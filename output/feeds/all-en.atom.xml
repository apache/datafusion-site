<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Apache DataFusion Blog</title><link href="https://datafusion.apache.org/blog/" rel="alternate"></link><link href="https://datafusion.apache.org/blog/feeds/all-en.atom.xml" rel="self"></link><id>https://datafusion.apache.org/blog/</id><updated>2025-04-10T00:00:00+00:00</updated><subtitle></subtitle><entry><title>tpchgen-rs World’s fastest open source TPC-H data generator, written in Rust</title><link href="https://datafusion.apache.org/blog/2025/04/10/fastest-tpch-generator" rel="alternate"></link><published>2025-04-10T00:00:00+00:00</published><updated>2025-04-10T00:00:00+00:00</updated><author><name>Andrew Lamb, Achraf B, and Sean Smith</name></author><id>tag:datafusion.apache.org,2025-04-10:/blog/2025/04/10/fastest-tpch-generator</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}x
--&gt;
&lt;style&gt;
/* Table borders */
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}
th, td {
  padding: 3px;
}
&lt;/style&gt;
&lt;p&gt;&lt;strong&gt;TLDR: TPC-H SF=100 in 1min using tpchgen-rs vs 30min+ with dbgen&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;3 members of the &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; community used Rust and open source
development to build &lt;a href="https://github.com/clflushopt/tpchgen-rs"&gt;tpchgen-rs&lt;/a&gt;, a fully open TPC-H data generator over …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}x
--&gt;
&lt;style&gt;
/* Table borders */
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}
th, td {
  padding: 3px;
}
&lt;/style&gt;
&lt;p&gt;&lt;strong&gt;TLDR: TPC-H SF=100 in 1min using tpchgen-rs vs 30min+ with dbgen&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;3 members of the &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; community used Rust and open source
development to build &lt;a href="https://github.com/clflushopt/tpchgen-rs"&gt;tpchgen-rs&lt;/a&gt;, a fully open TPC-H data generator over 20x
faster than any other implementation we know of.&lt;/p&gt;
&lt;p&gt;It is now possible to create the TPC-H SF=100 dataset in 72.23 seconds (1.4 GB/s
😎) on a Macbook Air M3 with 16GB of memory, compared to the classic &lt;code&gt;dbgen&lt;/code&gt;
which takes 30 minutes&lt;sup&gt;1&lt;/sup&gt; (0.05GB/sec). On the same machine, it takes less than
2 minutes to create all 3.6 GB of SF=100 in &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; format, which takes 44 minutes using &lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt;.
It is finally convenient and efficient to run TPC-H queries locally when testing
analytical engines such as DataFusion.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Time to create TPC-H parquet dataset for Scale Factor  1, 10, 100 and 1000" class="img-responsive" src="/blog/images/fastest-tpch-generator/parquet-performance.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Time to create TPC-H dataset for Scale Factor (see below) 1, 10,
100 and 1000 as 8 individual SNAPPY compressed parquet files using a 22 core GCP
VM with 88GB of memory. For Scale Factor(SF) 100 &lt;code&gt;tpchgen&lt;/code&gt; takes 1 minute and 14 seconds and
&lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt; takes 17 minutes and 48 seconds. For SF=1000, &lt;code&gt;tpchgen&lt;/code&gt; takes 10
minutes and 26 and uses about 5 GB of RAM at peak, and we could not measure
DuckDB&amp;rsquo;s time as it &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html#resource-usage-of-the-data-generator"&gt;requires 647 GB of RAM&lt;/a&gt;, more than the 88 GB that was
available on our test machine. The testing methodology is in the
&lt;a href="https://github.com/clflushopt/tpchgen-rs/blob/main/benchmarks/BENCHMARKS.md"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This blog explains what TPC-H is, how we ported the vintage C data generator to
Rust (yes, &lt;a href="https://www.reddit.com/r/rust/comments/4ri2gn/riir_rewrite_it_in_rust/"&gt;RWIR&lt;/a&gt;) and optimized its performance over the course of a few weeks
of part-time work. We began this project so we can easily generate TPC-H data in
&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; and &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Try it for yourself&lt;/h1&gt;
&lt;p&gt;The tool is entirely open source under the &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache 2.0 license&lt;/a&gt;. Visit the &lt;a href="https://github.com/clflushopt/tpchgen-rs"&gt;tpchgen-rs repository&lt;/a&gt; or try it for yourself by run the following commands after &lt;a href="https://www.rust-lang.org/tools/install"&gt;installing Rust&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ cargo install tpchgen-cli

# create SF=1 in classic TBL format
$ tpchgen-cli -s 1 

# create SF=10 in Parquet
$ tpchgen-cli -s 10 --format=parquet
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;What is TPC-H / dbgen?&lt;/h1&gt;
&lt;p&gt;The popular &lt;a href="https://www.tpc.org/tpch/"&gt;TPC-H&lt;/a&gt; benchmark (often referred to as TPCH) helps evaluate the
performance of database systems on &lt;a href="https://en.wikipedia.org/wiki/Online_analytical_processing"&gt;OLAP&lt;/a&gt; queries, the kind used to build BI
dashboards.&lt;/p&gt;
&lt;p&gt;TPC-H has become a de facto standard for analytic systems. While there are &lt;a href="https://www.vldb.org/pvldb/vol9/p204-leis.pdf"&gt;well
known&lt;/a&gt; limitations as the data and queries do not well represent many real world
use cases, the majority of analytic database papers and industrial systems still
use TPC-H query performance benchmarks as a baseline. You will inevitably find
multiple results for  &amp;ldquo;&lt;code&gt;TPCH Performance &amp;lt;your favorite database&amp;gt;&lt;/code&gt;&amp;rdquo; in any
search engine.&lt;/p&gt;
&lt;p&gt;The benchmark was created at a time when access to high performance analytical
systems was not widespread, so the &lt;a href="https://www.tpc.org/"&gt;Transaction Processing Performance Council&lt;/a&gt;
defined a process of formal result verification. More recently, given the broad
availability of free and open source database systems, it is common for users to
run and verify TPC-H performance themselves.&lt;/p&gt;
&lt;p&gt;TPC-H simulates a business environment with eight tables: &lt;code&gt;REGION&lt;/code&gt;, &lt;code&gt;NATION&lt;/code&gt;,
&lt;code&gt;SUPPLIER&lt;/code&gt;, &lt;code&gt;CUSTOMER&lt;/code&gt;, &lt;code&gt;PART&lt;/code&gt;, &lt;code&gt;PARTSUPP&lt;/code&gt;, &lt;code&gt;ORDERS&lt;/code&gt;, and &lt;code&gt;LINEITEM&lt;/code&gt;. These
tables are linked by foreign keys in a normalized schema representing a supply
chain with parts, suppliers, customers and orders. The benchmark itself is 22
SQL queries containing joins, aggregations, and sorting operations.&lt;/p&gt;
&lt;p&gt;The queries run against data created with &lt;code&gt;&lt;a href="https://github.com/electrum/tpch-dbgen"&gt;dbgen&lt;/a&gt;&lt;/code&gt;, a program
written in a pre &lt;a href="https://en.wikipedia.org/wiki/C99"&gt;C-99&lt;/a&gt; dialect, which generates data in a format called &lt;em&gt;TBL&lt;/em&gt;
(example in Figure 2). &lt;code&gt;dbgen&lt;/code&gt; creates data for each of the 8 tables for a
certain &lt;em&gt;Scale Factor&lt;/em&gt;, commonly abbreviated as SF. Example Scale Factors and
corresponding dataset sizes are shown in Table 1. There is no theoretical upper
bound on the Scale Factor.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;103|2844|845|3|23|40177.32|0.01|0.04|N|O|1996-09-11|1996-09-18|1996-09-26|NONE|FOB|ironic accou|
229|10540|801|6|29|42065.66|0.04|0.00|R|F|1994-01-14|1994-02-16|1994-01-22|NONE|FOB|uriously pending |
263|2396|649|1|22|28564.58|0.06|0.08|R|F|1994-08-24|1994-06-20|1994-09-09|NONE|FOB|efully express fo|
327|4172|427|2|9|9685.53|0.09|0.05|A|F|1995-05-24|1995-07-11|1995-06-05|NONE|AIR| asymptotes are fu|
450|5627|393|4|40|61304.80|0.05|0.03|R|F|1995-03-20|1995-05-25|1995-04-14|NONE|RAIL|ve. asymptote|
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Example TBL formatted output of &lt;code&gt;dbgen&lt;/code&gt; for the &lt;code&gt;LINEITEM&lt;/code&gt; table&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Data Size (TBL)&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Data Size (Parquet)&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.1
   &lt;/td&gt;
&lt;td&gt;103 Mb
   &lt;/td&gt;
&lt;td&gt;31 Mb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;1 Gb
   &lt;/td&gt;
&lt;td&gt;340 Mb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;10 Gb
   &lt;/td&gt;
&lt;td&gt;3.6 Gb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;107 Gb
   &lt;/td&gt;
&lt;td&gt;38 Gb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000
   &lt;/td&gt;
&lt;td&gt;1089 Gb
   &lt;/td&gt;
&lt;td&gt;379 Gb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: TPC-H data set sizes at different scale factors for both TBL and &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Why do we need a new TPC-H Data generator?&lt;/h1&gt;
&lt;p&gt;Despite the known limitations of the TPC-H benchmark, it is so well known that it
is used frequently in database performance analysis. To run TPC-H, you must first
load the data, using &lt;code&gt;dbgen&lt;/code&gt;, which is not ideal for several reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You must find and compile a copy of the 15+ year old C program (for example &lt;a href="https://github.com/electrum/tpch-dbgen"&gt;electrum/tpch-dbgen&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dbgen&lt;/code&gt; requires substantial time (Figure 3) and is not able to use more than one core.&lt;/li&gt;
&lt;li&gt;It outputs TBL format, which typically requires loading into your database (for example, &lt;a href="https://github.com/apache/datafusion/blob/507f6b6773deac69dd9d90dbe60831f5ea5abed1/datafusion/sqllogictest/test_files/tpch/create_tables.slt.part#L24-L124"&gt;here is how to do so&lt;/a&gt; in Apache DataFusion) prior to query.&lt;/li&gt;
&lt;li&gt;The implementation makes substantial assumptions about the operating environment, making it difficult to extend or embed into other systems.&lt;sup&gt;2&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Time to generate TPC-H data in TBL format" class="img-responsive" src="/blog/images/fastest-tpch-generator/tbl-performance.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Time to generate TPC-H data in TBL format. &lt;code&gt;tpchgen&lt;/code&gt; is
shown in blue. &lt;code&gt;tpchgen&lt;/code&gt; restricted to a single core is shown in red. Unmodified
&lt;code&gt;dbgen&lt;/code&gt; is shown in green and &lt;code&gt;dbgen&lt;/code&gt; modified to use &lt;code&gt;-O3&lt;/code&gt; optimization level
is shown in yellow.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dbgen&lt;/code&gt; is so inconvenient and takes so long that vendors often provide
preloaded TPC-H data, for example &lt;a href="https://docs.snowflake.com/en/user-guide/sample-data-tpch"&gt;Snowflake Sample Data&lt;/a&gt;, &lt;a href="https://docs.databricks.com/aws/en/discover/databricks-datasets"&gt;Databricks Sample
datasets&lt;/a&gt; and &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html#pre-generated-data-sets"&gt;DuckDB Pre-Generated Data Sets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition to pre-generated datasets, DuckDB also provides a &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html"&gt;TPC-H extension&lt;/a&gt; 
for generating TPC-H datasets within DuckDB. This is so much easier to use than
the current alternatives that it leads many researchers and other thought
leaders to use DuckDB to evaluate new ideas. For example, &lt;a href="https://github.com/lmwnshn"&gt;Wan Shen
Lim&lt;/a&gt; explicitly &lt;a href="https://github.com/apache/datafusion/issues/14373"&gt;mentioned the ease of creating the TPC-H dataset&lt;/a&gt; as one reason
the first student project of &lt;a href="https://15799.courses.cs.cmu.edu/spring2025/"&gt;CMU-799 Spring 2025&lt;/a&gt; used DuckDB.&lt;/p&gt;
&lt;p&gt;As beneficial as the DuckDB TPC-H extension is, it is non-ideal for several reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Creates data in a proprietary format, which requires export to use in other systems.&lt;/li&gt;
&lt;li&gt;Requires significant time (e.g. 17 minutes for Scale Factor 10).&lt;/li&gt;
&lt;li&gt;Requires unnecessarily large amounts of memory (e.g. 71 GB for Scale Factor 10)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above limitations makes it impractical to generate Scale Factor 100 and
above on laptops or standard workstations, though DuckDB offers &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html#pre-generated-data-sets"&gt;pre-computed
files&lt;/a&gt; for larger factors&lt;sup&gt;3&lt;/sup&gt;.&lt;/p&gt;
&lt;h1&gt;Why Rust?&lt;/h1&gt;
&lt;p&gt;Realistically we used Rust because we wanted to integrate the data generator
into &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; and &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt;. However, we also believe Rust is
superior to C/C++ due to its comparable performance, but much higher programmer
productivity (Figure 4). Productivity in this case refers to the ease of
optimizing and adding multithreading without introducing hard to debug memory
safety or concurrency issues.&lt;/p&gt;
&lt;p&gt;While Rust does allow unsafe access to memory (eliding bounds checking, for
example), when required for performance, our implementation is entirely memory
safe. The only &lt;a href="https://github.com/search?q=repo%3Aclflushopt%2Ftpchgen-rs%20unsafe&amp;amp;type=code"&gt;unsafe&lt;/a&gt; code is used to &lt;a href="https://github.com/clflushopt/tpchgen-rs/blob/c651da1fc309f9cb3872cbdf71e4796904dc62c6/tpchgen/src/text.rs#L72"&gt;skip&lt;/a&gt; UTF8 validation on known ASCII
strings.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lamb Theory on Evolution of Systems Languages" class="img-responsive" src="/blog/images/fastest-tpch-generator/lamb-theory.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Lamb Theory of System Language Evolution from &lt;a href="https://midas.bu.edu/assets/slides/andrew_lamb_slides.pdf"&gt;Boston University
MiDAS Fall 2024 (Data Systems Seminar)&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=CpnxuBwHbUc"&gt;recording&lt;/a&gt;. Special
thanks to &lt;a href="https://x.com/KurtFehlhauer"&gt;@KurtFehlhauer&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;How: The Journey&lt;/h1&gt;
&lt;p&gt;We did it together as a team in the open over the course of a few weeks.
&lt;a href="https://github.com/lmwnshn"&gt;Wan Shen Lim&lt;/a&gt; inspired the project by pointing out the benefits of &lt;a href="https://github.com/apache/datafusion/issues/14373"&gt;easy TPC-H
dataset creation&lt;/a&gt;  and &lt;a href="https://github.com/apache/datafusion/issues/14608#issuecomment-2651044600"&gt;suggesting we check out a Java port on February 11,
2025&lt;/a&gt;. Achraf made &lt;a href="https://github.com/clflushopt/tpchgen-rs/commit/53d3402680422a15349ece0a7ea3c3f001018ba0"&gt;first commit a few days later&lt;/a&gt; on February 16, and &lt;a href="https://github.com/clflushopt/tpchgen-rs/commit/9bb386a4c55b8cf93ffac1b98f29b5da990ee79e"&gt;Andrew
and Sean started helping on March 8, 2025&lt;/a&gt; and we &lt;a href="https://crates.io/crates/tpchgen/0.1.0"&gt;released version 0.1&lt;/a&gt; on
March 30, 2025.&lt;/p&gt;
&lt;h2&gt;Optimizing Single Threaded Performance&lt;/h2&gt;
&lt;p&gt;Archaf &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/16"&gt;completed the end to end conformance tests&lt;/a&gt;, to ensure correctness, and
an initial &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/12"&gt;cli check in&lt;/a&gt; on March 15, 2025.&lt;/p&gt;
&lt;p&gt;On a Macbook Pro M3 (Nov 2023), the initial performance numbers were actually
slower than the original Java implementation which was ported 😭. This wasn&amp;rsquo;t
surprising since the focus of the first version was to get a byte of byte
compatible port, and knew about the performance shortcomings and how to approach
them.&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;0m10.307s
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;1m26.530s
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;14m56.986s
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 2&lt;/strong&gt;: Performance of running &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/12"&gt;the initial tpchgen-cli&lt;/a&gt;, measured with
&lt;code&gt;time target/release/tpchgen-cli -s $SCALE_FACTOR&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;With this strong foundation we began optimizing the code using Rust&amp;rsquo;s low level
memory management to improve performance while retaining memory safely. We spent
several days obsessing over low level details and implemented a textbook like
list of optimizations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/19"&gt;Avoiding startup overhead&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/26"&gt;not&lt;/a&gt; &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/32"&gt;copying&lt;/a&gt; strings (many more PRs as well)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/27"&gt;Rust&amp;rsquo;s zero overhead abstractions for dates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/35"&gt;Static strings&lt;/a&gt; (entirely safely with static lifetimes)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/33"&gt;Generics to avoid virtual function call overhead&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/62"&gt;Moving lookups from runtime&lt;/a&gt; to load time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the time of writing, single threaded performance is now 2.5x-2.7x faster than the initial version, as shown in Table 3.&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Times faster&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;0m4.079s
   &lt;/td&gt;
&lt;td&gt;2.5x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;0m31.616s
   &lt;/td&gt;
&lt;td&gt;2.7x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;5m28.083s
   &lt;/td&gt;
&lt;td&gt;2.7x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 3&lt;/strong&gt;: Single threaded &lt;code&gt;tpchgen-cli&lt;/code&gt; performance, measured with &lt;code&gt;time target/release/tpchgen-cli -s $SCALE_FACTOR --num-threads=1&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Multi-threading&lt;/h2&gt;
&lt;p&gt;Then we applied &lt;a href="https://doc.rust-lang.org/book/ch16-00-concurrency.html"&gt;Rust&amp;rsquo;s fearless concurrency&lt;/a&gt; &amp;ndash; with a single, &lt;a href="https://github.com/clflushopt/tpchgen-rs/commit/ab720a70cdc80a711f4a3dda6bac05445106f499"&gt;small PR&lt;/a&gt; (272
net new lines) we updated the same memory safe code to run with multiple threads
and consume bounded memory using &lt;a href="https://thenewstack.io/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/"&gt;tokio for the thread scheduler&lt;/a&gt;&lt;sup&gt;4&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;As shown in Table 4, with this change, tpchgen-cli generates the full SF=100
dataset in 32 seconds (which is 3.3 GB/sec 🤯). Further investigation reveals
that at SF=100 our generator is actually IO bound (which is not the case for
&lt;code&gt;dbgen&lt;/code&gt; or &lt;code&gt;duckdb&lt;/code&gt;) &amp;ndash; it creates data &lt;strong&gt;faster than can be written to an SSD&lt;/strong&gt;.
When writing to &lt;code&gt;/dev/null&lt;/code&gt; tpchgen  generates the entire dataset in 25 seconds
(4 GB/s).&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Times faster than initial implementation&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Times faster than optimized single threaded&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;0m1.369s
   &lt;/td&gt;
&lt;td&gt;7.3x
   &lt;/td&gt;
&lt;td&gt;3x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;0m3.828s
   &lt;/td&gt;
&lt;td&gt;22.6x
   &lt;/td&gt;
&lt;td&gt;8.2x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;0m32.615s
   &lt;/td&gt;
&lt;td&gt;27.5x
   &lt;/td&gt;
&lt;td&gt;10x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100 (to /dev/null)
   &lt;/td&gt;
&lt;td&gt;0m25.088s
   &lt;/td&gt;
&lt;td&gt;35.7x
   &lt;/td&gt;
&lt;td&gt;13.1x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 4&lt;/strong&gt;: tpchgen-cli (multithreaded) performance measured with &lt;code&gt;time target/release/tpchgen-cli -s $SCALE_FACTOR&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Using Rust and async streams, the data generator is also fully streaming: memory
use does not increase with increasing data size / scale factors&lt;sup&gt;5&lt;/sup&gt;. The DuckDB
generator seems to &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html#resource-usage-of-the-data-generator"&gt;require far more memory&lt;/a&gt; than is commonly available on
developer laptops and memory use increases with scale factor. With &lt;code&gt;tpchgen-cli&lt;/code&gt;
it is perfectly possible to create data for SF=10000 or larger on a machine with
16GB of memory (assuming sufficient storage capacity).&lt;/p&gt;
&lt;h2&gt;Direct to parquet&lt;/h2&gt;
&lt;p&gt;At this point, &lt;code&gt;tpchgen-cli&lt;/code&gt; could very quickly generate the TBL format.
However, as described above, the TBL is annoying to work with, because&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It has no header&lt;/li&gt;
&lt;li&gt;It is like a CSV but the delimiter is &lt;code&gt;|&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Each line ends with an extra &lt;code&gt;|&lt;/code&gt; delimiter before the newline 🙄&lt;/li&gt;
&lt;li&gt;No system that we know can read them without additional configuration.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We next &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/54"&gt;added support for CSV&lt;/a&gt; generation (special thanks &lt;a href="https://github.com/niebayes"&gt;@niebayes&lt;/a&gt; from
Datalayers for finding and &lt;a href="https://github.com/clflushopt/tpchgen-rs/issues/73"&gt;fixing&lt;/a&gt; &lt;a href="https://github.com/clflushopt/tpchgen-rs/issues/65"&gt;bugs&lt;/a&gt;) which performs at the same
speed as TBL. While CSV files are far more standard than TBL, they must still be
parsed prior to load and automatic type inference may not deduce the types
needed for the TPC-H benchmarks (e.g. floating point vs Decimal).&lt;/p&gt;
&lt;p&gt;What would be far more useful is a typed, efficient columnar format such as
Apache Parquet which is supported by all modern query engines. So we &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/71"&gt;made&lt;/a&gt; a
&lt;a href="https://crates.io/crates/tpchgen-arrow"&gt;tpchgen-arrow&lt;/a&gt; crate to create &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; arrays directly and then &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/61"&gt;a small
300 line PR&lt;/a&gt; to feed those arrays to the &lt;a href="https://crates.io/crates/parquet"&gt;Rust Parquet writer&lt;/a&gt;, again using
tokio for parallelized but memory bound work.&lt;/p&gt;
&lt;p&gt;This approach was simple, fast and scalable, as shown in Table 5. Even though
creating Parquet files is significantly more computationally expensive than TBL
or CSV, tpchgen-cli creates the full SF=100 parquet format dataset in less than
45 seconds.&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Time to generate Parquet&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Speed compared to tbl generation&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;0m1.649s
   &lt;/td&gt;
&lt;td&gt;0.8x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;0m5.643s
   &lt;/td&gt;
&lt;td&gt;0.7x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;0m45.243s
   &lt;/td&gt;
&lt;td&gt;0.7x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100 (to /dev/null)
   &lt;/td&gt;
&lt;td&gt;0m45.153s
   &lt;/td&gt;
&lt;td&gt;0.5x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 5&lt;/strong&gt;: &lt;code&gt;tpchgen-cli&lt;/code&gt; Parquet generation performance measured with  &lt;code&gt;time
target/release/tpchgen-cli -s $SCALE_FACTOR --format=parquet&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;Conclusion 👊🎤&lt;/h1&gt;
&lt;p&gt;In just a few days, with some fellow database nerds and the power of Rust, we built something 10x better than what currently exists. We hope it inspires more research
into analytical systems using the TPC-H dataset and that people build awesome
things with it. For example, Sean has already added &lt;a href="https://github.com/GlareDB/glaredb/pull/3549"&gt;on-demand generation of
tables to GlareDB&lt;/a&gt;. Please consider joining us and helping out at
&lt;a href="https://github.com/clflushopt/tpchgen-rs"&gt;https://github.com/clflushopt/tpchgen-rs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We met while working together on Apache DataFusion in various capacities. If you
are looking for a community of like minded people hacking on databases, we
welcome you to &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;come join us&lt;/a&gt;. We are in the process of integrating this into
DataFusion (see &lt;a href="https://github.com/apache/datafusion/issues/14608"&gt;apache/datafusion#14608&lt;/a&gt;) if you are interested in helping 🎣&lt;/p&gt;
&lt;h1&gt;About the Authors:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/in/andrewalamb/"&gt;Andrew Lamb&lt;/a&gt; (&lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;) is a Staff Engineer at &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; and a PMC member of &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Achraf B (&lt;a href="https://github.com/clflushopt"&gt;@clflushopt&lt;/a&gt;) is a Software Engineer at &lt;a href="https://optable.co/"&gt;Optable&lt;/a&gt; where he works on data infrastructure.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/in/scsmithr/"&gt;Sean Smith&lt;/a&gt; (&lt;a href="https://github.com/scsmithr"&gt;@scsmithr&lt;/a&gt;) is the founder of &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt; focused on building a fast analytics database.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Footnotes themselves at the bottom. --&gt;
&lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;1&lt;/em&gt;: Actual Time: &lt;code&gt;30:35&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;2&lt;/em&gt;: It is possible to embed the dbgen code, which appears to be the approach taken by DuckDB. This approach was tried in GlareDB (&lt;a href="https://github.com/GlareDB/glaredb/pull/3313"&gt;GlareDB/glaredb#3313&lt;/a&gt;), but ultimately shelved given the amount of effort needed to adapt and isolate the dbgen code.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;3&lt;/em&gt;: It is pretty amazing to imagine the machine required to generate SF300 that had 1.8TB (!!) of RAM&lt;/p&gt;
&lt;p&gt;&lt;em&gt;4&lt;/em&gt;: We tried to &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/34"&gt;use Rayon (see discussion here)&lt;/a&gt;, but could not easily keep memory bounded.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;5&lt;/em&gt;: &lt;code&gt;tpchgen-cli&lt;/code&gt; memory usage is a function of the number of threads:  each thread needs some buffer space&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Python 46.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/03/30/datafusion-python-46.0.0" rel="alternate"></link><published>2025-03-30T00:00:00+00:00</published><updated>2025-03-30T00:00:00+00:00</updated><author><name>timsaucer</name></author><id>tag:datafusion.apache.org,2025-03-30:/blog/2025/03/30/datafusion-python-46.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/46.0.0/"&gt;datafusion-python 46.0.0&lt;/a&gt; has been released. This release
brings in all of the new features of the core &lt;a href="https://datafusion.apache.org/blog/2025/03/24/datafusion-46.0.0"&gt;DataFusion 46.0.0&lt;/a&gt; library. Since the last
blog post for &lt;a href="https://datafusion.apache.org/blog/2024/12/14/datafusion-python-43.1.0/"&gt;datafusion-python 43.1.0&lt;/a&gt;, a large number of improvements have been made
that can …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/46.0.0/"&gt;datafusion-python 46.0.0&lt;/a&gt; has been released. This release
brings in all of the new features of the core &lt;a href="https://datafusion.apache.org/blog/2025/03/24/datafusion-46.0.0"&gt;DataFusion 46.0.0&lt;/a&gt; library. Since the last
blog post for &lt;a href="https://datafusion.apache.org/blog/2024/12/14/datafusion-python-43.1.0/"&gt;datafusion-python 43.1.0&lt;/a&gt;, a large number of improvements have been made
that can be found in the &lt;a href="https://github.com/apache/datafusion-python/tree/main/dev/changelog"&gt;changelogs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We highly recommend reviewing the upstream &lt;a href="https://datafusion.apache.org/blog/2025/03/24/datafusion-46.0.0"&gt;DataFusion 46.0.0&lt;/a&gt; announcement.&lt;/p&gt;
&lt;h2&gt;Easier file reading&lt;/h2&gt;
&lt;p&gt;In these releases we have introduced two new ways to more easily read files into
DataFrames.&lt;/p&gt;
&lt;p&gt;PR &lt;a href="https://github.com/apache/datafusion-python/pull/982"&gt;#982&lt;/a&gt; introduced a series of easier read functions for Parquet, JSON, CSV, and
AVRO files. This introduces a concept of a global context that is available by
default when using these methods. Now instead of creating a default Session
Context and then calling the read methods, you can simply import these read
alternative methods and begin working with your DataFrames. Below is an example of
how easy to use this new approach is.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion.io import read_parquet
df = read_parquet(path="./examples/tpch/data/customer.parquet")
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PR &lt;a href="https://github.com/apache/datafusion-python/pull/980"&gt;#980&lt;/a&gt; adds a method for setting up a session context to use URL tables. With
this enabled, you can use a path to a local file as a table name. An example
of how to use this is demonstrated in the following snippet.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import datafusion
ctx = datafusion.SessionContext().enable_url_table()
df = ctx.table("./examples/tpch/data/customer.parquet")
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Registering Table Views&lt;/h2&gt;
&lt;p&gt;DataFusion supports registering a logical plan as a view with a session context. This
allows creating views in one part of your work flow and passinng the session
context to other places where that logical plan can be reused. This is an useful
feature for building up complex workflows and for code clarity. PR &lt;a href="https://github.com/apache/datafusion-python/pull/1016"&gt;#1016&lt;/a&gt; enables this
feature in &lt;code&gt;datafusion-python&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, supposing you have a DataFrame called &lt;code&gt;df1&lt;/code&gt;, you could use this code snippet
to register the view and then use it in another place:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ctx.register_view("view1", df1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then in another portion of your code which has access to the same session context
you can retrive the DataFrame with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df2 = ctx.table("view1")
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Asynchronous Iteration of Record Batches&lt;/h2&gt;
&lt;p&gt;Retrieving a &lt;code&gt;RecordBatch&lt;/code&gt; from a &lt;code&gt;RecordBatchStream&lt;/code&gt; was a synchronous call, which would
require the end user's code to wait for the data retrieval. This is described in
&lt;a href="https://github.com/apache/datafusion-python/issues/974"&gt;Issue 974&lt;/a&gt;. We continue to support this as a synchronous iterator, but we have also added
in the ability to retrieve the &lt;code&gt;RecordBatch&lt;/code&gt; using the Python asynchronous &lt;code&gt;anext&lt;/code&gt;
function.&lt;/p&gt;
&lt;h2&gt;Default ZSTD Compression for Parquet files&lt;/h2&gt;
&lt;p&gt;With PR &lt;a href="https://github.com/apache/datafusion-python/pull/981"&gt;#981&lt;/a&gt;, we change the saving of Parquet files to use zstd compression by default.
Previously the default was uncompressed, causing excessive disk storage. Zstd is an
excellent compression scheme that balances speed and compression ratio. Users can still
save their Parquet files uncompressed by passing in the appropriate value to the
&lt;code&gt;compression&lt;/code&gt; argument when calling &lt;code&gt;DataFrame.write_parquet&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;UDF Decorators&lt;/h2&gt;
&lt;p&gt;In PRs &lt;a href="https://github.com/apache/datafusion-python/pull/1040"&gt;#1040&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion-python/pull/1061"&gt;#1061&lt;/a&gt; we add methods to make creating user defined functions
easier and take advantage of Python decorators. With these PRs you can save a step
from defining a method and then defining a udf of that method. Instead you can
simply add the appropriate &lt;code&gt;udf&lt;/code&gt; decorator. Similar methods exist for aggregate
and window user defined functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@udf([pa.int64(), pa.int64()], pa.bool_(), "stable")
def my_custom_function(
    age: pa.Array,
    favorite_number: pa.Array,
) -&amp;gt; pa.Array:
    pass
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;code&gt;uv&lt;/code&gt; package management&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; is an extremely fast Python package manager, written in Rust. In the previous version
of &lt;code&gt;datafusion-python&lt;/code&gt; we had a combination of settings of PyPi and Conda. Instead, we
switch to using &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; is our primary method for dependency management.&lt;/p&gt;
&lt;p&gt;For most users of DataFusion, this change will be transparent. You can still install
via &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt;. For developers, the instructions in the repository have been updated.&lt;/p&gt;
&lt;h2&gt;Code cleanup&lt;/h2&gt;
&lt;p&gt;In an effort to improve our code cleanliness and ensure we are following Python best
practices, we use &lt;a href="https://docs.astral.sh/ruff/"&gt;ruff&lt;/a&gt; to perform Python linting. Until now we enabled only a portion
of the available linters available. In PRs &lt;a href="https://github.com/apache/datafusion-python/pull/1055"&gt;#1055&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion-python/pull/1062"&gt;#1062&lt;/a&gt;, we enable many more
of these linters and made code improvements to ensure we are following these
recommendations.&lt;/p&gt;
&lt;h2&gt;Improved Jupyter Notebook rendering&lt;/h2&gt;
&lt;p&gt;Since PR &lt;a href="https://github.com/apache/datafusion-python/pull/839"&gt;#839&lt;/a&gt; in DataFusion 41.0.0 we have been able to render DataFrames using html in
&lt;a href="https://jupyter.org/"&gt;jupyter&lt;/a&gt; notebooks. This is a big improvement over the &lt;code&gt;show&lt;/code&gt; command when we have the
ability to render tables. In PR &lt;a href="https://github.com/apache/datafusion-python/pull/1036"&gt;#1036&lt;/a&gt; we went a step further and added in a variety
of features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now html tables are scrollable, vertically and horizontally.&lt;/li&gt;
&lt;li&gt;When data are truncated, we report this to the user.&lt;/li&gt;
&lt;li&gt;Instead of showing a small number of rows, we collect up to 2 megabytes of data to
display. Since we have scrollable tables, we are able to make more data available
to the user without sacrificing notebook usability.&lt;/li&gt;
&lt;li&gt;We report explicitly when the DataFrame is empty. Previously we would not output
anything for an empty table. This indicator is helpful to users to ensure their plans
are written correctly. Sometimes a non-output can be overlooked.&lt;/li&gt;
&lt;li&gt;For long output of data, we generate a collapsed view of the data with an option
for the user to click on it to expand the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the below view you can see an example of some of these features such as the
expandable text and scroll bars.&lt;/p&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Example html rendering in a jupyter notebook." class="img-responsive" src="/blog/images/python-datafusion-46.0.0/html_rendering.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 1&lt;/b&gt;: With the html rendering enhancements, tables are more easily
   viewable in jupyter notebooks.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2&gt;Extension Documentation&lt;/h2&gt;
&lt;p&gt;We have recently added &lt;a href="https://datafusion.apache.org/python/contributor-guide/ffi.html"&gt;Extension Documentation&lt;/a&gt; to the DataFusion in Python website. We
have received many requests about how to better understand how to integrate DataFusion
in Python with other Rust libraries. To address these questions we wrote an article about
some of the difficulties that we encounter when using Rust libraries in Python and our
approach to addressing them.&lt;/p&gt;
&lt;h2&gt;Migration Guide&lt;/h2&gt;
&lt;p&gt;During the upgrade from &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; to &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/44.0.0.md"&gt;DataFusion 44.0.0&lt;/a&gt; as our upstream core
dependency, we discovered a few changes were necessary within our repository and our
unit tests. These notes serve to help guide users who may encounter similar issues when
upgrading.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;RuntimeConfig&lt;/code&gt; is now deprecated in favor of &lt;code&gt;RuntimeEnvBuilder&lt;/code&gt;. The migration is
fairly straightforward, and the corresponding classes have been marked as deprecated. For
end users it should be simply a matter of changing the class name.&lt;/li&gt;
&lt;li&gt;If you perform a &lt;code&gt;concat&lt;/code&gt; of a &lt;code&gt;string_view&lt;/code&gt; and &lt;code&gt;string&lt;/code&gt;, it will now return a
&lt;code&gt;string_view&lt;/code&gt; instead of a &lt;code&gt;string&lt;/code&gt;. This likely only impacts unit tests that are validating
return types. In general, it is recommended to switch to using &lt;code&gt;string_view&lt;/code&gt; whenever 
possible. You can see the blog articles &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1/"&gt;String View Pt 1&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-2/"&gt;Pt 2&lt;/a&gt; for more information
on these performance improvements.&lt;/li&gt;
&lt;li&gt;The function &lt;code&gt;date_part&lt;/code&gt; now returns an &lt;code&gt;int32&lt;/code&gt; instead of a &lt;code&gt;float64&lt;/code&gt;. This is likely
only impactful to unit tests.&lt;/li&gt;
&lt;li&gt;We have upgraded the Python minimum version to 3.9 since 3.8 is no longer officially
supported.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Coming Soon&lt;/h2&gt;
&lt;p&gt;There is a lot of excitement around the upcoming work. This list is not comprehensive, but
a glimpse into some of the upcoming work includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reusable DataFusion UDFs: The way user defined functions are currently written in
&lt;code&gt;datafusion-python&lt;/code&gt; is slightly different from those written for the upstream Rust
&lt;code&gt;datafusion&lt;/code&gt;. The core ideas are usually the same, but it means it takes effort for users
to re-implement functions already written for Rust projects to be usable in Python. Issue
&lt;a href="https://github.com/apache/datafusion-python/issues/1017"&gt;#1017&lt;/a&gt; addresses this topic. Work is well underway to make it easier to expose these
user functions through the FFI boundary. This means that the work that already exists in
repositories such as those found in the &lt;a href="https://github.com/datafusion-contrib"&gt;datafusion-contrib&lt;/a&gt; project can be easily
re-used in Python. This will provide a low effort way to expose significant functionality
to the DataFusion in Python community.&lt;/li&gt;
&lt;li&gt;Additional table providers: We have work well underway to provide a host of table providers
to &lt;code&gt;datafusion-python&lt;/code&gt; including: sqlite, duckdb, postgres, odbc, and mysql! In
&lt;a href="https://github.com/datafusion-contrib/datafusion-table-providers/issues/279"&gt;datafusion-contrib #279&lt;/a&gt; we track the progress of this excellent work. Once complete, users
will be able to &lt;code&gt;pip install&lt;/code&gt; this library and get easy access to all of these table
providers. This is another way we are leveraging the FFI work to greatly expand the usability
of &lt;code&gt;datafusion-python&lt;/code&gt; with relatively low effort.&lt;/li&gt;
&lt;li&gt;External catalog and schema providers: For users who wish to go beyond table providers
and have an entire custom catalog with schema, Issue &lt;a href="https://github.com/apache/datafusion-python/issues/1091"&gt;#1091&lt;/a&gt; tracks the progress of exposing
this in Python. With this work, if you have already written a Rust based table catalog you
will be able to interface it in Python similar to the work described for the table
providers above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is only a sample of the great work that is being done. If there are features you would
love to see, we encourage you to open an issue and join us as we build something wonderful.&lt;/p&gt;
&lt;h2&gt;Appreciation&lt;/h2&gt;
&lt;p&gt;We would like to thank everyone who has helped with these releases through their helpful
conversations, code review, issue descriptions, and code authoring. We would especially
like to thank the following authors of PRs who made these releases possible, listed in
alphabetical order by username: &lt;a href="https://github.com/chenkovsky"&gt;@chenkovsky&lt;/a&gt;, &lt;a href="https://github.com/CrystalZhou0529"&gt;@CrystalZhou0529&lt;/a&gt;, &lt;a href="https://github.com/ion-elgreco"&gt;@ion-elgreco&lt;/a&gt;,
&lt;a href="https://github.com/jsai28"&gt;@jsai28&lt;/a&gt;, &lt;a href="https://github.com/kevinjqliu"&gt;@kevinjqliu&lt;/a&gt;, &lt;a href="https://github.com/kylebarron"&gt;@kylebarron&lt;/a&gt;, &lt;a href="https://github.com/kosiew"&gt;@kosiew&lt;/a&gt;, &lt;a href="https://github.com/nirnayroy"&gt;@nirnayroy&lt;/a&gt;, and &lt;a href="https://github.com/Spaarsh"&gt;@Spaarsh&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you!&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;The DataFusion Python team is an active and engaging community and we would love
to have you join us and help the project.&lt;/p&gt;
&lt;p&gt;Here are some ways to get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learn more by visiting the &lt;a href="https://datafusion.apache.org/python/index.html"&gt;DataFusion Python project&lt;/a&gt; page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try out the project and provide feedback, file issues, and contribute code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Join us on &lt;a href="https://s.apache.org/slack-invite"&gt;ASF Slack&lt;/a&gt; or the &lt;a href="https://discord.gg/Qw5gKqHxUM"&gt;Arrow Rust Discord Server&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion 46.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/03/24/datafusion-46.0.0" rel="alternate"></link><published>2025-03-24T00:00:00+00:00</published><updated>2025-03-24T00:00:00+00:00</updated><author><name>Oznur Hanci and Berkay Sahin on behalf of the PMC</name></author><id>tag:datafusion.apache.org,2025-03-24:/blog/2025/03/24/datafusion-46.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We&amp;rsquo;re excited to announce the release of&amp;nbsp;&lt;strong&gt;Apache DataFusion 46.0.0&lt;/strong&gt;! This new version represents a significant milestone for the project, packing in a wide range of improvements and fixes. You can find the complete details in the&amp;nbsp;full &lt;a href="https://github.com/apache/datafusion/blob/branch-46/dev/changelog/46.0.0.md"&gt;changelog&lt;/a&gt;. We&amp;rsquo;ll highlight the most important changes below …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We&amp;rsquo;re excited to announce the release of&amp;nbsp;&lt;strong&gt;Apache DataFusion 46.0.0&lt;/strong&gt;! This new version represents a significant milestone for the project, packing in a wide range of improvements and fixes. You can find the complete details in the&amp;nbsp;full &lt;a href="https://github.com/apache/datafusion/blob/branch-46/dev/changelog/46.0.0.md"&gt;changelog&lt;/a&gt;. We&amp;rsquo;ll highlight the most important changes below and guide you through upgrading.&lt;/p&gt;
&lt;h2&gt;Breaking Changes&lt;/h2&gt;
&lt;p&gt;DataFusion 46.0.0 brings a few&amp;nbsp;&lt;strong&gt;breaking changes&lt;/strong&gt;&amp;nbsp;that may require adjustments to your code as described in the &lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html"&gt;Upgrade Guide&lt;/a&gt;. Here are the most notable ones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/datafusion/pull/14224#"&gt;Unified &lt;code&gt;DataSourceExec&lt;/code&gt; Execution Plan&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt;&amp;nbsp;DataFusion 46.0.0 introduces a major refactor of scan operators. The separate file-format-specific execution plan nodes (&lt;code&gt;ParquetExec&lt;/code&gt;,&amp;nbsp;&lt;code&gt;CsvExec&lt;/code&gt;,&amp;nbsp;&lt;code&gt;JsonExec&lt;/code&gt;,&amp;nbsp;&lt;code&gt;AvroExec&lt;/code&gt;, etc.) have been&amp;nbsp;&lt;strong&gt;deprecated and merged into a single &lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;plan&lt;/strong&gt;. Format-specific logic is now encapsulated in new&amp;nbsp;&lt;code&gt;DataSource&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;FileSource&lt;/code&gt;&amp;nbsp;traits. This change simplifies the execution model, but if you have code that directly references the old plan nodes, you&amp;rsquo;ll need to update it to use&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;(see the&amp;nbsp;&lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html"&gt;Upgrade Guide&lt;/a&gt;&amp;nbsp;for examples of the new API).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/7360#:~:text=2"&gt;**Error Handling Improvements&lt;/a&gt; (&lt;code&gt;DataFusionError::Collection&lt;/code&gt;):**&amp;nbsp;We began overhauling DataFusion&amp;rsquo;s approach to error handling. In this release, a new error variant&amp;nbsp;&lt;code&gt;DataFusionError::Collection&lt;/code&gt;&amp;nbsp;(and related mechanisms) has been introduced to aggregate multiple errors into one. This is part of a broader effort to provide richer error context and reduce internal panics. As a result, some error types or messages have changed. Downstream code that matches on specific&amp;nbsp;&lt;code&gt;DataFusionError&lt;/code&gt;&amp;nbsp;variants might need adjustment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Performance Improvements&lt;/h2&gt;
&lt;p&gt;DataFusion 46.0.0 comes with a slew of performance enhancements across the board. Here are some of the noteworthy optimizations in this release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faster&amp;nbsp;&lt;code&gt;median()&lt;/code&gt;&amp;nbsp;(no grouping):&lt;/strong&gt;&amp;nbsp;The&amp;nbsp;&lt;code&gt;median()&lt;/code&gt;&amp;nbsp;aggregate function got a special fast path when used without a&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;. By optimizing its accumulator, median calculation is about&amp;nbsp;&lt;strong&gt;2&amp;times; faster&lt;/strong&gt;&amp;nbsp;in the single-group case. If you use&amp;nbsp;&lt;code&gt;MEDIAN()&lt;/code&gt;&amp;nbsp;on large datasets (especially as a single value), you should notice reduced query times (PR &lt;a href="https://github.com/apache/datafusion/pull/14399"&gt;#14399&lt;/a&gt; by &lt;a href="https://github.com/2010YOUY01"&gt;@2010YOUY01&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimized&amp;nbsp;&lt;code&gt;FIRST_VALUE&lt;/code&gt;/&lt;code&gt;LAST_VALUE&lt;/code&gt;:&lt;/strong&gt;&amp;nbsp;The&amp;nbsp;&lt;code&gt;FIRST_VALUE&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;LAST_VALUE&lt;/code&gt;&amp;nbsp;window functions have been improved by avoiding an internal sort of rows. Instead of sorting each partition, the implementation now uses a direct approach to pick the first/last element. This yields&amp;nbsp;&lt;strong&gt;10&amp;ndash;100% performance improvement&lt;/strong&gt;&amp;nbsp;for these functions, depending on the scenario. Queries using&amp;nbsp;&lt;code&gt;FIRST_VALUE(...) OVER (PARTITION BY ... ORDER BY ...)&lt;/code&gt;&amp;nbsp;will run faster, especially when partitions are large (PR &lt;a href="https://github.com/apache/datafusion/pull/14402"&gt;#14402&lt;/a&gt; by &lt;a href="https://github.com/blaginin"&gt;@blaginin&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;repeat()&lt;/code&gt;&amp;nbsp;String Function Boost:&lt;/strong&gt;&amp;nbsp;Repeating strings is now more efficient &amp;ndash; the&amp;nbsp;&lt;code&gt;repeat(text, n)&lt;/code&gt;&amp;nbsp;function was optimized by about&amp;nbsp;&lt;strong&gt;50%&lt;/strong&gt;. This was achieved by reducing allocations and using a more efficient concatenation strategy. If you generate large repeated strings in queries, this can cut the time nearly in half (PR &lt;a href="https://github.com/apache/datafusion/pull/14697"&gt;#14697&lt;/a&gt; by &lt;a href="https://github.com/zjregee"&gt;@zjregee&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ultra-fast&amp;nbsp;&lt;code&gt;uuid()&lt;/code&gt;&amp;nbsp;UDF:&lt;/strong&gt;&amp;nbsp;The&amp;nbsp;&lt;code&gt;uuid()&lt;/code&gt;&amp;nbsp;function (which generates random UUID strings) received a major speed-up. It&amp;rsquo;s now roughly&amp;nbsp;&lt;strong&gt;40&amp;times; faster&lt;/strong&gt;&amp;nbsp;than before! The new implementation avoids unnecessary string copying and uses a more direct conversion to hex, making bulk UUID generation far more practical (PR &lt;a href="https://github.com/apache/datafusion/pull/14675"&gt;#14675&lt;/a&gt; by &lt;a href="https://github.com/simonvandel"&gt;@simonvandel&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accelerated&amp;nbsp;&lt;code&gt;chr()&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;to_hex()&lt;/code&gt;:&lt;/strong&gt;&amp;nbsp;Several scalar functions have been micro-optimized. The&amp;nbsp;&lt;code&gt;chr()&lt;/code&gt;&amp;nbsp;function (which returns the character for a given ASCII code) is about&amp;nbsp;&lt;strong&gt;4&amp;times; faster&lt;/strong&gt;&amp;nbsp;now, and the&amp;nbsp;&lt;code&gt;to_hex()&lt;/code&gt;&amp;nbsp;function (which converts numbers to hex string) is roughly&amp;nbsp;&lt;strong&gt;2&amp;times; faster&lt;/strong&gt;. These improvements may be most noticeable in tight loops or when these functions are applied to large arrays of values (PR &lt;a href="https://github.com/apache/datafusion/pull/14700"&gt;#14700&lt;/a&gt; for&amp;nbsp;&lt;code&gt;chr&lt;/code&gt;, &lt;a href="https://github.com/apache/datafusion/pull/14686"&gt;#14686&lt;/a&gt; for&amp;nbsp;&lt;code&gt;to_hex&lt;/code&gt; by &lt;a href="https://github.com/simonvandel"&gt;@simonvandel&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No More RowConverter in Grouped Ordering:&lt;/strong&gt;&amp;nbsp;We removed an inefficient step in the&amp;nbsp;&lt;em&gt;partial grouping&lt;/em&gt;&amp;nbsp;algorithm. The&amp;nbsp;&lt;code&gt;GroupOrderingPartial&lt;/code&gt;&amp;nbsp;operator no longer converts data to &amp;ldquo;row format&amp;rdquo; for each batch (via&amp;nbsp;&lt;code&gt;RowConverter&lt;/code&gt;). Instead, it uses a direct arrow-based approach to detect sort key changes. This eliminated overhead and yields a nice speedup for certain aggregation queries. (PR &lt;a href="https://github.com/apache/datafusion/pull/14566"&gt;#14566&lt;/a&gt; by &lt;a href="https://github.com/ctsk"&gt;@ctsk&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predicate Pruning for&amp;nbsp;&lt;code&gt;NOT LIKE&lt;/code&gt;:&lt;/strong&gt;&amp;nbsp;DataFusion&amp;rsquo;s parquet reader can now prune row groups using&amp;nbsp;&lt;code&gt;NOT LIKE&lt;/code&gt;&amp;nbsp;filters, similar to how it handles&amp;nbsp;&lt;code&gt;LIKE&lt;/code&gt;. This means if you have a filter such as&amp;nbsp;&lt;code&gt;column NOT LIKE 'prefix%'&lt;/code&gt;, DataFusion can use min/max statistics to skip reading files/parts that can be determined to either entirely match or not match the predicate. In particular, a pattern like&amp;nbsp;&lt;code&gt;NOT LIKE 'X%'&lt;/code&gt;&amp;nbsp;can skip data ranges that definitely start with "X". While a niche case, it contributes to query efficiency in those scenarios (PR &lt;a href="https://github.com/apache/datafusion/pull/14567"&gt;#14567&lt;/a&gt; by &lt;a href="https://github.com/UBarney"&gt;@UBarney&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Google Summer of Code 2025&lt;/h2&gt;
&lt;p&gt;Another exciting development:&amp;nbsp;&lt;strong&gt;Apache DataFusion has been accepted as a mentoring organization for Google Summer of Code (GSoC) 2025&lt;/strong&gt;! 🎉 This means that this summer, students from around the world will have the opportunity to contribute to DataFusion under the guidance of our committers. We have put together &lt;a href="https://datafusion.apache.org/contributor-guide/gsoc_project_ideas.html"&gt;a list of project ideas&lt;/a&gt; that candidates can choose from.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re interested, check out our&amp;nbsp;&lt;a href="https://datafusion.apache.org/contributor-guide/gsoc_application_guidelines.html"&gt;GSoC Application Guidelines&lt;/a&gt;. We encourage students to reach out, discuss ideas with us, and apply.&lt;/p&gt;
&lt;h2&gt;Highlighted New Features&lt;/h2&gt;
&lt;h3&gt;Improved Diagnostics&lt;/h3&gt;
&lt;p&gt;DataFusion 46.0.0 introduces a new&amp;nbsp;&lt;a href="https://github.com/apache/datafusion/issues/14429"&gt;&lt;strong&gt;SQL Diagnostics framework&lt;/strong&gt;&lt;/a&gt;&amp;nbsp;to make error messages more understandable. This comes in the form of new&amp;nbsp;&lt;code&gt;Diagnostic&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;DiagnosticEntry&lt;/code&gt;&amp;nbsp;types, which allow the system to attach rich context (like source query text spans) to error messages. In practical terms, certain planner errors will now point to the exact location in your SQL query that caused the issue. &lt;/p&gt;
&lt;p&gt;For example, if you reference an unknown table or miss a column in &lt;code&gt;GROUP BY&lt;/code&gt; the error message will include the query snippet causing the error. These diagnostics are meant for end-users of applications built on DataFusion, providing clearer messages instead of generic errors. Here&amp;rsquo;s an example:&lt;/p&gt;
&lt;p&gt;&lt;img alt="diagnostic-example" class="img-responsive" src="/blog/images/datafusion-46.0.0/diagnostic-example.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;Currently, diagnostics cover unresolved table/column references, missing &lt;code&gt;GROUP BY&lt;/code&gt; columns, ambiguous references, wrong number of UNION columns, type mismatches, and a few others. Future releases will extend this to more error types. This feature should greatly ease debugging of complex SQL by pinpointing errors directly in the query text. We thank &lt;a href="https://github.com/eliaperantoni"&gt;@eliaperantoni&lt;/a&gt; for his contributions in this project.&lt;/p&gt;
&lt;h3&gt;Unified&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;for Table Providers&lt;/h3&gt;
&lt;p&gt;As mentioned, DataFusion now uses a unified&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;for reading tables, which is both a breaking change and a feature.&amp;nbsp;&lt;em&gt;Why is this important?&lt;/em&gt;&amp;nbsp;The new approach simplifies how custom table providers are integrated and optimized. Namely, the optimizer can treat file scans uniformly and push down filters/limits more consistently when there is one execution plan that handles all data sources. The new&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;is paired with a&amp;nbsp;&lt;code&gt;DataSource&lt;/code&gt;&amp;nbsp;trait that encapsulates format-specific behaviors (Parquet, CSV, JSON, Avro, etc.) in a pluggable way.&lt;/p&gt;
&lt;p&gt;All built-in sources (Parquet, CSV, Avro, Arrow, JSON, etc.) have been migrated to this framework. This unification makes the codebase cleaner and sets the stage for future enhancements (like consistent metadata handling and limit pushdown across all formats). Check out PR &lt;a href="https://github.com/apache/datafusion/pull/14224"&gt;#14224&lt;/a&gt; for design details. We thank &lt;a href="https://github.com/mertak-synnada"&gt;@mertak-synnada&lt;/a&gt; and &lt;a href="https://github.com/ozankabak"&gt;@ozankabak&lt;/a&gt; for their contributions.&lt;/p&gt;
&lt;h3&gt;FFI Support for Scalar UDFs&lt;/h3&gt;
&lt;p&gt;DataFusion&amp;rsquo;s Foreign Function Interface (FFI) has been extended to support&amp;nbsp;&lt;a href="https://github.com/apache/datafusion/pull/14579"&gt;&lt;strong&gt;user-defined scalar functions&lt;/strong&gt;&lt;/a&gt;&amp;nbsp;defined in external languages. In 46.0.0, you can now expose a custom scalar UDF through the FFI layer and use it in DataFusion as if it were built-in. This is particularly exciting for the &lt;strong&gt;Python bindings&lt;/strong&gt; and other language integrations &amp;ndash; it means you could define a function in Python (or C, etc.) and register it with DataFusion&amp;rsquo;s Rust core via the FFI crate. Thanks, &lt;a href="https://github.com/timsaucer"&gt;@timsaucer&lt;/a&gt;!&lt;/p&gt;
&lt;h3&gt;New Statistics/Distribution Framework&lt;/h3&gt;
&lt;p&gt;This release, thanks mainly to &lt;a href="https://github.com/Fly-Style"&gt;@Fly-Style&lt;/a&gt; with contributions from &lt;a href="https://github.com/ozankabak"&gt;@ozankabak&lt;/a&gt; and &lt;a href="https://github.com/berkaysynnada"&gt;@berkaysynnada&lt;/a&gt;, includes the initial pieces of a&amp;nbsp;&lt;a href="https://github.com/apache/datafusion/pull/14699"&gt;**redesigned statistics framework&lt;/a&gt;.&lt;strong&gt; DataFusion&amp;rsquo;s optimizer can now represent column data distributions using a new&amp;nbsp;&lt;code&gt;Distribution&lt;/code&gt;&amp;nbsp;enum, instead of the old precision or range estimations. The supported distribution types currently include&amp;nbsp;&lt;/strong&gt;Uniform, Gaussian (normal), Exponential, Bernoulli&lt;strong&gt;, and a&amp;nbsp;&lt;/strong&gt;Generic**&amp;nbsp;catch-all.&lt;/p&gt;
&lt;p&gt;For example, if a filter expression is applied to a column with a known uniform distribution range, the optimizer can propagate that to estimate result selectivity more accurately. Similarly, comparisons (&lt;code&gt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, etc.) on columns yield Bernoulli distributions (with true/false probabilities) in this model.&lt;/p&gt;
&lt;p&gt;This is a foundational change with many follow-on PRs underway. Even though the immediate user-visible effect is limited (the optimizer didn't magically improve by an order of magnitude overnight), but it lays groundwork for more advanced query planning in the future. Over time, as statistics information encapsulated in &lt;code&gt;Distribution&lt;/code&gt;s get integrated, DataFusion will be able to make smarter decisions like more aggressive parquet pruning, better join orderings, and so on based on data distribution information. The core framework is now in place and is being hooked up to column and table level statistics.&lt;/p&gt;
&lt;h3&gt;Aggregate Monotonicity and Window Ordering&lt;/h3&gt;
&lt;p&gt;DataFusion 46.0.0 adds a new concept of &lt;a href="https://github.com/apache/datafusion/pull/14271"&gt;&lt;strong&gt;set-monotonicity&lt;/strong&gt;&lt;/a&gt; for certain transformations, which helps avoid unnecessary sort operations. In particular, the planner now understands when a &lt;strong&gt;window function introduces new orderings of data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For example, DataFusion now recognizes that a window-aggregate like &lt;code&gt;MAX&lt;/code&gt; on a column can produce a result that is &lt;strong&gt;monotonically increasing&lt;/strong&gt;, even if the input column is unordered &amp;mdash; depending on the window frame used.&lt;/p&gt;
&lt;p&gt;Consider the following query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT MAX(c1) OVER (
    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
) AS max_c1
FROM c1_table
ORDER BY max_c1;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In earlier versions of DataFusion, this query would require an additional SortExec on max_c1 to satisfy the ORDER BY clause. However, with the new set-monotonicity logic, the planner knows that MAX(...) OVER (...) produces values that are not smaller than the previous row, making the extra sort redundant. This leads to more efficient query execution.&lt;/p&gt;
&lt;p&gt;PR &lt;a href="https://github.com/apache/datafusion/pull/14271"&gt;#14271&lt;/a&gt; introduced the core monotonicity tracking for aggregates and window functions.
PR &lt;a href="https://github.com/apache/datafusion/pull/14813"&gt;#14813&lt;/a&gt; improved ordering preservation within various window frame types, and brought an extensive test coverage.
Huge thanks to &lt;a href="https://github.com/berkaysynnada"&gt;@berkaysynnada&lt;/a&gt; and &lt;a href="https://github.com/mertak-synnada"&gt;@mertak-synnada&lt;/a&gt; for designing and implementing this optimizer enhancement!&lt;/p&gt;
&lt;h3&gt;UNION [ALL | DISTINCT] BY NAME Support&lt;/h3&gt;
&lt;p&gt;DataFusion now supports UNION BY NAME and UNION ALL BY NAME, which align columns by name instead of position. This matches functionality found in systems like Spark and DuckDB and simplifies combining heterogeneously ordered result sets.&lt;/p&gt;
&lt;p&gt;You no longer need to rewrite column order manually &amp;mdash; just write:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT col1, col2 FROM t1
UNION ALL BY NAME
SELECT col2, col1 FROM t2;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under the hood, this is supported by the new union_by_name() and union_by_name_distinct() plan builder methods.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/rkrishn7"&gt;@rkrishn7&lt;/a&gt; for PR &lt;a href="https://github.com/apache/datafusion/pull/14538"&gt;#14538&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;New range() Table Function&lt;/h3&gt;
&lt;p&gt;A new table-valued function range(start, stop, step) has been added to make it easy to generate integer sequences &amp;mdash; similar to PostgreSQL&amp;rsquo;s generate_series() or Spark&amp;rsquo;s range().&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT * FROM range(1, 10, 2);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns: 1, 3, 5, 7, 9. It&amp;rsquo;s great for testing, cross joins, surrogate keys, and more.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/simonvandel"&gt;@simonvandel&lt;/a&gt; for PR &lt;a href="https://github.com/apache/datafusion/pull/14830"&gt;#14830&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Upgrade Guide and Changelog&lt;/h2&gt;
&lt;p&gt;Upgrading to 46.0.0 should be straightforward for most users, but do review the&amp;nbsp;&lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html"&gt;Upgrade Guide for DataFusion 46.0.0&lt;/a&gt;&amp;nbsp;for detailed steps and code changes. The upgrade guide covers the breaking changes mentioned (like replacing old exec nodes with&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;, updating UDF invocation to&amp;nbsp;&lt;code&gt;invoke_with_args&lt;/code&gt;, etc.) and provides code snippets to help with the transition. For a comprehensive list of all changes, please refer to the&amp;nbsp;&lt;strong&gt;changelog&lt;/strong&gt;&amp;nbsp;for 46.0.0 (linked above and in the repository). The changelog enumerates every merged PR in this release, including many smaller fixes and improvements that we couldn&amp;rsquo;t cover in this post.&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;Apache DataFusion is an open-source project, and we welcome involvement from anyone interested. Now is a great time to take 46.0.0 for a spin: try it out on your workloads, and let us know if you encounter any issues or have suggestions. You can report bugs or request features on our&amp;nbsp;GitHub issue tracker, or better yet, submit a pull request. Join our community discussions &amp;ndash; whether you have questions, want to share how you&amp;rsquo;re using DataFusion, or are looking to contribute, we&amp;rsquo;d love to hear from you. A list of open issues suitable for beginners is&amp;nbsp;&lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;&amp;nbsp;and you can find how to reach us on the&amp;nbsp;&lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication doc&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy querying!&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Efficient Filter Pushdown in Parquet</title><link href="https://datafusion.apache.org/blog/2025/03/21/parquet-pushdown" rel="alternate"></link><published>2025-03-21T00:00:00+00:00</published><updated>2025-03-21T00:00:00+00:00</updated><author><name>Xiangpeng Hao</name></author><id>tag:datafusion.apache.org,2025-03-21:/blog/2025/03/21/parquet-pushdown</id><summary type="html">&lt;style&gt;
figure {
  margin: 20px 0;
}

figure img {
  display: block;
  max-width: 80%;
}

figcaption {
  font-style: italic;
  margin-top: 10px;
  color: #555;
  font-size: 0.9em;
  max-width: 80%;
}
&lt;/style&gt;
&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog was first published on &lt;a href="https://blog.xiangpeng.systems/posts/parquet-pushdown/"&gt;Xiangpeng Hao's blog&lt;/a&gt;. Thanks to &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; for sponsoring this work as part of his PhD funding.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2025/03/20/parquet-pruning"&gt;previous post …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;style&gt;
figure {
  margin: 20px 0;
}

figure img {
  display: block;
  max-width: 80%;
}

figcaption {
  font-style: italic;
  margin-top: 10px;
  color: #555;
  font-size: 0.9em;
  max-width: 80%;
}
&lt;/style&gt;
&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog was first published on &lt;a href="https://blog.xiangpeng.systems/posts/parquet-pushdown/"&gt;Xiangpeng Hao's blog&lt;/a&gt;. Thanks to &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; for sponsoring this work as part of his PhD funding.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2025/03/20/parquet-pruning"&gt;previous post&lt;/a&gt;, we discussed how &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; prunes &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; files to skip irrelevant &lt;strong&gt;files/row_groups&lt;/strong&gt; (sometimes also &lt;a href="https://parquet.apache.org/docs/file-format/pageindex/"&gt;pages&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This post discusses how Parquet readers skip irrelevant &lt;strong&gt;rows&lt;/strong&gt; while scanning data,
leveraging Parquet's columnar layout by first reading only filter columns,
and then selectively reading other columns only for matching rows.&lt;/p&gt;
&lt;h2&gt;Why filter pushdown in Parquet?&lt;/h2&gt;
&lt;p&gt;Below is an example query that reads sensor data with filters on &lt;code&gt;date_time&lt;/code&gt; and &lt;code&gt;location&lt;/code&gt;.
Without filter pushdown, all rows from &lt;code&gt;location&lt;/code&gt;, &lt;code&gt;val&lt;/code&gt;, and &lt;code&gt;date_time&lt;/code&gt; columns are decoded before &lt;code&gt;location='office'&lt;/code&gt; is evaluated. Filter pushdown is especially useful when the filter is selective, i.e., removes many rows.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT val, location 
FROM sensor_data 
WHERE date_time &amp;gt; '2025-03-11' AND location = 'office';
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img alt="Parquet pruning skips irrelevant files/row_groups, while filter pushdown skips irrelevant rows. Without filter pushdown, all rows from location, val, and date_time columns are decoded before `location='office'` is evaluated. Filter pushdown is especially useful when the filter is selective, i.e., removes many rows." class="img-responsive" src="/blog/images/parquet-pushdown/pushdown-vs-no-pushdown.jpg" width="80%"/&gt;
&lt;figcaption&gt;
    Parquet pruning skips irrelevant files/row_groups, while filter pushdown skips irrelevant rows. Without filter pushdown, all rows from location, val, and date_time columns are decoded before `location='office'` is evaluated. Filter pushdown is especially useful when the filter is selective, i.e., removes many rows.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In our setup, sensor data is aggregated by date &amp;mdash; each day has its own Parquet file.
At planning time, DataFusion prunes the unneeded Parquet files, i.e., &lt;code&gt;2025-03-10.parquet&lt;/code&gt; and &lt;code&gt;2025-03-11.parquet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once the files to read are located, the &lt;a href="https://github.com/apache/datafusion/issues/3463"&gt;&lt;em&gt;DataFusion's current default implementation&lt;/em&gt;&lt;/a&gt; reads all the projected columns (&lt;code&gt;sensor_id&lt;/code&gt;, &lt;code&gt;val&lt;/code&gt;, and &lt;code&gt;location&lt;/code&gt;) into Arrow RecordBatches, then applies the filters over &lt;code&gt;location&lt;/code&gt; to get the final set of rows.&lt;/p&gt;
&lt;p&gt;A better approach is called &lt;strong&gt;filter pushdown&lt;/strong&gt; with &lt;strong&gt;late materialization&lt;/strong&gt;, which evaluates filter conditions first and only decodes data that passes these conditions.
In practice, this works by first processing only the filter columns (&lt;code&gt;date_time&lt;/code&gt; and &lt;code&gt;location&lt;/code&gt;), building a boolean mask of rows that satisfy our conditions, then using this mask to selectively decode only the relevant rows from other columns (&lt;code&gt;sensor_id&lt;/code&gt;, &lt;code&gt;val&lt;/code&gt;). 
This eliminates the waste of decoding rows that will be immediately filtered out.&lt;/p&gt;
&lt;p&gt;While simple in theory, practical implementations often make performance worse.&lt;/p&gt;
&lt;h2&gt;How can filter pushdown be slower?&lt;/h2&gt;
&lt;p&gt;At a high level, the Parquet reader first builds a filter mask -- essentially a boolean array indicating which rows meet the filter criteria -- and then uses this mask to selectively decode only the needed rows from the remaining columns in the projection.&lt;/p&gt;
&lt;p&gt;Let's dig into details of &lt;a href="https://github.com/apache/arrow-rs/blob/d5339f31a60a4bd8a4256e7120fe32603249d88e/parquet/src/arrow/async_reader/mod.rs#L618-L712"&gt;how filter pushdown is implemented&lt;/a&gt; in the current Rust Parquet reader implementation, illustrated in the following figure.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Implementation of filter pushdown in Rust Parquet readers" class="img-responsive" src="/blog/images/parquet-pushdown/baseline-impl.jpg" with="70%"/&gt;
&lt;figcaption&gt;
    Implementation of filter pushdown in Rust Parquet readers -- the first phase builds the filter mask, the second phase applies the filter mask to the other columns
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The filter pushdown has two phases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Build the filter mask (steps 1-3)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the filter mask to selectively decode other columns (steps 4-7), e.g., output step 3 is used as input for step 5 and 7.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Within each phase, it takes three steps from Parquet to Arrow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Decompress the Parquet pages using generic decompression algorithms like LZ4, Zstd, etc. (steps 1, 4, 6)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decode the page content into Arrow format (steps 2, 5, 7)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Evaluate the filter over Arrow data (step 3)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the figure above, we can see that &lt;code&gt;location&lt;/code&gt; is &lt;strong&gt;decompressed and decoded twice&lt;/strong&gt;, first when building the filter mask (steps 1, 2), and second when building the output (steps 4, 5).
This happens for all columns that appear both in the filter and output.&lt;/p&gt;
&lt;p&gt;The table below shows the corresponding CPU time on the &lt;a href="https://github.com/apache/datafusion/blob/main/benchmarks/queries/clickbench/queries.sql#L23"&gt;ClickBench query 22&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+------------+--------+-------------+--------+
| Decompress | Decode | Apply filter| Others |
+------------+--------+-------------+--------+
| 206 ms     | 117 ms | 22 ms       | 48 ms  |
+------------+--------+-------------+--------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, decompress/decode operations dominate the time spent. With filter pushdown, it needs to decompress/decode twice; but without filter pushdown, it only needs to do this once.
This explains why filter pushdown is slower in some cases.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Highly selective filters may skip the entire page; but as long as it reads one row from the page, it needs to decompress and often decode the entire page.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Attempt: cache filter columns&lt;/h2&gt;
&lt;p&gt;Intuitively, caching the filter columns and reusing them later could help.&lt;/p&gt;
&lt;p&gt;But naively caching decoded pages consumes prohibitively high memory:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It needs to cache Arrow arrays, which are on average &lt;a href="https://github.com/XiangpengHao/liquid-cache/blob/main/dev/doc/liquid-cache-vldb.pdf"&gt;4x larger than Parquet data&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It needs to cache the &lt;strong&gt;entire column chunk in memory&lt;/strong&gt;, because in Phase 1 it builds filters over the column chunk, and only use it in Phase 2.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The memory usage is proportional to the number of filter columns, which can be unboundedly high. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Worse, caching filter columns means it needs to read partially from Parquet and partially from cache, which is complex to implement, likely requiring a substantial change to the current implementation. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Feel the complexity:&lt;/strong&gt; consider building a cache that properly handles nested columns, multiple filters, and filters with multiple columns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Real solution&lt;/h2&gt;
&lt;p&gt;We need a solution that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Is simple to implement, i.e., doesn't require thousands of lines of code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Incurs minimal memory overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section describes my &lt;a href="https://github.com/apache/arrow-rs/pull/6921#issuecomment-2718792433"&gt;&amp;lt;700 LOC PR (with lots of comments and tests)&lt;/a&gt; that &lt;strong&gt;reduces total ClickBench time by 15%, with up to 2x lower latency for some queries, no obvious regression on other queries, and caches at most 2 pages (~2MB) per column in memory&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="New decoding pipeline, building filter mask and output columns are interleaved in a single pass, allowing us to cache minimal pages for minimal amount of time" class="img-responsive" src="/blog/images/parquet-pushdown/new-pipeline.jpg" width="80%"/&gt;
&lt;figcaption&gt;
    New decoding pipeline, building filter mask and output columns are interleaved in a single pass, allowing us to cache minimal pages for minimal amount of time
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The new pipeline interleaves the previous two phases into a single pass, so that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The page being decompressed is immediately used to build filter masks and output columns.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decompressed pages are cached for minimal time; after one pass (steps 1-6), the cache memory is released for the next pass. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This allows the cache to only hold 1 page at a time, and to immediately discard the previous page after it's used, significantly reducing the memory requirement for caching.&lt;/p&gt;
&lt;h3&gt;What pages are cached?&lt;/h3&gt;
&lt;p&gt;You may have noticed that only &lt;code&gt;location&lt;/code&gt; is cached, not &lt;code&gt;val&lt;/code&gt;, because &lt;code&gt;val&lt;/code&gt; is only used for output.
More generally, only columns that appear both in the filter and output are cached, and at most 1 page is cached for each such column.&lt;/p&gt;
&lt;p&gt;More examples:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT val 
FROM sensor_data 
WHERE date_time &amp;gt; '2025-03-11' AND location = 'office';
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, no columns are cached, because &lt;code&gt;val&lt;/code&gt; is not used for filtering.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT COUNT(*) 
FROM sensor_data 
WHERE date_time &amp;gt; '2025-03-11' AND location = 'office';
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, again, no columns are cached, because the output projection is empty after query plan optimization.&lt;/p&gt;
&lt;h3&gt;Then why cache 2 pages per column instead of 1?&lt;/h3&gt;
&lt;p&gt;This is another real-world nuance regarding how Parquet layouts the pages.&lt;/p&gt;
&lt;p&gt;Parquet by default encodes data using &lt;a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/"&gt;dictionary encoding&lt;/a&gt;, which writes a dictionary page as the first page of a column chunk, followed by the keys referencing the dictionary.&lt;/p&gt;
&lt;p&gt;You can see this in action using &lt;a href="https://parquet-viewer.xiangpeng.systems"&gt;parquet-viewer&lt;/a&gt;:&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Parquet viewer shows the page layout of a column chunk" class="img-responsive" src="/blog/images/parquet-pushdown/parquet-viewer.jpg" width="80%"/&gt;
&lt;figcaption&gt;
    Parquet viewer shows the page layout of a column chunk
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This means that to decode a page of data, it actually references two pages: the dictionary page and the data page.&lt;/p&gt;
&lt;p&gt;This is why it caches 2 pages per column: one dictionary page and one data page.
The data page slot will move forward as it reads the data; but the dictionary page slot always references the first page.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Cached two pages, one for dictionary (pinned), one for data (moves as it reads the data)" class="img-responsive" src="/blog/images/parquet-pushdown/cached-pages.jpg" width="80%"/&gt;
&lt;figcaption&gt;
    Cached two pages, one for dictionary (pinned), one for data (moves as it reads the data)
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2&gt;How does it perform?&lt;/h2&gt;
&lt;p&gt;Here are my results on &lt;a href="https://github.com/apache/datafusion/tree/main/benchmarks#clickbench"&gt;ClickBench&lt;/a&gt; on my AMD 9900X machine. The total time is reduced by 15%, with Q23 being 2.24x faster,
and queries that get slower are likely due to noise.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Query        ┃ no-pushdown ┃ new-pushdown ┃        Change ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
&amp;boxv; QQuery 0     &amp;boxv;      0.47ms &amp;boxv;       0.43ms &amp;boxv; +1.10x faster &amp;boxv;
&amp;boxv; QQuery 1     &amp;boxv;     51.10ms &amp;boxv;      50.10ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 2     &amp;boxv;     68.23ms &amp;boxv;      64.49ms &amp;boxv; +1.06x faster &amp;boxv;
&amp;boxv; QQuery 3     &amp;boxv;     90.68ms &amp;boxv;      86.73ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 4     &amp;boxv;    458.93ms &amp;boxv;     458.59ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 5     &amp;boxv;    522.06ms &amp;boxv;     478.50ms &amp;boxv; +1.09x faster &amp;boxv;
&amp;boxv; QQuery 6     &amp;boxv;     49.84ms &amp;boxv;      49.94ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 7     &amp;boxv;     55.09ms &amp;boxv;      55.77ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 8     &amp;boxv;    565.26ms &amp;boxv;     556.95ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 9     &amp;boxv;    575.83ms &amp;boxv;     575.05ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 10    &amp;boxv;    164.56ms &amp;boxv;     178.23ms &amp;boxv;  1.08x slower &amp;boxv;
&amp;boxv; QQuery 11    &amp;boxv;    177.20ms &amp;boxv;     191.32ms &amp;boxv;  1.08x slower &amp;boxv;
&amp;boxv; QQuery 12    &amp;boxv;    591.05ms &amp;boxv;     569.92ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 13    &amp;boxv;    861.06ms &amp;boxv;     848.59ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 14    &amp;boxv;    596.20ms &amp;boxv;     580.73ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 15    &amp;boxv;    554.96ms &amp;boxv;     548.77ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 16    &amp;boxv;   1175.08ms &amp;boxv;    1146.07ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 17    &amp;boxv;   1150.45ms &amp;boxv;    1121.49ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 18    &amp;boxv;   2634.75ms &amp;boxv;    2494.07ms &amp;boxv; +1.06x faster &amp;boxv;
&amp;boxv; QQuery 19    &amp;boxv;     90.15ms &amp;boxv;      89.24ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 20    &amp;boxv;    620.15ms &amp;boxv;     591.67ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 21    &amp;boxv;    782.38ms &amp;boxv;     703.15ms &amp;boxv; +1.11x faster &amp;boxv;
&amp;boxv; QQuery 22    &amp;boxv;   1927.94ms &amp;boxv;    1404.35ms &amp;boxv; +1.37x faster &amp;boxv;
&amp;boxv; QQuery 23    &amp;boxv;   8104.11ms &amp;boxv;    3610.76ms &amp;boxv; +2.24x faster &amp;boxv;
&amp;boxv; QQuery 24    &amp;boxv;    360.79ms &amp;boxv;     330.55ms &amp;boxv; +1.09x faster &amp;boxv;
&amp;boxv; QQuery 25    &amp;boxv;    290.61ms &amp;boxv;     252.54ms &amp;boxv; +1.15x faster &amp;boxv;
&amp;boxv; QQuery 26    &amp;boxv;    395.18ms &amp;boxv;     362.72ms &amp;boxv; +1.09x faster &amp;boxv;
&amp;boxv; QQuery 27    &amp;boxv;    891.76ms &amp;boxv;     959.39ms &amp;boxv;  1.08x slower &amp;boxv;
&amp;boxv; QQuery 28    &amp;boxv;   4059.54ms &amp;boxv;    4137.37ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 29    &amp;boxv;    235.88ms &amp;boxv;     228.99ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 30    &amp;boxv;    564.22ms &amp;boxv;     584.65ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 31    &amp;boxv;    741.20ms &amp;boxv;     757.87ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 32    &amp;boxv;   2652.48ms &amp;boxv;    2574.19ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 33    &amp;boxv;   2373.71ms &amp;boxv;    2327.10ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 34    &amp;boxv;   2391.00ms &amp;boxv;    2342.15ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 35    &amp;boxv;    700.79ms &amp;boxv;     694.51ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 36    &amp;boxv;    151.51ms &amp;boxv;     152.93ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 37    &amp;boxv;    108.18ms &amp;boxv;      86.03ms &amp;boxv; +1.26x faster &amp;boxv;
&amp;boxv; QQuery 38    &amp;boxv;    114.64ms &amp;boxv;     106.22ms &amp;boxv; +1.08x faster &amp;boxv;
&amp;boxv; QQuery 39    &amp;boxv;    260.80ms &amp;boxv;     239.13ms &amp;boxv; +1.09x faster &amp;boxv;
&amp;boxv; QQuery 40    &amp;boxv;     60.74ms &amp;boxv;      73.29ms &amp;boxv;  1.21x slower &amp;boxv;
&amp;boxv; QQuery 41    &amp;boxv;     58.75ms &amp;boxv;      67.85ms &amp;boxv;  1.15x slower &amp;boxv;
&amp;boxv; QQuery 42    &amp;boxv;     65.49ms &amp;boxv;      68.11ms &amp;boxv;     no change &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓
┃ Benchmark Summary           ┃            ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩
&amp;boxv; Total Time (no-pushdown)    &amp;boxv; 38344.79ms &amp;boxv;
&amp;boxv; Total Time (new-pushdown)   &amp;boxv; 32800.50ms &amp;boxv;
&amp;boxv; Average Time (no-pushdown)  &amp;boxv;   891.74ms &amp;boxv;
&amp;boxv; Average Time (new-pushdown) &amp;boxv;   762.80ms &amp;boxv;
&amp;boxv; Queries Faster              &amp;boxv;         13 &amp;boxv;
&amp;boxv; Queries Slower              &amp;boxv;          5 &amp;boxv;
&amp;boxv; Queries with No Change      &amp;boxv;         25 &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Despite being simple in theory, filter pushdown in Parquet is non-trivial to implement.
It requires understanding both the Parquet format and reader implementation details. 
The challenge lies in efficiently navigating through the dynamics of decoding, filter evaluation, and memory management.&lt;/p&gt;
&lt;p&gt;If you are interested in this level of optimization and want to help test, document and implement this type of optimization, come find us in the &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;DataFusion Community&lt;/a&gt;. We would love to have you. &lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.7.0 Release</title><link href="https://datafusion.apache.org/blog/2025/03/20/datafusion-comet-0.7.0" rel="alternate"></link><published>2025-03-20T00:00:00+00:00</published><updated>2025-03-20T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-03-20:/blog/2025/03/20/datafusion-comet-0.7.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.7.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.7.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately four weeks of development work and is the result of merging 46 PRs from 11
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.7.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Performance&lt;/h3&gt;
&lt;p&gt;Comet 0.7.0 has improved performance compared to the previous release due to improvements in the native shuffle 
implementation and performance improvements in DataFusion 46.&lt;/p&gt;
&lt;p&gt;For single-node TPC-H at 100 GB, Comet now delivers a &lt;strong&gt;greater than 2x speedup&lt;/strong&gt; compared to Spark using the same 
CPU and RAM. Even with &lt;strong&gt;half the resources&lt;/strong&gt;, Comet still provides a measurable performance improvement.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.7.0" class="img-responsive" src="/blog/images/comet-0.7.0/performance.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;These benchmarks were performed on a Linux workstation with PCIe 5, AMD 7950X CPU (16 cores), 128 GB RAM, and data 
stored locally in Parquet format on NVMe storage. Spark was running in Kubernetes with hard memory limits.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Shuffle Improvements&lt;/h2&gt;
&lt;p&gt;There are several improvements to shuffle in this release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When running in off-heap mode (which is the recommended approach), Comet was using the wrong memory allocator 
  implementation for some types of shuffle operation, which could result in OOM rather than spilling to disk.&lt;/li&gt;
&lt;li&gt;The number of spill files is drastically reduced. In previous releases, each instance of ShuffleMapTask could 
  potentially create a new spill file for each output partition each time that spill was invoked. Comet now creates 
  a maximum of one spill file per output partition per instance of ShuffleMapTask, which is appended to in subsequent 
  spills.&lt;/li&gt;
&lt;li&gt;There was a flaw in the memory accounting which resulted in Comet requesting approximately twice the amount of 
  memory that was needed, resulting in premature spilling. This is now resolved.&lt;/li&gt;
&lt;li&gt;The metric for number of spilled bytes is now accurate. It was previously reporting invalid information.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Improved Hash Join Performance&lt;/h2&gt;
&lt;p&gt;When using the &lt;code&gt;spark.comet.exec.replaceSortMergeJoin&lt;/code&gt; setting to replace sort-merge joins with hash joins, Comet 
will now do a better job of picking the optimal build side. Thanks to &lt;a href="https://github.com/hayman42"&gt;@hayman42&lt;/a&gt; for suggesting this, and thanks to the 
&lt;a href="https://github.com/apache/incubator-gluten/"&gt;Apache Gluten(incubating)&lt;/a&gt; project for the inspiration in implementing this feature.&lt;/p&gt;
&lt;h2&gt;Experimental Support for DataFusion&amp;rsquo;s Parquet Scan&lt;/h2&gt;
&lt;p&gt;It is now possible to configure Comet to use DataFusion&amp;rsquo;s Parquet reader instead of Comet&amp;rsquo;s current Parquet reader. This 
has the advantage of supporting complex types, and also has performance optimizations that are not present in Comet's 
existing reader.&lt;/p&gt;
&lt;p&gt;Support should still be considered experimental, but most of Comet&amp;rsquo;s unit tests are now passing with the new reader. 
Known issues include handling of &lt;code&gt;INT96&lt;/code&gt; timestamps and unsigned bytes and shorts.&lt;/p&gt;
&lt;p&gt;To enable DataFusion&amp;rsquo;s Parquet reader, either set &lt;code&gt;spark.comet.scan.impl=native_datafusion&lt;/code&gt; or set the environment 
variable &lt;code&gt;COMET_PARQUET_SCAN_IMPL=native_datafusion&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Complex Type Support&lt;/h2&gt;
&lt;p&gt;With DataFusion&amp;rsquo;s Parquet reader enabled, there is now some early support for reading structs from Parquet. This is 
not thoroughly tested yet. We would welcome additional testing from the community to help determine what is and isn&amp;rsquo;t 
working, as well as contributions to improve support for structs and other complex types. The tracking issue is 
&lt;a href="https://github.com/apache/datafusion-comet/issues/1043"&gt;https://github.com/apache/datafusion-comet/issues/1043&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Updates to supported Spark versions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Comet 0.7.0 is now tested against Spark 3.5.4 rather than 3.5.1&lt;/li&gt;
&lt;li&gt;This will be the last Comet release to support Spark 3.3.x&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Improved Tuning Guide&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://datafusion.apache.org/comet/user-guide/tuning.html"&gt;Comet Tuning Guide&lt;/a&gt; has been improved and now provides guidance on determining how much memory to allocate to 
Comet.&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Parquet Pruning in DataFusion: Read Only What Matters</title><link href="https://datafusion.apache.org/blog/2025/03/20/parquet-pruning" rel="alternate"></link><published>2025-03-20T00:00:00+00:00</published><updated>2025-03-20T00:00:00+00:00</updated><author><name>Xiangpeng Hao</name></author><id>tag:datafusion.apache.org,2025-03-20:/blog/2025/03/20/parquet-pruning</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog was first published on &lt;a href="https://blog.xiangpeng.systems/posts/parquet-to-arrow/"&gt;Xiangpeng Hao's blog&lt;/a&gt;. Thanks to &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; for sponsoring this work as part of his PhD funding.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; has become the industry standard for storing columnar data, and reading Parquet efficiently -- especially from remote storage -- is crucial for query performance.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog was first published on &lt;a href="https://blog.xiangpeng.systems/posts/parquet-to-arrow/"&gt;Xiangpeng Hao's blog&lt;/a&gt;. Thanks to &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; for sponsoring this work as part of his PhD funding.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; has become the industry standard for storing columnar data, and reading Parquet efficiently -- especially from remote storage -- is crucial for query performance.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; implements advanced Parquet pruning techniques to effectively read only the data that matters for a given query.&lt;/p&gt;
&lt;p&gt;Achieving high performance adds complexity.
This post provides an overview of the techniques used in DataFusion to selectively read Parquet files.&lt;/p&gt;
&lt;h3&gt;The pipeline&lt;/h3&gt;
&lt;p&gt;The diagram below illustrates the &lt;a href="https://docs.rs/datafusion/46.0.0/datafusion/datasource/physical_plan/parquet/source/struct.ParquetSource.html"&gt;Parquet reading pipeline&lt;/a&gt; in DataFusion, highlighting how data flows through various pruning stages before being converted to Arrow format:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Parquet pruning pipeline in DataFusion" class="img-responsive" src="/blog/images/parquet-pruning/read-parquet.jpg" width="100%"/&gt;&lt;/p&gt;
&lt;h4&gt;Background: Parquet file structure&lt;/h4&gt;
&lt;p&gt;As shown in the figure above, each Parquet file has multiple row groups. Each row group contains a set of columns, and each column contains a set of pages.&lt;/p&gt;
&lt;p&gt;Pages are the smallest units of data in Parquet files and typically contain compressed and encoded values for a specific column. This hierarchical structure enables efficient columnar access and forms the foundation for the pruning techniques we'll discuss.&lt;/p&gt;
&lt;p&gt;Check out &lt;a href="https://www.influxdata.com/blog/querying-parquet-millisecond-latency/"&gt;Querying Parquet with Millisecond Latency&lt;/a&gt; for more details on the Parquet file structure.&lt;/p&gt;
&lt;h4&gt;1. Read metadata&lt;/h4&gt;
&lt;p&gt;DataFusion first reads the &lt;a href="https://parquet.apache.org/docs/file-format/metadata/"&gt;Parquet metadata&lt;/a&gt; to understand the data in the file. 
Metadata often includes data schema, the exact location of each row group and column chunk, and their corresponding statistics (e.g., min/max values).
It also optionally includes &lt;a href="https://parquet.apache.org/docs/file-format/pageindex/"&gt;page-level stats&lt;/a&gt; and &lt;a href="https://www.influxdata.com/blog/using-parquets-bloom-filters/"&gt;Bloom filters&lt;/a&gt;.
This information is used to prune the file before reading the actual data.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/reader.rs#L118"&gt;Fetching metadata&lt;/a&gt; requires up to two network requests: one to read the footer size from the end of the file, and another to read the footer itself. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.influxdata.com/blog/how-good-parquet-wide-tables/"&gt;Decoding metadata&lt;/a&gt; is generally fast since it only requires parsing a small amount of data. However, for tables with hundreds or thousands of columns, the metadata can become quite large and decoding it can become a bottleneck. This is particularly noticeable when scanning many small files.&lt;/p&gt;
&lt;p&gt;Reading metadata is latency-critical, so DataFusion allows users to cache metadata through the &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/reader.rs#L39"&gt;ParquetFileReaderFactory&lt;/a&gt; trait.&lt;/p&gt;
&lt;h4&gt;2. Prune by projection&lt;/h4&gt;
&lt;p&gt;The simplest yet perhaps most effective pruning is to read only the columns that are needed.
This is because queries usually don't select all columns, e.g., &lt;code&gt;SELECT a FROM table&lt;/code&gt; only reads column &lt;code&gt;a&lt;/code&gt;.
As a &lt;strong&gt;columnar&lt;/strong&gt; format, Parquet allows DataFusion to &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/mod.rs#L778"&gt;only read&lt;/a&gt; the &lt;strong&gt;columns&lt;/strong&gt; that are needed.&lt;/p&gt;
&lt;p&gt;This projection pruning happens at the column level and can dramatically reduce I/O when working with wide tables where queries typically access only a small subset of columns.&lt;/p&gt;
&lt;h4&gt;3. Prune by row group stats and Bloom filters&lt;/h4&gt;
&lt;p&gt;Each row group has &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/physical_optimizer/pruning.rs#L81"&gt;basic stats&lt;/a&gt; like min/max values for each column.
DataFusion applies the query predicates to these stats to prune row groups, e.g., &lt;code&gt;SELECT * FROM table WHERE a &amp;gt; 10&lt;/code&gt; will only read row groups where &lt;code&gt;a&lt;/code&gt; has a max value greater than 10.&lt;/p&gt;
&lt;p&gt;Sometimes min/max stats are too simple to prune effectively, so Parquet also supports &lt;a href="https://www.influxdata.com/blog/using-parquets-bloom-filters/"&gt;Bloom filters&lt;/a&gt;. DataFusion &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/opener.rs#L202"&gt;uses Bloom filters when available&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bloom filters are particularly effective for equality predicates (&lt;code&gt;WHERE a = 10&lt;/code&gt;) and can significantly reduce the number of row groups that need to be read for point queries or queries with highly selective predicates.&lt;/p&gt;
&lt;h4&gt;4. Prune by page stats&lt;/h4&gt;
&lt;p&gt;Parquet optionally supports &lt;a href="https://github.com/apache/parquet-format/blob/master/PageIndex.md"&gt;page-level stats&lt;/a&gt; -- similar to row group stats but more fine-grained.
DataFusion implements &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/opener.rs#L219"&gt;page pruning&lt;/a&gt; when the stats are present.&lt;/p&gt;
&lt;p&gt;Page-level pruning provides an additional layer of filtering after row group pruning. It allows DataFusion to skip individual pages within a row group, further reducing the amount of data that needs to be read and decoded.&lt;/p&gt;
&lt;h4&gt;5. Read from storage&lt;/h4&gt;
&lt;p&gt;Now we (hopefully) have pruned the Parquet file into small ranges of bytes, i.e., the &lt;a href="https://github.com/apache/datafusion/blob/76a7789ace33ced54c973fa0d5fc9d1866e1bf19/datafusion/datasource-parquet/src/access_plan.rs#L86"&gt;Access Plan&lt;/a&gt;.
The last step is to &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/reader.rs#L103"&gt;make requests&lt;/a&gt; to fetch those bytes and decode them into Arrow RecordBatch. &lt;/p&gt;
&lt;h3&gt;Preview of coming attractions: filter pushdown&lt;/h3&gt;
&lt;p&gt;So far we have discussed techniques that prune the Parquet file using only the metadata, i.e., before reading the actual data.&lt;/p&gt;
&lt;p&gt;Filter pushdown, also known as predicate pushdown or late materialization, is a technique that prunes data during scanning, with filters being generated and applied in the Parquet reader.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Filter pushdown in DataFusion" class="img-responsive" src="/blog/images/parquet-pruning/filter-pushdown.jpg" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Unlike metadata-based pruning which works at the row group or page level, filter pushdown operates at the row level, allowing DataFusion to filter out individual rows that don't match the query predicates during the decoding process.&lt;/p&gt;
&lt;p&gt;DataFusion &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/row_filter.rs#L154"&gt;implements filter pushdown&lt;/a&gt; but has &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/common/src/config.rs#L382"&gt;not enabled it by default&lt;/a&gt; due to &lt;a href="https://github.com/apache/datafusion/issues/3463"&gt;some performance regressions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are working to remove the &lt;a href="https://github.com/apache/arrow-rs/issues/5523#issuecomment-2429470872"&gt;remaining performance issues&lt;/a&gt; and enable it by default, which we will discuss in the next blog post.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;DataFusion employs a multi-step approach to Parquet pruning, from column projection to row group stats, page stats, and potentially row-level filtering. 
Each step may reduce the amount of data to be read and processed, significantly improving query performance.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Using Ordering for Better Plans in Apache DataFusion</title><link href="https://datafusion.apache.org/blog/2025/03/11/ordering-analysis" rel="alternate"></link><published>2025-03-11T00:00:00+00:00</published><updated>2025-03-11T00:00:00+00:00</updated><author><name>Mustafa Akur, Andrew Lamb</name></author><id>tag:datafusion.apache.org,2025-03-11:/blog/2025/03/11/ordering-analysis</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/11631 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this blog post, we explain when an ordering requirement of an operator is satisfied by its input data. This analysis is essential for order-based optimizations and is often more complex than one might initially think.&lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Ordering Requirement&lt;/strong&gt; for an operator describes how the input data to that operator …&lt;/blockquote&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/11631 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this blog post, we explain when an ordering requirement of an operator is satisfied by its input data. This analysis is essential for order-based optimizations and is often more complex than one might initially think.&lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Ordering Requirement&lt;/strong&gt; for an operator describes how the input data to that operator must be sorted for the operator to compute the correct result. It is the job of the planner to make sure that these requirements are satisfied during execution (See DataFusion &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/enforce_sorting/struct.EnforceSorting.html" target="_blank"&gt;EnforceSorting&lt;/a&gt; for an implementation of such a rule).
&lt;/blockquote&gt;
&lt;p&gt;There are various use cases where this type of analysis can be useful such as the following examples.&lt;/p&gt;
&lt;h3&gt;Removing Unnecessary Sorts&lt;/h3&gt;
&lt;p&gt;Imagine a user wants to execute the following query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-SQL"&gt;SELECT hostname, log_line 
FROM telemetry ORDER BY time ASC limit 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we don't know anything about the &lt;code&gt;telemetry&lt;/code&gt; table we need to sort it by &lt;code&gt;time ASC&lt;/code&gt; and then retrieve the first 10 rows to get the correct result. However, if the table is already ordered by &lt;code&gt;time ASC&lt;/code&gt;, we can simply retrieve the first 10 rows. This approach executes much faster and uses less memory compared to resorting the entire table, even when the &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/struct.TopK.html"&gt;TopK&lt;/a&gt; operator is used. &lt;/p&gt;
&lt;p&gt;In order to avoid the sort the query optimizer must determine the data is already sorted. For simple queries the analysis is straightforward however it gets complicated fast. For example, what if your data is sorted by &lt;code&gt;[hostname, time ASC]&lt;/code&gt; and your query is&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT hostname, log_line 
FROM telemetry WHERE hostname = 'app.example.com' ORDER BY time ASC;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case a sort still isn't needed, but the analysis must reason about the sortedness of the stream when it knows &lt;code&gt;hostname&lt;/code&gt; has a single value.&lt;/p&gt;
&lt;h3&gt;Optimized Operator Implementations&lt;/h3&gt;
&lt;p&gt;As another use case, some operators can utilize the ordering information to change its underlying algorithm to execute more efficiently. Consider the following query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-SQL"&gt;SELECT COUNT(log_line) 
FROM telemetry GROUP BY hostname;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most analytic systems, including DataFusion, by default implement such a query using a hash table keyed on values of &lt;code&gt;hostname&lt;/code&gt; to store the counts. However, if the &lt;code&gt;telemetry&lt;/code&gt; table is sorted by &lt;code&gt;hostname&lt;/code&gt;,  there are much more efficient algorithms for grouping on &lt;code&gt;hostname&lt;/code&gt; values than hashing every value and storing it in memory. However, the more efficient algorithm can only be used when the input is sorted correctly. To see this in practice, check out the &lt;a href="https://github.com/apache/datafusion/tree/main/datafusion/physical-plan/src/aggregates/order"&gt;source&lt;/a&gt; for ordered variant of the &lt;code&gt;Aggregation&lt;/code&gt; in &lt;code&gt;DataFusion&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Streaming-Friendly Execution&lt;/h3&gt;
&lt;p&gt;Stream processing aims to produce results immediately as they become available ensuring minimal latency for real-time workloads. However, some operators need to consume all input data before producing any output. Consider the &lt;code&gt;Sort&lt;/code&gt; operation: before it can start generating output, the algorithm must first process all input data. As a result, data flow halts whenever such an operator is encountered until all input is consumed. When a physical query plan contains such an operator (&lt;code&gt;Sort&lt;/code&gt;, &lt;code&gt;CrossJoin&lt;/code&gt;, ..) we refer to this as pipeline breaking, meaning the query cannot be executed in a streaming fashion.&lt;/p&gt;
&lt;p&gt;For a query to be executed in a streaming fashion we need to satisfy 2 conditions:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logically Streamable&lt;/strong&gt;&lt;br/&gt;
It should be possible to generate what user wants in streaming fashion. Consider following query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-SQL"&gt;SELECT SUM(amount)  
FROM orders  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the user wants to compute the sum of all amounts in the orders table. By the nature of the query this requires scanning the entire table to generate a result making it impossible to execute in a streaming fashion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Streaming Aware Planner&lt;/strong&gt;&lt;br/&gt;
Being logically streamable does not guarantee that a query will execute in a streaming fashion. SQL is a declarative language, meaning it specifies 'WHAT' user wants. It is up to the planner 'HOW' to generate the result. In most cases there are many ways to compute the correct result for a given query. The query planner is responsible for choosing "a way" (ideally the best&lt;sup id="optimal1"&gt;&lt;a href="#optimal"&gt;*&lt;/a&gt;&lt;/sup&gt; one) among the all alternatives to generate what user asks for. If a plan contains a pipeline-breaking operator the execution will not be streaming&amp;mdash;even if the query is logically streamable. To generate truly streaming plans from logically streamable queries the planner must carefully analyze the existing orderings in the source tables to ensure that the final plan does not contain any pipeline-breaking operators.&lt;/p&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;Let's start by creating an example table that we will refer throughout the post. This table models the input data of an operator for the analysis:&lt;/p&gt;
&lt;h3&gt;Example Virtual Table&lt;/h3&gt;
&lt;style&gt;
  table {
    border-collapse: collapse;
    width: 80%;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
  }
  th, td {
    padding: 12px 16px;
    text-align: left;
    border-bottom: 1px solid #e0e0e0;
  }
  th {
    background-color: #f9f9f9;
    font-weight: 600;
  }
  tr:hover {
    background-color: #f1f1f1;
  }
&lt;/style&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;amount&lt;/th&gt; &lt;th&gt;price&lt;/th&gt; &lt;th&gt;hostname&lt;/th&gt;&lt;th&gt;currency&lt;/th&gt;&lt;th&gt;time_bin&lt;/th&gt; &lt;th&gt;time&lt;/th&gt; &lt;th&gt;price_cloned&lt;/th&gt; &lt;th&gt;time_cloned&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:01:30&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;08:01:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt; &lt;td&gt;26&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:11:30&lt;/td&gt; &lt;td&gt;26&lt;/td&gt; &lt;td&gt;08:11:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:41:30&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;08:41:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:55:15&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;08:55:15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;35&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:10:23&lt;/td&gt; &lt;td&gt;35&lt;/td&gt; &lt;td&gt;09:10:23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:20:33&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;09:20:33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:40:15&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;09:40:15&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;How can a table have multiple orderings?&lt;/strong&gt; At first glance it may seem counterintuitive for a table to have more than one valid ordering. However, during query execution such scenarios can arise.

For example consider the following query:


&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT time, date_bin('1 hour', time, '1970-01-01') as time_bin  
FROM table;
&lt;/code&gt;&lt;/pre&gt;

If we know that the table is ordered by &lt;code&gt;time ASC&lt;/code&gt; we can infer that &lt;code&gt;time_bin ASC&lt;/code&gt; is also a valid ordering. This is because the &lt;code&gt;date_bin&lt;/code&gt; function is monotonic, meaning it preserves the order of its input.

DataFusion leverages these functional dependencies to infer new orderings as data flows through different query operators. For details on the implementation see the &lt;a ,="" href="https://github.com/apache/datafusion/blob/main/datafusion/common/src/functional_dependencies.rs" target="_blank"&gt;source&lt;/a&gt; code.
&lt;/blockquote&gt;
&lt;p&gt;By inspection, you can see this table is sorted by the &lt;code&gt;amount&lt;/code&gt; column, but It is also sorted by &lt;code&gt;time&lt;/code&gt; and &lt;code&gt;time_bin&lt;/code&gt; as well as the compound &lt;code&gt;(time_bin, amount)&lt;/code&gt; and many other variations. While this example is an extreme case, real-world data often has multiple sort orders. &lt;/p&gt;
&lt;p&gt;A naive approach for analyzing whether the ordering requirement of an operator is satisfied by its input would be:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Store all the valid ordering expressions that the tables satisfies  &lt;/li&gt;
&lt;li&gt;Check whether the ordering requirement by the operator is among valid orderings.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This naive algorithm works and correct. However, listing all valid orderings can be quite lengthy and is of exponential complexity as the number of orderings grows. For the example table here is a (small) subset of the valid orderings:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[amount ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[amount ASC, price_cloned ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[hostname ASC, amount ASC, price_cloned ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[amount ASC, hostname ASC,  price_cloned ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[amount ASC, price_cloned ASC, hostname ASC]&lt;/code&gt;&lt;br/&gt;
.&lt;br/&gt;
.&lt;br/&gt;
.  &lt;/p&gt;
&lt;p&gt;As can be seen from the listing above storing all valid orderings is wasteful and contains significant redundancy. Here are some observations which suggest that we can do much better:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Storing a prefix of another valid ordering is redundant. If the table satisfies the lexicographic ordering&lt;sup id="fn1"&gt;&lt;a href="#footnote1"&gt;1&lt;/a&gt;&lt;/sup&gt;: &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, it already satisfies ordering &lt;code&gt;[amount ASC]&lt;/code&gt; trivially. Hence, once we store &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt; storing &lt;code&gt;[amount ASC]&lt;/code&gt; is redundant.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using all columns that are equal to each other in the listings is redundant. If we know the table is ordered by &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, it is also ordered by &lt;code&gt;[amount ASC, price_cloned ASC]&lt;/code&gt; since &lt;code&gt;price&lt;/code&gt; and &lt;code&gt;price_cloned&lt;/code&gt; are copy of each other. It is enough to use just one expression among the expressions that exact copy of each other.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Constant expressions can be inserted anywhere in a valid ordering with an arbitrary direction (e.g. &lt;code&gt;ASC&lt;/code&gt;, &lt;code&gt;DESC&lt;/code&gt;). Hence, if the table is ordered by &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, it is also ordered by: &lt;br/&gt;
&lt;code&gt;[hostname ASC, amount ASC, price ASC]&lt;/code&gt;,&lt;br/&gt;
&lt;code&gt;[hostname DESC, amount ASC, price ASC]&lt;/code&gt;,&lt;br/&gt;
&lt;code&gt;[amount ASC, hostname ASC, price ASC]&lt;/code&gt;,&lt;br/&gt;
   .&lt;br/&gt;
   .    &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is clearly redundant. For this reason, it is better to avoid explicitly encoding constant expressions in valid sort orders.&lt;/p&gt;
&lt;p&gt;In summary,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We should store only the longest lexicographic ordering (shouldn't use any prefix of it)&lt;/li&gt;
&lt;li&gt;Using expressions that are exact copies of each other is redundant.&lt;/li&gt;
&lt;li&gt;Ordering expressions shouldn't contain any constant expression.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Key Concepts for Analyzing Orderings&lt;/h2&gt;
&lt;p&gt;To solve the shortcomings above DataFusion needs to track of following properties for the table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constant Expressions  &lt;/li&gt;
&lt;li&gt;Equivalent Expression Groups (will be explained shortly)&lt;/li&gt;
&lt;li&gt;Succinct Valid Orderings (will be explained shortly)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Note:&lt;/strong&gt; These properties are implemented in the &lt;code&gt;EquivalenceProperties&lt;/code&gt; structure in &lt;code&gt;DataFusion&lt;/code&gt;, please see the &lt;a href="https://github.com/apache/datafusion/blob/f47ea73b87eec4af044f9b9923baf042682615b2/datafusion/physical-expr/src/equivalence/properties/mod.rs#L134" target="_blank"&gt;source&lt;/a&gt; for more details&lt;br/&gt;
&lt;/blockquote&gt;
&lt;p&gt;These properties allow us to analyze whether the ordering requirement is satisfied by the data already.&lt;/p&gt;
&lt;h3&gt;1. Constant Expressions&lt;/h3&gt;
&lt;p&gt;Constant expressions are those where each row in the expression has the same value across all rows. Although constant expressions may seem odd in a table they can arise after operations like &lt;code&gt;Filter&lt;/code&gt; or &lt;code&gt;Join&lt;/code&gt; occur. &lt;/p&gt;
&lt;p&gt;For instance in the example table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Columns &lt;code&gt;hostname&lt;/code&gt; and &lt;code&gt;currency&lt;/code&gt; are constant because every row in the table has the same value (&lt;code&gt;'app.example.com'&lt;/code&gt; for &lt;code&gt;hostname&lt;/code&gt;, and &lt;code&gt;'USD'&lt;/code&gt; for &lt;code&gt;currency&lt;/code&gt;) for these columns.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Note:&lt;/strong&gt; Constant expressions can arise during query execution. For example, in following query:&lt;br/&gt;
&lt;code&gt;SELECT hostname FROM logs&lt;/code&gt;&lt;br/&gt;&lt;code&gt;WHERE hostname='app.example.com'&lt;/code&gt; &lt;br/&gt;
    after filtering is done, for subsequent operators the &lt;code&gt;hostname&lt;/code&gt; column will be constant.
&lt;/blockquote&gt;
&lt;h3&gt;2. Equivalent Expression Groups&lt;/h3&gt;
&lt;p&gt;Equivalent expression groups are expressions that always hold the same value across rows. These expressions can be thought of as clones of each other and may arise from operations like &lt;code&gt;Filter&lt;/code&gt;, &lt;code&gt;Join&lt;/code&gt;, or &lt;code&gt;Projection&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the example table, the expressions &lt;code&gt;price&lt;/code&gt; and &lt;code&gt;price_cloned&lt;/code&gt; form one equivalence group, and &lt;code&gt;time&lt;/code&gt; and &lt;code&gt;time_cloned&lt;/code&gt; form another equivalence group.&lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Note:&lt;/strong&gt; Equivalent expression groups can arise during the query execution. For example, in the following query:&lt;br/&gt;
&lt;code&gt;SELECT time, time as time_cloned FROM logs&lt;/code&gt; &lt;br/&gt;
    after the projection is done, for subsequent operators &lt;code&gt;time&lt;/code&gt; and &lt;code&gt;time_cloned&lt;/code&gt; will form an equivalence group. As another example, in the following query:&lt;br/&gt;
&lt;code&gt;SELECT employees.id, employees.name, departments.department_name&lt;/code&gt;
&lt;code&gt;FROM employees&lt;/code&gt;
&lt;code&gt;JOIN departments ON employees.department_id = departments.id;&lt;/code&gt; &lt;br/&gt;
after joining, &lt;code&gt;employees.department_id&lt;/code&gt; and &lt;code&gt;departments.id&lt;/code&gt; will form an equivalence group.
&lt;/blockquote&gt;
&lt;h3&gt;3. Succinct Encoding of Valid Orderings&lt;/h3&gt;
&lt;p&gt;Valid orderings are the orderings that the table already satisfies. However, naively listing them requires exponential space as the number of columns grows as discussed before. Instead, we list all valid orderings after following constraints are applied:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do not use any constant expressions in the valid ordering construction&lt;/li&gt;
&lt;li&gt;Use only one entry (by convention the first entry) in the equivalent expression group.&lt;/li&gt;
&lt;li&gt;Lexicographic ordering shouldn't contain any leading ordering&lt;sup id="fn2"&gt;&lt;a href="#footnote2"&gt;2&lt;/a&gt;&lt;/sup&gt;except the first position &lt;sup id="fn3"&gt;&lt;a href="#footnote3"&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;Do not use any prefix of a valid lexicographic ordering&lt;sup id="fn4"&gt;&lt;a href="#footnote4"&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After applying the first and second constraint, the example table simplifies to &lt;/p&gt;
&lt;style&gt;
  table {
    border-collapse: collapse;
    width: 80%;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
  }
  th, td {
    padding: 12px 16px;
    text-align: left;
    border-bottom: 1px solid #e0e0e0;
  }
  th {
    background-color: #f9f9f9;
    font-weight: 600;
  }
  tr:hover {
    background-color: #f1f1f1;
  }
&lt;/style&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;th&gt;amount&lt;/th&gt; &lt;th&gt;price&lt;/th&gt;&lt;th&gt;time_bin&lt;/th&gt; &lt;th&gt;time&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt; &lt;td&gt;25&lt;/td&gt;&lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:01:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt; &lt;td&gt;26&lt;/td&gt;&lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:11:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;30&lt;/td&gt;&lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:41:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;32&lt;/td&gt;&lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:55:15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;35&lt;/td&gt;&lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:10:23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt; &lt;td&gt;18&lt;/td&gt;&lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:20:33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt; &lt;td&gt;22&lt;/td&gt;&lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:40:15&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br/&gt;
Following third and fourth constraints for the simplified table, the succinct valid orderings are:&lt;br/&gt;
&lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, &lt;br/&gt;
&lt;code&gt;[time_bin ASC]&lt;/code&gt;,&lt;br/&gt;
&lt;code&gt;[time ASC]&lt;/code&gt; &lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;p&gt;&lt;strong&gt;How can DataFusion find orderings?&lt;/strong&gt;&lt;/p&gt; 
DataFusion's &lt;code&gt;CREATE EXTERNAL TABLE&lt;/code&gt; has a &lt;code&gt;WITH ORDER&lt;/code&gt; clause (see &lt;a href="https://datafusion.apache.org/user-guide/sql/ddl.html#create-external-table"&gt;docs&lt;/a&gt;) to specify the known orderings of the table during table creation. For example the following query:&lt;br/&gt;
&lt;pre&gt;&lt;code&gt;
CREATE EXTERNAL TABLE source (
    amount INT NOT NULL,
    price DOUBLE NOT NULL,
    time TIMESTAMP NOT NULL,
    ...
)
STORED AS CSV
WITH ORDER (time ASC)
WITH ORDER (amount ASC, price ASC)
LOCATION '/path/to/FILE_NAME.csv'
OPTIONS ('has_header' 'true');
&lt;/code&gt;&lt;/pre&gt;
communicates that &lt;code&gt;source&lt;/code&gt; table has the orderings: &lt;code&gt;[time ASC]&lt;/code&gt; and &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;.&lt;br/&gt;
When orderings are communicated from the source, DataFusion tracks the orderings through each operator while optimizing the plan.&lt;br/&gt;
&lt;ul&gt;
&lt;li&gt;add new orderings (such as when "date_bin" function is applied to the "time" column)&lt;/li&gt;
&lt;li&gt;Remove orderings, if operation doesn't preserve the ordering of the data at its input&lt;/li&gt;
&lt;li&gt;Update equivalent groups&lt;/li&gt;
&lt;li&gt;Update constant expressions&lt;/li&gt;
&lt;/ul&gt;

Figure 1 shows an example how DataFusion generates an efficient plan for the query:
&lt;pre&gt;&lt;code&gt;
SELECT 
  row_number() OVER (ORDER BY time) as rn,
  time
FROM events
ORDER BY rn, time
&lt;/code&gt;&lt;/pre&gt;
using the orderings of the query intermediates.&lt;br/&gt;
&lt;br/&gt;
&lt;figure&gt;
&lt;img alt="Window Query Datafusion Optimization" class="img-responsive" src="/blog/images/ordering_analysis/query_window_plan.png" width="80%"/&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; DataFusion analyzes orderings of the sources and query intermediates to generate efficient plans&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Table Properties&lt;/h3&gt;
&lt;p&gt;In summary, for the example table, the following properties correctly describe the sort properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Constant Expressions&lt;/strong&gt; = &lt;code&gt;hostname, currency&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Equivalent Expression Groups&lt;/strong&gt; = &lt;code&gt;[price, price_cloned], [time, time_cloned]&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Valid Orderings&lt;/strong&gt; = &lt;code&gt;[amount ASC, price ASC], [time_bin ASC], [time ASC]&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Algorithm for Analyzing Ordering Requirements&lt;/h3&gt;
&lt;p&gt;After deriving these properties for the data, following algorithm can be used to check whether an ordering requirement is satisfied by the table:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Prune constant expressions&lt;/strong&gt;: Remove any constant expressions from the ordering requirement.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Normalize the requirement&lt;/strong&gt;: Replace each expression in the ordering requirement with the first entry from its equivalence group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;De-duplicate expressions&lt;/strong&gt;: If an expression appears more than once, remove duplicates, keeping only the first occurrence.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Match leading orderings&lt;/strong&gt;: Check whether the leading ordering requirement&lt;sup id="fn5"&gt;&lt;a href="#footnote5"&gt;5&lt;/a&gt;&lt;/sup&gt; matches the leading valid orderings&lt;sup id="fn6"&gt;&lt;a href="#footnote6"&gt;6&lt;/a&gt;&lt;/sup&gt; of table. If so:&lt;ul&gt;
&lt;li&gt;Remove the leading ordering requirement from the ordering requirement &lt;/li&gt;
&lt;li&gt;Remove the matching leading valid ordering from the valid orderings of table. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterate through the remaining expressions&lt;/strong&gt;: Go back to step 4 until ordering requirement is empty or leading ordering requirement is not found among the leading valid orderings of table.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If, at the end of the procedure above, the ordering requirement is an empty list, we can conclude that the requirement is satisfied by the table.&lt;/p&gt;
&lt;h3&gt;Example Walkthrough&lt;/h3&gt;
&lt;p&gt;Let's say the user provided a query such as the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT * FROM table
ORDER BY hostname DESC, amount ASC, time_bin ASC, price_cloned ASC, time ASC, currency ASC, price DESC;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the input has the same properties explained above&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Constant Expressions&lt;/strong&gt; = &lt;code&gt;hostname, currency&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Equivalent Expressions Groups&lt;/strong&gt; = &lt;code&gt;[price, price_cloned], [time, time_cloned]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Succinct Valid Orderings&lt;/strong&gt; = &lt;code&gt;[amount ASC, price ASC], [time_bin ASC], [time ASC]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to remove a sort the optimizer must check if the ordering requirement &lt;code&gt;[hostname DESC, amount ASC, time_bin ASC, price_cloned ASC, time ASC, currency ASC, price DESC]&lt;/code&gt; is satisfied by the properties.&lt;/p&gt;
&lt;h3&gt;Algorithm Steps&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prune constant expressions&lt;/strong&gt;:&lt;br/&gt;
   Remove &lt;code&gt;hostname&lt;/code&gt; and &lt;code&gt;currency&lt;/code&gt; from the requirement. The requirement becomes:&lt;br/&gt;
&lt;code&gt;[amount ASC, time_bin ASC, price_cloned ASC, time ASC, price DESC]&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normalize using equivalent groups&lt;/strong&gt;:&lt;br/&gt;
   Replace &lt;code&gt;price_cloned&lt;/code&gt; with &lt;code&gt;price&lt;/code&gt; and &lt;code&gt;time_cloned&lt;/code&gt; with &lt;code&gt;time&lt;/code&gt;. The requirement becomes:&lt;br/&gt;
&lt;code&gt;[amount ASC, time_bin ASC, price ASC, time ASC, price DESC]&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;De-duplicate expressions&lt;/strong&gt;:&lt;br/&gt;
   Since &lt;code&gt;price&lt;/code&gt; appears twice, we simplify the requirement to:&lt;br/&gt;
&lt;code&gt;[amount ASC, time_bin ASC, price ASC, time ASC]&lt;/code&gt; (keeping the first occurrence from the left side).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Match leading orderings&lt;/strong&gt;:&lt;br/&gt;
  Check if leading ordering requirement &lt;code&gt;amount ASC&lt;/code&gt; is among the leading valid orderings: &lt;code&gt;amount ASC, time_bin ASC, time ASC&lt;/code&gt;. Since this is the case, we remove &lt;code&gt;amount ASC&lt;/code&gt; from both the ordering requirement and the valid orderings of the table.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterate through the remaining expressions&lt;/strong&gt;:
Now, the problem is converted from&lt;br/&gt;
&lt;em&gt;"whether the requirement: &lt;code&gt;[amount ASC, time_bin ASC, price ASC, time ASC]&lt;/code&gt; is satisfied by valid orderings:  &lt;code&gt;[amount ASC, price ASC], [time_bin ASC], [time ASC]&lt;/code&gt;"&lt;/em&gt;&lt;br/&gt;
into&lt;br/&gt;
&lt;em&gt;"whether the requirement: &lt;code&gt;[time_bin ASC, price ASC, time ASC]&lt;/code&gt; is satisfied by valid orderings:  &lt;code&gt;[price ASC], [time_bin ASC], [time ASC]&lt;/code&gt;"&lt;/em&gt;&lt;br/&gt;
We go back to step 4 until the ordering requirement list is exhausted or its length no longer decreases.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At the end of stages above we end up with an empty ordering requirement list. Given this, we can conclude that the table satisfies the ordering requirement and thus no sort is required. &lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we described the conditions under which an ordering requirement is satisfied based on the properties of a table. We introduced key concepts such as constant expressions, equivalence groups, and valid orderings, and used them to determine whether a given ordering requirement are satisfied by an input table.&lt;/p&gt;
&lt;p&gt;This analysis plays a crucial role in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choosing more efficient algorithm variants&lt;/li&gt;
&lt;li&gt;Generating streaming-friendly plans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;DataFusion&lt;/code&gt; query engine employs this analysis (and many more) during its planning stage to ensure correct and efficient query execution. We &lt;a href="https://datafusion.apache.org/contributor-guide/index.html"&gt;welcome you&lt;/a&gt; to come and join the project.&lt;/p&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;!--
&lt;p id="footnote1"&gt;&lt;sup&gt;[1]&lt;/sup&gt;The ordering requirement refers to the condition that input data must be sorted in a certain way for a specific operator to function as intended.&lt;/p&gt;
--&gt;
&lt;p id="footnote1"&gt;&lt;sup&gt;[1]&lt;/sup&gt;Lexicographic order is a way of ordering sequences (like strings, list of expressions) based on the order of their components, similar to how words are ordered in a dictionary. It compares each element of the sequences one by one, from left to right.&lt;/p&gt;
&lt;p id="footnote2"&gt;&lt;sup&gt;[2]&lt;/sup&gt;Leading ordering is the first ordering in a lexicographic ordering list. As an example, for the ordering: &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, leading ordering will be: &lt;code&gt;amount ASC&lt;/code&gt;. &lt;/p&gt;
&lt;p id="footnote3"&gt;&lt;sup&gt;[3]&lt;/sup&gt;This means that, if we know that &lt;code&gt;[amount ASC]&lt;/code&gt; and &lt;code&gt;[time ASC]&lt;/code&gt; are both valid orderings for the table. We shouldn't enlist &lt;code&gt;[amount ASC, time ASC]&lt;/code&gt; or &lt;code&gt;[time ASC, amount ASC]&lt;/code&gt; as valid orderings. These orderings can be deduced if we know that table satisfies the ordering &lt;code&gt;[amount ASC]&lt;/code&gt; and &lt;code&gt;[time ASC]&lt;/code&gt;.&lt;/p&gt;
&lt;p id="footnote4"&gt;&lt;sup&gt;[4]&lt;/sup&gt;This means that, if ordering &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt; is a valid ordering for the table. We shouldn't enlist &lt;code&gt;[amount ASC]&lt;/code&gt; as valid ordering. Validity of it can be deduced from the ordering: &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;
&lt;p id="footnote5"&gt;&lt;sup&gt;[5]&lt;/sup&gt;Leading ordering requirement is the first ordering requirement in the list of lexicographic ordering requirement expression. As an example for the requirement: &lt;code&gt;[amount ASC, time_bin ASC, prices ASC, time ASC]&lt;/code&gt;, leading ordering requirement is: &lt;code&gt;amount ASC&lt;/code&gt;.&lt;/p&gt;
&lt;p id="footnote6"&gt;&lt;sup&gt;[6]&lt;/sup&gt;Leading valid orderings are the first ordering for each valid ordering list in the table. As an example, for the valid orderings: &lt;code&gt;[amount ASC, prices ASC], [time_bin ASC], [time ASC]&lt;/code&gt;, leading valid orderings will be: &lt;code&gt;amount ASC, time_bin ASC, time ASC&lt;/code&gt;. &lt;/p&gt;
&lt;p id="optimal"&gt;&lt;sup&gt;*&lt;/sup&gt;Best depends on the use case, &lt;code&gt;DataFusion&lt;/code&gt; has many various flags to communicate what user thinks the best plan is (e.g. streamable, fastest, lowest memory, etc.). See &lt;a href="https://datafusion.apache.org/user-guide/configs.html" target="_blank"&gt;configurations&lt;/a&gt; for detail.&lt;/p&gt;&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion 45.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/02/20/datafusion-45.0.0" rel="alternate"></link><published>2025-02-20T00:00:00+00:00</published><updated>2025-02-20T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-02-20:/blog/2025/02/20/datafusion-45.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/11631 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are very proud to announce &lt;a href="https://crates.io/crates/datafusion/45.0.0"&gt;DataFusion 45.0.0&lt;/a&gt;. This blog highlights some of the
many major improvements since we released &lt;a href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/"&gt;DataFusion 40.0.0&lt;/a&gt; and a preview of
what the community is thinking about in the next 6 months. It has been an exciting
period of development …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/11631 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are very proud to announce &lt;a href="https://crates.io/crates/datafusion/45.0.0"&gt;DataFusion 45.0.0&lt;/a&gt;. This blog highlights some of the
many major improvements since we released &lt;a href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/"&gt;DataFusion 40.0.0&lt;/a&gt; and a preview of
what the community is thinking about in the next 6 months. It has been an exciting
period of development for DataFusion!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; is an extensible query engine, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that
uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion is used by developers to
create new, fast data centric systems such as databases, dataframe libraries,
machine learning and streaming applications. While &lt;a href="https://datafusion.apache.org/user-guide/introduction.html#project-goals"&gt;DataFusion&amp;rsquo;s primary design
goal&lt;/a&gt; is to accelerate the creation of other data centric systems, it has a
reasonable experience directly out of the box as a &lt;a href="https://datafusion.apache.org/user-guide/dataframe.html"&gt;dataframe library&lt;/a&gt;,
&lt;a href="https://datafusion.apache.org/python/"&gt;python library&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/user-guide/cli/"&gt;command line SQL tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion's core thesis is that as a community, together we can build much more
advanced technology than any of us as individuals or companies could do alone. 
Without DataFusion, highly performant vectorized query engines would remain
the domain of a few large companies and world-class research institutions. 
With DataFusion, we can all build on top of a shared foundation, and focus on
what makes our projects unique.&lt;/p&gt;
&lt;h2&gt;Community Growth  📈&lt;/h2&gt;
&lt;p&gt;In the last 6 months, between &lt;code&gt;40.0.0&lt;/code&gt; and &lt;code&gt;45.0.0&lt;/code&gt;, our community continues to
grow in new and exciting ways.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We added several PMC members and new committers: &lt;a href="https://github.com/jayzhan211"&gt;@jayzhan211&lt;/a&gt; and &lt;a href="https://github.com/jonahgao"&gt;@jonahgao&lt;/a&gt; joined the PMC,
   &lt;a href="https://github.com/2010YOUY01"&gt;@2010YOUY01&lt;/a&gt;, &lt;a href="https://github.com/rachelint"&gt;@rachelint&lt;/a&gt;, &lt;a href="https://github.com/findepi/"&gt;@findpi&lt;/a&gt;, &lt;a href="https://github.com/iffyio"&gt;@iffyio&lt;/a&gt;, &lt;a href="https://github.com/goldmedal"&gt;@goldmedal&lt;/a&gt;, &lt;a href="https://github.com/Weijun-H"&gt;@Weijun-H&lt;/a&gt;, &lt;a href="https://github.com/Michael-J-Ward"&gt;@Michael-J-Ward&lt;/a&gt; and &lt;a href="https://github.com/korowa"&gt;@korowa&lt;/a&gt;
   joined as committers. See the &lt;a href="https://lists.apache.org/list.html?dev@datafusion.apache.org"&gt;mailing list&lt;/a&gt; for more details.&lt;/li&gt;
&lt;li&gt;In the &lt;a href="https://github.com/apache/arrow-datafusion"&gt;core DataFusion repo&lt;/a&gt; alone we reviewed and accepted almost 1600 PRs from 206 different
   committers, created over 1100 issues and closed 751 of them 🚀. All changes are listed in the detailed
   &lt;a href="https://github.com/apache/datafusion/tree/main/dev/changelog"&gt;changelogs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;DataFusion focused meetups happened in multiple cities around the world: &lt;a href="https://github.com/apache/datafusion/discussions/10341#discussioncomment-10110273"&gt;Hangzhou&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/11431"&gt;Belgrade&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/11213"&gt;New York&lt;/a&gt;, 
   &lt;a href="https://github.com/apache/datafusion/discussions/10348"&gt;Seattle&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/12894"&gt;Chicago&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/13165"&gt;Boston&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/discussions/12988"&gt;Amsterdam&lt;/a&gt; as well as a Rust NYC meetup in NYC focused on DataFusion.&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
$ git log --pretty=oneline 40.0.0..45.0.0 . | wc -l
     1532 (up from 1453)

$ git shortlog -sn 40.0.0..45.0.0 . | wc -l
     206 (up from 182)

https://crates.io/crates/datafusion/45.0.0
DataFusion 45 released Feb 7, 2025

https://crates.io/crates/datafusion/40.0.0
DataFusion 40 released July 12, 2024

Issues created in this time: 375 open, 751 closed (from 321 open, 781 closed)
https://github.com/apache/datafusion/issues?q=is%3Aissue+created%3A2024-07-12..2025-02-07

Issues closed: 956 (up from 911)
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+closed%3A2024-07-12..2025-02-07

PRs merged in this time 1597 (up from 1490)
https://github.com/apache/arrow-datafusion/pulls?q=is%3Apr+merged%3A2024-07-12..2025-02-07

--&gt;
&lt;p&gt;DataFusion has put in an application to be part of &lt;a href="https://summerofcode.withgoogle.com/"&gt;Google Summer of Code&lt;/a&gt; with a 
&lt;a href="https://github.com/apache/datafusion/issues/14478"&gt;number of ideas&lt;/a&gt; for projects with mentors already selected. Additionally, &lt;a href="https://github.com/apache/datafusion/issues/14373"&gt;some ideas&lt;/a&gt; on
how to make DataFusion an ideal selection for university database projects such as the 
&lt;a href="https://15445.courses.cs.cmu.edu/spring2025/"&gt;CMU database classes&lt;/a&gt; have been put forward.&lt;/p&gt;
&lt;p&gt;In addition, DataFusion has been appearing publicly more and more, both online and offline. Here are some highlights:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;a href="https://uwheel.rs/post/datafusion_uwheel/"&gt;demonstration of how uwheel&lt;/a&gt; is integrated into DataFusion&lt;/li&gt;
&lt;li&gt;Integrating StringView into DataFusion - &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;part 1&lt;/a&gt; and &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-two-influxdb/"&gt;part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://techontherocks.show/3"&gt;Building streams&lt;/a&gt; with DataFusion&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.haoxp.xyz/posts/caching-datafusion"&gt;Caching in DataFusion&lt;/a&gt;: Don't read twice&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.haoxp.xyz/posts/parquet-to-arrow/"&gt;Parquet pruning in DataFusion&lt;/a&gt;: Read no more than you need&lt;/li&gt;
&lt;li&gt;DataFusion is one of &lt;a href="https://www.crn.com/news/software/2024/the-10-coolest-open-source-software-tools-of-2024?page=3"&gt;The 10 coolest open source software tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.denormalized.io/blog/building-databases"&gt;Building databases over a weekend&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Improved Performance 🚀&lt;/h2&gt;
&lt;p&gt;DataFusion hit a milestone in its development by becoming &lt;a href="https://datafusion.apache.org/blog/2024/11/18/datafusion-fastest-single-node-parquet-clickbench/"&gt;the fastest single node engine&lt;/a&gt; 
for querying Apache Parquet files in &lt;a href="https://benchmark.clickhouse.com/"&gt;clickbench&lt;/a&gt; benchmark for the 43.0.0 release. A &lt;a href="https://github.com/apache/datafusion/issues/12821"&gt;lot 
of work&lt;/a&gt; went into making this happen! While other engines have subsequently gotten faster,
displacing DataFusion from the top spot, DataFusion still remains near the top and we &lt;a href="https://github.com/apache/datafusion/issues/14586"&gt;are planning
more improvements&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ClickBench performance results over time for DataFusion" class="img-responsive" src="/blog/images/datafusion-45.0.0/performance_over_time.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: ClickBench performance improved over 33% between DataFusion 33
(released Nov. 2023) and DataFusion 45 (released Feb. 2025). &lt;/p&gt;
&lt;p&gt;The task of &lt;a href="https://github.com/apache/datafusion/issues/10918"&gt;integrating&lt;/a&gt; the new &lt;a href="https://docs.rs/arrow/latest/arrow/array/struct.GenericByteViewArray.html"&gt;Arrow StringView&lt;/a&gt; which significantly improves performance 
for workloads that scan, filter and group by variable length string and binary data was completed 
and enabled by default in the past 6 months. The improvement is especially pronounced for Parquet 
files due to &lt;a href="https://github.com/apache/arrow-rs/issues/5530"&gt;upstream work in the parquet reader&lt;/a&gt;. Kudos to &lt;a href="https://github.com/XiangpengHong"&gt;@XiangpengHong&lt;/a&gt;, &lt;a href="https://github.com/AriesDevil"&gt;@AriesDevil&lt;/a&gt;, 
&lt;a href="https://github.com/PsiACE"&gt;@PsiACE&lt;/a&gt;, &lt;a href="https://github.com/Weijun-H"&gt;@Weijun-H&lt;/a&gt;, &lt;a href="https://github.com/a10y"&gt;@a10y&lt;/a&gt;, and &lt;a href="https://github.com/RinChanNOWWW"&gt;@RinChanNOWWW&lt;/a&gt; for driving this project.&lt;/p&gt;
&lt;h2&gt;Improved Quality 📋&lt;/h2&gt;
&lt;p&gt;DataFusion continues to improve overall in quality. In addition to ongoing bug
fixes, one of the most exciting improvements in the last 6 months was the addition of the 
&lt;a href="https://github.com/apache/datafusion/pull/13936"&gt;SQLite sqllogictest suite&lt;/a&gt; thanks to &lt;a href="https://github.com/Omega359"&gt;@Omega359&lt;/a&gt;. These tests run over 5 million 
sql statements on every push to the main branch.&lt;/p&gt;
&lt;p&gt;Support for &lt;a href="https://github.com/apache/datafusion/pull/13651"&gt;explicitly checking logical plan invariants&lt;/a&gt; was added by &lt;a href="https://github.com/wiedld"&gt;@wiedld&lt;/a&gt; which 
can help catch implicit changes that might cause problems during upgrades.&lt;/p&gt;
&lt;p&gt;We have also started other quality initiatives to make it &lt;a href="https://github.com/apache/datafusion/issues/13525"&gt;easier to use DataFusion&lt;/a&gt; 
based on &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt;'s experience along with more &lt;a href="https://github.com/apache/datafusion/issues/13661"&gt;extensive prerelease testing&lt;/a&gt;.  &lt;/p&gt;
&lt;h2&gt;Improved Documentation 📚&lt;/h2&gt;
&lt;p&gt;We continue to improve the documentation to make it easier to get started using DataFusion. 
During the last 6 months two projects were initiated to migrate the function documentation
from strictly static markdown files. First, &lt;a href="https://github.com/apache/datafusion/pull/12668"&gt;@Omega359&lt;/a&gt; to allow function
documentation to be generated from code and &lt;a href="https://github.com/jonathanc-n"&gt;@jonathanc-n&lt;/a&gt; and others helped with the migration,
then &lt;a href="https://github.com/comphead"&gt;@comphead&lt;/a&gt; lead a project to &lt;a href="https://github.com/apache/datafusion/pull/12822"&gt;create a doc macro&lt;/a&gt; to allow for an even easier way to write 
function documentation. A special thanks to &lt;a href="https://github.com/Chen-Yuan-Lai"&gt;@Chen-Yuan-Lai&lt;/a&gt; for migrating many functions to 
the new syntax.&lt;/p&gt;
&lt;p&gt;Additionally, the &lt;a href="https://github.com/apache/datafusion/pull/13877"&gt;examples&lt;/a&gt; were &lt;a href="https://github.com/apache/datafusion/pull/13905"&gt;refactored&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/pull/13950"&gt;cleaned up&lt;/a&gt; to improve their usefulness.&lt;/p&gt;
&lt;h2&gt;New Features ✨&lt;/h2&gt;
&lt;p&gt;There are too many new features in the last 6 months to list them all, but here
are some highlights:&lt;/p&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Uniform Window Functions:  &lt;code&gt;BuiltInWindowFunctions&lt;/code&gt; was removed and all now use UDFs (&lt;a href="https://github.com/jcsherin"&gt;@jcsherin&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Uniform Aggregate Functions: &lt;code&gt;BuiltInAggregateFunctions&lt;/code&gt; was removed and all now use UDFs&lt;/li&gt;
&lt;li&gt;As mentioned above function documentation was extracted from the markdown files&lt;/li&gt;
&lt;li&gt;Some new functions and sql support were added including '&lt;a href="https://github.com/apache/datafusion/pull/13799"&gt;show functions&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/11347"&gt;to_local_time&lt;/a&gt;',
  '&lt;a href="https://github.com/apache/datafusion/pull/12970"&gt;regexp_count&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/11969"&gt;map_extract&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/12211"&gt;array_distance&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/12329"&gt;array_any_value&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/12474"&gt;greatest&lt;/a&gt;',
  '&lt;a href="https://github.com/apache/datafusion/pull/13786"&gt;least&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/14217"&gt;arrays_overlap&lt;/a&gt;'&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;FFI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Foreign Function Interface work has started. This should allow for 
  &lt;a href="https://github.com/apache/datafusion/pull/12920"&gt;using table providers&lt;/a&gt; across languages and versions of DataFusion. This 
  is especially pertinent for integration with &lt;a href="https://delta-io.github.io/delta-rs/"&gt;delta-rs&lt;/a&gt; and other table formats.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Materialized Views&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/suremarc"&gt;@suremarc&lt;/a&gt; has added a &lt;a href="https://github.com/datafusion-contrib/datafusion-materialized-views"&gt;materialized view implementation&lt;/a&gt; in datafusion-contrib 🚀&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Substrait&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A lot of work was put into improving and enhancing substrait support (&lt;a href="https://github.com/Blizzara"&gt;@Blizzara&lt;/a&gt;, &lt;a href="https://github.com/westonpace"&gt;@westonpace&lt;/a&gt;, &lt;a href="https://github.com/tokoko"&gt;@tokoko&lt;/a&gt;, &lt;a href="https://github.com/vbarua"&gt;@vbarua&lt;/a&gt;, &lt;a href="https://github.com/LatrecheYasser"&gt;@LatrecheYasser&lt;/a&gt;, &lt;a href="https://github.com/notfilippo"&gt;@notfilippo&lt;/a&gt; and others)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Looking Ahead: The Next Six Months 🔭&lt;/h2&gt;
&lt;p&gt;One of the long term goals of &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;, DataFusion's PMC chair, has been to have 
&lt;a href="https://www.influxdata.com/blog/datafusion-2025-influxdb/"&gt;1000 DataFusion based projects&lt;/a&gt;. This may be the year that happens!&lt;/p&gt;
&lt;p&gt;The community has been &lt;a href="https://github.com/apache/datafusion/issues/14580"&gt;discussing what we will work on in the next six months&lt;/a&gt;.
Some major initiatives are likely to be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Performance&lt;/em&gt;: A &lt;a href="https://github.com/apache/datafusion/issues/14482"&gt;number of items have been identified&lt;/a&gt; as areas that could use additional work&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Memory usage&lt;/em&gt;: Tracking and improving memory usage, statistics and spilling to disk &lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://summerofcode.withgoogle.com/"&gt;Google Summer of Code&lt;/a&gt; (GSOC)&lt;/em&gt;: DataFusion is hopefully selected as a project and we start accepting and supporting student projects &lt;/li&gt;
&lt;li&gt;&lt;em&gt;FFI&lt;/em&gt;: Extending the FFI implementation to support to all types of UDF's and SessionContext&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Spark Functions&lt;/em&gt;: A &lt;a href="https://github.com/apache/datafusion/issues/5600"&gt;proposal has been made to add a crate&lt;/a&gt; covering spark compatible builtin functions &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;How to Get Involved&lt;/h2&gt;
&lt;p&gt;DataFusion is not a project built or driven by a single person, company, or
foundation. Rather, our community of users and contributors work together to
build a shared technology that none of us could have built alone.&lt;/p&gt;
&lt;p&gt;If you are interested in joining us we would love to have you. You can try out
DataFusion on some of your own data and projects and let us know how it goes,
contribute suggestions, documentation, bug reports, or a PR with documentation,
tests or code. A list of open issues suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt; and you
can find how to reach us on the &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication doc&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.6.0 Release</title><link href="https://datafusion.apache.org/blog/2025/02/17/datafusion-comet-0.6.0" rel="alternate"></link><published>2025-02-17T00:00:00+00:00</published><updated>2025-02-17T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-02-17:/blog/2025/02/17/datafusion-comet-0.6.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.6.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.6.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately four weeks of development work and is the result of merging 39 PRs from 12
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.6.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;Starting with this release, we now plan on releasing new versions of Comet more frequently, typically within 1-2 weeks
of each major DataFusion release. The main motivation for this change is to better support downstream Rust projects 
that depend on the &lt;a href="https://docs.rs/datafusion-comet-spark-expr/latest/datafusion_comet_spark_expr/"&gt;datafusion_comet_spark_expr&lt;/a&gt; crate.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;DataFusion Upgrade&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Comet 0.6.0 uses DataFusion 45.0.0&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;New Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Comet now supports &lt;code&gt;array_join&lt;/code&gt;, &lt;code&gt;array_intersect&lt;/code&gt;, and &lt;code&gt;arrays_overlap&lt;/code&gt;. Note that these expressions are not 
  yet guaranteed to be 100% compatible with Spark for all input data types, so these expressions are only enabled 
  with the configuration setting &lt;code&gt;spark.comet.expression.allowIncompatible=true&lt;/code&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Performance &amp;amp; Stability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Metrics from native execution are now updated in Spark every 3 seconds by default, rather than for each
  batch being processed. The mechanism for passing the metrics via JNI is also more efficient.&lt;/li&gt;
&lt;li&gt;New memory pool options "fair unified" and "unbounded" have been added. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/tuning.html"&gt;Comet Tuning Guide&lt;/a&gt; for more information.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Bug Fixes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hashing of decimal values with precision &amp;lt;= 18 is now compatible with Spark&lt;/li&gt;
&lt;li&gt;Comet falls back to Spark when hashing decimals with precision &amp;gt; 18&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Ballista 43.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/02/02/datafusion-ballista-43.0.0" rel="alternate"></link><published>2025-02-02T00:00:00+00:00</published><updated>2025-02-02T00:00:00+00:00</updated><author><name>milenkovicm</name></author><id>tag:datafusion.apache.org,2025-02-02:/blog/2025/02/02/datafusion-ballista-43.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are  pleased to announce version &lt;a href="https://github.com/apache/datafusion-ballista/blob/main/CHANGELOG.md#4300-2025-01-07"&gt;43.0.0&lt;/a&gt; of the &lt;a href="https://datafusion.apache.org/ballista/"&gt;DataFusion Ballista&lt;/a&gt;. Ballista allows existing &lt;a href="https://datafusion.apache.org"&gt;DataFusion&lt;/a&gt; applications to be scaled out on a cluster for use cases that are not practical to run on a single node.&lt;/p&gt;
&lt;h2&gt;Highlights of this release&lt;/h2&gt;
&lt;h3&gt;Seamless Integration with DataFusion&lt;/h3&gt;
&lt;p&gt;The primary objective of …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are  pleased to announce version &lt;a href="https://github.com/apache/datafusion-ballista/blob/main/CHANGELOG.md#4300-2025-01-07"&gt;43.0.0&lt;/a&gt; of the &lt;a href="https://datafusion.apache.org/ballista/"&gt;DataFusion Ballista&lt;/a&gt;. Ballista allows existing &lt;a href="https://datafusion.apache.org"&gt;DataFusion&lt;/a&gt; applications to be scaled out on a cluster for use cases that are not practical to run on a single node.&lt;/p&gt;
&lt;h2&gt;Highlights of this release&lt;/h2&gt;
&lt;h3&gt;Seamless Integration with DataFusion&lt;/h3&gt;
&lt;p&gt;The primary objective of this release has been to achieve a more seamless integration with the DataFusion ecosystem and try to achieve the same level of flexibility as DataFusion.&lt;/p&gt;
&lt;p&gt;In recent months, our development efforts have been directed toward providing a robust and extensible Ballista API. This new API empowers end-users to tailor Ballista's core functionality to their specific use cases. As a result, we have deprecated several experimental features from the Ballista core, allowing users to reintroduce them as custom extensions outside the core framework. This shift reduces the maintenance burden on Ballista's core maintainers and paves the way for optional features, such as &lt;a href="https://github.com/delta-io/delta-rs"&gt;delta-rs&lt;/a&gt; support, to be added externally when needed.&lt;/p&gt;
&lt;p&gt;The most significant enhancement in this release is the deprecation of &lt;code&gt;BallistaContext&lt;/code&gt;, which has been superseded by the DataFusion &lt;code&gt;SessionContext&lt;/code&gt;. This change enables DataFusion applications written in Rust to execute on a Ballista cluster with minimal modifications. Beyond simplifying migration and reducing maintenance overhead, this update introduces distributed write functionality to Ballista for the first time, significantly enhancing its capabilities.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;use ballista::prelude::*;
use datafusion::prelude::*;

#[tokio::main]
async fn main() -&amp;gt; datafusion::error::Result&amp;lt;()&amp;gt; {

  // Instead of creating classic SessionContext
  // let ctx = SessionContext::new();

  // create DataFusion SessionContext with ballista standalone cluster started
  // let ctx = SessionContext::standalone().await;

  // create DataFusion SessionContext with ballista remote cluster started
  let ctx = SessionContext::remote("df://localhost:50050").await;

  // register the table
  ctx.register_csv("example", "tests/data/example.csv", CsvReadOptions::new()).await?;

  // create a plan to run a SQL query
  let df = ctx.sql("SELECT a, MIN(b) FROM example WHERE a &amp;lt;= b GROUP BY a LIMIT 100").await?;

  // execute and print results
  df.show().await?;
  Ok(())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, Ballista&amp;rsquo;s versioning scheme has been aligned with that of DataFusion, ensuring that Ballista's version number reflects the compatible DataFusion version.&lt;/p&gt;
&lt;p&gt;At the moment there is a gap between DataFusion and Ballista, which we will try to bridge in the future.&lt;/p&gt;
&lt;h3&gt;Removal of Experimental Features&lt;/h3&gt;
&lt;p&gt;Ballista had grown in scope to include several experimental features in various states of completeness. Some features have been removed from this release in an effort to strip Ballista back to its core and make it easier to maintain and extend.&lt;/p&gt;
&lt;p&gt;Specifically, the caching subsystem, predefined object store registry, plugin subsystem, key-value stores for persistent scheduler state, and the UI have been removed.&lt;/p&gt;
&lt;h3&gt;Performance &amp;amp; Scalability&lt;/h3&gt;
&lt;p&gt;Ballista has significantly leveraged the advancements made in the DataFusion project over the past year. Benchmark results demonstrate notable improvements in performance, highlighting the impact of these enhancements:&lt;/p&gt;
&lt;p&gt;Per query comparison:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Per query comparison" class="img-responsive" src="/blog/images/datafusion-ballista-43.0.0/tpch_queries_compare.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Relative speedup:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Relative speedup graph" class="img-responsive" src="/blog/images/datafusion-ballista-43.0.0/tpch_queries_speedup_rel.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;The overall speedup is 2.9x&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overall speedup" class="img-responsive" src="/blog/images/datafusion-ballista-43.0.0/tpch_allqueries.png" width="50%"/&gt;&lt;/p&gt;
&lt;h3&gt;New Logo&lt;/h3&gt;
&lt;p&gt;Ballista now has a new logo, which is visually similar to other DataFusion projects.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="New logo" class="img-responsive" src="/blog/images/datafusion-ballista-43.0.0/ballista-logo.png" width="50%"/&gt;&lt;/p&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;Moving forward, Ballista will adopt the same release cadence as DataFusion, providing synchronized updates across the ecosystem.
Currently, there is no established long-term roadmap for Ballista. A plan will be formulated in the coming months based on community feedback and the availability of additional maintainers.&lt;/p&gt;
&lt;p&gt;In the short term, development efforts will concentrate on closing the feature gap between DataFusion and Ballista. Key priorities include implementing support for &lt;code&gt;INSERT INTO&lt;/code&gt;, enabling table &lt;code&gt;URL&lt;/code&gt; functionality, and achieving deeper integration with the Python ecosystem.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.5.0 Release</title><link href="https://datafusion.apache.org/blog/2025/01/17/datafusion-comet-0.5.0" rel="alternate"></link><published>2025-01-17T00:00:00+00:00</published><updated>2025-01-17T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-01-17:/blog/2025/01/17/datafusion-comet-0.5.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.5.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.5.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately 8 weeks of development work and is the result of merging 69 PRs from 15
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.5.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Performance&lt;/h3&gt;
&lt;p&gt;Comet 0.5.0 achieves a 1.9x speedup for single-node TPC-H @ 100 GB, an improvement from 1.7x in the previous release.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.5.0" class="img-responsive" src="/blog/images/comet-0.5.0/tpch_allqueries.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.5.0" class="img-responsive" src="/blog/images/comet-0.5.0/tpch_queries_compare.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;More benchmarking results can be found in the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Shuffle Improvements&lt;/h3&gt;
&lt;p&gt;Comet now supports multiple compression algorithms for compressing shuffle files. Previously, only ZSTD was supported
but Comet now also supports LZ4 and Snappy. The default is now LZ4, which matches the default in Spark. ZSTD may be
a better choice when the compression ratio is more important than CPU overhead.&lt;/p&gt;
&lt;p&gt;Previously, Comet used Arrow IPC to encode record batches into shuffle files. Although Arrow IPC is a good
general-purpose framework for serializing Arrow record batches, we found that we could get better performance using
a custom serialization approach optimized for Comet. One optimization is that the schema is encoded once per shuffle
operation rather than once per batch. There are some planned performance improvements in the Rust implementation of
Arrow IPC and Comet may switch back to Arrow IPC in the future.&lt;/p&gt;
&lt;p&gt;Comet provides two shuffle implementations. Comet native shuffle is the fastest and performs repartitioning in
native code. Comet columnar shuffle delegates to Spark to perform repartitioning and is used in cases where native
shuffle is not supported, such as with &lt;code&gt;RangePartitioning&lt;/code&gt;. Comet generally tries to use native shuffle first, then
columnar shuffle, and finally falls back to Spark if neither is supported. There was a bug in previous releases
where Comet would sometimes fall back to Spark shuffle if native shuffle was not supported and missed opportunities
to use columnar shuffle. This bug was fixed in this release but currently requires the configuration setting
&lt;code&gt;spark.comet.exec.shuffle.fallbackToColumnar=true&lt;/code&gt;. This will be enabled by default in the next release.&lt;/p&gt;
&lt;h3&gt;Memory Management&lt;/h3&gt;
&lt;p&gt;Comet 0.4.0 required Spark to be configured to use off-heap memory. In this release it is no longer required and
there are multiple options for configuring Comet to use on-heap memory instead. More details are available in the
&lt;a href="https://datafusion.apache.org/comet/user-guide/tuning.html"&gt;Comet Tuning Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Spark SQL Metrics&lt;/h3&gt;
&lt;p&gt;Comet now provides detailed metrics for native shuffle, showing time for repartitioning, encoding and compressing,
and writing to disk.&lt;/p&gt;
&lt;h3&gt;Crate Reorganization&lt;/h3&gt;
&lt;p&gt;One of the goals of the Comet project is to make Spark-compatible functionality available to other projects that
are based on DataFusion. In this release, many implementations of Spark-compatible expressions were moved from the
unpublished &lt;code&gt;datafusion-comet&lt;/code&gt; crate, which provides the native part of the Spark plugin, into the
&lt;code&gt;datafusion-comet-spark-expr&lt;/code&gt; crate. There is also ongoing work to reorganize this crate to move expressions into
subfolders named after the group name that Spark uses to organize expressions. For example, there are now subfolders
named &lt;code&gt;agg_funcs&lt;/code&gt;, &lt;code&gt;datetime_funcs&lt;/code&gt;, &lt;code&gt;hash_funcs&lt;/code&gt;, and so on.&lt;/p&gt;
&lt;h2&gt;Update on Complex Type Support&lt;/h2&gt;
&lt;p&gt;Good progress has been made with proof-of-concept work using DataFusion&amp;rsquo;s &lt;code&gt;ParquetExec&lt;/code&gt;, which has the advantage of
supporting complex types. This work is available on the &lt;code&gt;comet-parquet-exec&lt;/code&gt; branch, and the current focus is on
fixing test regressions, particularly regarding timestamp conversion issues.&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Python 43.1.0 Released</title><link href="https://datafusion.apache.org/blog/2024/12/14/datafusion-python-43.1.0" rel="alternate"></link><published>2024-12-14T00:00:00+00:00</published><updated>2024-12-14T00:00:00+00:00</updated><author><name>timsaucer</name></author><id>tag:datafusion.apache.org,2024-12-14:/blog/2024/12/14/datafusion-python-43.1.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/43.1.0/"&gt;datafusion-python 43.1.0&lt;/a&gt; has been released. This release
brings in all of the new features of the core &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; library. Since the last
blog post for &lt;a href="https://datafusion.apache.org/blog/2024/08/20/python-datafusion-40.0.0/"&gt;datafusion-python 40.1.0&lt;/a&gt;, a large number of improvements have been made
that can …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/43.1.0/"&gt;datafusion-python 43.1.0&lt;/a&gt; has been released. This release
brings in all of the new features of the core &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; library. Since the last
blog post for &lt;a href="https://datafusion.apache.org/blog/2024/08/20/python-datafusion-40.0.0/"&gt;datafusion-python 40.1.0&lt;/a&gt;, a large number of improvements have been made
that can be found in the &lt;a href="https://github.com/apache/datafusion-python/tree/main/dev/changelog"&gt;changelogs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We would like to point out four features that are particularly noteworthy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Arrow PyCapsule import and export&lt;/li&gt;
&lt;li&gt;User-Defined Window Functions&lt;/li&gt;
&lt;li&gt;Foreign Table Providers&lt;/li&gt;
&lt;li&gt;String View performance enhancements&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Arrow PyCapsule import and export&lt;/h2&gt;
&lt;p&gt;Arrow has stable C interface for moving data between different libraries, but difficulties
sometimes arise when different Python libraries expose this interface through different
methods, requiring developers to write function calls for each library they are attempting
to work with. A better approach is to use the &lt;a href="https://arrow.apache.org/docs/format/CDataInterface/PyCapsuleInterface.html"&gt;Arrow PyCapsule Interface&lt;/a&gt; which gives a
consistent method for exposing these data structures across libraries.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://github.com/apache/datafusion-python/pull/825"&gt;PR #825&lt;/a&gt;, we introduced support for both importing and exporting Arrow data in
&lt;code&gt;datafusion-python&lt;/code&gt;. With this improvement, you can now use a single function call to import
a table from &lt;strong&gt;any&lt;/strong&gt; Python library that implements the &lt;a href="https://arrow.apache.org/docs/format/CDataInterface/PyCapsuleInterface.html"&gt;Arrow PyCapsule Interface&lt;/a&gt;.
Many popular libaries, such as &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; and &lt;a href="https://pola.rs/"&gt;Polars&lt;/a&gt;
already support these interfaces.&lt;/p&gt;
&lt;p&gt;Suppose you have a Pandas and Polars DataFrames named &lt;code&gt;df_pandas&lt;/code&gt; or &lt;code&gt;df_polars&lt;/code&gt;, respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ctx = SessionContext()
df_dfn1 = ctx.from_arrow(df_pandas)
df_dfn1.show()

df_dfn2 = ctx.from_arrow(df_polars)
df_dfn2.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One great thing about using this interface is that as any new library is developed and
uses these stable interfaces, they will work out of the box with DataFusion!&lt;/p&gt;
&lt;p&gt;Additionally, DataFusion DataFrames allow for exporting via the PyCapsule interface. For example,
to convert a DataFrame to a PyArrow table, it is simply&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pyarrow as pa
table = pa.table(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;User-Defined Window Functions&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;datafusion-python 42.0.0&lt;/code&gt; we released User-Defined Window Support in &lt;a href="https://github.com/apache/datafusion-python/pull/880"&gt;PR #880&lt;/a&gt;.
For a detailed description of how these work please see the online documentation for
all &lt;a href="https://datafusion.apache.org/python/user-guide/common-operations/udf-and-udfa.html"&gt;user-defined functions&lt;/a&gt;. Additionally the &lt;a href="https://github.com/apache/datafusion-python/tree/main/examples"&gt;examples folder&lt;/a&gt; contains a complete
example demonstrating the four different modes of operation of window functions
within DataFusion.&lt;/p&gt;
&lt;h2&gt;Foreign Table Providers&lt;/h2&gt;
&lt;p&gt;In the core &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; release, support was added for a Foreign Function
Interface to table providers. This creates a stable way for sharing functionality
across different libraries, similar to the &lt;a href="https://arrow.apache.org/docs/format/CDataInterface.html"&gt;Arrow C data interface&lt;/a&gt; operates. This
enables libraries, such as &lt;a href="https://delta.io/docs/"&gt;delta lake&lt;/a&gt; and &lt;a href="https://github.com/datafusion-contrib/datafusion-table-providers"&gt;datafusion-contrib&lt;/a&gt; to write their own
table providers in Rust and expose them in Python without requiring a Rust dependency
on &lt;code&gt;datafusion-python&lt;/code&gt;. This is important because it allows these libraries to
operate with &lt;code&gt;datafusion-python&lt;/code&gt; regardless of which version of &lt;code&gt;datafusion&lt;/code&gt; they
were built against.&lt;/p&gt;
&lt;p&gt;To implement this feature in a table provider is quite simple. There is a complete
example in the &lt;a href="https://github.com/apache/datafusion-python/tree/main/examples"&gt;examples folder&lt;/a&gt;, but the relevant code is here, exposed as a
Python function via &lt;a href="https://pyo3.rs/"&gt;pyo3&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;    fn __datafusion_table_provider__&amp;lt;'py&amp;gt;(
        &amp;amp;self,
        py: Python&amp;lt;'py&amp;gt;,
    ) -&amp;gt; PyResult&amp;lt;Bound&amp;lt;'py, PyCapsule&amp;gt;&amp;gt; {
        let name = CString::new("datafusion_table_provider").unwrap();

        let provider = self
            .create_table()
            .map_err(|e| PyRuntimeError::new_err(e.to_string()))?;
        let provider = FFI_TableProvider::new(Arc::new(provider), false);

        PyCapsule::new_bound(py, provider, Some(name.clone()))
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's it! All of the work of converting the table provider to use the FFI interface
is performed by the core library.&lt;/p&gt;
&lt;h2&gt;String View performance enhancements&lt;/h2&gt;
&lt;p&gt;In the core &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; release, the option to enable StringView by default
was turned on. This leads to some significant performance enhancements, but it &lt;em&gt;may&lt;/em&gt;
require some changes to users of &lt;code&gt;datafusion-python&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To learn more about the excellent work on this feature please read &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1/"&gt;part 1&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-2/"&gt;part 2&lt;/a&gt;
of the blog post describing how these enhancements can lead to 20-200% performance
gains in some tests.&lt;/p&gt;
&lt;p&gt;During our testing we identified some cases where we needed to adjust workflows to
account for the fact that StringView is now the default type for string based operations.
First, when performing manipulations on string objects there is a perfomance loss when
needing to cast from string to string view or vice versa. To reap the best performance,
ideally all of your string type data will use StringView. For most users this should be
transparent. However if you specify a schema for reading or creating data, then you
likely need to change from &lt;code&gt;pa.string()&lt;/code&gt; to &lt;code&gt;pa.string_view()&lt;/code&gt;. For our testing, this
primarily happens during data loading operations and in unit tests.&lt;/p&gt;
&lt;p&gt;If you wish to disable StringView as the default type to retain the old approach,
you can do so following this example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion import SessionContext
from datafusion import SessionConfig
config = SessionConfig({"datafusion.execution.parquet.schema_force_view_types": "false"})
ctx = SessionContext(config=config)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Appreciation&lt;/h2&gt;
&lt;p&gt;We would like to thank everyone who has helped with these releases through their helpful
conversations, code review, issue descriptions, and code authoring. We would especially
like to thank the following authors of PRs who made these releases possible, listed in
alphabetical order by username: &lt;a href="https://github.com/andygrove"&gt;@andygrove&lt;/a&gt;, &lt;a href="https://github.com/drauschenbach"&gt;@drauschenbach&lt;/a&gt;, &lt;a href="https://github.com/emgeee"&gt;@emgeee&lt;/a&gt;, &lt;a href="https://github.com/ion-elgreco"&gt;@ion-elgreco&lt;/a&gt;,
&lt;a href="https://github.com/jcrist"&gt;@jcrist&lt;/a&gt;, &lt;a href="https://github.com/kosiew"&gt;@kosiew&lt;/a&gt;, &lt;a href="https://github.com/mesejo"&gt;@mesejo&lt;/a&gt;, &lt;a href="https://github.com/Michael-J-Ward"&gt;@Michael-J-Ward&lt;/a&gt;, and &lt;a href="https://github.com/sir-sigurd"&gt;@sir-sigurd&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you!&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;The DataFusion Python team is an active and engaging community and we would love
to have you join us and help the project.&lt;/p&gt;
&lt;p&gt;Here are some ways to get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learn more by visiting the &lt;a href="https://datafusion.apache.org/python/index.html"&gt;DataFusion Python project&lt;/a&gt;
page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try out the project and provide feedback, file issues, and contribute code.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.4.0 Release</title><link href="https://datafusion.apache.org/blog/2024/11/20/datafusion-comet-0.4.0" rel="alternate"></link><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-11-20:/blog/2024/11/20/datafusion-comet-0.4.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.4.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.4.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately six weeks of development work and is the result of merging 51 PRs from 10
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.4.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Performance &amp;amp; Stability&lt;/h3&gt;
&lt;p&gt;There are a number of performance and stability improvements in this release. Here is a summary of some of the
larger changes. Current benchmarking results can be found in the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Unified Memory Management&lt;/h4&gt;
&lt;p&gt;Comet now uses a unified memory management approach that shares an off-heap memory pool with Apache Spark, resulting
in a much simpler configuration. Comet now requires &lt;code&gt;spark.memory.offHeap.enabled=true&lt;/code&gt;. This approach provides a
holistic view of memory usage in Spark and Comet and makes it easier to optimize system performance.&lt;/p&gt;
&lt;h4&gt;Faster Joins&lt;/h4&gt;
&lt;p&gt;Apache Spark supports sort-merge and hash joins, which have similar performance characteristics. Spark defaults to
using sort-merge joins because they are less likely to result in OutOfMemory exceptions. In vectorized query
engines such as DataFusion, hash joins outperform sort-merge joins. Comet now has an experimental feature to
replace Spark sort-merge joins with hash joins for improved performance. This feature is experimental because
there is currently no spill-to-disk support in the hash join implementation. This feature can be enabled by
setting &lt;code&gt;spark.comet.exec.replaceSortMergeJoin=true&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Bloom Filter Aggregates&lt;/h4&gt;
&lt;p&gt;Spark&amp;rsquo;s optimizer can insert Bloom filter aggregations and filters to prune large result sets before a shuffle. However,
Comet would fall back to Spark for the aggregation. Comet now has native support for Bloom filter aggregations
after previously supporting Bloom filter testing. Users no longer need to set
&lt;code&gt;spark.sql.optimizer.runtime.bloomFilter.enabled=false&lt;/code&gt; when using Comet.&lt;/p&gt;
&lt;h4&gt;Complex Type support&lt;/h4&gt;
&lt;p&gt;This release has the following improvements to complex type support:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implemented &lt;code&gt;ArrayAppend&lt;/code&gt; and &lt;code&gt;GetArrayStructFields&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Implemented native cast between structs&lt;/li&gt;
&lt;li&gt;Implemented native cast from structs to string&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;One of the highest priority items on the roadmap is to add support for reading complex types (maps, structs, and arrays)
from Parquet sources, both when reading Parquet directly and from Iceberg.&lt;/p&gt;
&lt;p&gt;Comet currently has proprietary native code for decoding Parquet pages, native column readers for all of Spark&amp;rsquo;s
primitive types, and special handling for Spark-specific use cases such as timestamp rebasing and decimal type
promotion. This implementation does not yet support complex types. File IO, decryption, and decompression are handled
in JVM code, and Parquet pages are passed on to native code for decoding.&lt;/p&gt;
&lt;p&gt;Rather than add complex type support to this existing code, we are exploring two main options to allow us to
leverage more of the upstream Arrow and DataFusion code.&lt;/p&gt;
&lt;h3&gt;Use DataFusion&amp;rsquo;s ParquetExec&lt;/h3&gt;
&lt;p&gt;For use cases where DataFusion can support reading a Parquet source, Comet could create a native plan that uses
DataFusion&amp;rsquo;s ParquetExec. We are investigating using DataFusion&amp;rsquo;s SchemaAdapter to handle some Spark-specific
handling of timestamps and decimals.&lt;/p&gt;
&lt;h3&gt;Use Arrow&amp;rsquo;s Parquet Batch Reader&lt;/h3&gt;
&lt;p&gt;For use cases not supported by DataFusion&amp;rsquo;s ParquetExec, such as integrating with Iceberg, we are exploring
replacing our current native Parquet decoding logic with the Arrow readers provided by the Parquet crate.&lt;/p&gt;
&lt;p&gt;Iceberg already provides a vectorized Spark reader for Parquet. A &lt;a href="https://github.com/apache/iceberg/pull/9841"&gt;PR&lt;/a&gt; is open against Iceberg for adding a native
version based on Comet, and we hope to update this to leverage the improvements outlined above.&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Comparing approaches to User Defined Functions in Apache DataFusion using Python</title><link href="https://datafusion.apache.org/blog/2024/11/19/datafusion-python-udf-comparisons" rel="alternate"></link><published>2024-11-19T00:00:00+00:00</published><updated>2024-11-19T00:00:00+00:00</updated><author><name>timsaucer</name></author><id>tag:datafusion.apache.org,2024-11-19:/blog/2024/11/19/datafusion-python-udf-comparisons</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Personal Context&lt;/h2&gt;
&lt;p&gt;For a few months now I&amp;rsquo;ve been working with &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;, a
fast query engine written in Rust. From my experience the language that nearly all data scientists
are working in is Python. In general, data scientists often use &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;
for in-memory tasks and &lt;a href="https://spark.apache.org/"&gt;PySpark&lt;/a&gt; for larger …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Personal Context&lt;/h2&gt;
&lt;p&gt;For a few months now I&amp;rsquo;ve been working with &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;, a
fast query engine written in Rust. From my experience the language that nearly all data scientists
are working in is Python. In general, data scientists often use &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;
for in-memory tasks and &lt;a href="https://spark.apache.org/"&gt;PySpark&lt;/a&gt; for larger tasks that require
distributed processing.&lt;/p&gt;
&lt;p&gt;In addition to DataFusion, there is another Rust based newcomer to the DataFrame world,
&lt;a href="https://pola.rs/"&gt;Polars&lt;/a&gt;. The latter is growing extremely fast, and it serves many of the same
use cases as DataFusion. For my use cases, I'm interested in DataFusion because I want to be able
to build small scale tests rapidly and then scale them up to larger distributed systems with ease.
I do recommend evaluating Polars for in-memory work.&lt;/p&gt;
&lt;p&gt;Personally, I would love a single query approach that is fast for both in-memory usage and can
extend to large batch processing to exploit parallelization. I think DataFusion, coupled with
&lt;a href="https://datafusion.apache.org/ballista/"&gt;Ballista&lt;/a&gt; or
&lt;a href="https://github.com/apache/datafusion-ray"&gt;DataFusion-Ray&lt;/a&gt;, may provide this solution.&lt;/p&gt;
&lt;p&gt;As I&amp;rsquo;m testing, I&amp;rsquo;m primarily limiting my work to the
&lt;a href="https://datafusion.apache.org/python/"&gt;datafusion-python&lt;/a&gt; project, a wrapper around the Rust
DataFusion library. This wrapper gives you the speed advantages of keeping all of the data in the
Rust implementation and the ergonomics of working in Python. Personally, I would prefer to work
purely in Rust, but I also recognize that since the industry works in Python we should meet the
people where they are.&lt;/p&gt;
&lt;h2&gt;User-Defined Functions&lt;/h2&gt;
&lt;p&gt;The focus of this post is User-Defined Functions (UDFs). The DataFusion library gives a lot of
useful functions already for doing DataFrame manipulation. These are going to be similar to those
you find in other DataFrame libraries. You&amp;rsquo;ll be able to do simple arithmetic, create substrings of
columns, or find the average value across a group of rows. These cover most of the use cases
you&amp;rsquo;ll need in a DataFrame.&lt;/p&gt;
&lt;p&gt;However, there will always arise times when you want a custom function. With UDFs you open a
world of possibilities in your code. Sometimes there simply isn&amp;rsquo;t an easy way to use built-in
functions to achieve your goals.&lt;/p&gt;
&lt;p&gt;In the following, I&amp;rsquo;m going to demonstrate two example use cases. These are based on real world
problems I&amp;rsquo;ve encountered. Also I want to demonstrate the approach of &amp;ldquo;make it work, make it work
well, make it work fast&amp;rdquo; that is a motto I&amp;rsquo;ve seen thrown around in data science.&lt;/p&gt;
&lt;p&gt;I will demonstrate three approaches to writing UDFs. In order of increasing performance they are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Writing a pure Python function to do your computation&lt;/li&gt;
&lt;li&gt;Using the PyArrow libraries in Python to accelerate your processing&lt;/li&gt;
&lt;li&gt;Writing a UDF in Rust and exposing it to Python&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally I will demonstrate two variants of this. The first will be nearly identical to the
PyArrow library approach to simplify understanding how to connect the Rust code to Python. In the
second version we will do the iteration through the input arrays ourselves to give even greater
flexibility to the user.&lt;/p&gt;
&lt;p&gt;Here are the two example use cases, taken from my own work but generalized.&lt;/p&gt;
&lt;h3&gt;Use Case 1: Scalar Function&lt;/h3&gt;
&lt;p&gt;I have a DataFrame and a list of tuples that I&amp;rsquo;m interested in. I want to filter out the DataFrame
to only have values that match those tuples from certain columns in the DataFrame.&lt;/p&gt;
&lt;p&gt;To give a concrete example, we will use data generated for the &lt;a href="https://www.tpc.org/tpch/"&gt;TPC-H benchmarks&lt;/a&gt;.
Suppose I have a table of sales line items. There are many columns, but I am interested in three: a
part key (&lt;code&gt;p_partkey&lt;/code&gt;), supplier key (&lt;code&gt;p_suppkey&lt;/code&gt;), and return status (&lt;code&gt;p_returnflag&lt;/code&gt;). I want
only to return a DataFrame with a specific combination of these three values. That is, I want
to know if part number 1530 from supplier 4031 was sold (not returned), so I want a specific
combination of &lt;code&gt;p_partkey = 1530&lt;/code&gt;, &lt;code&gt;p_suppkey = 4031&lt;/code&gt;, and &lt;code&gt;p_returnflag = 'N'&lt;/code&gt;. I have a small
handful of these combinations I want to return.&lt;/p&gt;
&lt;p&gt;Probably the most ergonomic way to do this without UDF is to turn that list of tuples into a
DataFrame itself, perform a join, and select the columns from the original DataFrame. If we were
working in PySpark we would probably broadcast join the DataFrame created from the tuple list since
it is tiny. In practice, I have found that with some DataFrame libraries performing a filter rather
than a join can be significantly faster. This is worth profiling for your specific use case.&lt;/p&gt;
&lt;h3&gt;Use Case 2: Aggregate Function&lt;/h3&gt;
&lt;p&gt;I have a DataFrame with many values that I want to aggregate. I have already analyzed it and
determined there is a noise level below which I do not want to include in my analysis. I want to
compute a sum of only values that are above my noise threshold.&lt;/p&gt;
&lt;p&gt;This can be done fairly easy without leaning on a User Defined Aggegate Function (UDAF). You can
simply filter the DataFrame and then aggregate using the built-in &lt;code&gt;sum&lt;/code&gt; function. Here, we
demonstrate doing this as a UDF primarily as an example of how to write UDAFs. We will use the
PyArrow compute approach.&lt;/p&gt;
&lt;h2&gt;Pure Python approach&lt;/h2&gt;
&lt;p&gt;The fastest way (developer time, not code time) for me to implement the scalar problem solution
was to do something along the lines of &amp;ldquo;for each row, check the values of interest contains that
tuple&amp;rdquo;. I&amp;rsquo;ve published this as
&lt;a href="https://github.com/apache/datafusion-python/blob/main/examples/python-udf-comparisons.py"&gt;an example&lt;/a&gt;
in the &lt;a href="https://github.com/apache/datafusion-python"&gt;datafusion-python repository&lt;/a&gt;. Here is an
example of how this can be done:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;values_of_interest = [
    (1530, 4031, "N"),
    (6530, 1531, "N"),
    (5618, 619, "N"),
    (8118, 8119, "N"),
]

def is_of_interest_impl(
    partkey_arr: pa.Array,
    suppkey_arr: pa.Array,
    returnflag_arr: pa.Array,
) -&amp;gt; pa.Array:
    result = []
    for idx, partkey in enumerate(partkey_arr):
        partkey = partkey.as_py()
        suppkey = suppkey_arr[idx].as_py()
        returnflag = returnflag_arr[idx].as_py()
        value = (partkey, suppkey, returnflag)
        result.append(value in values_of_interest)

    return pa.array(result)

# Wrap our custom function with `datafusion.udf`, annotating expected 
# parameter and return types
is_of_interest = udf(
    is_of_interest_impl,
    [pa.int64(), pa.int64(), pa.utf8()],
    pa.bool_(),
    "stable",
)

df_udf_filter = df_lineitem.filter(
    is_of_interest(col("l_partkey"), col("l_suppkey"), col("l_returnflag"))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When working with a DataFusion UDF in Python, you define your function to take in some number of
expressions. During the evaluation, these will get computed into their corresponding values and
passed to your UDF as a PyArrow Array. We must return an Array also with the same number of
elements (rows). So the UDF example just iterates through all of the arrays and checks to see if
the tuple created from these columns matches any of those that we&amp;rsquo;re looking for.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll repeat because this is something that tripped me up the first time I wrote a UDF for
datafusion: &lt;strong&gt;DataFusion UDFs, even scalar UDFs, process an array of values at a time not a single
row.&lt;/strong&gt; This is different from some other DataFrame libraries and you may need to recognize a slight
change in mentality.&lt;/p&gt;
&lt;p&gt;Some important lines here are the lines like &lt;code&gt;partkey = partkey.as_py()&lt;/code&gt;. When we do this, we pay a
heavy cost. Now instead of keeping the analysis in the Rust code, we have to take the values in the
array and convert them over to Python objects. In this case we end up getting two numbers and a
string as real Python objects, complete with reference counting and all. Also we are iterating
through the array in Python rather than Rust native. These will &lt;strong&gt;significantly&lt;/strong&gt; slow down your
code. Any time you have to cross the barrier where you change values inside the Rust arrays into
Python objects or vice versa you will pay &lt;strong&gt;heavy&lt;/strong&gt; cost in that transformation. You will want to
design your UDFs to avoid this as much as possible.&lt;/p&gt;
&lt;h2&gt;Python approach using PyArrow compute&lt;/h2&gt;
&lt;p&gt;DataFusion uses &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; as its in-memory data format. This can
be seen in the way that Arrow Arrays are passed into the UDFs. We can take advantage of the fact
that &lt;a href="https://arrow.apache.org/docs/python/"&gt;PyArrow&lt;/a&gt;, the canonical Python Arrow implementation,
provides a variety of
useful functions. In the example below, we are only using a few of the boolean functions and the
equality function. Each of these functions takes two arrays and analyzes them row by row. In the
below example, we shift the logic around a little since we are now operating on an entire array of
values instead of checking a single row ourselves.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pyarrow.compute as pc

def udf_using_pyarrow_compute_impl(
    partkey_arr: pa.Array,
    suppkey_arr: pa.Array,
    returnflag_arr: pa.Array,
) -&amp;gt; pa.Array:
    results = None
    for partkey, suppkey, returnflag in values_of_interest:
        filtered_partkey_arr = pc.equal(partkey_arr, partkey)
        filtered_suppkey_arr = pc.equal(suppkey_arr, suppkey)
        filtered_returnflag_arr = pc.equal(returnflag_arr, returnflag)

        resultant_arr = pc.and_(filtered_partkey_arr, filtered_suppkey_arr)
        resultant_arr = pc.and_(resultant_arr, filtered_returnflag_arr)

        if results is None:
            results = resultant_arr
        else:
            results = pc.or_(results, resultant_arr)

    return results


udf_using_pyarrow_compute = udf(
    udf_using_pyarrow_compute_impl,
    [pa.int64(), pa.int64(), pa.utf8()],
    pa.bool_(),
    "stable",
)

df_udf_pyarrow_compute = df_lineitem.filter(
    udf_using_pyarrow_compute(col("l_partkey"), col("l_suppkey"), col("l_returnflag"))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The idea in the code above is that we will iterate through each of the values of interest, which we
expect to be small. For each of the columns, we compare the value of interest to it&amp;rsquo;s corresponding
array using &lt;code&gt;pyarrow.compute.equal&lt;/code&gt;. This will give use three boolean arrays. We have a match to
the tuple if we have a row in all three arrays that is true, so we use &lt;code&gt;pyarrow.compute.and_&lt;/code&gt;. Now
our return value from the UDF needs to include arrays for which any of the values of interest list
of tuples exists, so we take the result from the current loop and perform a &lt;code&gt;pyarrow.compute.or_&lt;/code&gt;
on it.&lt;/p&gt;
&lt;p&gt;From my benchmarking, switching from approach of converting values into Python objects to this
approach of using the PyArrow built-in functions leads to about a 10x speed improvement in this
simple problem.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that almost all of the PyArrow compute functions expect to take one or two arrays
as their arguments. If you need to write a UDF that is evaluating three or more columns, you&amp;rsquo;ll
need to do something akin to what we&amp;rsquo;ve shown here.&lt;/p&gt;
&lt;h2&gt;Rust UDF with Python wrapper&lt;/h2&gt;
&lt;p&gt;This is the most complicated approach, but has the potential to be the most performant. What we
will do here is write a Rust function to perform our computation and then expose that function to
Python. I know of two use cases where I would recommend this approach. The first is the case when
the PyArrow compute functions are insufficient for your needs. Perhaps your code is too complex or
could be greatly simplified if you pulled in some outside dependency. The second use case is when
you have written a UDF that you&amp;rsquo;re sharing across multiple projects and have hardened the approach.
It is possible that you can implement your function in Rust to give a speed improvement and then
every project that is using this shared UDF will benefit from those updates.&lt;/p&gt;
&lt;p&gt;When deciding to use this approach, it&amp;rsquo;s worth considering how much you think you&amp;rsquo;ll actually
benefit from the Rust implementation to decide if it&amp;rsquo;s worth the additional effort to maintain and
deploy the Python wheels you generate. It is certainly not necessary for every use case.&lt;/p&gt;
&lt;p&gt;Due to the excellent work by the Python arrow team, we can simplify our work to needing only two
dependencies on the Rust side, &lt;a href="https://github.com/apache/arrow-rs"&gt;arrow-rs&lt;/a&gt; and
&lt;a href="https://pyo3.rs/"&gt;pyo3&lt;/a&gt;. I have posted a &lt;a href="https://github.com/timsaucer/tuple_filter_example"&gt;minimal example&lt;/a&gt;.
You&amp;rsquo;ll need &lt;a href="https://github.com/PyO3/maturin"&gt;maturin&lt;/a&gt; to build the project, and you must use
release mode when building to get the expected performance.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;maturin develop --release
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you write your UDF in Rust you generally will need to take these steps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Write a function description that takes in some number of Python generic objects.&lt;/li&gt;
&lt;li&gt;Convert these objects to Arrow Arrays of the appropriate type(s).&lt;/li&gt;
&lt;li&gt;Perform your computation and create a resultant Array.&lt;/li&gt;
&lt;li&gt;Convert the array into a Python generic object.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the conversion to and from Python objects, we can take advantage of the
&lt;code&gt;ArrayData::from_pyarrow_bound&lt;/code&gt; and &lt;code&gt;ArrayData::to_pyarrow&lt;/code&gt; functions.  All that remains is to
perform your computation.&lt;/p&gt;
&lt;p&gt;We are going to demonstrate doing this computation in two ways. The first is to mimic what we&amp;rsquo;ve
done in the above approach using PyArrow. In the second we demonstrate iterating through the three
arrays ourselves.&lt;/p&gt;
&lt;p&gt;In our first approach, we can expect the performance to be nearly identical to when we used the
PyArrow compute functions. On the Rust side we will have slightly less overhead but the heavy
lifting portions of the code are essentially the same between this Rust implementation and the
PyArrow approach above.&lt;/p&gt;
&lt;p&gt;The reason for demonstrating this, even though it doesn&amp;rsquo;t provide a significant speedup over
Python, is to primarily demonstrate how to make the Python to Rust with Python wrapper
transition. In the second implementation you can see how we can iterate through all of the arrays
ourselves.&lt;/p&gt;
&lt;p&gt;In this first example, we are hard coding the values of interest, but in the following section
we demonstrate passing these in during initalization.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[pyfunction]
pub fn tuple_filter_fn(
    py: Python&amp;lt;'_&amp;gt;,
    partkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
    suppkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
    returnflag_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
) -&amp;gt; PyResult&amp;lt;Py&amp;lt;PyAny&amp;gt;&amp;gt; {
    let partkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
        ArrayData::from_pyarrow_bound(partkey_expr)?.into();
    let suppkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
        ArrayData::from_pyarrow_bound(suppkey_expr)?.into();
    let returnflag_arr: StringArray = ArrayData::from_pyarrow_bound(returnflag_expr)?.into();

    let values_of_interest = vec![
        (1530, 4031, "N".to_string()),
        (6530, 1531, "N".to_string()),
        (5618, 619, "N".to_string()),
        (8118, 8119, "N".to_string()),
    ];

    let mut res: Option&amp;lt;BooleanArray&amp;gt; = None;

    for (partkey, suppkey, returnflag) in &amp;amp;values_of_interest {
        let filtered_partkey_arr = BooleanArray::from_unary(&amp;amp;partkey_arr, |p| p == *partkey);
        let filtered_suppkey_arr = BooleanArray::from_unary(&amp;amp;suppkey_arr, |s| s == *suppkey);
        let filtered_returnflag_arr =
            BooleanArray::from_unary(&amp;amp;returnflag_arr, |s| s == returnflag);

        let part_and_supp = compute::and(&amp;amp;filtered_partkey_arr, &amp;amp;filtered_suppkey_arr)
            .map_err(|e| PyValueError::new_err(e.to_string()))?;
        let resultant_arr = compute::and(&amp;amp;part_and_supp, &amp;amp;filtered_returnflag_arr)
            .map_err(|e| PyValueError::new_err(e.to_string()))?;

        res = match res {
            Some(r) =&amp;gt; compute::or(&amp;amp;r, &amp;amp;resultant_arr).ok(),
            None =&amp;gt; Some(resultant_arr),
        };
    }

    res.unwrap().into_data().to_pyarrow(py)
}


#[pymodule]
fn tuple_filter_example(module: &amp;amp;Bound&amp;lt;'_, PyModule&amp;gt;) -&amp;gt; PyResult&amp;lt;()&amp;gt; {
    module.add_function(wrap_pyfunction!(tuple_filter_fn, module)?)?;
    Ok(())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use this we use the &lt;code&gt;udf&lt;/code&gt; function in &lt;code&gt;datafusion-python&lt;/code&gt; just as before.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion import udf
import pyarrow as pa
from tuple_filter_example import tuple_filter_fn

udf_using_custom_rust_fn = udf(
    tuple_filter_fn,
    [pa.int64(), pa.int64(), pa.utf8()],
    pa.bool_(),
    "stable",
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's it! We've now got a third party Rust UDF with Python wrappers working with DataFusion's
Python bindings!&lt;/p&gt;
&lt;h3&gt;Rust UDF with initialization&lt;/h3&gt;
&lt;p&gt;Looking at the code above, you can see that it is hard coding the values we're interested in. There
are many types of UDFs that don't require any additional data provided to them before they start
the computation. The code above is sloppy, so let's clean it up.&lt;/p&gt;
&lt;p&gt;We want to write the function to take some additional data. A limitation of the UDFs we create is
that they expect to operate on entire arrays of data at a time. We can get around this problem by
creating an initializer for our UDF. We do this by defining a Rust struct that contains the data we
need and implement two methods on this struct, &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;__call__&lt;/code&gt;. By doing this we will create a
Python object that is callable, so it can be the function we provide to &lt;code&gt;udf&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[pyclass]
pub struct TupleFilterClass {
    values_of_interest: Vec&amp;lt;(i64, i64, String)&amp;gt;,
}

#[pymethods]
impl TupleFilterClass {
    #[new]
    fn new(values_of_interest: Vec&amp;lt;(i64, i64, String)&amp;gt;) -&amp;gt; Self {
        Self {
            values_of_interest,
        }
    }

    fn __call__(
        &amp;amp;self,
        py: Python&amp;lt;'_&amp;gt;,
        partkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
        suppkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
        returnflag_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
    ) -&amp;gt; PyResult&amp;lt;Py&amp;lt;PyAny&amp;gt;&amp;gt; {
        let partkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
            ArrayData::from_pyarrow_bound(partkey_expr)?.into();
        let suppkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
            ArrayData::from_pyarrow_bound(suppkey_expr)?.into();
        let returnflag_arr: StringArray = ArrayData::from_pyarrow_bound(returnflag_expr)?.into();

        let mut res: Option&amp;lt;BooleanArray&amp;gt; = None;

        for (partkey, suppkey, returnflag) in &amp;amp;self.values_of_interest {
            let filtered_partkey_arr = BooleanArray::from_unary(&amp;amp;partkey_arr, |p| p == *partkey);
            let filtered_suppkey_arr = BooleanArray::from_unary(&amp;amp;suppkey_arr, |s| s == *suppkey);
            let filtered_returnflag_arr =
                BooleanArray::from_unary(&amp;amp;returnflag_arr, |s| s == returnflag);

            let part_and_supp = compute::and(&amp;amp;filtered_partkey_arr, &amp;amp;filtered_suppkey_arr)
                .map_err(|e| PyValueError::new_err(e.to_string()))?;
            let resultant_arr = compute::and(&amp;amp;part_and_supp, &amp;amp;filtered_returnflag_arr)
                .map_err(|e| PyValueError::new_err(e.to_string()))?;

            res = match res {
                Some(r) =&amp;gt; compute::or(&amp;amp;r, &amp;amp;resultant_arr).ok(),
                None =&amp;gt; Some(resultant_arr),
            };
        }

        res.unwrap().into_data().to_pyarrow(py)
    }
}

#[pymodule]
fn tuple_filter_example(module: &amp;amp;Bound&amp;lt;'_, PyModule&amp;gt;) -&amp;gt; PyResult&amp;lt;()&amp;gt; {
    module.add_class::&amp;lt;TupleFilterClass&amp;gt;()?;
    Ok(())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you write this, you don't have to call your constructor &lt;code&gt;new&lt;/code&gt;. The more important part is that
you have &lt;code&gt;#[new]&lt;/code&gt; designated on the function. With this you can provide any kinds of data you need
during processing. Using this initializer in Python is fairly straightforward.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion import udf
import pyarrow as pa
from tuple_filter_example import TupleFilterClass

tuple_filter_class = TupleFilterClass(values_of_interest)

udf_using_custom_rust_fn_with_data = udf(
    tuple_filter_class,
    [pa.int64(), pa.int64(), pa.utf8()],
    pa.bool_(),
    "stable",
    name="tuple_filter_with_data"
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you use this approach you will need to provide a &lt;code&gt;name&lt;/code&gt; argument to &lt;code&gt;udf&lt;/code&gt;. This is because our
class/struct does not get the &lt;code&gt;__qualname__&lt;/code&gt; attribute that the &lt;code&gt;udf&lt;/code&gt; function is looking for. You
can give this udf any name you choose.&lt;/p&gt;
&lt;h3&gt;Rust UDF with direct iteration&lt;/h3&gt;
&lt;p&gt;The final version of our scalar UDF is one where we implement it in Rust and iterate through all of
the arrays ourselves. If you are iterating through more than 3 arrays at a time I recommend looking
at &lt;a href="https://docs.rs/itertools/latest/itertools/macro.izip.html"&gt;izip&lt;/a&gt; in the
&lt;a href="https://crates.io/crates/itertools"&gt;itertools crate&lt;/a&gt;. For ease of understanding and since we only
have 3 arrays here I will just explicitly create my own tuple here.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[pyclass]
pub struct TupleFilterDirectIterationClass {
    values_of_interest: Vec&amp;lt;(i64, i64, String)&amp;gt;,
}

#[pymethods]
impl TupleFilterDirectIterationClass {
    #[new]
    fn new(values_of_interest: Vec&amp;lt;(i64, i64, String)&amp;gt;) -&amp;gt; Self {
        Self { values_of_interest }
    }

    fn __call__(
        &amp;amp;self,
        py: Python&amp;lt;'_&amp;gt;,
        partkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
        suppkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
        returnflag_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
    ) -&amp;gt; PyResult&amp;lt;Py&amp;lt;PyAny&amp;gt;&amp;gt; {
        let partkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
            ArrayData::from_pyarrow_bound(partkey_expr)?.into();
        let suppkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
            ArrayData::from_pyarrow_bound(suppkey_expr)?.into();
        let returnflag_arr: StringArray = ArrayData::from_pyarrow_bound(returnflag_expr)?.into();

        let values_to_search: Vec&amp;lt;(&amp;amp;i64, &amp;amp;i64, &amp;amp;str)&amp;gt; = (&amp;amp;self.values_of_interest)
            .iter()
            .map(|(a, b, c)| (a, b, c.as_str()))
            .collect();

        let values = partkey_arr
            .values()
            .iter()
            .zip(suppkey_arr.values().iter())
            .zip(returnflag_arr.iter())
            .map(|((a, b), c)| (a, b, c.unwrap_or_default()))
            .map(|v| values_to_search.contains(&amp;amp;v));

        let res: BooleanArray = BooleanBuffer::from_iter(values).into();

        res.into_data().to_pyarrow(py)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We convert the &lt;code&gt;values_of_interest&lt;/code&gt; into a vector of borrowed types so that we can do a fast search
without creating additional memory. The other option is to turn the &lt;code&gt;returnflag&lt;/code&gt; into a &lt;code&gt;String&lt;/code&gt;
but that memory allocation is unnecessary. After that we use two &lt;code&gt;zip&lt;/code&gt; operations so that we can
iterate over all three columns in a single pass. Since each &lt;code&gt;zip&lt;/code&gt; will return a tuple of two
elements, a quick &lt;code&gt;map&lt;/code&gt; turns them into the tuple format we need. Also, &lt;code&gt;StringArray&lt;/code&gt; is a little
different in the buffer it uses, so it is treated slightly differently from the others.&lt;/p&gt;
&lt;h2&gt;User Defined Aggregate Function&lt;/h2&gt;
&lt;p&gt;Writing a user defined aggregate function or user defined window function is slightly more complex
than scalar functions. This is because we must accumulate values and there is no guarantee that one
batch will contain all the values we are aggregating over. For this we need to define an
&lt;code&gt;Accumulator&lt;/code&gt; which will do a few things.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Process a batch and compute an internal state&lt;/li&gt;
&lt;li&gt;Share the state so that we can combine multiple batches&lt;/li&gt;
&lt;li&gt;Merge the results across multiple batches&lt;/li&gt;
&lt;li&gt;Return the final result&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the example below, we're going to look at customer orders and we want to know per customer ID,
how much they have ordered total. We want to ignore small orders, which we define as anything under
5000.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion import Accumulator, udaf
import pyarrow as pa
import pyarrow.compute as pc

IGNORE_THESHOLD = 5000.0
class AboveThresholdAccum(Accumulator):
    def __init__(self) -&amp;gt; None:
        self._sum = 0.0

    def update(self, values: pa.Array) -&amp;gt; None:
        over_threshold = pc.greater(values, pa.scalar(IGNORE_THESHOLD))
        sum_above = pc.sum(values.filter(over_threshold)).as_py()
        if sum_above is None:
            sum_above = 0.0
        self._sum = self._sum + sum_above

    def merge(self, states: List[pa.Array]) -&amp;gt; None:
        self._sum = self._sum + pc.sum(states[0]).as_py()

    def state(self) -&amp;gt; List[pa.Scalar]:
        return [pa.scalar(self._sum)]

    def evaluate(self) -&amp;gt; pa.Scalar:
        return pa.scalar(self._sum)

sum_above_threshold = udaf(AboveThresholdAccum, [pa.float64()], pa.float64(), [pa.float64()], 'stable')

df_orders.aggregate([col("o_custkey")],[sum_above_threshold(col("o_totalprice")).alias("sales")]).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we are doing a &lt;code&gt;sum&lt;/code&gt; we can keep a single value as our internal state. When we call &lt;code&gt;update()&lt;/code&gt;
we will process a single array and update the internal state, which we share with the &lt;code&gt;state()&lt;/code&gt;
function. For larger batches we may &lt;code&gt;merge()&lt;/code&gt; these states. It is important to note that the
&lt;code&gt;states&lt;/code&gt; in the &lt;code&gt;merge()&lt;/code&gt; function are an array of the values returned from &lt;code&gt;state()&lt;/code&gt;. It is
entirely possible that the &lt;code&gt;merge&lt;/code&gt; function is significantly different than the &lt;code&gt;update&lt;/code&gt;, though in
our example they are very similar.&lt;/p&gt;
&lt;p&gt;One example of implementing a user defined aggregate function where the &lt;code&gt;update()&lt;/code&gt; and &lt;code&gt;merge()&lt;/code&gt;
operations are different is computing an average. In &lt;code&gt;update()&lt;/code&gt; we would create a state that is both
a sum and a count. &lt;code&gt;state()&lt;/code&gt; would return a list of these two values, and &lt;code&gt;merge()&lt;/code&gt; would compute
the final result.&lt;/p&gt;
&lt;h2&gt;User Defined Window Functions&lt;/h2&gt;
&lt;p&gt;Writing a user defined window function is slightly more complex than an aggregate function due
to the variety of ways that window functions are called. I recommend reviewing the
&lt;a href="https://datafusion.apache.org/python/user-guide/common-operations/udf-and-udfa.html"&gt;online documentation&lt;/a&gt;
for a description of which functions need to be implemented. The details of how to implement
these generally follow the same patterns as described above for aggregate functions.&lt;/p&gt;
&lt;h2&gt;Performance Comparison&lt;/h2&gt;
&lt;p&gt;For the scalar functions above, we performed a timing evaluation, repeating the operation 100
times. For this simple example these are our results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+-----------------------------+--------------+---------+
| approach                    | Average Time | Std Dev |
+-----------------------------+--------------+---------+
| python udf                  | 4.969        | 0.062   |
| simple filter               | 1.075        | 0.022   |
| explicit filter             | 0.685        | 0.063   |
| pyarrow compute             | 0.529        | 0.017   |
| arrow rust compute          | 0.511        | 0.034   |
| arrow rust compute as class | 0.502        | 0.011   |
| rust custom iterator        | 0.478        | 0.009   |
+-----------------------------+--------------+---------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the conversion to Python objects is by far the worst performance. As soon as we drop
into using any functions that keep the data entirely on the Native (Rust or C/C++) side we see a
near 10x speed improvement. Then as we increase our complexity from using PyArrow compute functions
to implementing the UDF in Rust we see incremental improvements. Our fastest approach - iterating
through the arrays ourselves does operate nearly 10% faster than the PyArrow compute approach.&lt;/p&gt;
&lt;h2&gt;Final Thoughts and Recommendations&lt;/h2&gt;
&lt;p&gt;For anyone who is curious about &lt;a href="https://datafusion.apache.org/"&gt;DataFusion&lt;/a&gt; I highly recommend
giving it a try. This post was designed to make it easier for new users to the Python implementation
to work with User Defined Functions by giving a few examples of how one might implement these.&lt;/p&gt;
&lt;p&gt;When it comes to designing UDFs, I strongly recommend seeing if you can write your UDF using
&lt;a href="https://arrow.apache.org/docs/python/api/compute.html"&gt;PyArrow functions&lt;/a&gt; rather than pure Python
objects. As shown in the scalar example above, you can achieve a 10x speedup by using PyArrow
functions. If you must do something that isn't well represented by the PyArrow compute functions,
then I would consider using a Rust based UDF in the manner shown above.&lt;/p&gt;
&lt;p&gt;I would like to thank &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;, &lt;a href="https://github.com/andygrove"&gt;@andygrove&lt;/a&gt;, &lt;a href="https://github.com/comphead"&gt;@comphead&lt;/a&gt;, &lt;a href="https://github.com/emgeee"&gt;@emgeee&lt;/a&gt;, &lt;a href="https://github.com/kylebarron"&gt;@kylebarron&lt;/a&gt;, and &lt;a href="https://github.com/Omega359"&gt;@Omega359&lt;/a&gt;
for their helpful reviews and feedback.&lt;/p&gt;
&lt;p&gt;Lastly, the Apache Arrow and DataFusion community is an active group of very helpful people working
to make a great tool. If you want to get involved, please take a look at the
&lt;a href="https://datafusion.apache.org/python/"&gt;online documentation&lt;/a&gt; and jump in to help with one of the
&lt;a href="https://github.com/apache/datafusion-python/issues"&gt;open issues&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion is now the fastest single node engine for querying Apache Parquet files</title><link href="https://datafusion.apache.org/blog/2024/11/18/datafusion-fastest-single-node-parquet-clickbench" rel="alternate"></link><published>2024-11-18T00:00:00+00:00</published><updated>2024-11-18T00:00:00+00:00</updated><author><name>Andrew Lamb, Staff Engineer at InfluxData</name></author><id>tag:datafusion.apache.org,2024-11-18:/blog/2024/11/18/datafusion-fastest-single-node-parquet-clickbench</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;I am extremely excited to announce that &lt;a href="https://crates.io/crates/datafusion"&gt;Apache DataFusion&lt;/a&gt;  is the
fastest engine for querying Apache Parquet files in &lt;a href="https://benchmark.clickhouse.com/"&gt;ClickBench&lt;/a&gt;. It is faster
than &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt;, &lt;a href="https://clickhouse.com/chdb"&gt;chDB&lt;/a&gt; and &lt;a href="https://clickhouse.com/"&gt;Clickhouse&lt;/a&gt; using the same hardware. It also marks
the first time a &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;-based engine holds the top spot, which has previously
been …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;I am extremely excited to announce that &lt;a href="https://crates.io/crates/datafusion"&gt;Apache DataFusion&lt;/a&gt;  is the
fastest engine for querying Apache Parquet files in &lt;a href="https://benchmark.clickhouse.com/"&gt;ClickBench&lt;/a&gt;. It is faster
than &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt;, &lt;a href="https://clickhouse.com/chdb"&gt;chDB&lt;/a&gt; and &lt;a href="https://clickhouse.com/"&gt;Clickhouse&lt;/a&gt; using the same hardware. It also marks
the first time a &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;-based engine holds the top spot, which has previously
been held by traditional C/C++-based engines.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Apache DataFusion Logo" class="img-responsive" src="/blog/images/2x_bgwhite_original.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="ClickBench performance for DataFusion 43.0.0" class="img-responsive" src="/blog/images/clickbench-datafusion-43/perf.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: 2024-11-16 &lt;a href="https://benchmark.clickhouse.com/#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"&gt;ClickBench Results&lt;/a&gt; for the  &amp;lsquo;hot&amp;rsquo;[^1] run against the
partitioned 14 GB Parquet dataset (100 files, each ~140MB) on a &lt;code&gt;c6a.4xlarge&lt;/code&gt; (16
CPU / 32 GB  RAM) VM. Measurements are relative (&lt;code&gt;1.x&lt;/code&gt;) to results using
different hardware.&lt;/p&gt;
&lt;p&gt;Best in class performance on Parquet is now available to anyone. DataFusion&amp;rsquo;s
open design lets you start quickly with a full featured Query Engine, including
SQL, data formats, catalogs, and more, and then customize any behavior you need.
I predict the continued emergence of new classes of data systems now that
creators can focus the bulk of their innovation on areas such as query
languages, system integrations, and data formats rather than trying to play
catchup with core engine performance.&lt;/p&gt;
&lt;p&gt;ClickBench also includes results for proprietary storage formats, which require
costly load / export steps, making them useful in fewer use cases and thus much
less important than open formats (though the idea of use case specific formats
is interesting[^2]).&lt;/p&gt;
&lt;p&gt;This blog post highlights some of the techniques we used to achieve this
performance, and celebrates the teamwork involved.&lt;/p&gt;
&lt;h1&gt;A Strong History of Performance Improvements&lt;/h1&gt;
&lt;p&gt;Performance has long been a core focus for DataFusion's community, and 
speed attracts users and contributors. Recently, we seem to have been
even more focused on performance, including in July, 2024 when &lt;a href="https://www.linkedin.com/in/mehmet-ozan-kabak/"&gt;Mehmet Ozan
Kabak&lt;/a&gt;, CEO of &lt;a href="https://www.synnada.ai/"&gt;Synnada&lt;/a&gt;, again &lt;a href="https://github.com/apache/datafusion/issues/11442#issuecomment-2226834443"&gt;suggested focusing on performance&lt;/a&gt;. This
got many of us excited (who doesn&amp;rsquo;t love a challenge!), and we have subsequently
rallied to steadily improve the performance release on release as shown in
Figure 2.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ClickBench performance results over time for DataFusion" class="img-responsive" src="/blog/images/clickbench-datafusion-43/perf-over-time.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: ClickBench performance improved over 30% between DataFusion 34
(released Dec. 2023) and DataFusion 43 (released Nov. 2024).&lt;/p&gt;
&lt;p&gt;Like all good optimization efforts, ours took sustained effort as DataFusion ran
out of &lt;a href="https://www.influxdata.com/blog/aggregating-millions-groups-fast-apache-arrow-datafusion"&gt;single 2x performance improvements&lt;/a&gt; several years ago. Working together our
community of engineers from around the world[^3] and all experience levels[^4]
pulled it off (check out &lt;a href="https://github.com/apache/datafusion/issues/12821"&gt;this discussion&lt;/a&gt; to get a sense). It may be a "&lt;a href="https://db.cs.cmu.edu/seminar2024/"&gt;hobo
sandwich&lt;/a&gt;" [^5], but it is a tasty one!&lt;/p&gt;
&lt;p&gt;Of course, most of these techniques have been implemented and described before,
but until now they were only available in proprietary systems such as
&lt;a href="https://www.vertica.com/"&gt;Vertica&lt;/a&gt;, &lt;a href="https://www.databricks.com/product/photon"&gt;DataBricks
Photon&lt;/a&gt;, or
&lt;a href="https://www.snowflake.com/en/"&gt;Snowflake&lt;/a&gt; or in tightly integrated open source
systems such as &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt; or
&lt;a href="https://clickhouse.com/"&gt;ClickHouse&lt;/a&gt; which were not designed to be extended.&lt;/p&gt;
&lt;h2&gt;StringView&lt;/h2&gt;
&lt;p&gt;Performance improved for all queries when DataFusion switched to using Arrow
&lt;code&gt;StringView&lt;/code&gt;. Using &lt;code&gt;StringView&lt;/code&gt; &amp;ldquo;just&amp;rdquo; saves some copies and avoids one memory
access for certain comparisons. However, these copies and comparisons happen to
occur in many of the hottest loops during query processing, so optimizing them
resulted in measurable performance improvements.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Illustration of how take works with StringView" class="img-responsive" src="/blog/images/clickbench-datafusion-43/string-view-take.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Figure from &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;Using StringView / German Style Strings to Make
Queries Faster: Part 1&lt;/a&gt; showing how &lt;code&gt;StringView&lt;/code&gt; saves copying data in many cases.&lt;/p&gt;
&lt;p&gt;Using StringView to make DataFusion faster for ClickBench required substantial
careful, low level optimization work described in &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;Using StringView / German
Style Strings to Make Queries Faster: Part 1&lt;/a&gt; and &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-two-influxdb/"&gt;Part 2&lt;/a&gt;. However, it &lt;em&gt;also&lt;/em&gt;
required extending the rest of DataFusion&amp;rsquo;s operations to support the new type.
You can get a sense of the magnitude of the work required by looking at the 100+
pull requests linked to the epic in arrow-rs
(&lt;a href="https://github.com/apache/arrow-rs/issues/5374"&gt;here&lt;/a&gt;) and three major epics
(&lt;a href="https://github.com/apache/datafusion/issues/10918"&gt;here&lt;/a&gt;,
&lt;a href="https://github.com/apache/datafusion/issues/11790"&gt;here&lt;/a&gt; and
&lt;a href="https://github.com/apache/datafusion/issues/11752"&gt;here&lt;/a&gt;) in DataFusion.&lt;/p&gt;
&lt;p&gt;Here is a partial list of people involved in the project (I am sorry to those whom I forgot)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Arrow&lt;/strong&gt;:  &lt;a href="https://github.com/XiangpengHao"&gt;Xiangpeng Hao&lt;/a&gt; (InfluxData&amp;rsquo;s amazing 2024 summer intern and UW Madison PhD), &lt;a href="https://github.com/ariesdevil"&gt;Yijun Zhao&lt;/a&gt; from DataBend Labs, and &lt;a href="https://github.com/tustvold"&gt;Raphael Taylor-Davies&lt;/a&gt; laid the foundation.  &lt;a href="https://github.com/RinChanNOWWW"&gt;RinChanNOW&lt;/a&gt; from Tencent and &lt;a href="https://github.com/a10y"&gt;Andrew Duffy&lt;/a&gt; from SpiralDB helped push it along in the early days, and &lt;a href="https://github.com/viirya"&gt;Liang-Chi Hsieh&lt;/a&gt;, &lt;a href="https://github.com/Dandandan"&gt;Dani&amp;euml;l Heres&lt;/a&gt; reviewed and provided guidance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DataFusion&lt;/strong&gt;:  &lt;a href="https://github.com/XiangpengHao"&gt;Xiangpeng Hao&lt;/a&gt;, again charted the initial path and &lt;a href="https://github.com/Weijun-H"&gt;Weijun Huang&lt;/a&gt;, &lt;a href="https://github.com/dharanad"&gt;Dharan Aditya&lt;/a&gt; &lt;a href="https://github.com/Lordworms"&gt;Lordworms&lt;/a&gt;, &lt;a href="https://github.com/goldmedal"&gt;Jax Liu&lt;/a&gt;,  &lt;a href="https://github.com/wiedld"&gt;wiedld&lt;/a&gt;, &lt;a href="https://github.com/tlm365"&gt;Tai Le Manh&lt;/a&gt;, &lt;a href="https://github.com/my-vegetable-has-exploded"&gt;yi wang&lt;/a&gt;, &lt;a href="https://github.com/doupache"&gt;doupache&lt;/a&gt;, &lt;a href="https://github.com/jayzhan211"&gt;Jay Zhan&lt;/a&gt; , &lt;a href="https://github.com/xinlifoobar"&gt;Xin Li&lt;/a&gt;  and &lt;a href="https://github.com/Kev1n8"&gt;Kaifeng Zheng&lt;/a&gt; made it real.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DataFusion String Function Migration&lt;/strong&gt;:  &lt;a href="https://github.com/tshauck"&gt;Trent Hauck&lt;/a&gt; organized the effort and set the patterns, &lt;a href="https://github.com/goldmedal"&gt;Jax Liu&lt;/a&gt; made a clever testing framework, and &lt;a href="https://github.com/austin362667"&gt;Austin Liu&lt;/a&gt;, &lt;a href="https://github.com/demetribu"&gt;Dmitrii Bu&lt;/a&gt;, &lt;a href="https://github.com/tlm365"&gt;Tai Le Manh&lt;/a&gt;, &lt;a href="https://github.com/PsiACE"&gt;Chojan Shang&lt;/a&gt;, &lt;a href="https://github.com/devanbenz"&gt;WeblWabl&lt;/a&gt;, &lt;a href="https://github.com/Lordworms"&gt;Lordworms&lt;/a&gt;, &lt;a href="https://github.com/thinh2"&gt;iamthinh&lt;/a&gt;, &lt;a href="https://github.com/Omega359"&gt;Bruce Ritchie&lt;/a&gt;, &lt;a href="https://github.com/Kev1n8"&gt;Kaifeng Zheng&lt;/a&gt;, and &lt;a href="https://github.com/xinlifoobar"&gt;Xin Li&lt;/a&gt; bashed out the conversions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Parquet&lt;/h2&gt;
&lt;p&gt;Part of the reason for DataFusion's speed in ClickBench is reading Parquet files (really) quickly,
which reflects invested effort in the Parquet reading system (see &lt;a href="https://www.influxdata.com/blog/querying-parquet-millisecond-latency/"&gt;Querying
Parquet with Millisecond Latency&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/physical_plan/parquet/struct.ParquetExec.html"&gt;DataFusion ParquetExec&lt;/a&gt; (built on the &lt;a href="https://crates.io/crates/parquet"&gt;Rust Parquet Implementation&lt;/a&gt;) is now the most
sophisticated open source Parquet reader I know of. It has every optimization we
can think of for reading Parquet, including projection pushdown, predicate
pushdown (row group metadata, page index, and bloom filters), limit pushdown,
parallel reading, interleaved I/O, and late materialized filtering (coming soon &amp;trade;️
by default). Some recent work from &lt;a href="https://github.com/itsjunetime"&gt;June&lt;/a&gt;
&lt;a href="https://github.com/apache/datafusion/pull/12135"&gt;recently unblocked a remaining hurdle&lt;/a&gt; for enabling late materialized
filtering, and conveniently &lt;a href="https://github.com/XiangpengHao"&gt;Xiangpeng Hao&lt;/a&gt; is
working on the &lt;a href="https://github.com/apache/arrow-datafusion/issues/3463"&gt;final piece&lt;/a&gt; (no pressure😅)&lt;/p&gt;
&lt;h2&gt;Skipping Partial Aggregation When It Doesn't Help&lt;/h2&gt;
&lt;p&gt;Many ClickBench queries are aggregations that summarize millions of rows, a
common task for reporting and dashboarding. DataFusion uses state of the art
&lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.Accumulator.html#tymethod.state"&gt;two phase aggregation&lt;/a&gt; plans. Normally, two phase aggregation works well as the
first phase consolidates many rows immediately after reading, while the data is
still in cache. However, for certain &amp;ldquo;high cardinality&amp;rdquo; aggregate queries (that
have large numbers of groups), &lt;a href="https://github.com/apache/datafusion/issues/6937"&gt;the two phase aggregation strategy used in
DataFusion was inefficient&lt;/a&gt;,
manifesting in relatively slower performance compared to other engines for
ClickBench queries such as&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT "WatchID", "ClientIP", COUNT(*) AS c, ... 
FROM hits 
GROUP BY "WatchID", "ClientIP" /* &amp;lt;----- 13M Distinct Groups!!! */
ORDER BY c DESC 
LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For such queries, the first aggregation phase does not significantly
reduce the number of rows, which wastes significant effort. &lt;a href="https://github.com/korowa"&gt;Eduard
Karacharov&lt;/a&gt; contributed a &lt;a href="https://github.com/apache/datafusion/pull/11627"&gt;dynamic strategy&lt;/a&gt; to
bypass the first phase when it is not working efficiently, shown in Figure 4.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Two phase aggregation diagram from DataFusion API docs annotated to show first phase not helping" class="img-responsive" src="/blog/images/clickbench-datafusion-43/skipping-partial-aggregation.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Diagram from &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.Accumulator.html#tymethod.state"&gt;DataFusion API docs&lt;/a&gt; showing when the multi-phase
grouping is not effective&lt;/p&gt;
&lt;h2&gt;Optimized Multi-Column Grouping&lt;/h2&gt;
&lt;p&gt;Another method for improving analytic database performance is specialized (aka
highly optimized) versions of operations for different data types, which the
system picks at runtime based on the query. Like other systems, DataFusion has
specialized code for handling different types of group columns. For example,
there is &lt;a href="https://github.com/apache/datafusion/blob/73507c307487708deb321e1ba4e0d302084ca27e/datafusion/physical-plan/src/aggregates/group_values/single_group_by/primitive.rs"&gt;special code&lt;/a&gt; that handles &lt;code&gt;GROUP BY int_id&lt;/code&gt;  and &lt;a href="https://github.com/apache/datafusion/blob/73507c307487708deb321e1ba4e0d302084ca27e/datafusion/physical-plan/src/aggregates/group_values/single_group_by/bytes.rs"&gt;different special
code&lt;/a&gt; that handles &lt;code&gt;GROUP BY string_id&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;When a query groups by multiple columns, it is tricker to apply this technique.
For example &lt;code&gt;GROUP BY string_id, int_id&lt;/code&gt; and &lt;code&gt;GROUP BY int_id, string_id&lt;/code&gt; have
different optimal structures, but it is not possible to include specialized
versions for all possible combinations of group column types.&lt;/p&gt;
&lt;p&gt;DataFusion includes &lt;a href="https://github.com/apache/datafusion/blob/73507c307487708deb321e1ba4e0d302084ca27e/datafusion/physical-plan/src/aggregates/group_values/row.rs#L33-L39"&gt;a general Row based mechanism&lt;/a&gt; that works for any
combination of column types, but this general mechanism copies each value twice
as shown in Figure 5. The cost of this copy &lt;a href="https://github.com/apache/datafusion/issues/9403"&gt;is especially high for variable
length strings and binary data&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Row based storage for multiple group columns" class="img-responsive" src="/blog/images/clickbench-datafusion-43/row-based-storage.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: Prior to DataFusion 43.0.0, queries with multiple group columns
used Row based group storage and copied each group value twice. This copy
consumes a substantial amount of the query time for queries with many distinct
groups, such as several of the queries in ClickBench.&lt;/p&gt;
&lt;p&gt;Many optimizations in Databases boil down to simply avoiding copies, and this
was no exception. The trick was to figure out how to avoid copies without
causing per-column comparison overhead to dominate or complexity to get out of
hand. In a great example of diligent and disciplined engineering, &lt;a href="https://github.com/jayzhan211"&gt;Jay
Zhan&lt;/a&gt; tried &lt;a href="https://github.com/apache/datafusion/pull/10937"&gt;several&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/pull/10976"&gt;different&lt;/a&gt; approaches until arriving
at the [one shipped in DataFusion &lt;code&gt;43.0.0&lt;/code&gt;], shown in Figure 6.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Column based storage for multiple group columns" class="img-responsive" src="/blog/images/clickbench-datafusion-43/column-based-storage.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: DataFusion 43.0.0&amp;rsquo;s new columnar group storage copies each group
value exactly once, which is significantly faster when grouping by multiple
columns.&lt;/p&gt;
&lt;p&gt;Huge thanks as well to &lt;a href="https://github.com/eejbyfeldt"&gt;Emil Ejbyfeldt&lt;/a&gt; and
&lt;a href="https://github.com/Dandandan"&gt;Dani&amp;euml;l Heres&lt;/a&gt; for their help reviewing and to
&lt;a href="https://github.com/Rachelint"&gt;Rachelint (kamille&lt;/a&gt;) for reviewing and
contributing a faster &lt;a href="https://github.com/apache/datafusion/pull/12996"&gt;vectorized append and compare for multiple groups&lt;/a&gt; which
will be released in DataFusion 44. The discussion on &lt;a href="https://github.com/apache/datafusion/issues/9403"&gt;the ticket&lt;/a&gt; is another
great example of the power of the DataFusion community working together to build
great software.&lt;/p&gt;
&lt;h1&gt;What&amp;rsquo;s Next 🚀&lt;/h1&gt;
&lt;p&gt;Just as I expect the performance of other engines to improve, DataFusion has
several more performance improvements lined up itself:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/datafusion/pull/11943#top"&gt;Intermediate results blocked management&lt;/a&gt; (thanks again &lt;a href="https://github.com/Rachelint"&gt;Rachelint (kamille&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/datafusion/issues/3463"&gt;Enable parquet filter pushdown by default&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are also talking about what to focus on over the &lt;a href="https://github.com/apache/datafusion/issues/13274"&gt;next three
months&lt;/a&gt; and are always
looking for people to help! If you want to geek out (obsess??) about performance
and other features with engineers from around the world, &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;we would love you to
join us&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Additional Thanks&lt;/h1&gt;
&lt;p&gt;In addition to the people called out above, thanks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/pmcgleenon"&gt;Patrick McGleenon&lt;/a&gt; for running ClickBench and gathering this data (&lt;a href="https://github.com/apache/datafusion/issues/13099#issuecomment-2478314793"&gt;source&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Everyone I missed in the shoutouts &amp;ndash; there are so many of you. We appreciate everyone.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I have dreamed about DataFusion being on top of the ClickBench leaderboard for
several years. I often watched with envy improvements in systems backed by large
VC investments, internet companies, or world class research institutions, and
doubted that we could pull off something similar in an open source project with
always limited time.&lt;/p&gt;
&lt;p&gt;The fact that we have now surpassed those other systems in query performance I
think speaks to the power and possibility of focusing on community and aligning
our collective enthusiasm and skills towards a common goal. Of course, being on
the top in any particular benchmark is likely fleeting as other engines will
improve, but so will DataFusion!&lt;/p&gt;
&lt;p&gt;I love working on DataFusion &amp;ndash; the people, the quality of the code, my
interactions and the results we have achieved together far surpass my
expectations as well as most of my other software development experiences. I
can&amp;rsquo;t wait to see what people will build next, and hope to &lt;a href="https://github.com/apache/datafusion"&gt;see you
online&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;[^1]: Note that DuckDB is slightly faster on the &amp;lsquo;cold&amp;rsquo; run.&lt;/p&gt;
&lt;p&gt;[^2]: Want to try your hand at a custom format for ClickBench fame / glory?: &lt;a href="https://github.com/apache/datafusion/issues/13448"&gt;Make DataFusion the fastest engine in ClickBench with custom file format&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[^3]: We have contributors from North America, South American, Europe, Asia, Africa and Australia&lt;/p&gt;
&lt;p&gt;[^4]: Undergraduates, PhD, Junior engineers, and getting-kind-of-crotchety experienced engineers&lt;/p&gt;
&lt;p&gt;[^5]: Thanks to Andy Pavlo, I love that nomenclature&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.3.0 Release</title><link href="https://datafusion.apache.org/blog/2024/09/27/datafusion-comet-0.3.0" rel="alternate"></link><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-09-27:/blog/2024/09/27/datafusion-comet-0.3.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.3.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.3.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately four weeks of development work and is the result of merging 57 PRs from 12 
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.3.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Binary Releases&lt;/h3&gt;
&lt;p&gt;Comet jar files are now published to Maven central for amd64 and arm64 architectures (Linux only).&lt;/p&gt;
&lt;p&gt;Files can be found at https://central.sonatype.com/search?q=org.apache.datafusion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spark versions 3.3, 3.4, and 3.5 are supported.&lt;/li&gt;
&lt;li&gt;Scala versions 2.12 and 2.13 are supported.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;New Features&lt;/h3&gt;
&lt;p&gt;The following expressions are now supported natively:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DateAdd&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DateSub&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ElementAt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GetArrayElement&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ToJson&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Performance &amp;amp; Stability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Upgraded to DataFusion 42.0.0&lt;/li&gt;
&lt;li&gt;Reduced memory overhead due to some memory leaks being fixed&lt;/li&gt;
&lt;li&gt;Comet will now fall back to Spark for queries that use DPP, to avoid performance regressions because Comet does 
  not have native support for DPP yet&lt;/li&gt;
&lt;li&gt;Improved performance when converting Spark columnar data to Arrow format&lt;/li&gt;
&lt;li&gt;Faster decimal sum and avg functions &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Documentation Updates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Improved documentation for deploying Comet with Kubernetes and Helm in the &lt;a href="https://datafusion.apache.org/comet/user-guide/kubernetes.html"&gt;Comet Kubernetes Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More detailed architectural overview of Comet scan and execution in the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/plugin_overview.html"&gt;Comet Plugin Overview&lt;/a&gt; in the contributor guide&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Using StringView / German Style Strings to Make Queries Faster: Part 1- Reading Parquet</title><link href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1" rel="alternate"></link><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><author><name>Xiangpeng Hao, Andrew Lamb</name></author><id>tag:datafusion.apache.org,2024-09-13:/blog/2024/09/13/string-view-german-style-strings-part-1</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This is the first of a &lt;a href="../2024/09/13/string-view-german-style-strings-part-2/"&gt;two part&lt;/a&gt; blog series that was first published on the &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;InfluxData blog&lt;/a&gt;. Thanks to InfluxData for sponsoring this work as &lt;a href="https://haoxp.xyz/"&gt;Xiangpeng Hao&lt;/a&gt;'s summer intern project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This blog describes our experience implementing &lt;a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-view-layout"&gt;StringView&lt;/a&gt; in the &lt;a href="https://github.com/apache/arrow-rs"&gt;Rust implementation&lt;/a&gt; of &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;, and integrating …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This is the first of a &lt;a href="../2024/09/13/string-view-german-style-strings-part-2/"&gt;two part&lt;/a&gt; blog series that was first published on the &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;InfluxData blog&lt;/a&gt;. Thanks to InfluxData for sponsoring this work as &lt;a href="https://haoxp.xyz/"&gt;Xiangpeng Hao&lt;/a&gt;'s summer intern project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This blog describes our experience implementing &lt;a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-view-layout"&gt;StringView&lt;/a&gt; in the &lt;a href="https://github.com/apache/arrow-rs"&gt;Rust implementation&lt;/a&gt; of &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;, and integrating it into &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;, significantly accelerating string-intensive queries in the &lt;a href="https://benchmark.clickhouse.com/"&gt;ClickBench&lt;/a&gt; benchmark by 20%- 200% (Figure 1[^1]).&lt;/p&gt;
&lt;p&gt;Getting significant end-to-end performance improvements was non-trivial. Implementing StringView itself was only a fraction of the effort required. Among other things, we had to optimize UTF-8 validation, implement unintuitive compiler optimizations, tune block sizes, and time GC to realize the &lt;a href="https://www.influxdata.com/blog/flight-datafusion-arrow-parquet-fdap-architecture-influxdb/"&gt;FDAP ecosystem&lt;/a&gt;&amp;rsquo;s benefit. With other members of the open source community, we were able to overcome performance bottlenecks that could have killed the project. We would like to contribute by explaining the challenges and solutions in more detail so that more of the community can learn from our experience.&lt;/p&gt;
&lt;p&gt;StringView is based on a simple idea: avoid some string copies and accelerate comparisons with inlined prefixes. Like most great ideas, it is &amp;ldquo;obvious&amp;rdquo; only after &lt;a href="https://db.in.tum.de/~freitag/papers/p29-neumann-cidr20.pdf"&gt;someone describes it clearly&lt;/a&gt;. Although simple, straightforward implementation actually &lt;em&gt;slows down performance for almost every query&lt;/em&gt;. We must, therefore, apply astute observations and diligent engineering to realize the actual benefits from StringView.&lt;/p&gt;
&lt;p&gt;Although this journey was successful, not all research ideas are as lucky. To accelerate the adoption of research into industry, it is valuable to integrate research prototypes with practical systems. Understanding the nuances of real-world systems makes it more likely that research designs[^2] will lead to practical system improvements.&lt;/p&gt;
&lt;p&gt;StringView support was released as part of &lt;a href="https://crates.io/crates/arrow/52.2.0"&gt;arrow-rs v52.2.0&lt;/a&gt; and &lt;a href="https://crates.io/crates/datafusion/41.0.0"&gt;DataFusion v41.0.0&lt;/a&gt;. You can try it by setting the &lt;code&gt;schema_force_view_types&lt;/code&gt; &lt;a href="https://datafusion.apache.org/user-guide/configs.html"&gt;DataFusion configuration option&lt;/a&gt;, and we are&lt;a href="https://github.com/apache/datafusion/issues/11682"&gt; hard at work with the community to &lt;/a&gt;make it the default. We invite everyone to try it out, take advantage of the effort invested so far, and contribute to making it better.&lt;/p&gt;
&lt;p&gt;&lt;img alt="End to end performance improvements for ClickBench queries" class="img-responsive" src="/blog/images/string-view-1/figure1-performance.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: StringView improves string-intensive ClickBench query performance by 20% - 200%&lt;/p&gt;
&lt;h2&gt;What is StringView?&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Diagram of using StringArray and StringViewArray to represent the same string content" class="img-responsive" src="/blog/images/string-view-1/figure2-string-view.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: Use StringArray and StringViewArray to represent the same string content.&lt;/p&gt;
&lt;p&gt;The concept of inlined strings with prefixes (called &amp;ldquo;German Strings&amp;rdquo; &lt;a href="https://x.com/andy_pavlo/status/1813258735965643203"&gt;by Andy Pavlo&lt;/a&gt;, in homage to &lt;a href="https://www.tum.de/"&gt;TUM&lt;/a&gt;, where the &lt;a href="https://db.in.tum.de/~freitag/papers/p29-neumann-cidr20.pdf"&gt;Umbra paper that describes&lt;/a&gt; them originated) 
has been used in many recent database systems (&lt;a href="https://engineering.fb.com/2024/02/20/developer-tools/velox-apache-arrow-15-composable-data-management/"&gt;Velox&lt;/a&gt;, &lt;a href="https://pola.rs/posts/polars-string-type/"&gt;Polars&lt;/a&gt;, &lt;a href="https://duckdb.org/2021/12/03/duck-arrow.html"&gt;DuckDB&lt;/a&gt;, &lt;a href="https://cedardb.com/blog/german_strings/"&gt;CedarDB&lt;/a&gt;, etc.) 
and was introduced to Arrow as a new &lt;a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-view-layout"&gt;StringViewArray&lt;/a&gt;[^3] type. Arrow&amp;rsquo;s original &lt;a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-layout"&gt;StringArray&lt;/a&gt; is very memory efficient but less effective for certain operations. 
StringViewArray accelerates string-intensive operations via prefix inlining and a more flexible and compact string representation.&lt;/p&gt;
&lt;p&gt;A StringViewArray consists of three components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;&lt;em&gt;view&lt;/em&gt;&lt;/code&gt; array&lt;/li&gt;
&lt;li&gt;The buffers&lt;/li&gt;
&lt;li&gt;The buffer pointers (IDs) that map buffer offsets to their physical locations&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each &lt;code&gt;view&lt;/code&gt; is 16 bytes long, and its contents differ based on the string&amp;rsquo;s length:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;string length &amp;lt; 12 bytes: the first four bytes store the string length, and the remaining 12 bytes store the inlined string.&lt;/li&gt;
&lt;li&gt;string length &amp;gt; 12 bytes: the string is stored in a separate buffer. The length is again stored in the first 4 bytes, followed by the buffer id (4 bytes), the buffer offset (4 bytes), and the prefix (first 4 bytes) of the string.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figure 2 shows an example of the same logical content (left) using StringArray (middle) and StringViewArray (right):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first string &amp;ndash; &lt;code&gt;"Apache DataFusion"&lt;/code&gt; &amp;ndash; is 17 bytes long, and both StringArray and StringViewArray store the string&amp;rsquo;s bytes at the beginning of the buffer. The StringViewArray also inlines the first 4 bytes &amp;ndash; &lt;code&gt;"Apac"&lt;/code&gt; &amp;ndash; in the view.&lt;/li&gt;
&lt;li&gt;The second string, &lt;code&gt;"InfluxDB"&lt;/code&gt; is only 8 bytes long, so StringViewArray completely inlines the string content in the &lt;code&gt;view&lt;/code&gt; struct while StringArray stores the string in the buffer as well.&lt;/li&gt;
&lt;li&gt;The third string &lt;code&gt;"Arrow Rust Impl"&lt;/code&gt; is 15 bytes long and cannot be fully inlined. StringViewArray stores this in the same form as the first string.&lt;/li&gt;
&lt;li&gt;The last string &lt;code&gt;"Apache DataFusion"&lt;/code&gt; has the same content as the first string. It&amp;rsquo;s possible to use StringViewArray to avoid this duplication and reuse the bytes by pointing the view to the previous location.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StringViewArray provides three opportunities for outperforming StringArray:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Less copying via the offset + buffer format&lt;/li&gt;
&lt;li&gt;Faster comparisons using the inlined string prefix&lt;/li&gt;
&lt;li&gt;Reusing repeated string values with the flexible &lt;code&gt;view&lt;/code&gt; layout&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The rest of this blog post discusses how to apply these opportunities in real query scenarios to improve performance, what challenges we encountered along the way, and how we solved them.&lt;/p&gt;
&lt;h2&gt;Faster Parquet Loading&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; is the de facto format for storing large-scale analytical data commonly stored LakeHouse-style, such as &lt;a href="https://iceberg.apache.org"&gt;Apache Iceberg&lt;/a&gt; and &lt;a href="https://delta.io"&gt;Delta Lake&lt;/a&gt;. Efficiently loading data from Parquet is thus critical to query performance in many important real-world workloads.&lt;/p&gt;
&lt;p&gt;Parquet encodes strings (i.e., &lt;a href="https://docs.rs/parquet/latest/parquet/data_type/struct.ByteArray.html"&gt;byte array&lt;/a&gt;) in a slightly different format than required for the original Arrow StringArray. The string length is encoded inline with the actual string data (as shown in Figure 4 left). As mentioned previously, StringArray requires the data buffer to be continuous and compact&amp;mdash;the strings have to follow one after another. This requirement means that reading Parquet string data into an Arrow StringArray requires copying and consolidating the string bytes to a new buffer and tracking offsets in a separate array. Copying these strings is often wasteful. Typical queries filter out most data immediately after loading, so most of the copied data is quickly discarded.&lt;/p&gt;
&lt;p&gt;On the other hand, reading Parquet data as a StringViewArray can re-use the same data buffer as storing the Parquet pages because StringViewArray does not require strings to be contiguous. For example, in Figure 4, the StringViewArray directly references the buffer with the decoded Parquet page. The string &lt;code&gt;"Arrow Rust Impl"&lt;/code&gt; is represented by a &lt;code&gt;view&lt;/code&gt; with offset 37 and length 15 into that buffer.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram showing how StringViewArray can avoid copying by reusing decoded Parquet pages." class="img-responsive" src="/blog/images/string-view-1/figure4-copying.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 4: StringViewArray avoids copying by reusing decoded Parquet pages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mini benchmark&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reusing Parquet buffers is great in theory, but how much does saving a copy actually matter? We can run the following benchmark in arrow-rs to find out:&lt;/p&gt;
&lt;p&gt;Our benchmarking machine shows that loading &lt;em&gt;BinaryViewArray&lt;/em&gt; is almost 2x faster than loading BinaryArray (see next section about why this isn&amp;rsquo;t &lt;em&gt;String&lt;/em&gt; ViewArray).&lt;/p&gt;
&lt;p&gt;You can read more on this arrow-rs issue: &lt;a href="https://github.com/apache/arrow-rs/issues/5904"&gt;https://github.com/apache/arrow-rs/issues/5904&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;From Binary to Strings&lt;/h1&gt;
&lt;p&gt;You may wonder why we reported performance for BinaryViewArray when this post is about StringViewArray. Surprisingly, initially, our implementation to read StringViewArray from Parquet was much &lt;em&gt;slower&lt;/em&gt; than StringArray. Why? TLDR: Although reading StringViewArray copied less data, the initial implementation also spent much more time validating &lt;a href="https://en.wikipedia.org/wiki/UTF-8#:~:text=UTF%2D8%20is%20a%20variable,Unicode%20Standard"&gt;UTF-8&lt;/a&gt; (as shown in Figure 5).&lt;/p&gt;
&lt;p&gt;Strings are stored as byte sequences. When reading data from (potentially untrusted) Parquet files, a Parquet decoder must ensure those byte sequences are valid UTF-8 strings, and most programming languages, including Rust, include highly&lt;a href="https://doc.rust-lang.org/std/str/fn.from_utf8.html"&gt; optimized routines&lt;/a&gt; for doing so.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure showing time to load strings from Parquet and the effect of optimized UTF-8 validation." class="img-responsive" src="/blog/images/string-view-1/figure5-loading-strings.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 5: Time to load strings from Parquet. The UTF-8 validation advantage initially eliminates the advantage of reduced copying for StringViewArray.&lt;/p&gt;
&lt;p&gt;A StringArray can be validated in a single call to the UTF-8 validation function as it has a continuous string buffer. As long as the underlying buffer is UTF-8[^4], all strings in the array must be UTF-8. The Rust parquet reader makes a single function call to validate the entire buffer.&lt;/p&gt;
&lt;p&gt;However, validating an arbitrary StringViewArray requires validating each string with a separate call to the validation function, as the underlying buffer may also contain non-string data (for example, the lengths in Parquet pages).&lt;/p&gt;
&lt;p&gt;UTF-8 validation in Rust is highly optimized and favors longer strings (as shown in Figure 6), likely because it leverages SIMD instructions to perform parallel validation. The benefit of a single function call to validate UTF-8 over a function call for each string more than eliminates the advantage of avoiding the copy for StringViewArray.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure showing UTF-8 validation throughput vs string length." class="img-responsive" src="/blog/images/string-view-1/figure6-utf8-validation.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 6: UTF-8 validation throughput vs string length&amp;mdash;StringArray&amp;rsquo;s contiguous buffer can be validated much faster than StringViewArray&amp;rsquo;s buffer.&lt;/p&gt;
&lt;p&gt;Does this mean we should only use StringArray? No! Thankfully, there&amp;rsquo;s a clever way out. The key observation is that in many real-world datasets,&lt;a href="https://www.vldb.org/pvldb/vol17/p148-zeng.pdf"&gt; 99% of strings are shorter than 128 bytes&lt;/a&gt;, meaning the encoded length values are smaller than 128, &lt;strong&gt;in which case the length itself is also valid UTF-8&lt;/strong&gt; (in fact, it is &lt;a href="https://en.wikipedia.org/wiki/ASCII"&gt;ASCII&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This observation means we can optimize validating UTF-8 strings in Parquet pages by treating the length bytes as part of a single large string as long as the length &lt;em&gt;value&lt;/em&gt; is less than 128. Put another way, prior to this optimization, the length bytes act as string boundaries, which require a UTF-8 validation on each string. After this optimization, only those strings with lengths larger than 128 bytes (less than 1% of the strings in the ClickBench dataset) are string boundaries, significantly increasing the UTF-8 validation chunk size and thus improving performance.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/apache/arrow-rs/pull/6009/files"&gt;actual implementation&lt;/a&gt; is only nine lines of Rust (with 30 lines of comments). You can find more details in the related arrow-rs issue:&lt;a href="https://github.com/apache/arrow-rs/issues/5995"&gt; https://github.com/apache/arrow-rs/issues/5995&lt;/a&gt;. As expected, with this optimization, loading StringViewArray is almost 2x faster than loading StringArray.&lt;/p&gt;
&lt;h1&gt;Be Careful About Implicit Copies&lt;/h1&gt;
&lt;p&gt;After all the work to avoid copying strings when loading from Parquet, performance was still not as good as expected. We tracked the problem to a few implicit data copies that we weren't aware of, as described in&lt;a href="https://github.com/apache/arrow-rs/issues/6033"&gt; this issue&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The copies we eventually identified come from the following innocent-looking line of Rust code, where &lt;code&gt;self.buf&lt;/code&gt; is a &lt;a href="https://en.wikipedia.org/wiki/Reference_counting"&gt;reference counted&lt;/a&gt; pointer that should transform without copying into a buffer for use in StringViewArray.&lt;/p&gt;
&lt;p&gt;However, Rust-type coercion rules favored a blanket implementation that &lt;em&gt;did&lt;/em&gt; copy data. This implementation is shown in the following code block where the &lt;code&gt;impl&amp;lt;T: AsRef&amp;lt;[u8]&amp;gt;&amp;gt;&lt;/code&gt; will accept any type that implements &lt;code&gt;AsRef&amp;lt;[u8]&amp;gt;&lt;/code&gt; and copies the data to create a new buffer. To avoid copying, users need to explicitly call &lt;code&gt;from_vec&lt;/code&gt;, which consumes the &lt;code&gt;Vec&lt;/code&gt; and transforms it into a buffer.&lt;/p&gt;
&lt;p&gt;Diagnosing this implicit copy was time-consuming as it relied on subtle Rust language semantics. We needed to track every step of the data flow to ensure every copy was necessary. To help other users and prevent future mistakes, we also &lt;a href="https://github.com/apache/arrow-rs/pull/6043"&gt;removed&lt;/a&gt; the implicit API from arrow-rs in favor of an explicit API. Using this approach, we found and fixed several &lt;a href="https://github.com/apache/arrow-rs/pull/6039"&gt;other unintentional copies&lt;/a&gt; in the code base&amp;mdash;hopefully, the change will help other &lt;a href="https://github.com/spiraldb/vortex/pull/504"&gt;downstream users&lt;/a&gt; avoid unnecessary copies.&lt;/p&gt;
&lt;h1&gt;Help the Compiler by Giving it More Information&lt;/h1&gt;
&lt;p&gt;The Rust compiler&amp;rsquo;s automatic optimizations mostly work very well for a wide variety of use cases, but sometimes, it needs additional hints to generate the most efficient code. When profiling the performance of &lt;code&gt;view&lt;/code&gt; construction, we found, counterintuitively, that constructing &lt;strong&gt;long&lt;/strong&gt; strings was 10x faster than constructing &lt;strong&gt;short&lt;/strong&gt; strings, which made short strings slower on StringViewArray than on StringArray!&lt;/p&gt;
&lt;p&gt;As described in the first section, StringViewArray treats long and short strings differently. Short strings (&amp;lt;12 bytes) directly inline to the &lt;code&gt;view&lt;/code&gt; struct, while long strings only inline the first 4 bytes. The code to construct a &lt;code&gt;view&lt;/code&gt; looks something like this:&lt;/p&gt;
&lt;p&gt;It appears that both branches of the code should be fast: they both involve copying at most 16 bytes of data and some memory shift/store operations. How could the branch for short strings be 10x slower?&lt;/p&gt;
&lt;p&gt;Looking at the assembly code using &lt;a href="https://godbolt.org/"&gt;Compiler Explorer&lt;/a&gt;, we (with help from &lt;a href="https://github.com/aoli-al"&gt;Ao Li&lt;/a&gt;) found the compiler used CPU &lt;strong&gt;load instructions&lt;/strong&gt; to copy the fixed-sized 4 bytes to the &lt;code&gt;view&lt;/code&gt; for long strings, but it calls a function, &lt;a href="https://doc.rust-lang.org/std/ptr/fn.copy_nonoverlapping.html"&gt;&lt;code&gt;ptr::copy_non_overlapping&lt;/code&gt;&lt;/a&gt;, to copy the inlined bytes to the &lt;code&gt;view&lt;/code&gt; for short strings. The difference is that long strings have a prefix size (4 bytes) known at compile time, so the compiler directly uses efficient CPU instructions. But, since the size of the short string is unknown to the compiler, it has to call the general-purpose function &lt;code&gt;ptr::copy_non_coverlapping&lt;/code&gt;. Making a function call is significant unnecessary overhead compared to a CPU copy instruction.&lt;/p&gt;
&lt;p&gt;However, we know something the compiler doesn&amp;rsquo;t know: the short string size is not arbitrary&amp;mdash;it must be between 0 and 12 bytes, and we can leverage this information to avoid the function call. Our solution generates 13 copies of the function using generics, one for each of the possible prefix lengths. The code looks as follows, and &lt;a href="https://godbolt.org/z/685YPsd5G"&gt;checking the assembly code&lt;/a&gt;, we confirmed there are no calls to &lt;code&gt;ptr::copy_non_overlapping&lt;/code&gt;, and only native CPU instructions are used. For more details, see &lt;a href="https://github.com/apache/arrow-rs/issues/6034"&gt;the ticket&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;End-to-End Query Performance&lt;/h1&gt;
&lt;p&gt;In the previous sections, we went out of our way to make sure loading StringViewArray is faster than StringArray. Before going further, we wanted to verify if obsessing about reducing copies and function calls has actually improved end-to-end performance in real-life queries. To do this, we evaluated a ClickBench query (Q20) in DataFusion that counts how many URLs contain the word &lt;code&gt;"google"&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;This is a relatively simple query; most of the time is spent on loading the &amp;ldquo;URL&amp;rdquo; column to find matching rows. The query plan looks like this:&lt;/p&gt;
&lt;p&gt;We ran the benchmark in the DataFusion repo like this:&lt;/p&gt;
&lt;p&gt;With StringViewArray we saw a 24% end-to-end performance improvement, as shown in Figure 7. With the &lt;code&gt;--string-view&lt;/code&gt; argument, the end-to-end query time is &lt;code&gt;944.3 ms, 869.6 ms, 861.9 ms&lt;/code&gt; (three iterations). Without &lt;code&gt;--string-view&lt;/code&gt;, the end-to-end query time is &lt;code&gt;1186.1 ms, 1126.1 ms, 1138.3 ms&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure showing StringView improves end to end performance by 24 percent." class="img-responsive" src="/blog/images/string-view-1/figure7-end-to-end.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 7: StringView reduces end-to-end query time by 24% on ClickBench Q20.&lt;/p&gt;
&lt;p&gt;We also double-checked with detailed profiling and verified that the time reduction is indeed due to faster Parquet loading.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this first blog post, we have described what it took to improve the
performance of simply reading strings from Parquet files using StringView. While
this resulted in real end-to-end query performance improvements, in our &lt;a href="https://datafusion.apache.org/blog/2024/09/13/using-stringview-to-make-queries-faster-part-2.html"&gt;next
post&lt;/a&gt;, we explore additional optimizations enabled by StringView in DataFusion,
along with some of the pitfalls we encountered while implementing them.&lt;/p&gt;
&lt;h1&gt;Footnotes&lt;/h1&gt;
&lt;p&gt;[^1]: Benchmarked with AMD Ryzen 7600x (12 core, 24 threads, 32 MiB L3), WD Black SN770 NVMe SSD (5150MB/4950MB seq RW bandwidth)&lt;/p&gt;
&lt;p&gt;[^2]: Xiangpeng is a PhD student at the University of Wisconsin-Madison&lt;/p&gt;
&lt;p&gt;[^3]: There is also a corresponding &lt;em&gt;BinaryViewArray&lt;/em&gt; which is similar except that the data is not constrained to be UTF-8 encoded strings.&lt;/p&gt;
&lt;p&gt;[^4]: We also make sure that offsets do not break a UTF-8 code point, which is &lt;a href="https://github.com/apache/arrow-rs/blob/master/parquet/src/arrow/buffer/offset_buffer.rs#L62-L71"&gt;cheaply validated&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Using StringView / German Style Strings to make Queries Faster: Part 2 - String Operations</title><link href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-2" rel="alternate"></link><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><author><name>Xiangpeng Hao, Andrew Lamb</name></author><id>tag:datafusion.apache.org,2024-09-13:/blog/2024/09/13/string-view-german-style-strings-part-2</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog series was first published on the &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-two-influxdb/"&gt;InfluxData blog&lt;/a&gt;. Thanks to InfluxData for sponsoring this work as &lt;a href="https://haoxp.xyz/"&gt;Xiangpeng Hao&lt;/a&gt;'s summer intern project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1"&gt;first post&lt;/a&gt;, we discussed the nuances required to accelerate Parquet loading using StringViewArray by reusing buffers and reducing copies. 
In this second …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog series was first published on the &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-two-influxdb/"&gt;InfluxData blog&lt;/a&gt;. Thanks to InfluxData for sponsoring this work as &lt;a href="https://haoxp.xyz/"&gt;Xiangpeng Hao&lt;/a&gt;'s summer intern project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1"&gt;first post&lt;/a&gt;, we discussed the nuances required to accelerate Parquet loading using StringViewArray by reusing buffers and reducing copies. 
In this second part of the post, we describe the rest of the journey: implementing additional efficient operations for real query processing.&lt;/p&gt;
&lt;h2&gt;Faster String Operations&lt;/h2&gt;
&lt;h1&gt;Faster comparison&lt;/h1&gt;
&lt;p&gt;String comparison is ubiquitous; it is the core of 
&lt;a href="https://docs.rs/arrow/latest/arrow/compute/kernels/cmp/index.html"&gt;&lt;code&gt;cmp&lt;/code&gt;&lt;/a&gt;, 
&lt;a href="https://docs.rs/arrow/latest/arrow/compute/fn.min.html"&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;/&lt;a href="https://docs.rs/arrow/latest/arrow/compute/fn.max.html"&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;, 
and &lt;a href="https://docs.rs/arrow/latest/arrow/compute/kernels/comparison/fn.like.html"&gt;&lt;code&gt;like&lt;/code&gt;&lt;/a&gt;/&lt;a href="https://docs.rs/arrow/latest/arrow/compute/kernels/comparison/fn.ilike.html"&gt;&lt;code&gt;ilike&lt;/code&gt;&lt;/a&gt; kernels. StringViewArray is designed to accelerate such comparisons using the inlined prefix&amp;mdash;the key observation is that, in many cases, only the first few bytes of the string determine the string comparison results.&lt;/p&gt;
&lt;p&gt;For example, to compare the strings &lt;code&gt;InfluxDB&lt;/code&gt; with &lt;code&gt;Apache DataFusion&lt;/code&gt;, we only need to look at the first byte to determine the string ordering or equality. In this case, since &lt;code&gt;A&lt;/code&gt; is earlier in the alphabet than &lt;code&gt;I,&lt;/code&gt; &lt;code&gt;Apache DataFusion&lt;/code&gt; sorts first, and we know the strings are not equal. Despite only needing the first byte, comparing these strings when stored as a StringArray requires two memory accesses: 1) load the string offset and 2) use the offset to locate the string bytes. For low-level operations such as &lt;code&gt;cmp&lt;/code&gt; that are invoked millions of times in the very hot paths of queries, avoiding this extra memory access can make a measurable difference in query performance.&lt;/p&gt;
&lt;p&gt;For StringViewArray, typically, only one memory access is needed to load the view struct. Only if the result can not be determined from the prefix is the second memory access required. For the example above, there is no need for the second access. This technique is very effective in practice: the second access is never necessary for the more than &lt;a href="https://www.vldb.org/pvldb/vol17/p148-zeng.pdf"&gt;60% of real-world strings which are shorter than 12 bytes&lt;/a&gt;, as they are stored completely in the prefix.&lt;/p&gt;
&lt;p&gt;However, functions that operate on strings must be specialized to take advantage of the inlined prefix. In addition to low-level comparison kernels, we implemented &lt;a href="https://github.com/apache/arrow-rs/issues/5374"&gt;a wide range&lt;/a&gt; of other StringViewArray operations that cover the functions and operations seen in ClickBench queries. Supporting StringViewArray in all string operations takes quite a bit of effort, and thankfully the Arrow and DataFusion communities are already hard at work doing so (see &lt;a href="https://github.com/apache/datafusion/issues/11752"&gt;https://github.com/apache/datafusion/issues/11752&lt;/a&gt; if you want to help out).&lt;/p&gt;
&lt;h1&gt;Faster &lt;code&gt;take&lt;/code&gt;and&lt;code&gt;filter&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;After a filter operation such as &lt;code&gt;WHERE url &amp;lt;&amp;gt; ''&lt;/code&gt; to avoid processing empty urls, DataFusion will often &lt;em&gt;coalesce&lt;/em&gt; results to form a new array with only the passing elements. 
This coalescing ensures the batches are sufficiently sized to benefit from &lt;a href="https://www.vldb.org/pvldb/vol11/p2209-kersten.pdf"&gt;vectorized processing&lt;/a&gt; in subsequent steps.&lt;/p&gt;
&lt;p&gt;The coalescing operation is implemented using the &lt;a href="https://docs.rs/arrow/latest/arrow/compute/fn.take.html"&gt;take&lt;/a&gt; and &lt;a href="https://arrow.apache.org/rust/arrow/compute/kernels/filter/fn.filter.html"&gt;filter&lt;/a&gt; kernels in arrow-rs. For StringArray, these kernels require copying the string contents to a new buffer without &amp;ldquo;holes&amp;rdquo; in between. This copy can be expensive especially when the new array is large.&lt;/p&gt;
&lt;p&gt;However, &lt;code&gt;take&lt;/code&gt; and &lt;code&gt;filter&lt;/code&gt; for StringViewArray can avoid the copy by reusing buffers from the old array. The kernels only need to create a new list of  &lt;code&gt;view&lt;/code&gt;s that point at the same strings within the old buffers. 
Figure 1 illustrates the difference between the output of both string representations. StringArray creates two new strings at offsets 0-17 and 17-32, while StringViewArray simply points to the original buffer at offsets 0 and 25.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram showing Zero-copy &amp;lt;code&amp;gt;take&amp;lt;/code&amp;gt;/&amp;lt;code&amp;gt;filter&amp;lt;/code&amp;gt; for StringViewArray" class="img-responsive" src="/blog/images/string-view-2/figure1-zero-copy-take.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: Zero-copy &lt;code&gt;take&lt;/code&gt;/&lt;code&gt;filter&lt;/code&gt; for StringViewArray&lt;/p&gt;
&lt;h1&gt;When to GC?&lt;/h1&gt;
&lt;p&gt;Zero-copy &lt;code&gt;take/filter&lt;/code&gt; is great for generating large arrays quickly, but it is suboptimal for highly selective filters, where most of the strings are filtered out. When the cardinality drops, StringViewArray buffers become sparse&amp;mdash;only a small subset of the bytes in the buffer&amp;rsquo;s memory are referred to by any &lt;code&gt;view&lt;/code&gt;. This leads to excessive memory usage, especially in a &lt;a href="https://github.com/apache/datafusion/issues/11628"&gt;filter-then-coalesce scenario&lt;/a&gt;. For example, a StringViewArray with 10M strings may only refer to 1M strings after some filter operations; however, due to zero-copy take/filter, the (reused) 10M buffers can not be released/reused.&lt;/p&gt;
&lt;p&gt;To release unused memory, we implemented a &lt;a href="https://docs.rs/arrow/latest/arrow/array/struct.GenericByteViewArray.html#method.gc"&gt;garbage collection (GC)&lt;/a&gt; routine to consolidate the data into a new buffer to release the old sparse buffer(s). As the GC operation copies strings, similarly to StringArray, we must be careful about when to call it. If we call GC too early, we cause unnecessary copying, losing much of the benefit of StringViewArray. If we call GC too late, we hold large buffers for too long, increasing memory use and decreasing cache efficiency. The &lt;a href="https://pola.rs/posts/polars-string-type/"&gt;Polars blog&lt;/a&gt; on StringView also refers to the challenge presented by garbage collection timing.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;arrow-rs&lt;/code&gt; implements the GC process, but it is up to users to decide when to call it. We leverage the semantics of the query engine and observed that the &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/coalesce_batches/struct.CoalesceBatchesExec.html"&gt;&lt;code&gt;CoalseceBatchesExec&lt;/code&gt;&lt;/a&gt; operator, which merge smaller batches to a larger batch, is often used after the record cardinality is expected to shrink, which aligns perfectly with the scenario of GC in StringViewArray. 
We, therefore,&lt;a href="https://github.com/apache/datafusion/pull/11587"&gt; implemented the GC procedure&lt;/a&gt; inside &lt;code&gt;CoalseceBatchesExec&lt;/code&gt;[^5] with a heuristic that estimates when the buffers are too sparse.&lt;/p&gt;
&lt;h2&gt;The art of function inlining: not too much, not too little&lt;/h2&gt;
&lt;p&gt;Like string inlining, &lt;em&gt;function&lt;/em&gt; inlining is the process of embedding a short function into the caller to avoid the overhead of function calls (caller/callee save). 
Usually, the Rust compiler does a good job of deciding when to inline. However, it is possible to override its default using the &lt;a href="https://doc.rust-lang.org/reference/attributes/codegen.html#the-inline-attribute"&gt;&lt;code&gt;#[inline(always)]&lt;/code&gt; directive&lt;/a&gt;. 
In performance-critical code, inlined code allows us to organize large functions into smaller ones without paying the runtime cost of function invocation.&lt;/p&gt;
&lt;p&gt;However, function inlining is &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; always better, as it leads to larger function bodies that are harder for LLVM to optimize (for example, suboptimal &lt;a href="https://en.wikipedia.org/wiki/Register_allocation"&gt;register spilling&lt;/a&gt;) and risk overflowing the CPU&amp;rsquo;s instruction cache. We observed several performance regressions where function inlining caused &lt;em&gt;slower&lt;/em&gt; performance when implementing the StringViewArray comparison kernels. Careful inspection and tuning of the code was required to aid the compiler in generating efficient code. More details can be found in this PR: &lt;a href="https://github.com/apache/arrow-rs/pull/5900"&gt;https://github.com/apache/arrow-rs/pull/5900&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Buffer size tuning&lt;/h2&gt;
&lt;p&gt;StringViewArray permits multiple buffers, which enables a flexible buffer layout and potentially reduces the need to copy data. However, a large number of buffers slows down the performance of other operations. 
For example, &lt;a href="https://docs.rs/arrow/latest/arrow/array/trait.Array.html#tymethod.get_array_memory_size"&gt;&lt;code&gt;get_array_memory_size&lt;/code&gt;&lt;/a&gt; needs to sum the memory size of each buffer, which takes a long time with thousands of small buffers. 
In certain cases, we found that multiple calls to &lt;a href="https://docs.rs/arrow/latest/arrow/compute/fn.concat_batches.html"&gt;&lt;code&gt;concat_batches&lt;/code&gt;&lt;/a&gt; lead to arrays with millions of buffers, which was prohibitively expensive.&lt;/p&gt;
&lt;p&gt;For example, consider a StringViewArray with the previous default buffer size of 8 KB. With this configuration, holding 4GB of string data requires almost half a million buffers! Larger buffer sizes are needed for larger arrays, but we cannot arbitrarily increase the default buffer size, as small arrays would consume too much memory (most arrays require at least one buffer). Buffer sizing is especially problematic in query processing, as we often need to construct small batches of string arrays, and the sizes are unknown at planning time.&lt;/p&gt;
&lt;p&gt;To balance the buffer size trade-off, we again leverage the query processing (DataFusion) semantics to decide when to use larger buffers. While coalescing batches, we combine multiple small string arrays and set a smaller buffer size to keep the total memory consumption low. In string aggregation, we aggregate over an entire Datafusion partition, which can generate a large number of strings, so we set a larger buffer size (2MB).&lt;/p&gt;
&lt;p&gt;To assist situations where the semantics are unknown, we also &lt;a href="https://github.com/apache/arrow-rs/pull/6136"&gt;implemented&lt;/a&gt; a classic dynamic exponential buffer size growth strategy, which starts with a small buffer size (8KB) and doubles the size of each new buffer up to 2MB. We implemented this strategy in arrow-rs and enabled it by default so that other users of StringViewArray can also benefit from this optimization. See this issue for more details: &lt;a href="https://github.com/apache/arrow-rs/issues/6094"&gt;https://github.com/apache/arrow-rs/issues/6094&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;End-to-end query performance&lt;/h2&gt;
&lt;p&gt;We have made significant progress in optimizing StringViewArray filtering operations. Now, let&amp;rsquo;s test it in the real world to see how it works!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider ClickBench query 22, which selects multiple string fields (&lt;code&gt;URL&lt;/code&gt;, &lt;code&gt;Title&lt;/code&gt;, and &lt;code&gt;SearchPhase&lt;/code&gt;) and applies several filters.&lt;/p&gt;
&lt;p&gt;We ran the benchmark using the following command in the DataFusion repo. Again, the &lt;code&gt;--string-view&lt;/code&gt; option means we use StringViewArray instead of StringArray.&lt;/p&gt;
&lt;p&gt;To eliminate the impact of the faster Parquet reading using StringViewArray (see the first part of this blog), Figure 2 plots only the time spent in &lt;code&gt;FilterExec&lt;/code&gt;. Without StringViewArray, the filter takes 7.17s; with StringViewArray, the filter only takes 4.86s, a 32% reduction in time. Moreover, we see a 17% improvement in end-to-end query performance.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure showing StringViewArray reduces the filter time by 32% on ClickBench query 22." class="img-responsive" src="/blog/images/string-view-2/figure2-filter-time.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: StringViewArray reduces the filter time by 32% on ClickBench query 22.&lt;/p&gt;
&lt;h1&gt;Faster String Aggregation&lt;/h1&gt;
&lt;p&gt;So far, we have discussed how to exploit two StringViewArray features: reduced copy and faster filtering. This section focuses on reusing string bytes to repeat string values.&lt;/p&gt;
&lt;p&gt;As described in part one of this blog, if two strings have identical values, StringViewArray can use two different &lt;code&gt;view&lt;/code&gt;s pointing at the same buffer range, thus avoiding repeating the string bytes in the buffer. This makes StringViewArray similar to an Arrow &lt;a href="https://docs.rs/arrow/latest/arrow/array/struct.DictionaryArray.html"&gt;DictionaryArray&lt;/a&gt; that stores Strings&amp;mdash;both array types work well for strings with only a few distinct values.&lt;/p&gt;
&lt;p&gt;Deduplicating string values can significantly reduce memory consumption in StringViewArray. However, this process is expensive and involves hashing every string and maintaining a hash table, and so it cannot be done by default when creating a StringViewArray. We introduced an&lt;a href="https://docs.rs/arrow/latest/arrow/array/builder/struct.GenericByteViewBuilder.html#method.with_deduplicate_strings"&gt; opt-in string deduplication mode&lt;/a&gt; in arrow-rs for advanced users who know their data has a small number of distinct values, and where the benefits of reduced memory consumption outweigh the additional overhead of array construction.&lt;/p&gt;
&lt;p&gt;Once again, we leverage DataFusion query semantics to identify StringViewArray with duplicate values, such as aggregation queries with multiple group keys. For example, some &lt;a href="https://github.com/apache/datafusion/blob/main/benchmarks/queries/clickbench/queries.sql"&gt;ClickBench queries&lt;/a&gt; group by two columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;UserID&lt;/code&gt; (an integer with close to 1 M distinct values)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MobilePhoneModel&lt;/code&gt; (a string with less than a hundred distinct values)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this case, the output row count is&lt;code&gt;count(distinct UserID) * count(distinct MobilePhoneModel)&lt;/code&gt;,  which is 100M. Each string value of  &lt;code&gt;MobilePhoneModel&lt;/code&gt; is repeated 1M times. With StringViewArray, we can save space by pointing the repeating values to the same underlying buffer.&lt;/p&gt;
&lt;p&gt;Faster string aggregation with StringView is part of a larger project to &lt;a href="https://github.com/apache/datafusion/issues/7000"&gt;improve DataFusion aggregation performance&lt;/a&gt;. We have a &lt;a href="https://github.com/apache/datafusion/pull/11794"&gt;proof of concept implementation&lt;/a&gt; with StringView that can improve the multi-column string aggregation by 20%. We would love your help to get it production ready!&lt;/p&gt;
&lt;h1&gt;StringView Pitfalls&lt;/h1&gt;
&lt;p&gt;Most existing blog posts (including this one) focus on the benefits of using StringViewArray over other string representations such as StringArray. As we have discussed, even though it requires a significant engineering investment to realize, StringViewArray is a major improvement over StringArray in many cases.&lt;/p&gt;
&lt;p&gt;However, there are several cases where StringViewArray is slower than StringArray. For completeness, we have listed those instances here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tiny strings (when strings are shorter than 8 bytes)&lt;/strong&gt;: every element of the StringViewArray consumes at least 16 bytes of memory&amp;mdash;the size of the &lt;code&gt;view&lt;/code&gt; struct. For an array of tiny strings, StringViewArray consumes more memory than StringArray and thus can cause slower performance due to additional memory pressure on the CPU cache.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Many repeated short strings&lt;/strong&gt;: Similar to the first point, StringViewArray can be slower and require more memory than a DictionaryArray because 1) it can only reuse the bytes in the buffer when the strings are longer than 12 bytes and 2) 32-bit offsets are always used, even when a smaller size (8 bit or 16 bit) could represent all the distinct values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Filtering:&lt;/strong&gt; As we mentioned above, StringViewArrays often consume more memory than the corresponding StringArray, and memory bloat quickly dominates the performance without GC. However, invoking GC also reduces the benefits of less copying so must be carefully tuned.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Conclusion and Takeaways&lt;/h1&gt;
&lt;p&gt;In these two blog posts, we discussed what it takes to implement StringViewArray in arrow-rs and then integrate it into DataFusion. Our evaluations on ClickBench queries show that StringView can improve the performance of string-intensive workloads by up to 2x.&lt;/p&gt;
&lt;p&gt;Given that DataFusion already &lt;a href="https://benchmark.clickhouse.com/#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"&gt;performs very well on ClickBench&lt;/a&gt;, the level of end-to-end performance improvement using StringViewArray shows the power of this technique and, of course, is a win for DataFusion and the systems that build upon it.&lt;/p&gt;
&lt;p&gt;StringView is a big project that has received tremendous community support. Specifically, we would like to thank &lt;a href="https://github.com/tustvold"&gt;@tustvold&lt;/a&gt;, &lt;a href="https://github.com/ariesdevil"&gt;@ariesdevil&lt;/a&gt;, &lt;a href="https://github.com/RinChanNOWWW"&gt;@RinChanNOWWW&lt;/a&gt;, &lt;a href="https://github.com/ClSlaid"&gt;@ClSlaid&lt;/a&gt;, &lt;a href="https://github.com/2010YOUY01"&gt;@2010YOUY01&lt;/a&gt;, &lt;a href="https://github.com/chloro-pn"&gt;@chloro-pn&lt;/a&gt;, &lt;a href="https://github.com/a10y"&gt;@a10y&lt;/a&gt;, &lt;a href="https://github.com/Kev1n8"&gt;@Kev1n8&lt;/a&gt;, &lt;a href="https://github.com/Weijun-H"&gt;@Weijun-H&lt;/a&gt;, &lt;a href="https://github.com/PsiACE"&gt;@PsiACE&lt;/a&gt;, &lt;a href="https://github.com/tshauck"&gt;@tshauck&lt;/a&gt;, and &lt;a href="https://github.com/xinlifoobar"&gt;@xinlifoobar&lt;/a&gt; for their valuable contributions!&lt;/p&gt;
&lt;p&gt;As the introduction states, &amp;ldquo;German Style Strings&amp;rdquo; is a relatively straightforward research idea that avoid some string copies and accelerates comparisons. However, applying this (great) idea in practice requires a significant investment in careful software engineering. Again, we encourage the research community to continue to help apply research ideas to industrial systems, such as DataFusion, as doing so provides valuable perspectives when evaluating future research questions for the greatest potential impact.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;[^5]: There are additional optimizations possible in this operation that the community is working on, such as  &lt;a href="https://github.com/apache/datafusion/issues/7957"&gt;https://github.com/apache/datafusion/issues/7957&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.2.0 Release</title><link href="https://datafusion.apache.org/blog/2024/08/28/datafusion-comet-0.2.0" rel="alternate"></link><published>2024-08-28T00:00:00+00:00</published><updated>2024-08-28T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-08-28:/blog/2024/08/28/datafusion-comet-0.2.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.2.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.2.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately four weeks of development work and is the result of merging 87 PRs from 14 
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.2.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Docker Images&lt;/h3&gt;
&lt;p&gt;Docker images are now available from the &lt;a href="https://github.com/apache/datafusion-comet/pkgs/container/datafusion-comet/265110454?tag=spark-3.4-scala-2.12-0.2.0"&gt;GitHub Container Registry&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Performance improvements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Native shuffle is now enabled by default&lt;/li&gt;
&lt;li&gt;Improved handling of decimal types&lt;/li&gt;
&lt;li&gt;Reduced some redundant copying of batches in Filter/Scan operations&lt;/li&gt;
&lt;li&gt;Optimized performance of count aggregates&lt;/li&gt;
&lt;li&gt;Optimized performance of  CASE expressions for specific uses:&lt;/li&gt;
&lt;li&gt;CASE WHEN expr THEN column ELSE null END&lt;/li&gt;
&lt;li&gt;CASE WHEN expr THEN literal ELSE literal END&lt;/li&gt;
&lt;li&gt;Optimized performance of IS NOT NULL&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;New Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Window operations now support count and sum aggregates&lt;/li&gt;
&lt;li&gt;CreateArray&lt;/li&gt;
&lt;li&gt;GetStructField&lt;/li&gt;
&lt;li&gt;Support nested types in hash join&lt;/li&gt;
&lt;li&gt;Basic implementation of RLIKE expression&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Current Performance&lt;/h2&gt;
&lt;p&gt;We use benchmarks derived from the industry standard TPC-H and TPC-DS benchmarks for tracking progress with
performance. The following charts shows the time it takes to run the queries against 100 GB of data in
Parquet format using a single executor with eight cores. See the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;
for details of the environment used for these benchmarks.&lt;/p&gt;
&lt;h3&gt;Benchmark derived from TPC-H&lt;/h3&gt;
&lt;p&gt;Comet 0.2.0 provides a 62% speedup compared to Spark. This is slightly better than the Comet 0.1.0 release.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.2.0" class="img-responsive" src="/blog/images/comet-0.2.0/tpch_allqueries.png" width="100%"/&gt;&lt;/p&gt;
&lt;h3&gt;Benchmark derived from TPC-DS&lt;/h3&gt;
&lt;p&gt;Comet 0.2.0 provides a 21% speedup compared to Spark, which is a significant improvement compared to 
Comet 0.1.0, which did not provide any speedup for this benchmark.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-DS benchmark results for Comet 0.2.0" class="img-responsive" src="/blog/images/comet-0.2.0/tpcds_allqueries.png" width="100%"/&gt;&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Python 40.1.0 Released, Significant usability updates</title><link href="https://datafusion.apache.org/blog/2024/08/20/python-datafusion-40.0.0" rel="alternate"></link><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><author><name>timsaucer</name></author><id>tag:datafusion.apache.org,2024-08-20:/blog/2024/08/20/python-datafusion-40.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/40.1.0/"&gt;DataFusion in Python 40.1.0&lt;/a&gt; has been released. In addition to
bringing in all of the new features of the core &lt;a href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/"&gt;DataFusion 40.0.0&lt;/a&gt; package, this release
contains &lt;em&gt;significant&lt;/em&gt; updates to the user interface and documentation. We listened to the python …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/40.1.0/"&gt;DataFusion in Python 40.1.0&lt;/a&gt; has been released. In addition to
bringing in all of the new features of the core &lt;a href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/"&gt;DataFusion 40.0.0&lt;/a&gt; package, this release
contains &lt;em&gt;significant&lt;/em&gt; updates to the user interface and documentation. We listened to the python
user community to create a more &lt;em&gt;pythonic&lt;/em&gt; experience. If you have not used the python interface to
DataFusion before, this is an excellent time to give it a try!&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Until now, the python bindings for DataFusion have primarily been a thin layer to expose the
underlying Rust functionality. This has been worked well for early adopters to use DataFusion
within their Python projects, but some users have found it difficult to work with. As compared to
other DataFrame libraries, these issues were raised:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Most of the functions had little or no documentation. Users often had to refer to the Rust
documentation or code to learn how to use DataFusion. This alienated some python users.&lt;/li&gt;
&lt;li&gt;Users could not take advantage of modern IDE features such as type hinting. These are valuable
tools for rapid testing and development.&lt;/li&gt;
&lt;li&gt;Some of the interfaces felt &amp;ldquo;clunky&amp;rdquo; to users since some Python concepts do not always map well
to their Rust counterparts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This release aims to bring a better user experience to the DataFusion Python community.&lt;/p&gt;
&lt;h2&gt;What's Changed&lt;/h2&gt;
&lt;p&gt;The most significant difference is that we have added wrapper functions and classes for most of the
user facing interface. These wrappers, written in Python, contain both documentation and type
annotations.&lt;/p&gt;
&lt;p&gt;This documenation is now available on the &lt;a href="https://datafusion.apache.org/python/autoapi/datafusion/index.html"&gt;DataFusion in Python API&lt;/a&gt; website. There you can browse
the available functions and classes to see the breadth of available functionality.&lt;/p&gt;
&lt;p&gt;Modern IDEs use language servers such as
&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance"&gt;Pylance&lt;/a&gt; or
&lt;a href="https://jedi.readthedocs.io/en/latest/"&gt;Jedi&lt;/a&gt; to perform analysis of python code, provide useful
hints, and identify usage errors. These are major tools in the python user community. With this
release, users can fully use these tools in their workflow.&lt;/p&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Enhanced tooltips in an IDE." class="img-responsive" src="/blog/images/python-datafusion-40.0.0/vscode_hover_tooltip.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 1&lt;/b&gt;: With the enhanced python wrappers, users can see helpful tool tips with
   type annotations directly in modern IDEs.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;By having the type annotations, these IDEs can also identify quickly when a user has incorrectly
used a function's arguments as shown in Figure 2.&lt;/p&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 2: Error checking in static analysis" class="img-responsive" src="/blog/images/python-datafusion-40.0.0/pylance_error_checking.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 2&lt;/b&gt;: Modern Python language servers can perform static analysis and quickly find
   errors in the arguments to functions.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In addition to these wrapper libraries, we have enhancements to some of the functions to feel more
easy to use.&lt;/p&gt;
&lt;h3&gt;Improved DataFrame filter arguments&lt;/h3&gt;
&lt;p&gt;You can now apply multiple &lt;code&gt;filter&lt;/code&gt; statements in a single step. When using &lt;code&gt;DataFrame.filter&lt;/code&gt; you
can pass in multiple arguments, separated by a comma. These will act as a logical &lt;code&gt;AND&lt;/code&gt; of all of
the filter arguments. The following two statements are equivalent:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.filter(col("size") &amp;lt; col("max_size")).filter(col("color") == lit("green"))
df.filter(col("size") &amp;lt; col("max_size"), col("color") == lit("green"))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Comparison against literal values&lt;/h3&gt;
&lt;p&gt;It is very common to write DataFrame operations that compare an expression to some fixed value.
For example, filtering a DataFrame might have an operation such as &lt;code&gt;df.filter(col("size") &amp;lt; lit(16))&lt;/code&gt;.
To make these common operations more ergonomic, you can now simply use &lt;code&gt;df.filter(col("size") &amp;lt; 16)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the right hand side of the comparison operator, you can now use any Python value that can be
coerced into a &lt;code&gt;Literal&lt;/code&gt;. This gives an easy to ready expression. For example, consider these few
lines from one of the
&lt;a href="https://github.com/apache/datafusion-python/tree/main/examples/tpch"&gt;TPC-H examples&lt;/a&gt; provided in
the DataFusion Python repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = (
    df_lineitem.filter(col("l_shipdate") &amp;gt;= lit(date))
    .filter(col("l_discount") &amp;gt;= lit(DISCOUNT) - lit(DELTA))
    .filter(col("l_discount") &amp;lt;= lit(DISCOUNT) + lit(DELTA))
    .filter(col("l_quantity") &amp;lt; lit(QUANTITY))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code mirrors closely how these filters would need to be applied in rust. With this new
release, the user can simplify these lines. Also shown in the example below is that &lt;code&gt;filter()&lt;/code&gt;
now accepts a variable number of arguments and filters on all such arguments (boolean AND).&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = df_lineitem.filter(
    col("l_shipdate") &amp;gt;= date,
    col("l_discount") &amp;gt;= DISCOUNT - DELTA,
    col("l_discount") &amp;lt;= DISCOUNT + DELTA,
    col("l_quantity") &amp;lt; QUANTITY,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Select columns by name&lt;/h3&gt;
&lt;p&gt;It is very common for users to perform &lt;code&gt;DataFrame&lt;/code&gt; selection where they simply want a column. For
this we have had the function &lt;code&gt;select_columns("a", "b")&lt;/code&gt; or the user could perform
&lt;code&gt;select(col("a"), col("b"))&lt;/code&gt;. In the new release, we accept either full expressions in &lt;code&gt;select()&lt;/code&gt;
or strings of the column names. You can mix these as well.&lt;/p&gt;
&lt;p&gt;Where before you may have to do an operation like&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df_subset = df.select(col("a"), col("b"), f.abs(col("c")))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now simplify this to&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df_subset = df.select("a", "b", f.abs(col("c")))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Creating named structs&lt;/h3&gt;
&lt;p&gt;Creating a &lt;code&gt;struct&lt;/code&gt; with named fields was previously difficult to use and allowed for potential
user errors when specifying the name of each field. Now we have a cleaner interface where the
user passes a list of tuples containing the name of the field and the expression to create.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.select(f.named_struct([
  ("a", col("a")),
  ("b", col("b"))
]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;While most of the user facing classes and functions have been exposed, there are a few that require
exposure. Namely the classes in &lt;code&gt;datafusion.object_store&lt;/code&gt; and the logical plans used by
&lt;code&gt;datafusion.substrait&lt;/code&gt;. The team is working on
&lt;a href="https://github.com/apache/datafusion-python/issues/767"&gt;these issues&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Additionally, in the next release of DataFusion there have been improvements made to the user-defined
aggregate and window functions to make them easier to use. We plan on
&lt;a href="https://github.com/apache/datafusion-python/issues/780"&gt;bringing these enhancements&lt;/a&gt; to this project.&lt;/p&gt;
&lt;h2&gt;Thank You&lt;/h2&gt;
&lt;p&gt;We would like to thank the following members for their very helpful discussions regarding these
updates: &lt;a href="https://github.com/andygrove"&gt;@andygrove&lt;/a&gt;, &lt;a href="https://github.com/max-muoto"&gt;@max-muoto&lt;/a&gt;, &lt;a href="https://github.com/slyons"&gt;@slyons&lt;/a&gt;, &lt;a href="https://github.com/Throne3d"&gt;@Throne3d&lt;/a&gt;, &lt;a href="https://github.com/Michael-J-Ward"&gt;@Michael-J-Ward&lt;/a&gt;, &lt;a href="https://github.com/datapythonista"&gt;@datapythonista&lt;/a&gt;,
&lt;a href="https://github.com/austin362667"&gt;@austin362667&lt;/a&gt;, &lt;a href="https://github.com/kylebarron"&gt;@kylebarron&lt;/a&gt;, &lt;a href="https://github.com/simicd"&gt;@simicd&lt;/a&gt;. The &lt;a href="https://github.com/apache/datafusion-python/pull/750"&gt;primary PR (#750)&lt;/a&gt; that includes these updates
had an extensive conversation, leading to a significantly improved end product. Again, thank you
to all who provided input!&lt;/p&gt;
&lt;p&gt;We would like to give an special thank you to &lt;a href="https://github.com/3ok"&gt;@3ok&lt;/a&gt; who created the initial version of the wrapper
definitions. The work they did was time consuming and required exceptional attention to detail. It
provided enormous value to starting this project. Thank you!&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;The DataFusion Python team is an active and engaging community and we would love
to have you join us and help the project.&lt;/p&gt;
&lt;p&gt;Here are some ways to get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learn more by visiting the &lt;a href="https://datafusion.apache.org/python/index.html"&gt;DataFusion Python project&lt;/a&gt;
page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try out the project and provide feedback, file issues, and contribute code.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion 40.0.0 Released</title><link href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0" rel="alternate"></link><published>2024-07-24T00:00:00+00:00</published><updated>2024-07-24T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-07-24:/blog/2024/07/24/datafusion-40.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/9602 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are proud to announce &lt;a href="https://crates.io/crates/datafusion/40.0.0"&gt;DataFusion 40.0.0&lt;/a&gt;. This blog highlights some of the
many major improvements since we released &lt;a href="https://datafusion.apache.org/blog/2024/01/19/datafusion-34.0.0/"&gt;DataFusion 34.0.0&lt;/a&gt; and a preview of
what the community is thinking about in the next 6 months. We are hoping to make
more regular blog posts …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/9602 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are proud to announce &lt;a href="https://crates.io/crates/datafusion/40.0.0"&gt;DataFusion 40.0.0&lt;/a&gt;. This blog highlights some of the
many major improvements since we released &lt;a href="https://datafusion.apache.org/blog/2024/01/19/datafusion-34.0.0/"&gt;DataFusion 34.0.0&lt;/a&gt; and a preview of
what the community is thinking about in the next 6 months. We are hoping to make
more regular blog posts -- if you are interested in helping write them, please
reach out!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; is an extensible query engine, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that
uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion is used by developers to
create new, fast data centric systems such as databases, dataframe libraries,
machine learning and streaming applications. While &lt;a href="https://datafusion.apache.org/user-guide/introduction.html#project-goals"&gt;DataFusion&amp;rsquo;s primary design
goal&lt;/a&gt; is to accelerate the creation of other data centric systems, it has a
reasonable experience directly out of the box as a &lt;a href="https://datafusion.apache.org/python/"&gt;dataframe library&lt;/a&gt; and
&lt;a href="https://datafusion.apache.org/user-guide/cli/"&gt;command line SQL tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion's core thesis is that as a community, together we can build much more
advanced technology than any of us as individuals or companies could do alone. 
Without DataFusion, highly performant vectorized query engines would remain
the domain of a few large companies and world-class research institutions. 
With DataFusion, we can all build on top of a shared foundation, and focus on
what makes our projects unique.&lt;/p&gt;
&lt;h2&gt;Community Growth  📈&lt;/h2&gt;
&lt;p&gt;In the last 6 months, between &lt;code&gt;34.0.0&lt;/code&gt; and &lt;code&gt;40.0.0&lt;/code&gt;, our community continues to
grow in new and exciting ways.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DataFusion became a top level Apache Software Foundation project (read the
   &lt;a href="https://news.apache.org/foundation/entry/apache-software-foundation-announces-new-top-level-project-apache-datafusion"&gt;press release&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/blog/2024/05/07/datafusion-tlp/"&gt;blog post&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;We added several PMC members and new
   committers: &lt;a href="https://github.com/comphead"&gt;@comphead&lt;/a&gt;, &lt;a href="https://github.com/mustafasrepo"&gt;@mustafasrepo&lt;/a&gt;, &lt;a href="https://github.com/ozankabak"&gt;@ozankabak&lt;/a&gt;, and &lt;a href="https://github.com/waynexia"&gt;@waynexia&lt;/a&gt; joined the PMC,
   &lt;a href="https://github.com/jonahgao"&gt;@jonahgao&lt;/a&gt; and &lt;a href="https://github.com/lewiszlw"&gt;@lewiszlw&lt;/a&gt; joined as committers. See the &lt;a href="https://lists.apache.org/list.html?dev@datafusion.apache.org"&gt;mailing list&lt;/a&gt; for
   more details.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datafusion.apache.org/comet/"&gt;DataFusion Comet&lt;/a&gt; was &lt;a href="https://arrow.apache.org/blog/2024/03/06/comet-donation/"&gt;donated&lt;/a&gt; and is nearing its first release.&lt;/li&gt;
&lt;li&gt;In the &lt;a href="https://github.com/apache/arrow-datafusion"&gt;core DataFusion repo&lt;/a&gt; alone we reviewed and accepted almost 1500 PRs from 182 different
   committers, created over 1000 issues and closed 781 of them 🚀. This is up
   almost 50% from our last post (1000 PRs from 124 committers with 650 issues
   created in our last post) 🤯. All changes are listed in the detailed
   &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;DataFusion focused meetups happened or are happening in multiple cities 
   around the world: &lt;a href="https://github.com/apache/datafusion/discussions/8522"&gt;Austin&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/10800"&gt;San Francisco&lt;/a&gt;, &lt;a href="https://www.huodongxing.com/event/5761971909400?td=1965290734055"&gt;Hangzhou&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/11213"&gt;New York&lt;/a&gt;, and
   &lt;a href="https://github.com/apache/datafusion/discussions/11431"&gt;Belgrade&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Many new projects started in the &lt;a href="https://github.com/datafusion-contrib"&gt;datafusion-contrib&lt;/a&gt; organization, including
   &lt;a href="https://github.com/datafusion-contrib/datafusion-table-providers"&gt;Table Providers&lt;/a&gt;, &lt;a href="https://github.com/datafusion-contrib/datafusion-sqlancer"&gt;SQLancer&lt;/a&gt;, &lt;a href="https://github.com/datafusion-contrib/datafusion-functions-variant"&gt;Open Variant&lt;/a&gt;, &lt;a href="https://github.com/datafusion-contrib/datafusion-functions-json"&gt;JSON&lt;/a&gt;, and &lt;a href="https://github.com/datafusion-contrib/datafusion-orc"&gt;ORC&lt;/a&gt;.  &lt;/li&gt;
&lt;/ol&gt;
&lt;!--
$ git log --pretty=oneline 34.0.0..40.0.0 . | wc -l
     1453 (up from 1009)

$ git shortlog -sn 34.0.0..40.0.0 . | wc -l
      182 (up from 124)

https://crates.io/crates/datafusion/34.0.0
DataFusion 34 released Dec 17, 2023

https://crates.io/crates/datafusion/40.0.0
DataFusion 34 released July 12, 2024

Issues created in this time: 321 open, 781 closed (up from 214 open, 437 closed)
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+created%3A2023-12-17..2024-07-12

Issues closed: 911 (up from 517)
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+closed%3A2023-12-17..2024-07-12

PRs merged in this time 1490 (up from 908)
https://github.com/apache/arrow-datafusion/pulls?q=is%3Apr+merged%3A2023-12-17..2024-07-12

--&gt;
&lt;p&gt;In addition, DataFusion has been appearing publicly more and more, both online and offline. Here are some highlights:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://dl.acm.org/doi/10.1145/3626246.3653368"&gt;Apache Arrow DataFusion: A Fast, Embeddable, Modular Analytic Query Engine&lt;/a&gt;, was presented in &lt;a href="https://2024.sigmod.org/"&gt;SIGMOD '24&lt;/a&gt;, one of the major database conferences&lt;/li&gt;
&lt;li&gt;As part of the trend to define "the POSIX of databases" in &lt;a href="https://db.cs.cmu.edu/papers/2024/whatgoesaround-sigmodrec2024.pdf"&gt;"What Goes Around Comes Around... And Around..."&lt;/a&gt; from Andy Pavlo and Mike Stonebraker&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cpard.xyz/posts/datafusion/"&gt;"Why you should keep an eye on Apache DataFusion and its community"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tisonkun.org/2024/07/15/datafusion-meetup-san-francisco/"&gt;Apache DataFusion offline meetup in the Bay Area&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Improved Performance 🚀&lt;/h2&gt;
&lt;p&gt;Performance is a key feature of DataFusion, and the community continues to work
to keep DataFusion state of the art in this area. One major area DataFusion
improved is the time it takes to convert a SQL query into a plan that can be
executed. Planning is now almost 2x faster for TPC-DS and TPC-H queries, and
over 10x faster for some queries with many columns.&lt;/p&gt;
&lt;p&gt;Here is a chart showing the improvement due to the concerted effort of many
contributors including &lt;a href="https://github.com/jackwener"&gt;@jackwener&lt;/a&gt;, &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;, &lt;a href="https://github.com/Lordworms"&gt;@Lordworms&lt;/a&gt;, &lt;a href="https://github.com/dmitrybugakov"&gt;@dmitrybugakov&lt;/a&gt;,
&lt;a href="https://github.com/appletreeisyellow"&gt;@appletreeisyellow&lt;/a&gt;, &lt;a href="https://github.com/ClSlaid"&gt;@ClSlaid&lt;/a&gt;, &lt;a href="https://github.com/rohitrastogi"&gt;@rohitrastogi&lt;/a&gt;, &lt;a href="https://github.com/emgeee"&gt;@emgeee&lt;/a&gt;, &lt;a href="https://github.com/kevinmingtarja"&gt;@kevinmingtarja&lt;/a&gt;,
and &lt;a href="https://github.com/peter-toth"&gt;@peter-toth&lt;/a&gt; over several months (see &lt;a href="https://github.com/apache/arrow-datafusion/issues/8045"&gt;ticket&lt;/a&gt; for more details)&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/datafusion-40.0.0/improved-planning-time.png" width="700"/&gt;&lt;/p&gt;
&lt;p&gt;DataFusion is now up to 40% faster for queries that &lt;code&gt;GROUP BY&lt;/code&gt; a single string
or binary column due to a &lt;a href="https://github.com/apache/datafusion/pull/8827"&gt;specialization for single
Uft8/LargeUtf8/Binary/LargeBinary&lt;/a&gt;. We are working on improving performance when
there are [multiple variable length columns in the &lt;code&gt;GROUP BY&lt;/code&gt; clause].&lt;/p&gt;
&lt;p&gt;We are also in the final phases of &lt;a href="https://github.com/apache/datafusion/issues/10918"&gt;integrating&lt;/a&gt; the new &lt;a href="https://docs.rs/arrow/latest/arrow/array/struct.GenericByteViewArray.html"&gt;Arrow StringView&lt;/a&gt;
which significantly improves performance for workloads that scan, filter and
group by variable length string and binary data. We expect the improvement to be
especially pronounced for Parquet files due to &lt;a href="https://github.com/apache/arrow-rs/issues/5530"&gt;upstream work in the parquet
reader&lt;/a&gt;. Kudos to &lt;a href="https://github.com/XiangpengHong"&gt;@XiangpengHong&lt;/a&gt;, &lt;a href="https://github.com/AriesDevil"&gt;@AriesDevil&lt;/a&gt;, &lt;a href="https://github.com/PsiACE"&gt;@PsiACE&lt;/a&gt;, &lt;a href="https://github.com/Weijun-H"&gt;@Weijun-H&lt;/a&gt;,
&lt;a href="https://github.com/a10y"&gt;@a10y&lt;/a&gt;, and &lt;a href="https://github.com/RinChanNOWWW"&gt;@RinChanNOWWW&lt;/a&gt; for driving this project.&lt;/p&gt;
&lt;h2&gt;Improved Quality 📋&lt;/h2&gt;
&lt;p&gt;DataFusion continues to improve overall in quality. In addition to ongoing bug
fixes, one of the most exciting improvements is the addition of a new &lt;a href="https://github.com/datafusion-contrib/datafusion-sqlancer"&gt;SQLancer&lt;/a&gt;
based &lt;a href="https://github.com/apache/datafusion/issues/11030"&gt;DataFusion Fuzzing&lt;/a&gt; suite thanks to &lt;a href="https://github.com/2010YOUY01"&gt;@2010YOUY01&lt;/a&gt; that has already found
several bugs and thanks to &lt;a href="https://github.com/jonahgao"&gt;@jonahgao&lt;/a&gt;, &lt;a href="https://github.com/tshauck"&gt;@tshauck&lt;/a&gt;, &lt;a href="https://github.com/xinlifoobar"&gt;@xinlifoobar&lt;/a&gt;,
&lt;a href="https://github.com/LorrensP-2158466"&gt;@LorrensP-2158466&lt;/a&gt; for fixing them so fast.&lt;/p&gt;
&lt;h2&gt;Improved Documentation 📚&lt;/h2&gt;
&lt;p&gt;We continue to improve the documentation to make it easier to get started using DataFusion with
the &lt;a href="https://datafusion.apache.org/library-user-guide/index.html"&gt;Library Users Guide&lt;/a&gt;, &lt;a href="https://docs.rs/datafusion/latest/datafusion/index.html"&gt;API documentation&lt;/a&gt;, and &lt;a href="https://github.com/apache/datafusion/tree/main/datafusion-examples"&gt;Examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some notable new examples include:
* &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/sql_analysis.rs"&gt;sql_analysis.rs&lt;/a&gt; to analyse SQL queries with DataFusion structures (thanks &lt;a href="https://github.com/LorrensP-2158466"&gt;@LorrensP-2158466&lt;/a&gt;)
* &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/function_factory.rs"&gt;function_factory.rs&lt;/a&gt; to create custom functions via SQL (thanks &lt;a href="https://github.com/milenkovicm"&gt;@milenkovicm&lt;/a&gt;)
* &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/plan_to_sql.rs"&gt;plan_to_sql.rs&lt;/a&gt; to generate SQL from DataFusion Expr and LogicalPlan (thanks &lt;a href="https://github.com/edmondop"&gt;@edmondop&lt;/a&gt;)
* &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs"&gt;parquet_index.rs&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs"&gt;advanced_parquet_index.rs&lt;/a&gt; for parquet indexing, described more below (thanks &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;)&lt;/p&gt;
&lt;h2&gt;New Features ✨&lt;/h2&gt;
&lt;p&gt;There are too many new features in the last 6 months to list them all, but here
are some highlights:&lt;/p&gt;
&lt;h1&gt;SQL&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Support for &lt;code&gt;UNNEST&lt;/code&gt; (thanks &lt;a href="https://github.com/duongcongtoai"&gt;@duongcongtoai&lt;/a&gt;, &lt;a href="https://github.com/JasonLi-cn"&gt;@JasonLi-cn&lt;/a&gt; and &lt;a href="https://github.com/jayzhan211"&gt;@jayzhan211&lt;/a&gt;) &lt;/li&gt;
&lt;li&gt;Support for &lt;a href="https://github.com/apache/datafusion/issues/462"&gt;Recursive CTEs&lt;/a&gt; (thanks &lt;a href="https://github.com/jonahgao"&gt;@jonahgao&lt;/a&gt; and &lt;a href="https://github.com/matthewgapp"&gt;@matthewgapp&lt;/a&gt;) &lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;CREATE FUNCTION&lt;/code&gt; (see below) &lt;/li&gt;
&lt;li&gt;Many new SQL functions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DataFusion now has much improved support for structured types such &lt;code&gt;STRUCT&lt;/code&gt;,
&lt;code&gt;LIST&lt;/code&gt;/&lt;code&gt;ARRAY&lt;/code&gt; and &lt;code&gt;MAP&lt;/code&gt;. For example, you can now create &lt;code&gt;STRUCT&lt;/code&gt; literals 
in SQL like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;&amp;gt; select {'foo': {'bar': 2}};
+--------------------------------------------------------------+
| named_struct(Utf8("foo"),named_struct(Utf8("bar"),Int64(2))) |
+--------------------------------------------------------------+
| {foo: {bar: 2}}                                              |
+--------------------------------------------------------------+
1 row(s) fetched.
Elapsed 0.002 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;SQL Unparser (SQL Formatter)&lt;/h1&gt;
&lt;p&gt;DataFusion now supports converting &lt;code&gt;Expr&lt;/code&gt;s and &lt;code&gt;LogicalPlan&lt;/code&gt;s BACK to SQL text.
This can be useful in query federation to push predicates down into other
systems that only accept SQL, and for building systems that generate SQL.&lt;/p&gt;
&lt;p&gt;For example, you can now convert a logical expression back to SQL text:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Form a logical expression that represents the SQL "a &amp;lt; 5 OR a = 8"
let expr = col("a").lt(lit(5)).or(col("a").eq(lit(8)));
// convert the expression back to SQL text
let sql = expr_to_sql(&amp;amp;expr)?.to_string();
assert_eq!(sql, "a &amp;lt; 5 OR a = 8");
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also do complex things like parsing SQL, modifying the plan, and convert
it back to SQL:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let df = ctx
  // Use SQL to read some data from the parquet file
  .sql("SELECT int_col, double_col, CAST(date_string_col as VARCHAR) FROM alltypes_plain")
  .await?;
// Programmatically add new filters `id &amp;gt; 1 and tinyint_col &amp;lt; double_col`
let df = df.filter(col("id").gt(lit(1)).and(col("tinyint_col").lt(col("double_col"))))?
// Convert the new logical plan back to SQL
let sql = plan_to_sql(df.logical_plan())?.to_string();
assert_eq!(sql, 
           "SELECT alltypes_plain.int_col, alltypes_plain.double_col, CAST(alltypes_plain.date_string_col AS VARCHAR) \
           FROM alltypes_plain WHERE ((alltypes_plain.id &amp;gt; 1) AND (alltypes_plain.tinyint_col &amp;lt; alltypes_plain.double_col))")
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/plan_to_sql.rs"&gt;Plan to SQL example&lt;/a&gt; or the APIs &lt;a href="https://docs.rs/datafusion/latest/datafusion/sql/unparser/fn.expr_to_sql.html"&gt;expr_to_sql&lt;/a&gt; and &lt;a href="https://docs.rs/datafusion/latest/datafusion/sql/unparser/fn.plan_to_sql.html"&gt;plan_to_sql&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h1&gt;Low Level APIs for Fast Parquet Access (indexing)&lt;/h1&gt;
&lt;p&gt;With their rising prevalence, supporting efficient access to Parquet files
stored remotely on object storage is important. Part of doing this efficiently
is minimizing the number of object store requests made by caching metadata and
skipping over parts of the file that are not needed (e.g. via an index).&lt;/p&gt;
&lt;p&gt;DataFusion's Parquet reader has long internally supported &lt;a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"&gt;advanced predicate
pushdown&lt;/a&gt; by reading the parquet metadata from the file footer and pruning based
on row group and data page statistics. DataFusion now also supports users
supplying their own low level pruning information via the [&lt;code&gt;ParquetAccessPlan&lt;/code&gt;]
API.&lt;/p&gt;
&lt;p&gt;This API can be used along with index information to selectively skip decoding
parts of the file. For example, Spice AI used this feature to add &lt;a href="https://github.com/spiceai/spiceai/pull/1891"&gt;efficient
support&lt;/a&gt; for reading from DeltaLake tables and handling &lt;a href="https://docs.delta.io/latest/delta-deletion-vectors.html"&gt;deletion vectors&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;        &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;   If the RowSelection does not include any
        &amp;boxv;          ...          &amp;boxv;   rows from a particular Data Page, that
        &amp;boxv;                       &amp;boxv;   Data Page is not fetched or decoded.
        &amp;boxv; &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv;   Note this requires a PageIndex
        &amp;boxv; &amp;boxv;     &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv; &amp;boxv;
Row     &amp;boxv; &amp;boxv;     &amp;boxv;DataPage 0&amp;boxv;  &amp;boxv; &amp;boxv;                 &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
Groups  &amp;boxv; &amp;boxv;     &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxv; &amp;boxv;                 &amp;boxv;                    &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv; &amp;boxv;                 &amp;boxv;    ParquetExec     &amp;boxv;
        &amp;boxv; &amp;boxv; ... &amp;boxv;DataPage 1&amp;boxv; ◀&amp;boxvh; &amp;boxvh; &amp;boxh; &amp;boxh; &amp;boxh;           &amp;boxv;  (Parquet Reader)  &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxv; &amp;boxv;      &amp;boxur; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh;&amp;boxv;                    &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv; &amp;boxv;                 &amp;boxv; &amp;boxDR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxDL;  &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxv;DataPage 2&amp;boxv;  &amp;boxv; &amp;boxv; If only rows    &amp;boxv; &amp;boxV;ParquetMetadata&amp;boxV;  &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxv; &amp;boxv; from DataPage 1 &amp;boxv; &amp;boxUR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxUL;  &amp;boxv;
        &amp;boxv; &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv; are selected,   &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
        &amp;boxv;                       &amp;boxv; only DataPage 1
        &amp;boxv;          ...          &amp;boxv; is fetched and
        &amp;boxv;                       &amp;boxv; decoded
        &amp;boxv; &amp;boxDR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxDL; &amp;boxv;
        &amp;boxv; &amp;boxV;  Thrift metadata  &amp;boxV; &amp;boxv;
        &amp;boxv; &amp;boxUR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxUL; &amp;boxv;
        &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
         Parquet File
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs"&gt;parquet_index.rs&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs"&gt;advanced_parquet_index.rs&lt;/a&gt; examples for more details. &lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt; and &lt;a href="https://github.com/Ted-Jiang"&gt;@Ted-Jiang&lt;/a&gt; for this feature.  &lt;/p&gt;
&lt;h2&gt;Building Systems is Easier with DataFusion 🛠️&lt;/h2&gt;
&lt;p&gt;In addition to many incremental API improvements, there are several new APIs that make
it easier to build systems on top of DataFusion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Faster and easier to use &lt;a href="https://docs.rs/datafusion/latest/datafusion/common/tree_node/trait.TreeNode.html#overview"&gt;TreeNode API&lt;/a&gt; for traversing and manipulating plans and expressions.&lt;/li&gt;
&lt;li&gt;All functions now use the same &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/trait.ScalarUDFImpl.html"&gt;Scalar User Defined Function API&lt;/a&gt;, making it easier to customize
  DataFusion's behavior without sacrificing performance. See &lt;a href="https://github.com/apache/arrow-datafusion/issues/8045"&gt;ticket&lt;/a&gt; for more details.&lt;/li&gt;
&lt;li&gt;DataFusion can now be compiled to &lt;a href="https://github.com/apache/datafusion/discussions/9834"&gt;WASM&lt;/a&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;User Defined SQL Parsing Extensions&lt;/h1&gt;
&lt;p&gt;As of DataFusion 40.0.0, you can use the [&lt;code&gt;ExprPlanner&lt;/code&gt;] trait to extend
DataFusion's SQL planner to support custom operators or syntax.&lt;/p&gt;
&lt;p&gt;For example the &lt;a href="https://github.com/datafusion-contrib/datafusion-functions-json"&gt;datafusion-functions-json&lt;/a&gt; project uses this API to support
JSON operators in SQL queries. It provides a custom implementation for
planning JSON operators such as &lt;code&gt;-&amp;gt;&lt;/code&gt; and &lt;code&gt;-&amp;gt;&amp;gt;&lt;/code&gt; with code like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct MyCustomPlanner;

impl ExprPlanner for MyCustomPlanner {
    // Provide custom implementation for planning a binary operators
    // such as `-&amp;gt;` and `-&amp;gt;&amp;gt;`
    fn plan_binary_op(
        &amp;amp;self,
        expr: RawBinaryExpr,
        _schema: &amp;amp;DFSchema,
    ) -&amp;gt; Result&amp;lt;PlannerResult&amp;lt;RawBinaryExpr&amp;gt;&amp;gt; {
        match &amp;amp;expr.op {
           BinaryOperator::Arrow =&amp;gt; { /* plan -&amp;gt; operator */ }
           BinaryOperator::LongArrow =&amp;gt; { /* plan -&amp;gt;&amp;gt; operator */ }
           ...
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/samuelcolvin"&gt;@samuelcolvin&lt;/a&gt;, &lt;a href="https://github.com/jayzhan211"&gt;@jayzhan211&lt;/a&gt; and &lt;a href="https://github.com/dharanad"&gt;@dharanad&lt;/a&gt; for helping make this
feature happen.&lt;/p&gt;
&lt;h1&gt;Pluggable Support for &lt;code&gt;CREATE FUNCTION&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;DataFusion's new [&lt;code&gt;FunctionFactory&lt;/code&gt;] API let's users provide a handler for
&lt;code&gt;CREATE FUNCTION&lt;/code&gt; SQL statements. This feature lets you build systems that
support defining functions in SQL such as&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;-- SQL based functions
CREATE FUNCTION my_func(DOUBLE, DOUBLE) RETURNS DOUBLE
    RETURN $1 + $3
;

-- ML Models
CREATE FUNCTION iris(FLOAT[]) RETURNS FLOAT[] 
LANGUAGE TORCH AS 'models:/iris@champion';

-- WebAssembly
CREATE FUNCTION func(FLOAT[]) RETURNS FLOAT[] 
LANGUAGE WASM AS 'func.wasm'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Huge thanks to &lt;a href="https://github.com/milenkovicm"&gt;@milenkovicm&lt;/a&gt; for this feature. There is an example of how to
make macro like functions in &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/function_factory.rs"&gt;function_factory.rs&lt;/a&gt;. It would be
great if &lt;a href="https://github.com/apache/datafusion/issues/9326"&gt;someone made a demo&lt;/a&gt; showing how to create WASMs 🎣.&lt;/p&gt;
&lt;h2&gt;Looking Ahead: The Next Six Months 🔭&lt;/h2&gt;
&lt;p&gt;The community has been &lt;a href="https://github.com/apache/datafusion/issues/11442"&gt;discussing what we will work on in the next six months&lt;/a&gt;.
Some major initiatives from that discussion are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Performance&lt;/em&gt;: Improve the speed of &lt;a href="https://github.com/apache/arrow-datafusion/issues/7000"&gt;aggregating "high cardinality"&lt;/a&gt;
  data when there are many (e.g. millions) of distinct groups as well as additional
  ideas to improve parquet performance. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Modularity&lt;/em&gt;: Make DataFusion even more modular, by completely unifying
   built in and user &lt;a href="https://github.com/apache/datafusion/issues/8708"&gt;aggregate functions&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/issues/8709"&gt;window functions&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;LogicalTypes&lt;/em&gt;: &lt;a href="https://github.com/apache/datafusion/issues/11513"&gt;Introduce Logical Types&lt;/a&gt; to make it easier to use
   different encodings like &lt;code&gt;StringView&lt;/code&gt;, &lt;code&gt;RunEnd&lt;/code&gt; and &lt;code&gt;Dictionary&lt;/code&gt; arrays as well
   as user defined types. Thanks &lt;a href="https://github.com/notfilippo"&gt;@notfilippo&lt;/a&gt; for driving this. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Improved Documentation&lt;/em&gt;: Write blog posts and videos explaining
   how to use DataFusion for real-world use cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Testing&lt;/em&gt;: Improve CI infrastructure and test coverage, more fuzz
   testing, and better functional and performance regression testing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;DataFusion is not a project built or driven by a single person, company, or
foundation. Rather, our community of users and contributors work together to
build a shared technology that none of us could have built alone.&lt;/p&gt;
&lt;p&gt;If you are interested in joining us we would love to have you. You can try out
DataFusion on some of your own data and projects and let us know how it goes,
contribute suggestions, documentation, bug reports, or a PR with documentation,
tests or code. A list of open issues suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt; and you
can find how to reach us on the &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication doc&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.1.0 Release</title><link href="https://datafusion.apache.org/blog/2024/07/20/datafusion-comet-0.1.0" rel="alternate"></link><published>2024-07-20T00:00:00+00:00</published><updated>2024-07-20T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-07-20:/blog/2024/07/20/datafusion-comet-0.1.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce the first official source release of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce the first official source release of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers five months of development work since the project was &lt;a href="https://datafusion.apache.org/blog/2024/03/06/comet-donation/"&gt;donated&lt;/a&gt; to the Apache DataFusion
project and is the result of merging 343 PRs from 41 contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.1.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This first release supports 15 &lt;a href="https://datafusion.apache.org/comet/user-guide/datatypes.html#"&gt;data types&lt;/a&gt;, 13 &lt;a href="https://datafusion.apache.org/comet/user-guide/operators.html#"&gt;operators&lt;/a&gt;, and 106 &lt;a href="https://datafusion.apache.org/comet/user-guide/expressions.html#"&gt;expressions&lt;/a&gt;. Comet is compatible with Apache
Spark versions 3.3, 3.4, and 3.5. There is also experimental support for preview versions of Spark 4.0.&lt;/p&gt;
&lt;h2&gt;Project Status&lt;/h2&gt;
&lt;p&gt;The project's recent focus has been on fixing correctness and stability issues and implementing additional
native operators and expressions so that a broader range of queries can be executed natively.&lt;/p&gt;
&lt;p&gt;Here are some of the highlights since the project was donated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implemented native support for:&lt;/li&gt;
&lt;li&gt;SortMergeJoin&lt;/li&gt;
&lt;li&gt;HashJoin&lt;/li&gt;
&lt;li&gt;BroadcastHashJoin&lt;/li&gt;
&lt;li&gt;Columnar Shuffle&lt;/li&gt;
&lt;li&gt;More aggregate expressions&lt;/li&gt;
&lt;li&gt;Window aggregates&lt;/li&gt;
&lt;li&gt;Many Spark-compatible CAST expressions&lt;/li&gt;
&lt;li&gt;Implemented a simple Spark Fuzz Testing utility to find correctness issues&lt;/li&gt;
&lt;li&gt;Published a &lt;a href="https://datafusion.apache.org/comet/user-guide/overview.html"&gt;User Guide&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/comet/contributor-guide/contributing.html"&gt;Contributors Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Created a &lt;a href="https://github.com/apache/datafusion-benchmarks"&gt;DataFusion Benchmarks&lt;/a&gt; repository with scripts and documentation for running benchmarks derived&lt;br/&gt;
  from TPC-H and TPC-DS with DataFusion and Comet&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Current Performance&lt;/h2&gt;
&lt;p&gt;Comet already delivers a modest performance speedup for many queries, enabling faster data processing and
shorter time-to-insights.&lt;/p&gt;
&lt;p&gt;We use benchmarks derived from the industry standard TPC-H and TPC-DS benchmarks for tracking progress with
performance. The following chart shows the time it takes to run the 22 TPC-H queries against 100 GB of data in
Parquet format using a single executor with eight cores. See the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;
for details of the environment used for these benchmarks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.1.0" class="img-responsive" src="/blog/images/comet-0.1.0/tpch_allqueries.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Comet reduces the overall execution time from 626 seconds to 407 seconds, a 54% speedup (1.54x faster).&lt;/p&gt;
&lt;p&gt;Running the same queries with DataFusion standalone using the same number of cores results in a 3.9x speedup
compared to Spark. Although this isn&amp;rsquo;t a fair comparison (DataFusion does not have shuffle or match Spark
semantics in some cases, for example), it does give some idea about the potential future performance of
Comet. Comet aims to provide a 2x-4x speedup for a wide range of queries once more operators and expressions
can run natively.&lt;/p&gt;
&lt;p&gt;The following chart shows how much Comet currently accelerates each query from the benchmark.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.1.0" class="img-responsive" src="/blog/images/comet-0.1.0/tpch_queries_speedup.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;These benchmarks can be reproduced in any environment using the documentation in the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;. We
encourage you to run these benchmarks in your environment or, even better, try Comet out with your existing Spark jobs.&lt;/p&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;Comet is an open-source project, and contributors are welcome to work on any features they are interested in, but
here are some current focus areas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improve Performance &amp;amp; Reliability:&lt;/li&gt;
&lt;li&gt;Implement the remaining features needed so that all TPC-H queries can run entirely natively&lt;/li&gt;
&lt;li&gt;Implement spill support in SortMergeJoin&lt;/li&gt;
&lt;li&gt;Enable columnar shuffle by default&lt;/li&gt;
&lt;li&gt;Fully support Spark version 4.0.0&lt;/li&gt;
&lt;li&gt;Support more Spark operators and expressions&lt;/li&gt;
&lt;li&gt;We would like to support many more expressions natively in Comet, and this is a great place to start
    contributing. The contributors' guide has a section covering &lt;a href="https://datafusion.apache.org/comet/contributor-guide/adding_a_new_expression.html"&gt;adding support for new expressions&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Move more Spark expressions into the &lt;a href="https://crates.io/crates/datafusion-comet-spark-expr"&gt;datafusion-comet-spark-expr&lt;/a&gt; crate. Although the main focus of the Comet
  project is to provide an accelerator for Apache Spark, we also publish a standalone crate containing
  Spark-compatible expressions that can be used by any project using DataFusion, without adding any dependencies
  on JVM or Apache Spark.&lt;/li&gt;
&lt;li&gt;Release Process &amp;amp; Documentation&lt;/li&gt;
&lt;li&gt;Implement a binary release process so that we can publish JAR files to Maven for all supported platforms&lt;/li&gt;
&lt;li&gt;Add documentation for running Spark and Comet in Kubernetes, and add example Dockerfiles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project, and there is a Comet community video call held every four weeks on Wednesdays at 11:30 a.m. Eastern Time,
which is 16:30 UTC during Eastern Standard Time and 15:30 UTC during Eastern Daylight Time. See the
&lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;Comet Community Meeting&lt;/a&gt; Google Document for the next scheduled meeting date, the video call link, and
recordings of previous calls.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Announcing Apache Arrow DataFusion is now Apache DataFusion</title><link href="https://datafusion.apache.org/blog/2024/05/07/datafusion-tlp" rel="alternate"></link><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-05-07:/blog/2024/05/07/datafusion-tlp</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;TLDR; &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; DataFusion --&amp;gt; &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Arrow PMC and newly created DataFusion PMC are happy to announce that as of
April 16, 2024 the Apache Arrow DataFusion subproject is now a top level
&lt;a href="https://www.apache.org/"&gt;Apache Software Foundation&lt;/a&gt; project.&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Apache DataFusion is a fast, extensible query engine for building …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;TLDR; &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; DataFusion --&amp;gt; &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Arrow PMC and newly created DataFusion PMC are happy to announce that as of
April 16, 2024 the Apache Arrow DataFusion subproject is now a top level
&lt;a href="https://www.apache.org/"&gt;Apache Software Foundation&lt;/a&gt; project.&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Apache DataFusion is a fast, extensible query engine for building high-quality
data-centric systems in Rust, using the Apache Arrow in-memory format.&lt;/p&gt;
&lt;p&gt;When DataFusion was &lt;a href="https://arrow.apache.org/blog/2019/02/04/datafusion-donation/"&gt;donated to the Apache Software Foundation&lt;/a&gt; in 2019, the
DataFusion community was not large enough to stand on its own and the Arrow
project agreed to help support it. The community has grown significantly since
2019, benefiting immensely from being part of Arrow and following &lt;a href="https://www.apache.org/theapacheway/"&gt;The Apache
Way&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Why now?&lt;/h2&gt;
&lt;p&gt;The community &lt;a href="https://github.com/apache/datafusion/discussions/6475"&gt;discussed graduating to a top level project publicly&lt;/a&gt; for almost
a year, as the project seemed ready to stand on its own and would benefit from
more focused governance. For example, earlier in DataFusion's life many
contributed to both &lt;a href="https://github.com/apache/arrow-rs"&gt;arrow-rs&lt;/a&gt; and DataFusion, but as DataFusion has matured many
contributors, committers and PMC members focused more and more exclusively on
DataFusion.&lt;/p&gt;
&lt;h2&gt;Looking forward&lt;/h2&gt;
&lt;p&gt;The future looks bright. There are now &lt;a href="https://datafusion.apache.org/user-guide/introduction.html#known-users"&gt;10s of known projects built with
DataFusion&lt;/a&gt;, and that number continues to grow. We recently held our &lt;a href="https://github.com/apache/datafusion/discussions/8522"&gt;first in
person meetup&lt;/a&gt; passed &lt;a href="https://github.com/apache/datafusion/stargazers"&gt;5000 stars&lt;/a&gt; on GitHub, &lt;a href="https://github.com/apache/datafusion/issues/8373#issuecomment-2025133714"&gt;wrote a paper that was accepted
at SIGMOD 2024&lt;/a&gt;, and began work on &lt;a href="https://github.com/apache/datafusion-comet"&gt;Comet&lt;/a&gt;, an &lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt; accelerator
&lt;a href="https://arrow.apache.org/blog/2024/03/06/comet-donation/"&gt;initially donated by Apple&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you to everyone in the Arrow community who helped DataFusion grow and
mature over the years, and we look forward to continuing our collaboration as
projects. All future blogs and announcements will be posted on the &lt;a href="https://datafusion.apache.org/"&gt;Apache
DataFusion&lt;/a&gt; website.&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;If you are interested in joining the community, we would love to have you join
us. Get in touch using &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;Communication Doc&lt;/a&gt; and learn how to get involved in the
&lt;a href="https://datafusion.apache.org/contributor-guide/index.html"&gt;Contributor Guide&lt;/a&gt;. We welcome everyone to try DataFusion on their
own data and projects and let us know how it goes, contribute suggestions,
documentation, bug reports, or a PR with documentation, tests or code.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Announcing Apache Arrow DataFusion Comet</title><link href="https://datafusion.apache.org/blog/2024/03/06/comet-donation" rel="alternate"></link><published>2024-03-06T00:00:00+00:00</published><updated>2024-03-06T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-03-06:/blog/2024/03/06/comet-donation</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The Apache Arrow PMC is pleased to announce the donation of the &lt;a href="https://github.com/apache/arrow-datafusion-comet"&gt;Comet project&lt;/a&gt;,
a native Spark SQL Accelerator built on &lt;a href="https://arrow.apache.org/datafusion"&gt;Apache Arrow DataFusion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Comet is an Apache Spark plugin that uses Apache Arrow DataFusion to
accelerate Spark workloads. It is designed as a drop-in
replacement for Spark's JVM …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The Apache Arrow PMC is pleased to announce the donation of the &lt;a href="https://github.com/apache/arrow-datafusion-comet"&gt;Comet project&lt;/a&gt;,
a native Spark SQL Accelerator built on &lt;a href="https://arrow.apache.org/datafusion"&gt;Apache Arrow DataFusion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Comet is an Apache Spark plugin that uses Apache Arrow DataFusion to
accelerate Spark workloads. It is designed as a drop-in
replacement for Spark's JVM based SQL execution engine and offers significant
performance improvements for some workloads as shown below.&lt;/p&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Adaptive Arrow schema architecture overview." class="img-responsive" src="/blog/images/datafusion-comet/comet-architecture.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 1&lt;/b&gt;: With Comet, users interact with the same Spark ecosystem, tools
    and APIs such as Spark SQL. Queries still run through Spark's query optimizer and planner. 
    However, the execution is delegated to Comet,
    which is significantly faster and more resource efficient than a JVM based
    implementation.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Comet is one of a growing class of projects that aim to accelerate Spark using
native columnar engines such as the proprietary &lt;a href="https://www.databricks.com/product/photon"&gt;Databricks Photon Engine&lt;/a&gt; and
open source projects &lt;a href="https://incubator.apache.org/projects/gluten.html"&gt;Gluten&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/spark-rapids"&gt;Spark RAPIDS&lt;/a&gt;, and &lt;a href="https://github.com/kwai/blaze"&gt;Blaze&lt;/a&gt; (also built using
DataFusion).&lt;/p&gt;
&lt;p&gt;Comet was originally implemented at Apple and the engineers who worked on the
project are also significant contributors to Arrow and DataFusion. Bringing 
Comet into the Apache Software Foundation will accelerate its development and 
grow its community of contributors and users.&lt;/p&gt;
&lt;h1&gt;Get Involved&lt;/h1&gt;
&lt;p&gt;Comet is still in the early stages of development and we would love to have you
join us and help shape the project. We are working on an initial release, and 
expect to post another update with more details at that time.&lt;/p&gt;
&lt;p&gt;Before then, here are some ways to get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learn more by visiting the &lt;a href="https://github.com/apache/arrow-datafusion-comet"&gt;Comet project&lt;/a&gt; page and reading the &lt;a href="https://lists.apache.org/thread/0q1rb11jtpopc7vt1ffdzro0omblsh0s"&gt;mailing list
  discussion&lt;/a&gt; about the initial donation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Help us plan out the &lt;a href="https://github.com/apache/arrow-datafusion-comet/issues/19"&gt;roadmap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try out the project and provide feedback, file issues, and contribute code.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 34.0.0 Released, Looking Forward to 2024</title><link href="https://datafusion.apache.org/blog/2024/01/19/datafusion-34.0.0" rel="alternate"></link><published>2024-01-19T00:00:00+00:00</published><updated>2024-01-19T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-01-19:/blog/2024/01/19/datafusion-34.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We recently &lt;a href="https://crates.io/crates/datafusion/34.0.0"&gt;released DataFusion 34.0.0&lt;/a&gt;. This blog highlights some of the major
improvements since we &lt;a href="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/."&gt;released DataFusion 26.0.0&lt;/a&gt; (spoiler alert there are many)
and a preview of where the community plans to focus in the next 6 months.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We recently &lt;a href="https://crates.io/crates/datafusion/34.0.0"&gt;released DataFusion 34.0.0&lt;/a&gt;. This blog highlights some of the major
improvements since we &lt;a href="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/."&gt;released DataFusion 26.0.0&lt;/a&gt; (spoiler alert there are many)
and a preview of where the community plans to focus in the next 6 months.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query engine, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that
uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion is used by developers to
create new, fast data centric systems such as databases, dataframe libraries,
machine learning and streaming applications. While &lt;a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#project-goals"&gt;DataFusion&amp;rsquo;s primary design
goal&lt;/a&gt; is to accelerate creating other data centric systems, it has a
reasonable experience directly out of the box as a &lt;a href="https://arrow.apache.org/datafusion-python/"&gt;dataframe library&lt;/a&gt; and
&lt;a href="https://arrow.apache.org/datafusion/user-guide/cli.html"&gt;command line SQL tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This may also be our last update on the Apache Arrow Site. Future
updates will likely be on the DataFusion website as we are working to &lt;a href="https://github.com/apache/arrow-datafusion/discussions/6475"&gt;graduate
to a top level project&lt;/a&gt; (Apache Arrow DataFusion &amp;rarr; Apache DataFusion!) which
will help focus governance and project growth. Also exciting, our &lt;a href="https://github.com/apache/arrow-datafusion/discussions/8522"&gt;first
DataFusion in person meetup&lt;/a&gt; is planned for March 2024.&lt;/p&gt;
&lt;p&gt;DataFusion is very much a community endeavor. Our core thesis is that as a
community we can build much more advanced technology than any of us as
individuals or companies could alone. In the last 6 months between &lt;code&gt;26.0.0&lt;/code&gt; and
&lt;code&gt;34.0.0&lt;/code&gt;, community growth has been strong. We accepted and reviewed over a
thousand PRs from 124 different committers, created over 650 issues and closed 517
of them.
You can find a list of all changes in the detailed &lt;a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt;.&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 26.0.0..34.0.0 . | wc -l
     1009

$ git shortlog -sn 26.0.0..34.0.0 . | wc -l
      124

https://crates.io/crates/datafusion/26.0.0
DataFusion 26 released June 7, 2023

https://crates.io/crates/datafusion/34.0.0
DataFusion 34 released Dec 17, 2023

Issues created in this time: 214 open, 437 closed
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+created%3A2023-06-23..2023-12-17

Issues closes: 517
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+closed%3A2023-06-23..2023-12-17+

PRs merged in this time 908
https://github.com/apache/arrow-datafusion/pulls?q=is%3Apr+merged%3A2023-06-23..2023-12-17
--&gt;
&lt;h1&gt;Improved Performance 🚀&lt;/h1&gt;
&lt;p&gt;Performance is a key feature of DataFusion, DataFusion is 
more than 2x faster on &lt;a href="https://benchmark.clickhouse.com/"&gt;ClickBench&lt;/a&gt; compared to version &lt;code&gt;25.0.0&lt;/code&gt;, as shown below:&lt;/p&gt;
&lt;!--
  Scripts: https://github.com/alamb/datafusion-duckdb-benchmark/tree/datafusion-25-34
  Spreadsheet: https://docs.google.com/spreadsheets/d/1FtI3652WIJMC5LmJbLfT3G06w0JQIxEPG4yfMafexh8/edit#gid=1879366976
  Average runtime on 25.0.0: 7.2s (for the queries that actually ran)
  Average runtime on 34.0.0: 3.6s (for the same queries that ran in 25.0.0)
--&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Adaptive Arrow schema architecture overview." class="img-responsive" src="/blog/images/datafusion-34.0.0/compare-new.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 1&lt;/b&gt;: Performance improvement between &lt;code&gt;25.0.0&lt;/code&gt; and &lt;code&gt;34.0.0&lt;/code&gt; on ClickBench. 
    Note that DataFusion &lt;code&gt;25.0.0&lt;/code&gt;, could not run several queries due to 
    unsupported SQL (Q9, Q11, Q12, Q14) or memory requirements (Q33).
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Adaptive Arrow schema architecture overview." class="img-responsive" src="/blog/images/datafusion-34.0.0/compare.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 2&lt;/b&gt;: Total query runtime for DataFusion &lt;code&gt;34.0.0&lt;/code&gt; and DataFusion &lt;code&gt;25.0.0&lt;/code&gt;.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Here are some specific enhancements we have made to improve performance:
* &lt;a href="https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping/"&gt;2-3x better aggregation performance with many distinct groups&lt;/a&gt;
* Partially ordered grouping / streaming grouping
* [Specialized operator for "TopK" &lt;code&gt;ORDER BY LIMIT XXX&lt;/code&gt;] 
* [Specialized operator for &lt;code&gt;min(col) GROUP BY .. ORDER by min(col) LIMIT XXX&lt;/code&gt;]
* &lt;a href="https://github.com/apache/arrow-datafusion/pull/8126"&gt;Improved join performance&lt;/a&gt;
* Eliminate redundant sorting with sort order aware optimizers&lt;/p&gt;
&lt;h1&gt;New Features ✨&lt;/h1&gt;
&lt;h2&gt;DML / Insert / Creating Files&lt;/h2&gt;
&lt;p&gt;DataFusion now supports writing data in parallel, to individual or multiple
files, using &lt;code&gt;Parquet&lt;/code&gt;, &lt;code&gt;CSV&lt;/code&gt;, &lt;code&gt;JSON&lt;/code&gt;, &lt;code&gt;ARROW&lt;/code&gt; and user defined formats.
&lt;a href="https://github.com/apache/arrow-datafusion/pull/7655"&gt;Benchmark results&lt;/a&gt; show improvements up to 5x in some cases.&lt;/p&gt;
&lt;p&gt;Similarly to reading, data can now be written to any [&lt;code&gt;ObjectStore&lt;/code&gt;]
implementation, including AWS S3, Azure Blob Storage, GCP Cloud Storage, local
files, and user defined implementations. While reading from &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/listing/struct.ListingTable.html#features"&gt;hive style
partitioned tables&lt;/a&gt; has long been supported, it is now possible to write to such
tables as well.&lt;/p&gt;
&lt;p&gt;For example, to write to a local file:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ CREATE EXTERNAL TABLE awesome_table(x INT) STORED AS PARQUET LOCATION '/tmp/my_awesome_table';
0 rows in set. Query took 0.003 seconds.

❯ INSERT INTO awesome_table SELECT x * 10 FROM my_source_table;
+-------+
| count |
+-------+
| 3     |
+-------+
1 row in set. Query took 0.024 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also write to files with the [&lt;code&gt;COPY&lt;/code&gt;], similarly to [DuckDB&amp;rsquo;s &lt;code&gt;COPY&lt;/code&gt;]:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ COPY (SELECT x + 1 FROM my_source_table) TO '/tmp/output.json';
+-------+
| count |
+-------+
| 3     |
+-------+
1 row in set. Query took 0.014 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ cat /tmp/output.json
{"x":1}
{"x":2}
{"x":3}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Improved &lt;code&gt;STRUCT&lt;/code&gt; and &lt;code&gt;ARRAY&lt;/code&gt; support&lt;/h2&gt;
&lt;p&gt;DataFusion &lt;code&gt;34.0.0&lt;/code&gt; has much improved &lt;code&gt;STRUCT&lt;/code&gt; and &lt;code&gt;ARRAY&lt;/code&gt;
support, including a full range of &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/scalar_functions.html#struct-functions"&gt;struct functions&lt;/a&gt; and &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/scalar_functions.html#array-functions"&gt;array functions&lt;/a&gt;.&lt;/p&gt;
&lt;!--
❯ create table my_table as values ([1,2,3]), ([2]), ([4,5]);
--&gt;
&lt;p&gt;For example, you can now use &lt;code&gt;[]&lt;/code&gt; syntax and &lt;code&gt;array_length&lt;/code&gt; to access and inspect arrays:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ SELECT column1, 
         column1[1] AS first_element, 
         array_length(column1) AS len 
  FROM my_table;
+-----------+---------------+-----+
| column1   | first_element | len |
+-----------+---------------+-----+
| [1, 2, 3] | 1             | 3   |
| [2]       | 2             | 1   |
| [4, 5]    | 4             | 2   |
+-----------+---------------+-----+
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ SELECT column1, column1['c0'] FROM  my_table;
+------------------+----------------------+
| column1          | my_table.column1[c0] |
+------------------+----------------------+
| {c0: foo, c1: 1} | foo                  |
| {c0: bar, c1: 2} | bar                  |
+------------------+----------------------+
2 rows in set. Query took 0.002 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Other Features&lt;/h2&gt;
&lt;p&gt;Other notable features include:
* Support aggregating datasets that exceed memory size, with &lt;a href="https://github.com/apache/arrow-datafusion/pull/7400"&gt;group by spill to disk&lt;/a&gt;
* All operators now track and limit their memory consumption, including Joins&lt;/p&gt;
&lt;h1&gt;Building Systems is Easier with DataFusion 🛠️&lt;/h1&gt;
&lt;h2&gt;Documentation&lt;/h2&gt;
&lt;p&gt;It is easier than ever to get started using DataFusion with the
new &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/index.html"&gt;Library Users Guide&lt;/a&gt; as well as significantly improved the &lt;a href="https://docs.rs/datafusion/latest/datafusion/index.html"&gt;API documentation&lt;/a&gt;. &lt;/p&gt;
&lt;h2&gt;User Defined Window and Table Functions&lt;/h2&gt;
&lt;p&gt;In addition to DataFusion's &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-a-scalar-udf"&gt;User Defined Scalar Functions&lt;/a&gt;, and &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-an-aggregate-udf"&gt;User Defined Aggregate Functions&lt;/a&gt;, DataFusion now supports &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-a-window-udf"&gt;User Defined Window Functions&lt;/a&gt; 
 and &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-a-user-defined-table-function"&gt;User Defined Table Functions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, [the &lt;code&gt;datafusion-cli&lt;/code&gt;] implements a DuckDB style [&lt;code&gt;parquet_metadata&lt;/code&gt;]
function as a user defined table function (&lt;a href="https://github.com/apache/arrow-datafusion/blob/3f219bc929cfd418b0e3d3501f8eba1d5a2c87ae/datafusion-cli/src/functions.rs#L222-L248"&gt;source code here&lt;/a&gt;): &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ SELECT 
      path_in_schema, row_group_id, row_group_num_rows, stats_min, stats_max, total_compressed_size 
FROM 
      parquet_metadata('hits.parquet')
WHERE path_in_schema = '"WatchID"' 
LIMIT 3;

+----------------+--------------+--------------------+---------------------+---------------------+-----------------------+
| path_in_schema | row_group_id | row_group_num_rows | stats_min           | stats_max           | total_compressed_size |
+----------------+--------------+--------------------+---------------------+---------------------+-----------------------+
| "WatchID"      | 0            | 450560             | 4611687214012840539 | 9223369186199968220 | 3883759               |
| "WatchID"      | 1            | 612174             | 4611689135232456464 | 9223371478009085789 | 5176803               |
| "WatchID"      | 2            | 344064             | 4611692774829951781 | 9223363791697310021 | 3031680               |
+----------------+--------------+--------------------+---------------------+---------------------+-----------------------+
3 rows in set. Query took 0.053 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Growth of DataFusion 📈&lt;/h3&gt;
&lt;p&gt;DataFusion has been appearing more publically in the wild. For example
* New projects built using DataFusion such as &lt;a href="https://lancedb.com/"&gt;lancedb&lt;/a&gt;, &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt;, &lt;a href="https://www.arroyo.dev/"&gt;Arroyo&lt;/a&gt;, and &lt;a href="https://github.com/cmu-db/optd"&gt;optd&lt;/a&gt;.
* Public talks such as &lt;a href="https://www.youtube.com/watch?v=AJU9rdRNk9I"&gt;Apache Arrow Datafusion: Vectorized
  Execution Framework For Maximum Performance&lt;/a&gt; in &lt;a href="https://www.bagevent.com/event/8432178"&gt;CommunityOverCode Asia 2023&lt;/a&gt; 
* Blogs posts such as &lt;a href="https://www.synnada.ai/blog/apache-arrow-arrow-datafusion-ai-native-data-infra-an-interview-with-our-ceo-ozan"&gt;Apache Arrow, Arrow/DataFusion, AI-native Data Infra&lt;/a&gt;,
  &lt;a href="https://www.influxdata.com/blog/flight-datafusion-arrow-parquet-fdap-architecture-influxdb/"&gt;Flight, DataFusion, Arrow, and Parquet: Using the FDAP Architecture to build InfluxDB 3.0&lt;/a&gt;, and 
  &lt;a href="https://www.linkedin.com/pulse/guide-user-defined-functions-apache-arrow-datafusion-dade-aderemi/"&gt;A Guide to User-Defined Functions in Apache Arrow DataFusion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We have also &lt;a href="https://github.com/apache/arrow-datafusion/issues/6782"&gt;submitted a paper&lt;/a&gt; to &lt;a href="https://2024.sigmod.org/"&gt;SIGMOD 2024&lt;/a&gt;, one of the
premiere database conferences, describing DataFusion in a technically formal
style and making the case that it is possible to create a modular and extensive query engine 
without sacrificing performance. We hope this paper helps people 
evaluating DataFusion for their needs understand it better.&lt;/p&gt;
&lt;h1&gt;DataFusion in 2024 🥳&lt;/h1&gt;
&lt;p&gt;Some major initiatives from contributors we know of this year are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Modularity&lt;/em&gt;: Make DataFusion even more modular, such as &lt;a href="https://github.com/apache/arrow-datafusion/issues/8045"&gt;unifying
   built in and user functions&lt;/a&gt;, making it easier to customize 
   DataFusion's behavior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Community Growth&lt;/em&gt;: Graduate to our own top level Apache project, and
   subsequently add more committers and PMC members to keep pace with project
   growth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Use case white papers&lt;/em&gt;: Write blog posts and videos explaining
   how to use DataFusion for real-world use cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Testing&lt;/em&gt;: Improve CI infrastructure and test coverage, more fuzz
   testing, and better functional and performance regression testing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Planning Time&lt;/em&gt;: Reduce the time taken to plan queries, both &lt;a href="https://github.com/apache/arrow-datafusion/issues/7698"&gt;wide
   tables of 1000s of columns&lt;/a&gt;, and in &lt;a href="https://github.com/apache/arrow-datafusion/issues/5637"&gt;general&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Aggregate Performance&lt;/em&gt;: Improve the speed of &lt;a href="https://github.com/apache/arrow-datafusion/issues/7000"&gt;aggregating "high cardinality"&lt;/a&gt; data
   when there are many (e.g. millions) of distinct groups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Statistics&lt;/em&gt;: &lt;a href="https://github.com/apache/arrow-datafusion/issues/8227"&gt;Improved statistics handling&lt;/a&gt; with an eye towards more
   sophisticated expression analysis and cost models.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion we would love to have you
join us. You can try out DataFusion on some of your own data and projects and
let us know how it goes, contribute suggestions, documentation, bug reports, or
a PR with documentation, tests or code. A list of open issues
suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As the community grows, we are also looking to restart biweekly calls /
meetings. Timezones are always a challenge for such meetings, but we hope to
have two calls that can work for most attendees. If you are interested
in helping, or just want to say hi, please drop us a note via one of 
the methods listed in our &lt;a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html"&gt;Communication Doc&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Aggregating Millions of Groups Fast in Apache Arrow DataFusion 28.0.0</title><link href="https://datafusion.apache.org/blog/2023/08/05/datafusion_fast_grouping" rel="alternate"></link><published>2023-08-05T00:00:00+00:00</published><updated>2023-08-05T00:00:00+00:00</updated><author><name>alamb, Dandandan, tustvold</name></author><id>tag:datafusion.apache.org,2023-08-05:/blog/2023/08/05/datafusion_fast_grouping</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- Converted from Google Docs using https://www.buymeacoffee.com/docstomarkdown --&gt;
&lt;h2&gt;Aggregating Millions of Groups Fast in Apache Arrow DataFusion&lt;/h2&gt;
&lt;p&gt;Andrew Lamb, Dani&amp;euml;l Heres, Raphael Taylor-Davies,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: this article was originally published on the &lt;a href="https://www.influxdata.com/blog/aggregating-millions-groups-fast-apache-arrow-datafusion"&gt;InfluxData Blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;TLDR&lt;/h2&gt;
&lt;p&gt;Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. &lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt;&amp;rsquo;s parallel aggregation capability …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- Converted from Google Docs using https://www.buymeacoffee.com/docstomarkdown --&gt;
&lt;h2&gt;Aggregating Millions of Groups Fast in Apache Arrow DataFusion&lt;/h2&gt;
&lt;p&gt;Andrew Lamb, Dani&amp;euml;l Heres, Raphael Taylor-Davies,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: this article was originally published on the &lt;a href="https://www.influxdata.com/blog/aggregating-millions-groups-fast-apache-arrow-datafusion"&gt;InfluxData Blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;TLDR&lt;/h2&gt;
&lt;p&gt;Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. &lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt;&amp;rsquo;s parallel aggregation capability is 2-3x faster in the &lt;a href="https://crates.io/crates/datafusion/28.0.0"&gt;newly released version &lt;code&gt;28.0.0&lt;/code&gt;&lt;/a&gt; for queries with a large number (10,000 or more) of groups.&lt;/p&gt;
&lt;p&gt;Improving aggregation performance matters to all users of DataFusion. For example, both InfluxDB, a &lt;a href="https://github.com/influxdata/influxdb"&gt;time series data platform&lt;/a&gt; and Coralogix, a &lt;a href="https://coralogix.com/?utm_source=InfluxDB&amp;amp;utm_medium=Blog&amp;amp;utm_campaign=organic"&gt;full-stack observability&lt;/a&gt; platform, aggregate vast amounts of raw data to monitor and create insights for our customers. Improving DataFusion&amp;rsquo;s performance lets us provide better user experiences by generating insights faster with fewer resources. Because DataFusion is open source and released under the permissive &lt;a href="https://github.com/apache/arrow-datafusion/blob/main/LICENSE.txt"&gt;Apache 2.0&lt;/a&gt; license, the whole DataFusion community benefits as well.&lt;/p&gt;
&lt;p&gt;With the new optimizations, DataFusion&amp;rsquo;s grouping speed is now close to DuckDB, a system that regularly reports &lt;a href="https://duckdblabs.github.io/db-benchmark/"&gt;great&lt;/a&gt; &lt;a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html#experiments"&gt;grouping&lt;/a&gt; benchmark performance numbers. Figure 1 contains a representative sample of &lt;a href="https://github.com/ClickHouse/ClickBench/tree/main"&gt;ClickBench&lt;/a&gt; on a single Parquet file, and the full results are at the end of this article.&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/datafusion_fast_grouping/summary.png" width="700"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Query performance for ClickBench queries on queries 16, 17, 18 and 19 on a single Parquet file for DataFusion &lt;code&gt;27.0.0&lt;/code&gt;, DataFusion &lt;code&gt;28.0.0&lt;/code&gt; and DuckDB &lt;code&gt;0.8.1&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction to high cardinality grouping&lt;/h2&gt;
&lt;p&gt;Aggregation is a fancy word for computing summary statistics across many rows that have the same value in one or more columns. We call the rows with the same values &lt;em&gt;groups&lt;/em&gt; and &amp;ldquo;high cardinality&amp;rdquo; means there are a large number of distinct groups in the dataset. At the time of writing, a &amp;ldquo;large&amp;rdquo; number of groups in analytic engines is around 10,000.&lt;/p&gt;
&lt;p&gt;For example the &lt;a href="https://github.com/ClickHouse/ClickBench"&gt;ClickBench&lt;/a&gt; &lt;em&gt;hits&lt;/em&gt; dataset contains 100 million anonymized user clicks across a set of websites. ClickBench Query 17 is:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT "UserID", "SearchPhrase", COUNT(*)
FROM hits
GROUP BY "UserID", "SearchPhrase"
ORDER BY COUNT(*)
DESC LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In English, this query finds &amp;ldquo;the top ten (user, search phrase) combinations, across all clicks&amp;rdquo; and produces the following results (there are no search phrases for the top ten users):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;+---------------------+--------------+-----------------+
| UserID              | SearchPhrase | COUNT(UInt8(1)) |
+---------------------+--------------+-----------------+
| 1313338681122956954 |              | 29097           |
| 1907779576417363396 |              | 25333           |
| 2305303682471783379 |              | 10597           |
| 7982623143712728547 |              | 6669            |
| 7280399273658728997 |              | 6408            |
| 1090981537032625727 |              | 6196            |
| 5730251990344211405 |              | 6019            |
| 6018350421959114808 |              | 5990            |
| 835157184735512989  |              | 5209            |
| 770542365400669095  |              | 4906            |
+---------------------+--------------+-----------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ClickBench dataset contains&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;99,997,497 total rows[^1]&lt;/li&gt;
&lt;li&gt;17,630,976 different users (distinct UserIDs)[^2]&lt;/li&gt;
&lt;li&gt;6,019,103 different search phrases[^3]&lt;/li&gt;
&lt;li&gt;24,070,560 distinct combinations[^4] of (UserID, SearchPhrase)
Thus, to answer the query, DataFusion must map each of the 100M different input rows into one of the &lt;strong&gt;24 million different groups&lt;/strong&gt;, and keep count of how many such rows there are in each group.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The solution&lt;/h2&gt;
&lt;p&gt;Like most concepts in databases and other analytic systems, the basic ideas of this algorithm are straightforward and taught in introductory computer science courses. You could compute the query with a program such as this[^5]:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
from collections import defaultdict
from operator import itemgetter

# read file
hits = pd.read_parquet('hits.parquet', engine='pyarrow')

# build groups
counts = defaultdict(int)
for index, row in hits.iterrows():
    group = (row['UserID'], row['SearchPhrase']);
    # update the dict entry for the corresponding key
    counts[group] += 1

# Print the top 10 values
print (dict(sorted(counts.items(), key=itemgetter(1), reverse=True)[:10]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach, while simple, is both slow and very memory inefficient. It requires over 40 seconds to compute the results for less than 1% of the dataset[^6]. Both DataFusion &lt;code&gt;28.0.0&lt;/code&gt; and DuckDB &lt;code&gt;0.8.1&lt;/code&gt; compute results in under 10 seconds for the &lt;em&gt;entire&lt;/em&gt; dataset.&lt;/p&gt;
&lt;p&gt;To answer this query quickly and efficiently, you have to write your code such that it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Keeps all cores busy aggregating via parallelized computation&lt;/li&gt;
&lt;li&gt;Updates aggregate values quickly, using vectorizable loops that are easy for compilers to translate into the high performance &lt;a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data"&gt;SIMD&lt;/a&gt; instructions available in modern CPUs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The rest of this article explains how grouping works in DataFusion and the improvements we made in &lt;code&gt;28.0.0&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Two phase parallel partitioned grouping&lt;/h3&gt;
&lt;p&gt;Both DataFusion &lt;code&gt;27.0.&lt;/code&gt; and &lt;code&gt;28.0.0&lt;/code&gt; use state-of-the-art, two phase parallel hash partitioned grouping, similar to other high-performance vectorized engines like &lt;a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html"&gt;DuckDB&amp;rsquo;s Parallel Grouped Aggregates&lt;/a&gt;. In pictures this looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;            ▲                        ▲
            &amp;boxv;                        &amp;boxv;
            &amp;boxv;                        &amp;boxv;
            &amp;boxv;                        &amp;boxv;
&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;        GroupBy        &amp;boxv;  &amp;boxv;      GroupBy      &amp;boxv;      Step 4
&amp;boxv;        (Final)        &amp;boxv;  &amp;boxv;      (Final)      &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
            ▲                        ▲
            &amp;boxv;                        &amp;boxv;
            &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
                         &amp;boxv;
                         &amp;boxv;
            &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
            &amp;boxv;       Repartition       &amp;boxv;               Step 3
            &amp;boxv;         HASH(x)         &amp;boxv;
            &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
                         ▲
                         &amp;boxv;
            &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
            &amp;boxv;                       &amp;boxv;
            &amp;boxv;                       &amp;boxv;
 &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
 &amp;boxv;      GroupyBy      &amp;boxv;  &amp;boxv;       GroupBy       &amp;boxv;      Step 2
 &amp;boxv;     (Partial)      &amp;boxv;  &amp;boxv;      (Partial)      &amp;boxv;
 &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
            ▲                       ▲
         &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxul;                       &amp;boxur;&amp;boxh;&amp;boxdl;
         &amp;boxv;                            &amp;boxv;
    .&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;.                  .&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;.
 ,&amp;boxh;'           '&amp;boxh;.            ,&amp;boxh;'           '&amp;boxh;.
;      Input      :          ;      Input      :      Step 1
:    Stream 1     ;          :    Stream 2     ;
 ╲               ╱            ╲               ╱
  '&amp;boxh;.         ,&amp;boxh;'              '&amp;boxh;.         ,&amp;boxh;'
     `&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;'                    `&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Two phase repartitioned grouping: data flows from bottom (source) to top (results) in two phases. First (Steps 1 and 2), each core reads the data into a core-specific hash table, computing intermediate aggregates without any cross-core coordination. Then (Steps 3 and 4) DataFusion divides the data (&amp;ldquo;repartitions&amp;rdquo;) into distinct subsets by group value, and each subset is sent to a specific core which computes the final aggregate.&lt;/p&gt;
&lt;p&gt;The two phases are critical for keeping cores busy in a multi-core system. Both phases use the same hash table approach (explained in the next section), but differ in how the groups are distributed and the partial results emitted from the accumulators. The first phase aggregates data as soon as possible after it is produced. However, as shown in Figure 2, the groups can be anywhere in any input, so the same group is often found on many different cores. The second phase uses a hash function to redistribute data evenly across the cores, so each group value is processed by exactly one core which emits the final results for that group.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  2  &amp;boxv;    &amp;boxv;  4  &amp;boxv;   2. After Repartitioning: each
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;   group key  appears in exactly
    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;   one partition
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  2  &amp;boxv;    &amp;boxv;  4  &amp;boxv;
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;

&amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh;

    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
    &amp;boxv;  2  &amp;boxv;    &amp;boxv;  2  &amp;boxv;
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  2  &amp;boxv;
    &amp;boxv;  3  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  4  &amp;boxv;    &amp;boxv;  1  &amp;boxv;
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    1. Input Stream: groups
      ...        ...      values are spread
    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    arbitrarily over each input
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  4  &amp;boxv;
    &amp;boxv;  4  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  1  &amp;boxv;
    &amp;boxv;  4  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  3  &amp;boxv;    &amp;boxv;  2  &amp;boxv;
    &amp;boxv;  2  &amp;boxv;    &amp;boxv;  2  &amp;boxv;
    &amp;boxv;  2  &amp;boxv;    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;

    Core A      Core B

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Group value distribution across 2 cores during aggregation phases. In the first phase, every group value &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt;, &lt;code&gt;3&lt;/code&gt;, &lt;code&gt;4&lt;/code&gt;, is present in the input stream processed by each core. In the second phase, after repartitioning, the group values &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt; are processed by core A, and values &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt; are processed only by core B.&lt;/p&gt;
&lt;p&gt;There are some additional subtleties in the &lt;a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/core/src/physical_plan/aggregates/row_hash.rs"&gt;DataFusion implementation&lt;/a&gt; not mentioned above due to space constraints, such as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The policy of when to emit data from the first phase&amp;rsquo;s hash table (e.g. because the data is partially sorted)&lt;/li&gt;
&lt;li&gt;Handling specific filters per aggregate (due to the &lt;code&gt;FILTER&lt;/code&gt; SQL clause)&lt;/li&gt;
&lt;li&gt;Data types of intermediate values (which may not be the same as the final output for some aggregates such as &lt;code&gt;AVG&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Action taken when memory use exceeds its budget.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Hash grouping&lt;/h3&gt;
&lt;p&gt;DataFusion queries can compute many different aggregate functions for each group, both &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/aggregate_functions.html"&gt;built in&lt;/a&gt; and/or user defined &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/struct.AggregateUDF.html"&gt;&lt;code&gt;AggregateUDFs&lt;/code&gt;&lt;/a&gt;. The state for each aggregate function, called an &lt;em&gt;accumulator&lt;/em&gt;, is tracked with a hash table (DataFusion uses the excellent &lt;a href="https://docs.rs/hashbrown/latest/hashbrown/index.html"&gt;HashBrown&lt;/a&gt; &lt;a href="https://docs.rs/hashbrown/latest/hashbrown/raw/struct.RawTable.html"&gt;RawTable API&lt;/a&gt;), which logically stores the &amp;ldquo;index&amp;rdquo;  identifying the specific group value.&lt;/p&gt;
&lt;h3&gt;Hash grouping in &lt;code&gt;27.0.0&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;As shown in Figure 3, DataFusion &lt;code&gt;27.0.0&lt;/code&gt; stores the data in a &lt;a href="https://github.com/apache/arrow-datafusion/blob/4d93b6a3802151865b68967bdc4c7d7ef425b49a/datafusion/core/src/physical_plan/aggregates/utils.rs#L38-L50"&gt;&lt;code&gt;GroupState&lt;/code&gt;&lt;/a&gt; structure which, unsurprisingly, tracks the state for each group. The state for each group consists of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The actual value of the group columns, in &lt;a href="https://docs.rs/arrow-row/latest/arrow_row/index.html"&gt;Arrow Row&lt;/a&gt; format.&lt;/li&gt;
&lt;li&gt;In-progress accumulations (e.g. the running counts for the &lt;code&gt;COUNT&lt;/code&gt; aggregate) for each group, in one of two possible formats (&lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/expr/src/accumulator.rs#L24-L49"&gt;&lt;code&gt;Accumulator&lt;/code&gt;&lt;/a&gt;  or &lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/row_accumulator.rs#L26-L46"&gt;&lt;code&gt;RowAccumulator&lt;/code&gt;&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Scratch space for tracking which rows match each aggregate in each batch.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;                           &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
                           &amp;boxv;                                      &amp;boxv;
                           &amp;boxv;                  ...                 &amp;boxv;
                           &amp;boxv; ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ &amp;boxv;
                           &amp;boxv; ┃                                  ┃ &amp;boxv;
    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;            &amp;boxv; ┃ &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; ┃ &amp;boxv;
    &amp;boxv;         &amp;boxv;            &amp;boxv; ┃ &amp;boxv;group values: OwnedRow        &amp;boxv; ┃ &amp;boxv;
    &amp;boxv; &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv;            &amp;boxv; ┃ &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; ┃ &amp;boxv;
    &amp;boxv; &amp;boxv;  5  &amp;boxv; &amp;boxv;            &amp;boxv; ┃ &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; ┃ &amp;boxv;
    &amp;boxv; &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl; &amp;boxv;            &amp;boxv; ┃ &amp;boxv;Row accumulator:              &amp;boxv; ┃ &amp;boxv;
    &amp;boxv; &amp;boxv;  9  &amp;boxv;&amp;boxh;&amp;boxvh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;       &amp;boxv; ┃ &amp;boxv;Vec&amp;lt;u8&amp;gt;                       &amp;boxv; ┃ &amp;boxv;
    &amp;boxv; &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; ┃ &amp;boxv;
    &amp;boxv; &amp;boxv; ... &amp;boxv; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxv;  1  &amp;boxv; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxv;Accumulator 1 &amp;boxv;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxv; ... &amp;boxv; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxv;Accumulator 2 &amp;boxv;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv;         &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv; Box&amp;lt;dyn Accumulator&amp;gt; &amp;boxv;         ┃ &amp;boxv;
    Hash Table     &amp;boxv;       &amp;boxv; ┃ &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;         ┃ &amp;boxv;
                   &amp;boxv;       &amp;boxv; ┃ &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;      ┃ &amp;boxv;
                   &amp;boxv;       &amp;boxv; ┃ &amp;boxv;scratch indices: Vec&amp;lt;u32&amp;gt;&amp;boxv;      ┃ &amp;boxv;
                   &amp;boxv;       &amp;boxv; ┃ &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;      ┃ &amp;boxv;
                   &amp;boxv;       &amp;boxv; ┃ GroupState                       ┃ &amp;boxv;
                   &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;▶ &amp;boxv; ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ &amp;boxv;
                           &amp;boxv;                                      &amp;boxv;
  Hash table tracks an     &amp;boxv;                 ...                  &amp;boxv;
  index into group_states  &amp;boxv;                                      &amp;boxv;
                           &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
                           group_states: Vec&amp;lt;GroupState&amp;gt;

                           There is one GroupState PER GROUP

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Hash group operator structure in DataFusion &lt;code&gt;27.0.0&lt;/code&gt;. A hash table maps each group to a GroupState which contains all the per-group states.&lt;/p&gt;
&lt;p&gt;To compute the aggregate, DataFusion performs the following steps for each input batch:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Calculate hash using &lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/hash_utils.rs#L264-L307"&gt;efficient vectorized code&lt;/a&gt;, specialized for each data type.&lt;/li&gt;
&lt;li&gt;Determine group indexes for each input row using the hash table (creating new entries for newly seen groups).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/blob/4ab8be57dee3bfa72dd105fbd7b8901b873a4878/datafusion/core/src/physical_plan/aggregates/row_hash.rs#L562-L602"&gt;Update Accumulators for each group that had input rows,&lt;/a&gt; assembling the rows into a contiguous range for vectorized accumulator if there are a sufficient number of them.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;DataFusion also stores the hash values in the table to avoid potentially costly hash recomputation when resizing the hash table.&lt;/p&gt;
&lt;p&gt;This scheme works very well for a relatively small number of distinct groups: all accumulators are efficiently updated with large contiguous batches of rows.&lt;/p&gt;
&lt;p&gt;However, this scheme is not ideal for high cardinality grouping due to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Multiple allocations per group&lt;/strong&gt; for the group value row format, as well as for the &lt;code&gt;RowAccumulator&lt;/code&gt;s and each  &lt;code&gt;Accumulator&lt;/code&gt;. The &lt;code&gt;Accumulator&lt;/code&gt; may have additional allocations within it as well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-vectorized updates:&lt;/strong&gt; Accumulator updates often fall back to a slower non-vectorized form because the number of distinct groups is large (and thus number of values per group is small) in each input batch.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Hash grouping in &lt;code&gt;28.0.0&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;For &lt;code&gt;28.0.0&lt;/code&gt;, we rewrote the core group by implementation following traditional system optimization principles: fewer allocations, type specialization, and aggressive vectorization.&lt;/p&gt;
&lt;p&gt;DataFusion &lt;code&gt;28.0.0&lt;/code&gt; uses the same RawTable and still stores group indexes. The major differences, as shown in Figure 4, are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Group values are stored either&lt;ol&gt;
&lt;li&gt;Inline in the &lt;code&gt;RawTable&lt;/code&gt; (for single columns of primitive types), where the conversion to Row format costs more than its benefit&lt;/li&gt;
&lt;li&gt;In a separate &lt;a href="https://docs.rs/arrow-row/latest/arrow_row/struct.Row.html"&gt;Rows&lt;/a&gt; structure with a single contiguous allocation for all groups values, rather than an allocation per group. Accumulators manage the state for all the groups internally, so the code to update intermediate values is a tight type specialized loop. The new &lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/mod.rs#L66-L75"&gt;&lt;code&gt;GroupsAccumulator&lt;/code&gt;&lt;/a&gt; interface results in highly efficient type accumulator update loops.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;     &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv; &amp;boxdr; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxdl;  &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;&amp;boxv;     &amp;boxv; ┏━━━━━━━━━━━━━━━━━━━┓ &amp;boxv;
&amp;boxv;                &amp;boxv;                 &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxdr; &amp;boxh; &amp;boxh; &amp;boxdl;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv; ┃ &amp;boxv;
&amp;boxv;                &amp;boxv;    X   &amp;boxv;  5  &amp;boxv;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxv;  value1   &amp;boxv; &amp;boxv; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxvr; &amp;boxh; &amp;boxh; &amp;boxvl;&amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv; ┃ &amp;boxv;
&amp;boxv;                &amp;boxv;    Q   &amp;boxv;  9  &amp;boxv;&amp;boxh;&amp;boxh;&amp;boxvh;&amp;boxvh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv; ┃  &amp;boxv;     ...      &amp;boxv; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxvr; &amp;boxh; &amp;boxh; &amp;boxvl;&amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl;  &amp;boxv;&amp;boxv;  &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxvh;&amp;boxh;╋&amp;boxh;▶&amp;boxv;              &amp;boxv; ┃ &amp;boxv;
&amp;boxv;                &amp;boxv;   ...  &amp;boxv; ... &amp;boxv;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxvr; &amp;boxh; &amp;boxh; &amp;boxvl;&amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxv;  valueN   &amp;boxv; &amp;boxv; ┃ &amp;boxv;
&amp;boxv;                &amp;boxv;    H   &amp;boxv;  1  &amp;boxv;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxvr; &amp;boxh; &amp;boxh; &amp;boxvl;&amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;values: Vec&amp;lt;T&amp;gt;&amp;boxv; ┃ &amp;boxv;
&amp;boxv;     Rows       &amp;boxv;   ...  &amp;boxv; ... &amp;boxv;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxur; &amp;boxh; &amp;boxh; &amp;boxul;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃                   ┃ &amp;boxv;
&amp;boxv;  &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh;   &amp;boxv;                 &amp;boxv;&amp;boxv;     &amp;boxv; ┃ GroupsAccumulator ┃ &amp;boxv;
&amp;boxv;                &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;&amp;boxv;     &amp;boxv; ┗━━━━━━━━━━━━━━━━━━━┛ &amp;boxv;
&amp;boxv;                  Hash Table       &amp;boxv;     &amp;boxv;                       &amp;boxv;
&amp;boxv;                                   &amp;boxv;     &amp;boxv;          ...          &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;     &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
  GroupState                               Accumulators


Hash table value stores group_indexes     One  GroupsAccumulator
and group values.                         per aggregate. Each
                                          stores the state for
Group values are stored either inline     *ALL* groups, typically
in the hash table or in a single          using a native Vec&amp;lt;T&amp;gt;
allocation using the arrow Row format
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: Hash group operator structure in DataFusion &lt;code&gt;28.0.0&lt;/code&gt;. Group values are stored either directly in the hash table, or in a single allocation using the arrow Row format. The hash table contains group indexes. A single &lt;code&gt;GroupsAccumulator&lt;/code&gt; stores the per-aggregate state for &lt;em&gt;all&lt;/em&gt; groups.&lt;/p&gt;
&lt;p&gt;This new structure improves performance significantly for high cardinality groups due to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reduced allocations&lt;/strong&gt;: There are no longer any individual allocations per group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contiguous native accumulator states&lt;/strong&gt;: Type-specialized accumulators store the values for all groups in a single contiguous allocation using a &lt;a href="https://doc.rust-lang.org/std/vec/struct.Vec.html"&gt;Rust Vec&amp;lt;T&amp;gt;&lt;/a&gt; of some native type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vectorized state update&lt;/strong&gt;: The inner aggregate update loops, which are type-specialized and in terms of native &lt;code&gt;Vec&lt;/code&gt;s, are well-vectorized by the Rust compiler (thanks &lt;a href="https://llvm.org/"&gt;LLVM&lt;/a&gt;!).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Notes&lt;/h3&gt;
&lt;p&gt;Some vectorized grouping implementations store the accumulator state row-wise directly in the hash table, which often uses modern CPU caches efficiently. Managing accumulator state in columnar fashion may sacrifice some cache locality, however it ensures the size of the hash table remains small, even when there are large numbers of groups and aggregates, making it easier for the compiler to vectorize the accumulator update.&lt;/p&gt;
&lt;p&gt;Depending on the cost of recomputing hash values, DataFusion &lt;code&gt;28.0.0&lt;/code&gt; may or may not store the hash values in the table. This optimizes the tradeoff between the cost of computing the hash value (which is expensive for strings, for example) vs. the cost of storing it in the hash table.&lt;/p&gt;
&lt;p&gt;One subtlety that arises from pushing state updates into GroupsAccumulators is that each accumulator must handle similar variations with/without filtering and with/without nulls in the input. DataFusion &lt;code&gt;28.0.0&lt;/code&gt; uses a templated &lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/accumulate.rs#L28-L54"&gt;&lt;code&gt;NullState&lt;/code&gt;&lt;/a&gt; which encapsulates these common patterns across accumulators.&lt;/p&gt;
&lt;p&gt;The code structure is heavily influenced by the fact DataFusion is implemented using &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, a new(ish) systems programming language focused on speed and safety. Rust heavily discourages many of the traditional pointer casting &amp;ldquo;tricks&amp;rdquo; used in C/C++ hash grouping implementations. The DataFusion aggregation code is almost entirely &lt;a href="https://doc.rust-lang.org/nomicon/meet-safe-and-unsafe.html#:~:text=Safe%20Rust%20is%20the%20true,Undefined%20Behavior%20(a.k.a.%20UB)."&gt;&lt;code&gt;safe&lt;/code&gt;&lt;/a&gt;, deviating into &lt;code&gt;unsafe&lt;/code&gt; only when necessary. (Rust is a great choice because it makes DataFusion fast, easy to embed, and prevents many crashes and security issues often associated with multi-threaded C/C++ code).&lt;/p&gt;
&lt;h2&gt;ClickBench results&lt;/h2&gt;
&lt;p&gt;The full results of running the &lt;a href="https://github.com/ClickHouse/ClickBench/tree/main"&gt;ClickBench&lt;/a&gt; queries against the single Parquet file with DataFusion &lt;code&gt;27.0.0&lt;/code&gt;, DataFusion &lt;code&gt;28.0.0&lt;/code&gt;, and DuckDB &lt;code&gt;0.8.1&lt;/code&gt; are below. These numbers were run on a GCP &lt;code&gt;e2-standard-8 machine&lt;/code&gt; with 8 cores and 32 GB of RAM, using the scripts &lt;a href="https://github.com/alamb/datafusion-duckdb-benchmark"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As the industry moves towards data systems assembled from components, it is increasingly important that they exchange data using open standards such as &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; and &lt;a href="https://parquet.apache.org/"&gt;Parquet&lt;/a&gt; rather than custom storage and in-memory formats. Thus, this benchmark uses a single input Parquet file representative of many DataFusion users and aligned with the current trend in analytics of avoiding a costly load/transformation into a custom storage format prior to query.&lt;/p&gt;
&lt;p&gt;DataFusion now reaches near-DuckDB-speeds querying Parquet data. While we don&amp;rsquo;t plan to engage in a benchmarking shootout with a team that literally wrote &lt;a href="https://dl.acm.org/doi/abs/10.1145/3209950.3209955"&gt;Fair Benchmarking Considered Difficult&lt;/a&gt;, hopefully everyone can agree that DataFusion &lt;code&gt;28.0.0&lt;/code&gt; is a significant improvement.&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/datafusion_fast_grouping/full.png" width="700"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: Performance of DataFusion &lt;code&gt;27.0.0&lt;/code&gt;, DataFusion &lt;code&gt;28.0.0&lt;/code&gt;, and DuckDB &lt;code&gt;0.8.1&lt;/code&gt; on all 43 ClickBench queries against a single &lt;code&gt;hits.parquet&lt;/code&gt; file. Lower is better.&lt;/p&gt;
&lt;h3&gt;Notes&lt;/h3&gt;
&lt;p&gt;DataFusion &lt;code&gt;27.0.0&lt;/code&gt; was not able to run several queries due to either planner bugs (Q9, Q11, Q12, 14) or running out of memory (Q33). DataFusion &lt;code&gt;28.0.0&lt;/code&gt; solves those issues.&lt;/p&gt;
&lt;p&gt;DataFusion is faster than DuckDB for query 21 and 22, likely due to optimized implementations of string pattern matching.&lt;/p&gt;
&lt;h2&gt;Conclusion: performance matters&lt;/h2&gt;
&lt;p&gt;Improving aggregation performance by more than a factor of two allows developers building products and projects with DataFusion to spend more time on value-added domain specific features. We believe building systems with DataFusion is much faster than trying to build something similar from scratch. DataFusion increases productivity because it eliminates the need to rebuild well-understood, but costly to implement, analytic database technology. While we&amp;rsquo;re pleased with the improvements in DataFusion &lt;code&gt;28.0.0&lt;/code&gt;, we are by no means done and are pursuing &lt;a href="https://github.com/apache/arrow-datafusion/issues/7000"&gt;(Even More) Aggregation Performance&lt;/a&gt;. The future for performance is bright.&lt;/p&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;DataFusion is a &lt;a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html"&gt;community effort&lt;/a&gt; and this work was not possible without contributions from many in the community. A special shout out to &lt;a href="https://github.com/sunchao"&gt;sunchao&lt;/a&gt;, &lt;a href="https://github.com/jyshen"&gt;yjshen&lt;/a&gt;, &lt;a href="https://github.com/yahoNanJing"&gt;yahoNanJing&lt;/a&gt;, &lt;a href="https://github.com/mingmwang"&gt;mingmwang&lt;/a&gt;, &lt;a href="https://github.com/ozankabak"&gt;ozankabak&lt;/a&gt;, &lt;a href="https://github.com/mustafasrepo"&gt;mustafasrepo&lt;/a&gt;, and everyone else who contributed ideas, reviews, and encouragement &lt;a href="https://github.com/apache/arrow-datafusion/pull/6800"&gt;during&lt;/a&gt; this &lt;a href="https://github.com/apache/arrow-datafusion/pull/6904"&gt;work&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;About DataFusion&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query engine and database toolkit, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that uses &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion, along with &lt;a href="https://calcite.apache.org/"&gt;Apache Calcite&lt;/a&gt;, Facebook&amp;rsquo;s &lt;a href="https://github.com/facebookincubator/velox"&gt;Velox&lt;/a&gt;, and similar technology are part of the next generation &amp;ldquo;&lt;a href="https://www.usenix.org/publications/login/winter2018/khurana"&gt;Deconstructed Database&lt;/a&gt;&amp;rdquo; architectures, where new systems are built on a foundation of fast, modular components, rather than as a single tightly integrated system.&lt;/p&gt;
&lt;!-- Footnotes themselves at the bottom. --&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;[^1]: &lt;code&gt;SELECT COUNT(*) FROM 'hits.parquet';&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[^2]: &lt;code&gt;SELECT COUNT(DISTINCT "UserID") as num_users FROM 'hits.parquet';&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[^3]: &lt;code&gt;SELECT COUNT(DISTINCT "SearchPhrase") as num_phrases FROM 'hits.parquet';&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[^4]: &lt;code&gt;SELECT COUNT(*) FROM (SELECT DISTINCT "UserID", "SearchPhrase" FROM 'hits.parquet')&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[^5]: Full script at &lt;a href="https://github.com/alamb/datafusion-duckdb-benchmark/blob/main/hash.py"&gt;hash.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[^6]: &lt;a href="https://datasets.clickhouse.com/hits_compatible/athena_partitioned/hits_%7B%7D.parquet"&gt;hits_0.parquet&lt;/a&gt;, one of the files from the partitioned ClickBench dataset, which has &lt;code&gt;100,000&lt;/code&gt; rows and is 117 MB in size. The entire dataset has &lt;code&gt;100,000,000&lt;/code&gt; rows in a single 14 GB Parquet file. The script did not complete on the entire dataset after 40 minutes, and used 212 GB RAM at peak.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 26.0.0</title><link href="https://datafusion.apache.org/blog/2023/06/24/datafusion-25.0.0" rel="alternate"></link><published>2023-06-24T00:00:00+00:00</published><updated>2023-06-24T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2023-06-24:/blog/2023/06/24/datafusion-25.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;It has been a whirlwind 6 months of DataFusion development since &lt;a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0"&gt;our
last update&lt;/a&gt;: the community has grown, many features have been added,
performance improved and we are &lt;a href="https://github.com/apache/arrow-datafusion/discussions/6475"&gt;discussing&lt;/a&gt; branching out to our own
top level Apache Project.&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query engine and database
toolkit …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;It has been a whirlwind 6 months of DataFusion development since &lt;a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0"&gt;our
last update&lt;/a&gt;: the community has grown, many features have been added,
performance improved and we are &lt;a href="https://github.com/apache/arrow-datafusion/discussions/6475"&gt;discussing&lt;/a&gt; branching out to our own
top level Apache Project.&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query engine and database
toolkit, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory
format.&lt;/p&gt;
&lt;p&gt;DataFusion, along with &lt;a href="https://calcite.apache.org"&gt;Apache Calcite&lt;/a&gt;, Facebook's &lt;a href="https://github.com/facebookincubator/velox"&gt;Velox&lt;/a&gt; and
similar technology are part of the next generation "&lt;a href="https://www.usenix.org/publications/login/winter2018/khurana"&gt;Deconstructed
Database&lt;/a&gt;" architectures, where new systems are built on a foundation
of fast, modular components, rather as a single tightly integrated
system.&lt;/p&gt;
&lt;p&gt;While single tightly integrated systems such as &lt;a href="https://spark.apache.org/"&gt;Spark&lt;/a&gt;, &lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt; and
&lt;a href="https://www.pola.rs/"&gt;Pola.rs&lt;/a&gt; are great pieces of technology, our community believes that
anyone developing new data heavy application, such as those common in
machine learning in the next 5 years, will &lt;strong&gt;require&lt;/strong&gt; a high
performance, vectorized, query engine to remain relevant. The only
practical way to gain access to such technology without investing many
millions of dollars to build a new tightly integrated engine, is
though open source projects like DataFusion and similar enabling
technologies such as &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; and &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion is targeted primarily at developers creating other data
intensive analytics, and offers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High performance, native, parallel streaming execution engine&lt;/li&gt;
&lt;li&gt;Mature &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html"&gt;SQL support&lt;/a&gt;, featuring  subqueries, window functions, grouping sets, and more&lt;/li&gt;
&lt;li&gt;Built in support for Parquet, Avro, CSV, JSON and Arrow formats and easy extension for others&lt;/li&gt;
&lt;li&gt;Native DataFrame API and &lt;a href="https://arrow.apache.org/datafusion-python/"&gt;python bindings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.rs/datafusion/latest/datafusion/index.html"&gt;Well documented&lt;/a&gt; source code and architecture, designed to be customized to suit downstream project needs&lt;/li&gt;
&lt;li&gt;High quality, easy to use code &lt;a href="https://crates.io/crates/datafusion/versions"&gt;released every 2 weeks to crates.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Welcoming, open community, governed by the highly regarded and well understood &lt;a href="https://www.apache.org/"&gt;Apache Software Foundation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The rest of this post highlights some of the improvements we have made
to DataFusion over the last 6 months and a preview of where we are
heading. You can see a list of all changes in the detailed
&lt;a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;(Even) Better Performance&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://voltrondata.com/resources/speeds-and-feeds-hardware-and-software-matter"&gt;Various&lt;/a&gt; benchmarks show DataFusion to be quite close or &lt;a href="https://github.com/tustvold/access-log-bench"&gt;even
faster&lt;/a&gt; to the state of the art in analytic performance (at the moment
this seems to be DuckDB). We continually work on improving performance
(see &lt;a href="https://github.com/apache/arrow-datafusion/issues/5546"&gt;#5546&lt;/a&gt; for a list) and would love additional help in this area.&lt;/p&gt;
&lt;p&gt;DataFusion now reads single large Parquet files significantly faster by
&lt;a href="https://github.com/apache/arrow-datafusion/pull/5057"&gt;parallelizing across multiple cores&lt;/a&gt;. Native speeds for reading JSON
and CSV files are also up to 2.5x faster thanks to improvements
upstream in arrow-rs &lt;a href="https://github.com/apache/arrow-rs/pull/3479#issuecomment-1384353159"&gt;JSON reader&lt;/a&gt; and &lt;a href="https://github.com/apache/arrow-rs/pull/3365"&gt;CSV reader&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, we have integrated the &lt;a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/"&gt;arrow-rs Row Format&lt;/a&gt; into DataFusion resulting in up to &lt;a href="https://github.com/apache/arrow-datafusion/pull/6163"&gt;2-3x faster sorting and merging&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Improved Documentation and Website&lt;/h2&gt;
&lt;p&gt;Part of growing the DataFusion community is ensuring that DataFusion's
features are understood and that it is easy to contribute and
participate. To that end the &lt;a href="https://arrow.apache.org/datafusion/"&gt;website&lt;/a&gt; has been cleaned up, &lt;a href="https://docs.rs/datafusion/latest/datafusion/index.html#architecture"&gt;the
architecture guide&lt;/a&gt; expanded, the &lt;a href="https://arrow.apache.org/datafusion/contributor-guide/roadmap.html"&gt;roadmap&lt;/a&gt; updated, and several
overview talks created:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apr 2023 &lt;em&gt;Query Engine&lt;/em&gt;: &lt;a href="https://youtu.be/NVKujPxwSBA"&gt;recording&lt;/a&gt; and &lt;a href="https://docs.google.com/presentation/d/1D3GDVas-8y0sA4c8EOgdCvEjVND4s2E7I6zfs67Y4j8/edit#slide=id.p"&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;April 2023 &lt;em&gt;Logical Plan and Expressions&lt;/em&gt;: &lt;a href="https://youtu.be/EzZTLiSJnhY"&gt;recording&lt;/a&gt; and &lt;a href="https://docs.google.com/presentation/d/1ypylM3-w60kVDW7Q6S99AHzvlBgciTdjsAfqNP85K30"&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;April 2023 &lt;em&gt;Physical Plan and Execution&lt;/em&gt;: &lt;a href="https://youtu.be/2jkWU3_w6z0"&gt;recording&lt;/a&gt; and &lt;a href="https://docs.google.com/presentation/d/1cA2WQJ2qg6tx6y4Wf8FH2WVSm9JQ5UgmBWATHdik0hg"&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;New Features&lt;/h2&gt;
&lt;h3&gt;More Streaming, Less Memory&lt;/h3&gt;
&lt;p&gt;We have made significant progress on the &lt;a href="https://github.com/apache/arrow-datafusion/issues/4285"&gt;streaming execution roadmap&lt;/a&gt;
such as &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html#method.unbounded_output"&gt;unbounded datasources&lt;/a&gt;, &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/aggregates/enum.GroupByOrderMode.html"&gt;streaming group by&lt;/a&gt;, sophisticated
&lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/global_sort_selection/index.html"&gt;sort&lt;/a&gt; and &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/repartition/index.html"&gt;repartitioning&lt;/a&gt; improvements in the optimizer, and support
for &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/joins/struct.SymmetricHashJoinExec.html"&gt;symmetric hash join&lt;/a&gt; (read more about that in the great &lt;a href="https://www.synnada.ai/blog/general-purpose-stream-joins-via-pruning-symmetric-hash-joins"&gt;Synnada
Blog Post&lt;/a&gt; on the topic). Together, these features both 1) make it
easier to build streaming systems using DataFusion that can
incrementally generate output before (or ever) seeing the end of the
input and 2) allow general queries to use less memory and generate their
results faster.&lt;/p&gt;
&lt;p&gt;We have also improved the runtime &lt;a href="https://docs.rs/datafusion/latest/datafusion/execution/memory_pool/index.html"&gt;memory management&lt;/a&gt; system so that
DataFusion now stays within its declared memory budget &lt;a href="https://github.com/apache/arrow-datafusion/issues/3941"&gt;generate
runtime errors&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;DML Support (&lt;code&gt;INSERT&lt;/code&gt;, &lt;code&gt;DELETE&lt;/code&gt;, &lt;code&gt;UPDATE&lt;/code&gt;, etc)&lt;/h3&gt;
&lt;p&gt;Part of building high performance data systems includes writing data,
and DataFusion supports several features for creating new files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;INSERT INTO&lt;/code&gt; and &lt;code&gt;SELECT ... INTO&lt;/code&gt; support for memory backed and CSV tables&lt;/li&gt;
&lt;li&gt;New &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/insert/trait.DataSink.html"&gt;API for writing data into TableProviders&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are working on easier to use &lt;a href="https://github.com/apache/arrow-datafusion/issues/5654"&gt;COPY INTO&lt;/a&gt; syntax, better support
for writing parquet, JSON, and AVRO, and more -- see our &lt;a href="https://github.com/apache/arrow-datafusion/issues/6569"&gt;tracking epic&lt;/a&gt;
for more details.&lt;/p&gt;
&lt;h3&gt;Timestamp and Intervals&lt;/h3&gt;
&lt;p&gt;One mark of the maturity of a SQL engine is how it handles the tricky
world of timestamp, date, times and interval arithmetic. DataFusion is
feature complete in this area and behaves as you would expect,
supporting queries such as&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT now() + '1 month' FROM my_table;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still have a long tail of &lt;a href="https://github.com/apache/arrow-datafusion/issues/3148"&gt;date and time improvements&lt;/a&gt;, which we are working on as well.&lt;/p&gt;
&lt;h3&gt;Querying Structured Types (&lt;code&gt;List&lt;/code&gt; and &lt;code&gt;Struct&lt;/code&gt;s)&lt;/h3&gt;
&lt;p&gt;Arrow and Parquet &lt;a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/"&gt;support nested data&lt;/a&gt; well and DataFusion lets you
easily query such &lt;code&gt;Struct&lt;/code&gt; and &lt;code&gt;List&lt;/code&gt;. For example, you can use
DataFusion to read and query the &lt;a href="https://data.mendeley.com/datasets/ct8f9skv97"&gt;JSON Datasets for Exploratory OLAP -
Mendeley Data&lt;/a&gt; like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;----------
-- Explore structured data using SQL
----------
SELECT delete FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL limit 10;
+---------------------------------------------------------------------------------------------------------------------------+
| delete                                                                                                                    |
+---------------------------------------------------------------------------------------------------------------------------+
| {status: {id: {$numberLong: 135037425050320896}, id_str: 135037425050320896, user_id: 334902461, user_id_str: 334902461}} |
| {status: {id: {$numberLong: 134703982051463168}, id_str: 134703982051463168, user_id: 405383453, user_id_str: 405383453}} |
| {status: {id: {$numberLong: 134773741740765184}, id_str: 134773741740765184, user_id: 64823441, user_id_str: 64823441}}   |
| {status: {id: {$numberLong: 132543659655704576}, id_str: 132543659655704576, user_id: 45917834, user_id_str: 45917834}}   |
| {status: {id: {$numberLong: 133786431926697984}, id_str: 133786431926697984, user_id: 67229952, user_id_str: 67229952}}   |
| {status: {id: {$numberLong: 134619093570560002}, id_str: 134619093570560002, user_id: 182430773, user_id_str: 182430773}} |
| {status: {id: {$numberLong: 134019857527214080}, id_str: 134019857527214080, user_id: 257396311, user_id_str: 257396311}} |
| {status: {id: {$numberLong: 133931546469076993}, id_str: 133931546469076993, user_id: 124539548, user_id_str: 124539548}} |
| {status: {id: {$numberLong: 134397743350296576}, id_str: 134397743350296576, user_id: 139836391, user_id_str: 139836391}} |
| {status: {id: {$numberLong: 127833661767823360}, id_str: 127833661767823360, user_id: 244442687, user_id_str: 244442687}} |
+---------------------------------------------------------------------------------------------------------------------------+

----------
-- Select some deeply nested fields
----------
SELECT
  delete['status']['id']['$numberLong'] as delete_id,
  delete['status']['user_id'] as delete_user_id
FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL LIMIT 10;

+--------------------+----------------+
| delete_id          | delete_user_id |
+--------------------+----------------+
| 135037425050320896 | 334902461      |
| 134703982051463168 | 405383453      |
| 134773741740765184 | 64823441       |
| 132543659655704576 | 45917834       |
| 133786431926697984 | 67229952       |
| 134619093570560002 | 182430773      |
| 134019857527214080 | 257396311      |
| 133931546469076993 | 124539548      |
| 134397743350296576 | 139836391      |
| 127833661767823360 | 244442687      |
+--------------------+----------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Subqueries All the Way Down&lt;/h3&gt;
&lt;p&gt;DataFusion can run many different subqueries by rewriting them to
joins. It has been able to run the full suite of TPC-H queries for at
least the last year, but recently we have implemented significant
improvements to this logic, sufficient to run almost all queries in
the TPC-DS benchmark as well.&lt;/p&gt;
&lt;h2&gt;Community and Project Growth&lt;/h2&gt;
&lt;p&gt;The six months since &lt;a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0"&gt;our last update&lt;/a&gt; saw significant growth in
the DataFusion community. Between versions &lt;code&gt;17.0.0&lt;/code&gt; and &lt;code&gt;26.0.0&lt;/code&gt;,
DataFusion merged 711 PRs from 107 distinct contributors, not
including all the work that goes into our core dependencies such as
&lt;a href="https://crates.io/crates/arrow"&gt;arrow&lt;/a&gt;,
&lt;a href="https://crates.io/crates/parquet"&gt;parquet&lt;/a&gt;, and
&lt;a href="https://crates.io/crates/object_store"&gt;object_store&lt;/a&gt;, that much of
the same community helps support.&lt;/p&gt;
&lt;p&gt;In addition, we have added 7 new committers and 1 new PMC member to
the Apache Arrow project, largely focused on DataFusion, and we
learned about some of the cool &lt;a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#known-users"&gt;new systems&lt;/a&gt; which are using
DataFusion. Given the growth of the community and interest in the
project, we also clarified the &lt;a href="https://github.com/apache/arrow-datafusion/discussions/6441"&gt;mission statement&lt;/a&gt; and are
&lt;a href="https://github.com/apache/arrow-datafusion/discussions/6475"&gt;discussing&lt;/a&gt; "graduate"ing DataFusion to a new top level
Apache Software Foundation project.&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 17.0.0..26.0.0 . | wc -l
     711

$ git shortlog -sn 17.0.0..26.0.0 . | wc -l
      107
--&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;Kudos to everyone in the community who has contributed ideas,
discussions, bug reports, documentation and code. It is exciting to be
innovating on the next generation of database architectures together!&lt;/p&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to
have you join us. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check out our &lt;a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html"&gt;Communication Doc&lt;/a&gt; for more ways to engage with the
community.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 16.0.0 Project Update</title><link href="https://datafusion.apache.org/blog/2023/01/19/datafusion-16.0.0" rel="alternate"></link><published>2023-01-19T00:00:00+00:00</published><updated>2023-01-19T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2023-01-19:/blog/2023/01/19/datafusion-16.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible
query execution framework, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;,
that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its
in-memory format. It is targeted primarily at developers creating data
intensive analytics, and offers mature
&lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html"&gt;SQL support&lt;/a&gt;,
a DataFrame API, and many extension points.&lt;/p&gt;
&lt;p&gt;Systems based on DataFusion perform very well in benchmarks …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible
query execution framework, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;,
that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its
in-memory format. It is targeted primarily at developers creating data
intensive analytics, and offers mature
&lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html"&gt;SQL support&lt;/a&gt;,
a DataFrame API, and many extension points.&lt;/p&gt;
&lt;p&gt;Systems based on DataFusion perform very well in benchmarks,
especially considering they operate directly on parquet files rather
than first loading into a specialized format.  Some recent highlights
include &lt;a href="https://benchmark.clickhouse.com/"&gt;clickbench&lt;/a&gt; and the
&lt;a href="https://www.cloudfuse.io/dashboards/standalone-engines"&gt;Cloudfuse.io standalone query
engines&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;DataFusion is also part of a longer term trend, articulated clearly by
&lt;a href="http://www.cs.cmu.edu/~pavlo/"&gt;Andy Pavlo&lt;/a&gt; in his &lt;a href="https://ottertune.com/blog/2022-databases-retrospective/"&gt;2022 Databases
Retrospective&lt;/a&gt;.
Database frameworks are proliferating and it is likely that all OLAP
DBMSs and other data heavy applications, such as machine learning,
will &lt;strong&gt;require&lt;/strong&gt; a vectorized, highly performant query engine in the next
5 years to remain relevant.  The only practical way to make such
technology so widely available without many millions of dollars of
investment is though open source engine such as DataFusion or
&lt;a href="https://github.com/facebookincubator/velox"&gt;Velox&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The rest of this post describes the improvements made to DataFusion
over the last three months and some hints of where we are heading.&lt;/p&gt;
&lt;h2&gt;Community Growth&lt;/h2&gt;
&lt;p&gt;We again saw significant growth in the DataFusion community since &lt;a href="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/"&gt;our last update&lt;/a&gt;. There are some interesting metrics on &lt;a href="https://ossrank.com/p/1573-apache-arrow-datafusion"&gt;OSSRank&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The DataFusion 16.0.0 release consists of 543 PRs from 73 distinct contributors, not including all the work that goes into dependencies such as &lt;a href="https://crates.io/crates/arrow"&gt;arrow&lt;/a&gt;, &lt;a href="https://crates.io/crates/parquet"&gt;parquet&lt;/a&gt;, and &lt;a href="https://crates.io/crates/object_store"&gt;object_store&lt;/a&gt;, that much of the same community helps support. Thank you all for your help&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 13.0.0..16.0.0 . | wc -l
     543

$ git shortlog -sn 13.0.0..16.0.0 . | wc -l
      73
--&gt;
&lt;p&gt;Several &lt;a href="https://github.com/apache/arrow-datafusion#known-uses"&gt;new systems based on DataFusion&lt;/a&gt; were recently added:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/GreptimeTeam/greptimedb"&gt;Greptime DB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://synnada.ai/"&gt;Synnada&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PRQL/prql-query"&gt;PRQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/parseablehq/parseable"&gt;Parseable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/splitgraph/seafowl"&gt;SeaFowl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Performance 🚀&lt;/h2&gt;
&lt;p&gt;Performance and efficiency are core values for
DataFusion. While there is still a gap between DataFusion and the best of
breed, tightly integrated systems such as &lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt;
and &lt;a href="https://www.pola.rs/"&gt;Polars&lt;/a&gt;, DataFusion is
closing the gap quickly. Performance highlights from the last three
months:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Up to 30% Faster Sorting and Merging using the new &lt;a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/"&gt;Row Format&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"&gt;Advanced predicate pushdown&lt;/a&gt;, directly on parquet, directly from object storage, enabling sub millisecond filtering. &lt;!-- Andrew nots: we should really get this turned on by default --&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;70%&lt;/code&gt; faster &lt;code&gt;IN&lt;/code&gt; expressions evaluation (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4057"&gt;#4057&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Sort and partition aware optimizations (&lt;a href="https://github.com/apache/arrow-datafusion/issues/3969"&gt;#3969&lt;/a&gt; and  &lt;a href="https://github.com/apache/arrow-datafusion/issues/4691"&gt;#4691&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Filter selectivity analysis (&lt;a href="https://github.com/apache/arrow-datafusion/issues/3868"&gt;#3868&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Runtime Resource Limits&lt;/h2&gt;
&lt;p&gt;Previously, DataFusion could potentially use unbounded amounts of memory for certain queries that included Sorts, Grouping or Joins.&lt;/p&gt;
&lt;p&gt;In version 16.0.0, it is possible to limit DataFusion's memory usage for Sorting and Grouping. We are looking for help adding similar limiting for Joins as well as expanding our algorithms to optionally spill to secondary storage. See &lt;a href="https://github.com/apache/arrow-datafusion/issues/3941"&gt;#3941&lt;/a&gt; for more detail.&lt;/p&gt;
&lt;h2&gt;SQL Window Functions&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Window_function_(SQL)"&gt;SQL Window Functions&lt;/a&gt; are useful for a variety of analysis and DataFusion's implementation support expanded significantly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Custom window frames such as &lt;code&gt;... OVER (ORDER BY ... RANGE BETWEEN 0.2 PRECEDING AND 0.2 FOLLOWING)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Unbounded window frames such as &lt;code&gt;... OVER (ORDER BY ... RANGE UNBOUNDED ROWS PRECEDING)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Support for the &lt;code&gt;NTILE&lt;/code&gt; window function (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4676"&gt;#4676&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;GROUPS&lt;/code&gt; mode (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4155"&gt;#4155&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Improved Joins&lt;/h1&gt;
&lt;p&gt;Joins are often the most complicated operations to handle well in
analytics systems and DataFusion 16.0.0 offers significant improvements
such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cost based optimizer (CBO) automatically reorders join evaluations, selects algorithms (Merge / Hash), and pick build side based on available statistics and join type (&lt;code&gt;INNER&lt;/code&gt;, &lt;code&gt;LEFT&lt;/code&gt;, etc) (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4219"&gt;#4219&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Fast non &lt;code&gt;column=column&lt;/code&gt; equijoins such as &lt;code&gt;JOIN ON a.x + 5 = b.y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Better performance on non-equijoins (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4562"&gt;#4562&lt;/a&gt;) &lt;!-- TODO is this a good thing to mention as any time this is usd the query is going to go slow or the data size is small --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Streaming Execution&lt;/h1&gt;
&lt;p&gt;One emerging use case for Datafusion is as a foundation for
streaming-first data platforms. An important prerequisite
is support for incremental execution for queries that can be computed
incrementally.&lt;/p&gt;
&lt;p&gt;With this release, DataFusion now supports the following streaming features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data ingestion from infinite files such as FIFOs (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4694"&gt;#4694&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Detection of pipeline-breaking queries in streaming use cases (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4694"&gt;#4694&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Automatic input swapping for joins so probe side is a data stream (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4694"&gt;#4694&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Intelligent elision of pipeline-breaking sort operations whenever possible (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4691"&gt;#4691&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Incremental execution for more types of queries; e.g. queries involving finite window frames (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4777"&gt;#4777&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are a major steps forward, and we plan even more improvements over the next few releases.&lt;/p&gt;
&lt;h1&gt;Better Support for Distributed Catalogs&lt;/h1&gt;
&lt;p&gt;16.0.0 has been enhanced support for asynchronous catalogs (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4607"&gt;#4607&lt;/a&gt;)
to better support distributed metadata stores such as
&lt;a href="https://delta.io/"&gt;Delta.io&lt;/a&gt; and &lt;a href="https://iceberg.apache.org/"&gt;Apache
Iceberg&lt;/a&gt; which require asynchronous I/O
during planning to access remote catalogs. Previously, DataFusion
required synchronous access to all relevant catalog information.&lt;/p&gt;
&lt;h1&gt;Additional SQL Support&lt;/h1&gt;
&lt;p&gt;SQL support continues to improve, including some of these highlights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add TPC-DS query planning regression tests &lt;a href="https://github.com/apache/arrow-datafusion/issues/4719"&gt;#4719&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;PREPARE&lt;/code&gt; statement &lt;a href="https://github.com/apache/arrow-datafusion/issues/4490"&gt;#4490&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Automatic coercions ast between Date and Timestamp &lt;a href="https://github.com/apache/arrow-datafusion/issues/4726"&gt;#4726&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support type coercion for timestamp and utf8 &lt;a href="https://github.com/apache/arrow-datafusion/issues/4312"&gt;#4312&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Full support for time32 and time64 literal values (&lt;code&gt;ScalarValue&lt;/code&gt;) &lt;a href="https://github.com/apache/arrow-datafusion/issues/4156"&gt;#4156&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;New functions, incuding &lt;code&gt;uuid()&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/4041"&gt;#4041&lt;/a&gt;, &lt;code&gt;current_time&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/4054"&gt;#4054&lt;/a&gt;, &lt;code&gt;current_date&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/4022"&gt;#4022&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Compressed CSV/JSON support &lt;a href="https://github.com/apache/arrow-datafusion/issues/3642"&gt;#3642&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The community has also invested in new &lt;a href="https://github.com/apache/arrow-datafusion/blob/master/datafusion/core/tests/sqllogictests/README.md"&gt;sqllogic based&lt;/a&gt; tests to keep improving DataFusion's quality with less effort.&lt;/p&gt;
&lt;h1&gt;Plan Serialization and Substrait&lt;/h1&gt;
&lt;p&gt;DataFusion now supports serialization of physical plans, with a custom protocol buffers format. In addition, we are adding initial support for &lt;a href="https://substrait.io/"&gt;Substrait&lt;/a&gt;, a Cross-Language Serialization for Relational Algebra&lt;/p&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together!&lt;/p&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to
have you join us. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is
&lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check out our &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;
&lt;h2&gt;Appendix: Contributor Shoutout&lt;/h2&gt;
&lt;p&gt;Here is a list of people who have contributed PRs to this project over the last three releases, derived from &lt;code&gt;git shortlog -sn 13.0.0..16.0.0 .&lt;/code&gt; Thank you all!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   113  Andrew Lamb
    58  jakevin
    46  Raphael Taylor-Davies
    30  Andy Grove
    19  Batuhan Taskaya
    19  Remzi Yang
    17  ygf11
    16  Burak
    16  Jeffrey
    16  Marco Neumann
    14  Kun Liu
    12  Yang Jiang
    10  mingmwang
     9  Dani&amp;euml;l Heres
     9  Mustafa akur
     9  comphead
     9  mvanschellebeeck
     9  xudong.w
     7  dependabot[bot]
     7  yahoNanJing
     6  Brent Gardner
     5  AssHero
     4  Jiayu Liu
     4  Wei-Ting Kuo
     4  askoa
     3  Andr&amp;eacute; Calado Coroado
     3  Jie Han
     3  Jon Mease
     3  Metehan Y&amp;inodot;ld&amp;inodot;r&amp;inodot;m
     3  Nga Tran
     3  Ruihang Xia
     3  baishen
     2  Berkay &amp;Scedil;ahin
     2  Dan Harris
     2  Dongyan Zhou
     2  Eduard Karacharov
     2  Kikkon
     2  Liang-Chi Hsieh
     2  Marko Milenkovi&amp;cacute;
     2  Martin Grigorov
     2  Roman Nozdrin
     2  Tim Van Wassenhove
     2  r.4ntix
     2  unconsolable
     2  unvalley
     1  Ajaya Agrawal
     1  Alexander Spies
     1  ArkashaJavelin
     1  Artjoms Iskovs
     1  BoredPerson
     1  Christian Salvati
     1  Creampanda
     1  Data Psycho
     1  Francis Du
     1  Francis Le Roy
     1  LFC
     1  Marko Grujic
     1  Matt Willian
     1  Matthijs Brobbel
     1  Max Burke
     1  Mehmet Ozan Kabak
     1  Rito Takeuchi
     1  Roman Zeyde
     1  Vrishabh
     1  Zhang Li
     1  ZuoTiJia
     1  byteink
     1  cfraz89
     1  nbr
     1  xxchan
     1  yujie.zhang
     1  zembunia
     1  哇呜哇呜呀咦耶
&lt;/code&gt;&lt;/pre&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow Ballista 0.9.0 Release</title><link href="https://datafusion.apache.org/blog/2022/10/28/ballista-0.9.0" rel="alternate"></link><published>2022-10-28T00:00:00+00:00</published><updated>2022-10-28T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-10-28:/blog/2022/10/28/ballista-0.9.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/arrow-ballista"&gt;Ballista&lt;/a&gt; is an Arrow-native distributed SQL query engine implemented in Rust.&lt;/p&gt;
&lt;p&gt;Ballista 0.9.0 is now available and is the most significant release since the project was &lt;a href="http://arrow.apache.org/blog/2021/04/12/ballista-donation/"&gt;donated&lt;/a&gt; to Apache
Arrow in 2021.&lt;/p&gt;
&lt;p&gt;This release represents 4 weeks of work, with 66 commits from 14 contributors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    22  Andy …&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/arrow-ballista"&gt;Ballista&lt;/a&gt; is an Arrow-native distributed SQL query engine implemented in Rust.&lt;/p&gt;
&lt;p&gt;Ballista 0.9.0 is now available and is the most significant release since the project was &lt;a href="http://arrow.apache.org/blog/2021/04/12/ballista-donation/"&gt;donated&lt;/a&gt; to Apache
Arrow in 2021.&lt;/p&gt;
&lt;p&gt;This release represents 4 weeks of work, with 66 commits from 14 contributors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    22  Andy Grove
    12  yahoNanJing
     6  Dani&amp;euml;l Heres
     4  Brent Gardner
     4  dependabot[bot]
     4  r.4ntix
     3  Stefan Stanciulescu
     3  mingmwang
     2  Ken Suenobu
     2  Yang Jiang
     1  Metehan Y&amp;inodot;ld&amp;inodot;r&amp;inodot;m
     1  Trent Feda
     1  askoa
     1  yangzhong
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes
and improvements have been made: we refer you to the &lt;a href="https://github.com/apache/arrow-ballista/blob/0.9.0-rc2/ballista/CHANGELOG.md"&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Support for Cloud Object Stores and Distributed File Systems&lt;/h3&gt;
&lt;p&gt;This is the first release of Ballista to have documented support for querying data from distributed file systems and
object stores. Currently, S3 and HDFS are supported. Support for Google Cloud Storage and Azure Blob Storage is planned
for the next release.&lt;/p&gt;
&lt;h3&gt;Flight SQL &amp;amp; JDBC support&lt;/h3&gt;
&lt;p&gt;The Ballista scheduler now implements the &lt;a href="https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/"&gt;Flight SQL protocol&lt;/a&gt;, enabling any compliant Flight SQL client
to connect to and run queries against a Ballista cluster.&lt;/p&gt;
&lt;p&gt;The Apache Arrow Flight SQL JDBC driver can be used to connect Business Intelligence tools to a Ballista cluster.&lt;/p&gt;
&lt;h3&gt;Python Bindings&lt;/h3&gt;
&lt;p&gt;It is now possible to connect to a Ballista cluster from Python and execute queries using both the DataFrame and SQL
interfaces.&lt;/p&gt;
&lt;h3&gt;Scheduler Web User Interface and REST API&lt;/h3&gt;
&lt;p&gt;The scheduler now has a web user interface for monitoring queries. It is also possible to view graphical query plans
that show how the query was executed, along with metrics.&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/2022-10-28-ballista-web-ui.png" width="800"/&gt;&lt;/p&gt;
&lt;p&gt;The REST API that powers the user interface can also be accessed directly.&lt;/p&gt;
&lt;h3&gt;Simplified Kubernetes Deployment&lt;/h3&gt;
&lt;p&gt;Ballista now provides a &lt;a href="https://github.com/apache/arrow-ballista/tree/master/helm"&gt;Helm chart&lt;/a&gt; for simplified Kubernetes deployment.&lt;/p&gt;
&lt;h3&gt;User Guide&lt;/h3&gt;
&lt;p&gt;The user guide is published at &lt;a href="https://arrow.apache.org/ballista/"&gt;https://arrow.apache.org/ballista/&lt;/a&gt; and provides
deployment instructions for Docker, Docker Compose, and Kubernetes, as well as references for configuring and
tuning Ballista.&lt;/p&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;The Ballista community is currently focused on the following tasks for the next release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support for Azure Blob Storage and Google Cloud Storage&lt;/li&gt;
&lt;li&gt;Improve benchmark performance by implementing more query optimizations&lt;/li&gt;
&lt;li&gt;Improve scheduler web user interface&lt;/li&gt;
&lt;li&gt;Publish Docker images to GitHub Container Registry&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The detailed list of issues planned for the 0.10.0 release can be found in the &lt;a href="https://github.com/apache/arrow-ballista/issues/361"&gt;tracking issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;Ballista has a friendly community and we welcome contributions. A good place to start is to following the instructions
in the &lt;a href="https://arrow.apache.org/ballista/"&gt;user guide&lt;/a&gt; and try using Ballista with your own SQL queries and ETL pipelines, and file issues
for any bugs or feature suggestions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 13.0.0 Project Update</title><link href="https://datafusion.apache.org/blog/2022/10/25/datafusion-13.0.0" rel="alternate"></link><published>2022-10-25T00:00:00+00:00</published><updated>2022-10-25T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-10-25:/blog/2022/10/25/datafusion-13.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; &lt;a href="https://crates.io/crates/datafusion"&gt;&lt;code&gt;13.0.0&lt;/code&gt;&lt;/a&gt; is released, and this blog contains an update on the project for the 5 months since our &lt;a href="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/"&gt;last update in May 2022&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; &lt;a href="https://crates.io/crates/datafusion"&gt;&lt;code&gt;13.0.0&lt;/code&gt;&lt;/a&gt; is released, and this blog contains an update on the project for the 5 months since our &lt;a href="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/"&gt;last update in May 2022&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL processes, and database systems. You may want to check out DataFusion to extend your Rust project to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support &lt;a href="https://docs.rs/datafusion/13.0.0/datafusion/dataframe/struct.DataFrame.html"&gt;DataFrame API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support a Domain Specific Query Language&lt;/li&gt;
&lt;li&gt;Easily and quickly read and process Parquet, JSON, Avro or CSV data.&lt;/li&gt;
&lt;li&gt;Read from remote object stores such as AWS S3, Azure Blob Storage, GCP.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even though DataFusion is 4 years "young," it has seen significant community growth in the last few months and the momentum continues to accelerate.&lt;/p&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;DataFusion is used as the engine in &lt;a href="https://github.com/apache/arrow-datafusion#known-uses"&gt;many open source and commercial projects&lt;/a&gt; and was one of the early open source projects to provide this capability. 2022 has validated our belief in the need for such a &lt;a href="https://docs.google.com/presentation/d/1iNX_35sWUakee2q3zMFPyHE4IV2nC3lkCK_H6Y2qK84/edit#slide=id.p"&gt;"LLVM for database and AI systems"&lt;/a&gt;&lt;a href="https://www.slideshare.net/AndrewLamb32/20220623-apache-arrow-and-datafusion-changing-the-game-for-implementing-database-systemspdf"&gt;(alternate link)&lt;/a&gt; with announcements such as the &lt;a href="https://engineering.fb.com/2022/08/31/open-source/velox/"&gt;release of FaceBook's Velox&lt;/a&gt; engine, the major investments in &lt;a href="https://arrow.apache.org/docs/cpp/streaming_execution.html"&gt;Acero&lt;/a&gt; as well as the continued popularity of &lt;a href="https://calcite.apache.org/"&gt;Apache Calcite&lt;/a&gt; and other similar technologies.&lt;/p&gt;
&lt;p&gt;While Velox and Acero focus on execution engines, DataFusion provides the entire suite of components needed to build most analytic systems, including a SQL frontend, a dataframe API, and  extension points for just about everything. Some &lt;a href="https://github.com/apache/arrow-datafusion#known-uses"&gt;DataFusion users&lt;/a&gt; use a subset of the features such as the frontend (e.g. &lt;a href="https://dask-sql.readthedocs.io/en/latest/"&gt;dask-sql&lt;/a&gt;) or the execution engine, (e.g.  &lt;a href="https://github.com/blaze-init/blaze"&gt;Blaze&lt;/a&gt;), and some use many different components to build both SQL based and customized DSL based systems such as &lt;a href="https://github.com/influxdata/influxdb_iox/pulls"&gt;InfluxDB IOx&lt;/a&gt; and &lt;a href="https://github.com/vegafusion/vegafusion"&gt;VegaFusion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of DataFusion&amp;rsquo;s advantages is its implementation in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; and thus its easy integration with the broader Rust ecosystem. Rust continues to be a major source of benefit, from the &lt;a href="https://www.influxdata.com/blog/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/"&gt;ease of parallelization with the high quality and standardized &lt;code&gt;async&lt;/code&gt; ecosystem&lt;/a&gt; , as well as its modern dependency management system and wonderful performance. &lt;!-- I wonder if we should link to clickbench?? --&gt;&lt;/p&gt;
&lt;!--While we haven’t invested in the benchmarking ratings game datafusion continues to be quite speedy (todo quantity this, with some evidence) – maybe clickbench?--&gt;
&lt;!--
Maybe we can do this un a future post
# DataFusion in Action

While DataFusion really shines as an embeddable query engine, if you want to try it out and get a feel for its power, you can use the basic[`datafusion-cli`](https://docs.rs/datafusion-cli/13.0.0/datafusion_cli/) tool to get a sense for what is possible to add in your application

(TODO example here of using datafusion-cli to query from local parquet files on disk)

TODO: also mention you can use the same thing to query data from S3
--&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;We have increased the frequency of DataFusion releases to monthly instead of quarterly. This
makes it easier for the increasing number of projects that now depend on DataFusion.&lt;/p&gt;
&lt;p&gt;We have also completed the "graduation" of &lt;a href="https://github.com/apache/arrow-ballista"&gt;Ballista to its own top-level arrow-ballista repository&lt;/a&gt;
which decouples the two projects and allows each project to move even faster.&lt;/p&gt;
&lt;p&gt;Along with numerous other bug fixes and smaller improvements, here are some of the major advances:&lt;/p&gt;
&lt;h1&gt;Improved Support for Cloud Object Stores&lt;/h1&gt;
&lt;p&gt;DataFusion now supports many major cloud object stores (Amazon S3, Azure Blob Storage, and Google Cloud Storage) "out of the box" via the &lt;a href="https://crates.io/crates/object_store"&gt;object_store&lt;/a&gt; crate. Using this integration, DataFusion optimizes reading parquet files by reading only the parts of the files that are needed.&lt;/p&gt;
&lt;h2&gt;Advanced SQL&lt;/h2&gt;
&lt;p&gt;DataFusion now supports correlated subqueries, by rewriting them as joins. See the &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/subqueries.html"&gt;Subquery&lt;/a&gt; page in the User Guide for more information.&lt;/p&gt;
&lt;p&gt;In addition to numerous other small improvements, the following SQL features are now supported:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ROWS&lt;/code&gt;, &lt;code&gt;RANGE&lt;/code&gt;, &lt;code&gt;PRECEDING&lt;/code&gt; and &lt;code&gt;FOLLOWING&lt;/code&gt; in &lt;code&gt;OVER&lt;/code&gt; clauses &lt;a href="https://github.com/apache/arrow-datafusion/issues/3570"&gt;#3570&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ROLLUP&lt;/code&gt; and &lt;code&gt;CUBE&lt;/code&gt; grouping set expressions  &lt;a href="https://github.com/apache/arrow-datafusion/issues/2446"&gt;#2446&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SUM DISTINCT&lt;/code&gt; aggregate support  &lt;a href="https://github.com/apache/arrow-datafusion/issues/2405"&gt;#2405&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IN&lt;/code&gt; and &lt;code&gt;NOT IN&lt;/code&gt; Subqueries by rewriting them to &lt;code&gt;SEMI&lt;/code&gt; / &lt;code&gt;ANTI&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2885"&gt;#2421&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Non equality predicates in  &lt;code&gt;ON&lt;/code&gt; clause of  &lt;code&gt;LEFT&lt;/code&gt;, &lt;code&gt;RIGHT,&lt;/code&gt;and &lt;code&gt;FULL&lt;/code&gt; joins &lt;a href="https://github.com/apache/arrow-datafusion/issues/2591"&gt;#2591&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Exact &lt;code&gt;MEDIAN&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3009"&gt;#3009&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GROUPING SETS&lt;/code&gt;/&lt;code&gt;CUBE&lt;/code&gt;/&lt;code&gt;ROLLUP&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2716"&gt;#2716&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;More DDL Support&lt;/h1&gt;
&lt;p&gt;Just as it is important to query, it is also important to give users the ability to define their data sources. We have added:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CREATE VIEW&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2279"&gt;#2279&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DESCRIBE &amp;lt;table&amp;gt;&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2642"&gt;#2642&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Custom / Dynamic table provider factories &lt;a href="https://github.com/apache/arrow-datafusion/issues/3311"&gt;#3311&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SHOW CREATE TABLE&lt;/code&gt; for support for views &lt;a href="https://github.com/apache/arrow-datafusion/issues/2830"&gt;#2830&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Faster Execution&lt;/h1&gt;
&lt;p&gt;Performance is always an important goal for DataFusion, and there are a number of significant new optimizations such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimizations of TopK (queries with a &lt;code&gt;LIMIT&lt;/code&gt; or &lt;code&gt;OFFSET&lt;/code&gt; clause):  &lt;a href="https://github.com/apache/arrow-datafusion/issues/3527"&gt;#3527&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/2521"&gt;#2521&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reduce &lt;code&gt;left&lt;/code&gt;/&lt;code&gt;right&lt;/code&gt;/&lt;code&gt;full&lt;/code&gt; joins to &lt;code&gt;inner&lt;/code&gt; join &lt;a href="https://github.com/apache/arrow-datafusion/issues/2750"&gt;#2750&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convert  cross joins to inner joins when possible &lt;a href="https://github.com/apache/arrow-datafusion/issues/3482"&gt;#3482&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sort preserving &lt;code&gt;SortMergeJoin&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2699"&gt;#2699&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improvements in group by and sort performance &lt;a href="https://github.com/apache/arrow-datafusion/issues/2375"&gt;#2375&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adaptive &lt;code&gt;regex_replace&lt;/code&gt; implementation &lt;a href="https://github.com/apache/arrow-datafusion/issues/3518"&gt;#3518&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Optimizer Enhancements&lt;/h1&gt;
&lt;p&gt;Internally the optimizer has been significantly enhanced as well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Casting / coercion now happens during logical planning &lt;a href="https://github.com/apache/arrow-datafusion/issues/3396"&gt;#3185&lt;/a&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3636"&gt;#3636&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More sophisticated expression analysis and simplification is available&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Parquet&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The parquet reader can now read directly from parquet files on remote object storage &lt;a href="https://github.com/apache/arrow-datafusion/issues/2677"&gt;#2489&lt;/a&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3051"&gt;#3051&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Experimental support for &amp;ldquo;predicate pushdown&amp;rdquo; with late materialization after filtering during the scan (another blog post on this topic is coming soon).&lt;/li&gt;
&lt;li&gt;Support reading directly from AWS S3 and other object stores via &lt;code&gt;datafusion-cli&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3631"&gt;#3631&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;DataType Support&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Support for &lt;code&gt;TimestampTz&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3660"&gt;#3660&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Expanded support for the &lt;code&gt;Decimal&lt;/code&gt; type, including  &lt;code&gt;IN&lt;/code&gt; list and better built in coercion.&lt;/li&gt;
&lt;li&gt;Expanded support for date/time manipulation such as  &lt;code&gt;date_bin&lt;/code&gt; built-in function , timestamp &lt;code&gt;+/-&lt;/code&gt; interval, &lt;code&gt;TIME&lt;/code&gt; literal values &lt;a href="https://github.com/apache/arrow-datafusion/issues/3010"&gt;#3010&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3110"&gt;#3110&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3034"&gt;#3034&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Binary operations (&lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;XOR&lt;/code&gt;, etc):  &lt;a href="https://github.com/apache/arrow-datafusion/issues/1619"&gt;#3037&lt;/a&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3430"&gt;#3420&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IS TRUE/FALSE&lt;/code&gt; and &lt;code&gt;IS [NOT] UNKNOWN&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3235"&gt;#3235&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3246"&gt;#3246&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Upcoming Work&lt;/h2&gt;
&lt;p&gt;With the community growing and code accelerating, there is so much great stuff on the horizon. Some features we expect to land in the next few months:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/3462"&gt;Complete Parquet Pushdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/3148"&gt;Additional date/time support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cost models, Nested Join Optimizations, analysis framework &lt;a href="https://github.com/apache/arrow-datafusion/issues/128"&gt;#128&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3843"&gt;#3843&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3845"&gt;#3845&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Community Growth&lt;/h1&gt;
&lt;p&gt;The DataFusion 9.0.0 and 13.0.0 releases consists of 433 PRs from 64 distinct contributors. This does not count all the work that goes into our dependencies such as &lt;a href="https://crates.io/crates/arrow"&gt;arrow&lt;/a&gt;,  &lt;a href="https://crates.io/crates/parquet"&gt;parquet&lt;/a&gt;, and &lt;a href="https://crates.io/crates/object_store"&gt;object_store&lt;/a&gt;, that much of the same community helps nurture.&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 9.0.0..13.0.0 . | wc -l
433

$ git shortlog -sn 9.0.0..13.0.0 . | wc -l
65
--&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together!&lt;/p&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to
have you join us on our journey to create the most advanced open
source query engine. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is
&lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check out our &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;
&lt;h2&gt;Appendix: Contributor Shoutout&lt;/h2&gt;
&lt;p&gt;To give a sense of the number of people who contribute to this project regularly, we present for your consideration the following list derived from &lt;code&gt;git shortlog -sn 9.0.0..13.0.0 .&lt;/code&gt; Thank you all again!&lt;/p&gt;
&lt;!-- Note: combined kmitchener and Kirk Mitchener --&gt;
&lt;pre&gt;&lt;code&gt;    87  Andy Grove
    71  Andrew Lamb
    29  Kun Liu
    29  Kirk Mitchener
    17  Wei-Ting Kuo
    14  Yang Jiang
    12  Raphael Taylor-Davies
    11  Batuhan Taskaya
    10  Brent Gardner
    10  Remzi Yang
    10  comphead
    10  xudong.w
     8  AssHero
     7  Ruihang Xia
     6  Dan Harris
     6  Dani&amp;euml;l Heres
     6  Ian Alexander Joiner
     6  Mike Roberts
     6  askoa
     4  BaymaxHWY
     4  gorkem
     4  jakevin
     3  George Andronchik
     3  Sarah Yurick
     3  Stuart Carnie
     2  Dalton Modlin
     2  Dmitry Patsura
     2  JasonLi
     2  Jon Mease
     2  Marco Neumann
     2  yahoNanJing
     1  Adilet Sarsembayev
     1  Ayush Dattagupta
     1  Dezhi Wu
     1  Dhamotharan Sritharan
     1  Eduard Karacharov
     1  Francis Du
     1  Harbour Zheng
     1  Isma&amp;euml;l Mej&amp;iacute;a
     1  Jack Klamer
     1  Jeremy Dyer
     1  Jiayu Liu
     1  Kamil Konior
     1  Liang-Chi Hsieh
     1  Martin Grigorov
     1  Matthijs Brobbel
     1  Mehmet Ozan Kabak
     1  Metehan Y&amp;inodot;ld&amp;inodot;r&amp;inodot;m
     1  Morgan Cassels
     1  Nitish Tiwari
     1  Renjie Liu
     1  Rito Takeuchi
     1  Robert Pack
     1  Thomas Cameron
     1  Vrishabh
     1  Xin Hao
     1  Yijie Shen
     1  byteink
     1  kamille
     1  mateuszkj
     1  nvartolomei
     1  yourenawo
     1  &amp;Ouml;zg&amp;uuml;r Akkurt
&lt;/code&gt;&lt;/pre&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 8.0.0 Release</title><link href="https://datafusion.apache.org/blog/2022/05/16/datafusion-8.0.0" rel="alternate"></link><published>2022-05-16T00:00:00+00:00</published><updated>2022-05-16T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-05-16:/blog/2022/05/16/datafusion-8.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that
uses Apache Arrow as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;,
a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that
uses Apache Arrow as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;,
a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth
checking out.&lt;/p&gt;
&lt;p&gt;DataFusion's SQL, &lt;code&gt;DataFrame&lt;/code&gt;, and manual &lt;code&gt;PlanBuilder&lt;/code&gt; API let users access a sophisticated query optimizer and
execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of
today's multicore hardware. Being written in Rust means DataFusion can offer &lt;em&gt;both&lt;/em&gt; the safety of a dynamic language and
the resource efficiency of a compiled language.&lt;/p&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 8.0.0 release (and also the release of version 0.7.0 of
the Ballista subproject). This covers 3 months of development work and includes 279 commits from the following 49
distinct contributors.&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 7.0.0..8.0.0 datafusion datafusion-cli datafusion-examples ballista ballista-cli ballista-examples | wc -l
279

$ git shortlog -sn 7.0.0..8.0.0 datafusion datafusion-cli datafusion-examples ballista ballista-cli ballista-examples | wc -l
49

(feynman han, feynman.h, Feynman Han were assumed to be the same person)
--&gt;
&lt;pre&gt;&lt;code&gt;    39  Andy Grove
    33  Andrew Lamb
    21  DuRipeng
    20  Yijie Shen
    19  Yang Jiang
    17  Raphael Taylor-Davies
    11  Dan Harris
    11  Matthew Turner
    11  yahoNanJing
     9  dependabot[bot]
     8  jakevin
     6  Kun Liu
     5  Jiayu Liu
     4  Dani&amp;euml;l Heres
     4  mingmwang
     4  xudong.w
     3  Carol (Nichols || Goulding)
     3  Dmitry Patsura
     3  Eduard Karacharov
     3  Jeremy Dyer
     3  Kaushik
     3  Rich
     3  comphead
     3  gaojun2048
     3  Feynman Han
     2  Jie Han
     2  Jon Mease
     2  Tim Van Wassenhove
     2  Yt
     2  Zhang Li
     2  silence-coding
     1  Alexander Spies
     1  George Andronchik
     1  Guillaume Balaine
     1  Hao Xin
     1  Jiacai Liu
     1  J&amp;ouml;rn Horstmann
     1  Liang-Chi Hsieh
     1  Max Burke
     1  NaincyKumariKnoldus
     1  Nga Tran
     1  Patrick More
     1  Pierre Zemb
     1  Remzi Yang
     1  Sergey Melnychuk
     1  Stephen Carman
     1  doki
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following sections highlight some of the changes in this release. Of course, many other bug fixes and
improvements have been made and we encourage you to check out the
&lt;a href="https://github.com/apache/arrow-datafusion/blob/8.0.0/datafusion/CHANGELOG.md"&gt;changelog&lt;/a&gt; for full details.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;h2&gt;DDL Support&lt;/h2&gt;
&lt;p&gt;DDL support has been expanded to include the following commands for creating databases, schemas, and views. This
allows DataFusion to be used more effectively from the CLI.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CREATE DATABASE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREATE VIEW&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREATE SCHEMA&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREATE EXTERNAL TABLE&lt;/code&gt; now supports JSON files, &lt;code&gt;IF NOT EXISTS&lt;/code&gt;, and partition columns&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;SQL Support&lt;/h2&gt;
&lt;p&gt;The SQL query planner now supports a number of new SQL features, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Subqueries&lt;/em&gt;: when used via &lt;code&gt;IN&lt;/code&gt;, &lt;code&gt;EXISTS&lt;/code&gt;, and as scalars&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Grouping Sets&lt;/em&gt;: &lt;code&gt;CUBE&lt;/code&gt; and &lt;code&gt;ROLLUP&lt;/code&gt; grouping sets.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Aggregate functions&lt;/em&gt;: &lt;code&gt;approx_percentile&lt;/code&gt;, &lt;code&gt;approx_percentile_cont&lt;/code&gt;, &lt;code&gt;approx_percentile_cont_with_weight&lt;/code&gt;, &lt;code&gt;approx_distinct&lt;/code&gt;, &lt;code&gt;approx_median&lt;/code&gt; and &lt;code&gt;array&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;null&lt;/code&gt; literals&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bitwise operations&lt;/em&gt;: for example '&lt;code&gt;|&lt;/code&gt;'&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are also many bug fixes and improvements around normalizing identifiers consistently.&lt;/p&gt;
&lt;p&gt;We continue our tradition of incrementally releasing support for new
features as they are developed. Thus, while the physical plan may not yet
support all new features, it gets more complete each release. These
changes also make DataFusion an increasingly compelling choice for
projects looking for a SQL parser and query planner that can produce
optimized logical plans that can be translated to
their own execution engine.&lt;/p&gt;
&lt;h2&gt;Query Execution &amp;amp; Internals&lt;/h2&gt;
&lt;p&gt;There are several notable improvements and new features in the query execution engine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;ExecutionContext&lt;/code&gt; has been renamed to &lt;code&gt;SessionContext&lt;/code&gt; and now supports multi-tenancy&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;ExecutionPlan&lt;/code&gt; trait is no longer &lt;code&gt;async&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A new serialization API for serializing plans to bytes (based on protobuf)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, we have added several foundational features to drive even
more advanced query processing into DataFusion, focusing on running
arbitrary queries larger than available memory, and pushing the
envelope for performance of sorting, grouping, and joining even
further:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Morsel-Driven Scheduler based on &lt;a href="https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf"&gt;"Morsel-Driven Parallelism: A NUMA-Aware Query
  Evaluation Framework for the Many-Core Age"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consolidated object store implementation and integration with parquet decoding&lt;/li&gt;
&lt;li&gt;Memory Limited Spilling sort operator&lt;/li&gt;
&lt;li&gt;Memory Limited Sort-Merge join operator&lt;/li&gt;
&lt;li&gt;High performance JIT code generation for tuple comparisons&lt;/li&gt;
&lt;li&gt;Memory efficient Row Format&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Improved file support&lt;/h2&gt;
&lt;p&gt;DataFusion now supports JSON, both for reading and writing. There are also new DataFrame methods for writing query
results to files in CSV, Parquet, and JSON format.&lt;/p&gt;
&lt;h2&gt;Ballista&lt;/h2&gt;
&lt;p&gt;Ballista continues to mature and now supports a wider range of operators and expressions. There are also improvements
to the scheduler to support UDFs, and there are some robustness improvements, such as cleaning up work directories
and persisting session configs to allow schedulers to restart and continue processing in-flight jobs.&lt;/p&gt;
&lt;h2&gt;Upcoming Work&lt;/h2&gt;
&lt;p&gt;Here are some of the initiatives that the community plans on working on prior to the next release.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a &lt;a href="https://docs.google.com/document/d/1jNRbadyStSrV5kifwn0khufAwq6OnzGczG4z8oTQJP4/edit?usp=sharing"&gt;proposal to move Ballista to its own top-level arrow-ballista repository&lt;/a&gt;
 to decouple DataFusion and Ballista releases and to allow each project to have documentation better targeted at
  its particular audience.&lt;/li&gt;
&lt;li&gt;We plan on increasing the frequency of DataFusion releases, with monthly releases now instead of quarterly. This
  is driven by requests from the increasing number of projects that now depend on DataFusion.&lt;/li&gt;
&lt;li&gt;There is ongoing work to implement new optimizer rules to rewrite queries containing subquery expressions as
  joins, to support a wider range of queries.&lt;/li&gt;
&lt;li&gt;The new scheduler based on morsel-driven execution will continue to evolve in this next release, with work to
  refine IO abstractions to improve performance and integration with the new scheduler.&lt;/li&gt;
&lt;li&gt;Improved performance for Sort, Grouping and Joins&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, and learning about state-of-the-art query processing, we would
love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects
and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable
for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Check out our new &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Introducing Apache Arrow DataFusion Contrib</title><link href="https://datafusion.apache.org/blog/2022/03/21/datafusion-contrib" rel="alternate"></link><published>2022-03-21T00:00:00+00:00</published><updated>2022-03-21T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-03-21:/blog/2022/03/21/datafusion-contrib</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Apache Arrow &lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Apache Arrow &lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion's pluggable design makes creating extensions at various points particular easy to build.&lt;/p&gt;
&lt;p&gt;DataFusion's  SQL, &lt;code&gt;DataFrame&lt;/code&gt;, and manual &lt;code&gt;PlanBuilder&lt;/code&gt; API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer &lt;em&gt;both&lt;/em&gt; the safety of dynamic languages as well as the resource efficiency of a compiled language.&lt;/p&gt;
&lt;p&gt;The DataFusion team is pleased to announce the creation of the &lt;a href="https://github.com/datafusion-contrib"&gt;DataFusion-Contrib&lt;/a&gt; GitHub organization to support and accelerate other projects.  While the core DataFusion library remains under Apache governance, the contrib organization provides a more flexible testing ground for new DataFusion features and a home for DataFusion extensions.  With this announcement, we are pleased to introduce the following inaugural DataFusion-Contrib repositories.&lt;/p&gt;
&lt;h2&gt;DataFusion-Python&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-python"&gt;project&lt;/a&gt; provides Python bindings to the core Rust implementation of DataFusion, which allows users to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work with familiar SQL or DataFrame APIs to run queries in a safe, multi-threaded environment, returning results in Python&lt;/li&gt;
&lt;li&gt;Create User Defined Functions and User Defined Aggregate Functions for complex operations&lt;/li&gt;
&lt;li&gt;Pay no overhead to copy between Python and underlying Rust execution engine (by way of Apache Arrow arrays)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Upcoming enhancements&lt;/h3&gt;
&lt;p&gt;The team is focusing on exposing more features from the underlying Rust implementation of DataFusion and improving documentation.&lt;/p&gt;
&lt;h3&gt;How to install&lt;/h3&gt;
&lt;p&gt;From &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install datafusion
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install datafusion
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;DataFusion-ObjectStore-S3&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-objectstore-s3"&gt;crate&lt;/a&gt; provides an &lt;code&gt;ObjectStore&lt;/code&gt; implementation for querying data stored in S3 or S3 compatible storage. This makes it almost as easy to query data that lives on S3 as lives in local files&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ability to create &lt;code&gt;S3FileSystem&lt;/code&gt; to register as part of DataFusion &lt;code&gt;ExecutionContext&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Register files or directories stored on S3 with &lt;code&gt;ctx.register_listing_table&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Upcoming enhancements&lt;/h3&gt;
&lt;p&gt;The current priority is adding python bindings for &lt;code&gt;S3FileSystem&lt;/code&gt;.  After that there will be async improvements as DataFusion adopts more of that functionality and we are looking into S3 Select functionality.&lt;/p&gt;
&lt;h3&gt;How to Install&lt;/h3&gt;
&lt;p&gt;Add the below to your &lt;code&gt;Cargo.toml&lt;/code&gt; in your Rust Project with DataFusion.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-toml"&gt;datafusion-objectstore-s3 = "0.1.0"
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;DataFusion-Substrait&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://substrait.io/"&gt;Substrait&lt;/a&gt; is an emerging standard that provides a cross-language serialization format for relational algebra (e.g. expressions and query plans).&lt;/p&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-substrait"&gt;crate&lt;/a&gt; provides a Substrait producer and consumer for DataFusion.  A producer converts a DataFusion logical plan into a Substrait protobuf and a consumer does the reverse.&lt;/p&gt;
&lt;p&gt;Examples of how to use this crate can be found &lt;a href="https://github.com/datafusion-contrib/datafusion-substrait/blob/main/src/lib.rs"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Potential Use Cases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Replace custom DataFusion protobuf serialization.&lt;/li&gt;
&lt;li&gt;Make it easier to pass query plans over FFI boundaries, such as from Python to Rust&lt;/li&gt;
&lt;li&gt;Allow Apache Calcite query plans to be executed in DataFusion&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;DataFusion-BigTable&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-bigtable"&gt;crate&lt;/a&gt; implements &lt;a href="https://cloud.google.com/bigtable"&gt;Bigtable&lt;/a&gt; as a data source and physical executor for DataFusion queries.  It currently supports both UTF-8 string and 64-bit big-endian signed integers in Bigtable.  From a SQL perspective it supports both simple and composite row keys with &lt;code&gt;=&lt;/code&gt;, &lt;code&gt;IN&lt;/code&gt;, and &lt;code&gt;BETWEEN&lt;/code&gt; operators as well as projection pushdown.  The physical execution for queries is handled by this crate while any subsequent aggregation, group bys, or joins are handled in DataFusion.&lt;/p&gt;
&lt;h3&gt;Upcoming Enhancements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Predicate pushdown&lt;/li&gt;
&lt;li&gt;Value range&lt;/li&gt;
&lt;li&gt;Value Regex&lt;/li&gt;
&lt;li&gt;Timestamp range&lt;/li&gt;
&lt;li&gt;Multithreaded&lt;/li&gt;
&lt;li&gt;Partition aware execution&lt;/li&gt;
&lt;li&gt;Production ready&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;How to Install&lt;/h3&gt;
&lt;p&gt;Add the below to your &lt;code&gt;Cargo.toml&lt;/code&gt; in your Rust Project with DataFusion.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-toml"&gt;datafusion-bigtable = "0.1.0"
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;DataFusion-HDFS&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-objectstore-hdfs"&gt;crate&lt;/a&gt; introduces &lt;code&gt;HadoopFileSystem&lt;/code&gt; as a remote &lt;code&gt;ObjectStore&lt;/code&gt; which provides the ability to query HDFS files.  For HDFS access the &lt;a href="https://github.com/yahoNanJing/fs-hdfs"&gt;fs-hdfs&lt;/a&gt; library is used.&lt;/p&gt;
&lt;h2&gt;DataFusion-Tokomak&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-tokomak"&gt;crate&lt;/a&gt; provides an e-graph based DataFusion optimization framework based on the Rust &lt;a href="https://egraphs-good.github.io"&gt;egg&lt;/a&gt; library.  An e-graph is a data structure that powers the equality saturation optimization technique.&lt;/p&gt;
&lt;p&gt;As context, the optimizer framework within DataFusion is currently &lt;a href="https://github.com/apache/arrow-datafusion/issues/1972"&gt;under review&lt;/a&gt; with the objective of implementing a more strategic long term solution that is more efficient and simpler to develop.&lt;/p&gt;
&lt;p&gt;Some of the benefits of using &lt;code&gt;egg&lt;/code&gt; within DataFusion are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implements optimized algorithms that are hard to match with manually written optimization passes&lt;/li&gt;
&lt;li&gt;Makes it easy and less verbose to add optimization rules&lt;/li&gt;
&lt;li&gt;Plugin framework to add more complex optimizations&lt;/li&gt;
&lt;li&gt;Egg does not depend on rule order and can lead to a higher level of optimization by being able to apply multiple rules at the same time until it converges&lt;/li&gt;
&lt;li&gt;Allows for cost-based optimizations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is an exciting new area for DataFusion with lots of opportunity for community involvement!&lt;/p&gt;
&lt;h2&gt;DataFusion-Tui&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-tui"&gt;DataFusion-tui&lt;/a&gt; aka &lt;code&gt;dft&lt;/code&gt; provides a feature rich terminal application for using DataFusion.  It has drawn inspiration and several features from &lt;code&gt;datafusion-cli&lt;/code&gt;.  In contrast to &lt;code&gt;datafusion-cli&lt;/code&gt; the objective of this tool is to provide a light SQL IDE experience for querying data with DataFusion.  This includes features such as the following which are currently implemented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tab Management to provide clean and structured organization of DataFusion queries, results, &lt;code&gt;ExecutionContext&lt;/code&gt; information, and logs&lt;/li&gt;
&lt;li&gt;SQL Editor&lt;ul&gt;
&lt;li&gt;Text editor for writing SQL queries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Query History&lt;ul&gt;
&lt;li&gt;History of executed queries, their execution time, and the number of returned rows&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ExecutionContext&lt;/code&gt; information&lt;ul&gt;
&lt;li&gt;Expose information on which physical optimizers are used and which &lt;code&gt;ExecutionConfig&lt;/code&gt; settings are set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Logs&lt;ul&gt;
&lt;li&gt;Logs from &lt;code&gt;dft&lt;/code&gt;, DataFusion, and any dependent libraries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Support for custom &lt;code&gt;ObjectStore&lt;/code&gt;s&lt;/li&gt;
&lt;li&gt;S3&lt;/li&gt;
&lt;li&gt;Preload DDL from &lt;code&gt;~/.datafusionrc&lt;/code&gt; to enable having local "database" available at startup&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Upcoming Enhancements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SQL Editor&lt;/li&gt;
&lt;li&gt;Command to write query results to file&lt;/li&gt;
&lt;li&gt;Multiple SQL editor tabs&lt;/li&gt;
&lt;li&gt;Expose more information from &lt;code&gt;ExecutionContext&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A help tab that provides information on functions&lt;/li&gt;
&lt;li&gt;Query custom &lt;code&gt;TableProvider&lt;/code&gt;s such as &lt;a href="https://github.com/delta-io/delta-rs"&gt;DeltaTable&lt;/a&gt; or &lt;a href="https://github.com/datafusion-contrib/datafusion-bigtable"&gt;BigTable&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;DataFusion-Streams&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-streams"&gt;DataFusion-Stream&lt;/a&gt; is a new testing ground for creating a &lt;code&gt;StreamProvider&lt;/code&gt; in DataFusion that will enable querying streaming data sources such as Apache Kafka.  The implementation for this feature is currently being designed and is under active review.  Once the design is finalized the trait and attendant data structures will be added back to the core DataFusion crate.&lt;/p&gt;
&lt;h2&gt;DataFusion-Java&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-java"&gt;project&lt;/a&gt; created an initial set of Java bindings to DataFusion.  The project is currently in maintenance mode and is looking for maintainers to drive future development.&lt;/p&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, and learning about state of
the art query processing, we would love to have you join us on the journey! You
can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The best way to find out about creating new extensions within DataFusion-Contrib is reaching out on the &lt;code&gt;#arrow-rust&lt;/code&gt; channel of the Apache Software Foundation &lt;a href="https://join.slack.com/t/the-asf/shared_invite/zt-vlfbf7ch-HkbNHiU_uDlcH_RvaHv9gQ"&gt;Slack&lt;/a&gt; workspace.&lt;/p&gt;
&lt;p&gt;You can also check out our new &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more ways to engage with the community.&lt;/p&gt;
&lt;p&gt;Links for each DataFusion-Contrib repository are provided above if you would like to contribute to those.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 7.0.0 Release</title><link href="https://datafusion.apache.org/blog/2022/02/28/datafusion-7.0.0" rel="alternate"></link><published>2022-02-28T00:00:00+00:00</published><updated>2022-02-28T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-02-28:/blog/2022/02/28/datafusion-7.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out.&lt;/p&gt;
&lt;p&gt;DataFusion's  SQL, &lt;code&gt;DataFrame&lt;/code&gt;, and manual &lt;code&gt;PlanBuilder&lt;/code&gt; API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer &lt;em&gt;both&lt;/em&gt; the safety of dynamic languages as well as the resource efficiency of a compiled language.&lt;/p&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 7.0.0 release. This covers 4 months of development work
and includes 195 commits from the following 37 distinct contributors.&lt;/p&gt;
&lt;!--
git log --pretty=oneline 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     134

git shortlog -sn 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
      29

      Carlos and xudong963 are same individual
--&gt;
&lt;pre&gt;&lt;code&gt;    44  Andrew Lamb
    24  Kun Liu
    23  Jiayu Liu
    17  xudong.w
    11  Yijie Shen
     9  Matthew Turner
     7  Liang-Chi Hsieh
     5  Lin Ma
     4  Stephen Carman
     4  James Katz
     4  Dmitry Patsura
     4  QP Hou
     3  dependabot[bot]
     3  Remzi Yang
     3  Yang
     3  ic4y
     3  Dani&amp;euml;l Heres
     2  Andy Grove
     2  Raphael Taylor-Davies
     2  Jason Tianyi Wang
     2  Dan Harris
     2  Sergey Melnychuk
     1  Nitish Tiwari
     1  Dom
     1  Eduard Karacharov
     1  Javier Goday
     1  Boaz
     1  Marko Mikulicic
     1  Max Burke
     1  Carol (Nichols || Goulding)
     1  Phillip Cloud
     1  Rich
     1  Toby Hede
     1  Will Jones
     1  r.4ntix
     1  rdettai
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following section highlights some of the improvements in this release. Of course, many other bug fixes and improvements have also been made and we refer you to the complete &lt;a href="https://github.com/apache/arrow-datafusion/blob/7.0.0/datafusion/CHANGELOG.md"&gt;changelog&lt;/a&gt; for the full detail.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;DataFusion Crate&lt;/li&gt;
&lt;li&gt;The DataFusion crate is being split into multiple crates to decrease compilation times and improve the development experience. Initially, &lt;code&gt;datafusion-common&lt;/code&gt; (the core DataFusion components) and &lt;code&gt;datafusion-expr&lt;/code&gt; (DataFusion expressions, functions, and operators) have been split out. There will be additional splits after the 7.0 release.&lt;/li&gt;
&lt;li&gt;Performance Improvements and Optimizations&lt;/li&gt;
&lt;li&gt;Arrow&amp;rsquo;s dyn scalar kernels are now used to enable efficient operations on &lt;code&gt;DictionaryArray&lt;/code&gt;s &lt;a href="https://github.com/apache/arrow-datafusion/pull/1685"&gt;#1685&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Switch from &lt;code&gt;std::sync::Mutex&lt;/code&gt; to &lt;code&gt;parking_lot::Mutex&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1720"&gt;#1720&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;New Features&lt;/li&gt;
&lt;li&gt;Support for memory tracking and spilling to disk&lt;ul&gt;
&lt;li&gt;MemoryMananger and DiskManager &lt;a href="https://github.com/apache/arrow-datafusion/pull/1526"&gt;#1526&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Out of core sort &lt;a href="https://github.com/apache/arrow-datafusion/pull/1526"&gt;#1526&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;New metrics&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Gauge&lt;/code&gt; and &lt;code&gt;CurrentMemoryUsage&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1682"&gt;#1682&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Spill_count&lt;/code&gt; and &lt;code&gt;spilled_bytes&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1641"&gt;#1641&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;New math functions&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Approx_quantile&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1539"&gt;#1529&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;stddev&lt;/code&gt; and &lt;code&gt;variance&lt;/code&gt; (sample and population) &lt;a href="https://github.com/apache/arrow-datafusion/pull/1525"&gt;#1525&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;corr&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1561"&gt;#1561&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Support decimal type &lt;a href="https://github.com/apache/arrow-datafusion/pull/1394"&gt;#1394&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1407"&gt;#1407&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1408"&gt;#1408&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1431"&gt;#1431&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1483"&gt;#1483&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1554"&gt;#1554&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1640"&gt;#1640&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for reading Parquet files with evolved schemas &lt;a href="https://github.com/apache/arrow-datafusion/pull/1622"&gt;#1622&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1709"&gt;#1709&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for registering &lt;code&gt;DataFrame&lt;/code&gt; as table &lt;a href="https://github.com/apache/arrow-datafusion/pull/1699"&gt;#1699&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for the &lt;code&gt;substring&lt;/code&gt; function &lt;a href="https://github.com/apache/arrow-datafusion/pull/1621"&gt;#1621&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support &lt;code&gt;array_agg(distinct ...)&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1579"&gt;#1579&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support &lt;code&gt;sort&lt;/code&gt; on unprojected columns &lt;a href="https://github.com/apache/arrow-datafusion/pull/1415"&gt;#1415&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Additional Integration Points&lt;/li&gt;
&lt;li&gt;A new public Expression simplification API &lt;a href="https://github.com/apache/arrow-datafusion/pull/1717"&gt;#1717&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib"&gt;DataFusion-Contrib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A new GitHub organization created as a home for both &lt;code&gt;DataFusion&lt;/code&gt; extensions and as a testing ground for new features.&lt;ul&gt;
&lt;li&gt;Extensions&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-python"&gt;DataFusion-Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-java"&gt;DataFusion-Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-hdfs-native"&gt;DataFusion-hdsfs-native&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-objectstore-s3"&gt;DataFusion-ObjectStore-s3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;New Features&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-streams"&gt;DataFusion-Streams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jorgecarleitao/arrow2"&gt;Arrow2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An &lt;a href="https://github.com/apache/arrow-datafusion/tree/arrow2"&gt;Arrow2 Branch&lt;/a&gt; has been created.  There are ongoing discussions in &lt;a href="https://github.com/apache/arrow-datafusion/issues/1532"&gt;DataFusion&lt;/a&gt; and &lt;a href="https://github.com/apache/arrow-rs/issues/1176"&gt;arrow-rs&lt;/a&gt; about migrating &lt;code&gt;DataFusion&lt;/code&gt; to &lt;code&gt;Arrow2&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Documentation and Roadmap&lt;/h1&gt;
&lt;p&gt;We are working to consolidate the documentation into the &lt;a href="https://arrow.apache.org/datafusion"&gt;official site&lt;/a&gt;.  You can find more details there on topics such as the &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html"&gt;SQL status&lt;/a&gt;  and a &lt;a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#introduction"&gt;user guide&lt;/a&gt;. This is also an area we would love to get help from the broader community &lt;a href="https://github.com/apache/arrow-datafusion/issues/1821"&gt;#1821&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To provide transparency on DataFusion&amp;rsquo;s priorities to users and developers a three month roadmap will be published at the beginning of each quarter.  This can be found here &lt;a href="https://arrow.apache.org/datafusion/specification/roadmap.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Upcoming Attractions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Ballista is gaining momentum, and several groups are now evaluating and contributing to the project.&lt;/li&gt;
&lt;li&gt;Some of the proposed improvements&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/1701"&gt;Improvements Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/1675"&gt;Extensibility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/1702"&gt;File system access&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/1704"&gt;Cluster state&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Continued improvements for working with limited resources and large datasets&lt;/li&gt;
&lt;li&gt;Memory limited joins&lt;a href="https://github.com/apache/arrow-datafusion/issues/1599"&gt;#1599&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sort-merge join&lt;a href="https://github.com/apache/arrow-datafusion/issues/141"&gt;#141&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1776"&gt;#1776&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Introduce row based bytes representation &lt;a href="https://github.com/apache/arrow-datafusion/pull/1708"&gt;#1708&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, and learning about state of
the art query processing, we would love to have you join us on the journey! You
can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Check out our new &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 6.0.0 Release</title><link href="https://datafusion.apache.org/blog/2021/11/19/2021-11-8-datafusion-6.0.0.md" rel="alternate"></link><published>2021-11-19T00:00:00+00:00</published><updated>2021-11-19T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2021-11-19:/blog/2021/11/19/2021-11-8-datafusion-6.0.0.md</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an embedded
query engine which leverages the unique features of
&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache
Arrow&lt;/a&gt; to provide a system that is high
performance, easy to connect, easy to embed, and high quality.&lt;/p&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 6.0.0 release. This covers …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an embedded
query engine which leverages the unique features of
&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache
Arrow&lt;/a&gt; to provide a system that is high
performance, easy to connect, easy to embed, and high quality.&lt;/p&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 6.0.0 release. This covers 4 months of development work
and includes 134 commits from the following 28 distinct contributors.&lt;/p&gt;
&lt;!--
git log --pretty=oneline 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     134

git shortlog -sn 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
      29

      Carlos and xudong963 are same individual
--&gt;
&lt;pre&gt;&lt;code&gt;    28  Andrew Lamb
    26  Jiayu Liu
    13  xudong963
     9  rdettai
     9  QP Hou
     6  Matthew Turner
     5  Dani&amp;euml;l Heres
     4  Guillaume Balaine
     3  Francis Du
     3  Marco Neumann
     3  Jon Mease
     3  Nga Tran
     2  Yijie Shen
     2  Ruihang Xia
     2  Liang-Chi Hsieh
     2  baishen
     2  Andy Grove
     2  Jason Tianyi Wang
     1  Nan Zhu
     1  Antoine Wendlinger
     1  Kriszti&amp;aacute;n Sz&amp;udblac;cs
     1  Mike Seddon
     1  Conner Murphy
     1  Patrick More
     1  Taehoon Moon
     1  Tiphaine Ruy
     1  adsharma
     1  lichuan6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes
and improvements have been made: we refer you to the complete
&lt;a href="https://github.com/apache/arrow-datafusion/blob/6.0.0/datafusion/CHANGELOG.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;New Website&lt;/h1&gt;
&lt;p&gt;Befitting a growing project, DataFusion now has its
&lt;a href="https://arrow.apache.org/datafusion/"&gt;own website&lt;/a&gt; hosted as part of the
main &lt;a href="https://arrow.apache.org"&gt;Apache Arrow Website&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Roadmap&lt;/h1&gt;
&lt;p&gt;The community worked to gather their thoughts about where we are
taking DataFusion into a public
&lt;a href="https://arrow.apache.org/datafusion/specification/roadmap.html"&gt;Roadmap&lt;/a&gt;
for the first time&lt;/p&gt;
&lt;h1&gt;New Features&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Runtime operator metrics collection framework&lt;/li&gt;
&lt;li&gt;Object store abstraction for unified access to local or remote storage&lt;/li&gt;
&lt;li&gt;Hive style table partitioning support, for Parquet, CSV, Avro and Json files&lt;/li&gt;
&lt;li&gt;DataFrame API support for: &lt;code&gt;except&lt;/code&gt;, &lt;code&gt;intersect&lt;/code&gt;, &lt;code&gt;show&lt;/code&gt;, &lt;code&gt;limit&lt;/code&gt; and window functions&lt;/li&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;li&gt;&lt;code&gt;EXPLAIN ANALYZE&lt;/code&gt; with runtime metrics&lt;/li&gt;
&lt;li&gt;&lt;code&gt;trim ( [ LEADING | TRAILING | BOTH ] [ FROM ] string text [, characters text ] )&lt;/code&gt; syntax&lt;/li&gt;
&lt;li&gt;Postgres style regular expression matching operators &lt;code&gt;~&lt;/code&gt;, &lt;code&gt;~*&lt;/code&gt;, &lt;code&gt;!~&lt;/code&gt;, and &lt;code&gt;!~*&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;SQL set operators &lt;code&gt;UNION&lt;/code&gt;, &lt;code&gt;INTERSECT&lt;/code&gt;, and &lt;code&gt;EXCEPT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cume_dist&lt;/code&gt;, &lt;code&gt;percent_rank&lt;/code&gt; window functions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;digest&lt;/code&gt;, &lt;code&gt;blake2s&lt;/code&gt;, &lt;code&gt;blake2b&lt;/code&gt;, &lt;code&gt;blake3&lt;/code&gt; crypto functions&lt;/li&gt;
&lt;li&gt;HyperLogLog based &lt;code&gt;approx_distinct&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;is distinct from&lt;/code&gt; and &lt;code&gt;is not distinct from&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREATE TABLE AS SELECT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Accessing elements of nested &lt;code&gt;Struct&lt;/code&gt; and &lt;code&gt;List&lt;/code&gt; columns (e.g. &lt;code&gt;SELECT struct_column['field_name'], array_column[0] FROM ...&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Boolean expressions in &lt;code&gt;CASE&lt;/code&gt; statement&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DROP TABLE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VALUES&lt;/code&gt; List&lt;/li&gt;
&lt;li&gt;Postgres regex match operators&lt;/li&gt;
&lt;li&gt;Support for Avro format&lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;ScalarValue::Struct&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Automatic schema inference for CSV files&lt;/li&gt;
&lt;li&gt;Better interactive editing support in &lt;code&gt;datafusion-cli&lt;/code&gt; as well as &lt;code&gt;psql&lt;/code&gt; style commands such as &lt;code&gt;\d&lt;/code&gt;, &lt;code&gt;\?&lt;/code&gt;, and &lt;code&gt;\q&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Generic constant evaluation and simplification framework&lt;/li&gt;
&lt;li&gt;Added common subexpression eliminate query plan optimization rule&lt;/li&gt;
&lt;li&gt;Python binding 0.4.0 with all Datafusion 6.0.0 features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With these new features, we are also now passing TPC-H queries 8, 13 and 21.&lt;/p&gt;
&lt;p&gt;For the full list of new features with their relevant PRs, see the
&lt;a href="https://github.com/apache/arrow-datafusion/blob/6.0.0/datafusion/CHANGELOG.md"&gt;enhancements section&lt;/a&gt;
in the changelog.&lt;/p&gt;
&lt;h1&gt;&lt;code&gt;async&lt;/code&gt; planning and decoupling file format from table layout&lt;/h1&gt;
&lt;p&gt;Driven by the need to support Hive style table partitioning, @rdettai
introduced the following design change to the Datafusion core.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The code for reading specific file formats (&lt;code&gt;Parquet&lt;/code&gt;, &lt;code&gt;Avro&lt;/code&gt;, &lt;code&gt;CSV&lt;/code&gt;, and
&lt;code&gt;JSON&lt;/code&gt;) was separated from the logic that handles grouping sets of
files into execution partitions.&lt;/li&gt;
&lt;li&gt;The query planning process was made &lt;code&gt;async&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result, we are able to replace the old &lt;code&gt;Parquet&lt;/code&gt;, &lt;code&gt;CSV&lt;/code&gt; and &lt;code&gt;JSON&lt;/code&gt; table
providers with a single &lt;code&gt;ListingTable&lt;/code&gt; table provider.&lt;/p&gt;
&lt;p&gt;This also sets up DataFusion and its plug-in ecosystem to
supporting a wide range of catalogs and various object store implementations.
You can read more about this change in the
&lt;a href="https://docs.google.com/document/d/1Bd4-PLLH-pHj0BquMDsJ6cVr_awnxTuvwNJuWsTHxAQ"&gt;design document&lt;/a&gt;
and on the &lt;a href="https://github.com/apache/arrow-datafusion/pull/1010"&gt;arrow-datafusion#1010 PR&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to have you! You
can help by trying out DataFusion on some of your own data and projects and filing bug reports and helping to
improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for
beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;
and the full list is &lt;a href="https://github.com/apache/arrow-datafusion/issues"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check out our new &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow Ballista 0.5.0 Release</title><link href="https://datafusion.apache.org/blog/2021/08/18/ballista-0.5.0" rel="alternate"></link><published>2021-08-18T00:00:00+00:00</published><updated>2021-08-18T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2021-08-18:/blog/2021/08/18/ballista-0.5.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;Ballista extends DataFusion to provide support for distributed queries. This is the first release of Ballista since 
the project was &lt;a href="https://arrow.apache.org/blog/2021/04/12/ballista-donation/"&gt;donated&lt;/a&gt; to the Apache Arrow project 
and includes 80 commits from 11 contributors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git shortlog -sn 4.0.0..5.0.0 ballista/rust/client ballista/rust/core ballista/rust …&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;Ballista extends DataFusion to provide support for distributed queries. This is the first release of Ballista since 
the project was &lt;a href="https://arrow.apache.org/blog/2021/04/12/ballista-donation/"&gt;donated&lt;/a&gt; to the Apache Arrow project 
and includes 80 commits from 11 contributors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git shortlog -sn 4.0.0..5.0.0 ballista/rust/client ballista/rust/core ballista/rust/executor ballista/rust/scheduler
  27  Andy Grove
  15  Jiayu Liu
  12  Andrew Lamb
   8  Ximo Guanter
   6  Dani&amp;euml;l Heres
   5  QP Hou
   2  Jorge Leitao
   1  Javier Goday
   1  K.I. (Dennis) Jung
   1  Mike Seddon
   1  sathis
&lt;/code&gt;&lt;/pre&gt;
&lt;!--
$ git log --pretty=oneline 4.0.0..5.0.0 ballista/rust/client ballista/rust/core ballista/rust/executor ballista/rust/scheduler ballista-examples/ | wc -l
80
--&gt;
&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes 
and improvements have been made: we refer you to the &lt;a href="https://github.com/apache/arrow-datafusion/blob/5.0.0/ballista/CHANGELOG.md"&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Performance and Scalability&lt;/h1&gt;
&lt;p&gt;Ballista is now capable of running complex SQL queries at scale and supports scalable distributed joins. We have been 
benchmarking using individual queries from the TPC-H benchmark at scale factors up to 1000 (1 TB). When running against 
CSV files, performance is generally very close to DataFusion, and significantly faster in some cases due to the fact 
that the scheduler limits the number of concurrent tasks that run at any given time. Performance against large Parquet 
datasets is currently non ideal due to some issues (&lt;a href="https://github.com/apache/arrow-datafusion/issues/867"&gt;#867&lt;/a&gt;, 
&lt;a href="https://github.com/apache/arrow-datafusion/issues/868"&gt;#868&lt;/a&gt;) that we hope to resolve for the next release. &lt;/p&gt;
&lt;h1&gt;New Features&lt;/h1&gt;
&lt;p&gt;The main new features in this release are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ballista queries can now be executed by calling DataFrame.collect()&lt;/li&gt;
&lt;li&gt;The shuffle mechanism has been re-implemented&lt;/li&gt;
&lt;li&gt;Distributed hash-partitioned joins are now supported&lt;/li&gt;
&lt;li&gt;Keda autoscaling is supported&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To get started with Ballista, refer to the &lt;a href="https://docs.rs/ballista/0.5.0/ballista/"&gt;crate documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that the basic functionality is in place, the focus for the next release will be to improve the performance and
scalability as well as improving the documentation.&lt;/p&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to Ballista, we would love to have you! You
can help by trying out Ballista on some of your own data and projects and filing bug reports and helping to
improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for
beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;
and the full list is &lt;a href="https://github.com/apache/arrow-datafusion/issues"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 5.0.0 Release</title><link href="https://datafusion.apache.org/blog/2021/08/18/datafusion-5.0.0" rel="alternate"></link><published>2021-08-18T00:00:00+00:00</published><updated>2021-08-18T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2021-08-18:/blog/2021/08/18/datafusion-5.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 5.0.0 release. This covers 4 months of development work 
and includes 211 commits from the following 31 distinct contributors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git shortlog -sn 4.0.0..5.0.0 datafusion datafusion-cli datafusion-examples
    61  Jiayu Liu
    47  Andrew Lamb
    27 …&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 5.0.0 release. This covers 4 months of development work 
and includes 211 commits from the following 31 distinct contributors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git shortlog -sn 4.0.0..5.0.0 datafusion datafusion-cli datafusion-examples
    61  Jiayu Liu
    47  Andrew Lamb
    27  Dani&amp;euml;l Heres
    13  QP Hou
    13  Andy Grove
     4  Javier Goday
     4  sathis
     3  Ruan Pearce-Authers
     3  Raphael Taylor-Davies
     3  Jorge Leitao
     3  Cui Wenzheng
     3  Mike Seddon
     3  Edd Robinson
     2  思维
     2  Liang-Chi Hsieh
     2  Michael Lu
     2  Parth Sarthy
     2  Patrick More
     2  Rich
     1  Charlie Evans
     1  Gang Liao
     1  Agata Naomichi
     1  Ritchie Vink
     1  Evan Chan
     1  Ruihang Xia
     1  Todd Treece
     1  Yichen Wang
     1  baishen
     1  Nga Tran
     1  rdettai
     1  Marco Neumann
&lt;/code&gt;&lt;/pre&gt;
&lt;!--
$ git log --pretty=oneline 4.0.0..5.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     211
--&gt;
&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes 
and improvements have been made: we refer you to the complete 
&lt;a href="https://github.com/apache/arrow-datafusion/blob/5.0.0/datafusion/CHANGELOG.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Performance&lt;/h1&gt;
&lt;p&gt;There have been numerous performance improvements in this release. The following chart shows the relative 
performance of individual TPC-H queries compared to the previous release.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;TPC-H @ scale factor 100, in parquet format. Concurrency 24.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/2021-08-18-datafusion500perf.png"/&gt;&lt;/p&gt;
&lt;p&gt;We also extended support for more TPC-H queries: q7, q8, q9 and q13 are running successfully in DataFusion 5.0.&lt;/p&gt;
&lt;h1&gt;New Features&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Initial support for SQL-99 Analytics (WINDOW functions)&lt;/li&gt;
&lt;li&gt;Improved JOIN support: cross join, semi-join, anti join, and fixes to null handling&lt;/li&gt;
&lt;li&gt;Improved EXPLAIN support&lt;/li&gt;
&lt;li&gt;Initial implementation of metrics in the physical plan&lt;/li&gt;
&lt;li&gt;Support for SELECT DISTINCT&lt;/li&gt;
&lt;li&gt;Support for Json and NDJson formatted inputs&lt;/li&gt;
&lt;li&gt;Query column with relations&lt;/li&gt;
&lt;li&gt;Added more datetime related functions: &lt;code&gt;now&lt;/code&gt;, &lt;code&gt;date_trunc&lt;/code&gt;, &lt;code&gt;to_timestamp_millis&lt;/code&gt;, &lt;code&gt;to_timestamp_micros&lt;/code&gt;, &lt;code&gt;to_timestamp_seconds&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Streaming Dataframe.collect&lt;/li&gt;
&lt;li&gt;Support table column aliases&lt;/li&gt;
&lt;li&gt;Answer count(*), min() and max() queries using only statistics&lt;/li&gt;
&lt;li&gt;Non-equi-join filters in JOIN conditions&lt;/li&gt;
&lt;li&gt;Modulus operation&lt;/li&gt;
&lt;li&gt;Support group by column positions&lt;/li&gt;
&lt;li&gt;Added constant folding query optimizer&lt;/li&gt;
&lt;li&gt;Hash partitioned aggregation&lt;/li&gt;
&lt;li&gt;Added &lt;code&gt;random&lt;/code&gt; SQL function&lt;/li&gt;
&lt;li&gt;Implemented count distinct for floats and dictionary types&lt;/li&gt;
&lt;li&gt;Re-exported arrow and parquet crates in Datafusion&lt;/li&gt;
&lt;li&gt;General row group pruning logic that&amp;rsquo;s agnostic to storage format&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to have you! You 
can help by trying out DataFusion on some of your own data and projects and filing bug reports and helping to 
improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for 
beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt; 
and the full list is &lt;a href="https://github.com/apache/arrow-datafusion/issues"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Ballista: A Distributed Scheduler for Apache Arrow</title><link href="https://datafusion.apache.org/blog/2021/04/12/ballista-donation" rel="alternate"></link><published>2021-04-12T00:00:00+00:00</published><updated>2021-04-12T00:00:00+00:00</updated><author><name>agrove</name></author><id>tag:datafusion.apache.org,2021-04-12:/blog/2021/04/12/ballista-donation</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are excited to announce that &lt;a href="https://github.com/apache/arrow-datafusion/tree/master/ballista"&gt;Ballista&lt;/a&gt; has been donated 
to the Apache Arrow project. &lt;/p&gt;
&lt;p&gt;Ballista is a distributed compute platform primarily implemented in Rust, and powered by Apache Arrow. It is built
on an architecture that allows other programming languages (such as Python, C++, and Java) to be supported …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are excited to announce that &lt;a href="https://github.com/apache/arrow-datafusion/tree/master/ballista"&gt;Ballista&lt;/a&gt; has been donated 
to the Apache Arrow project. &lt;/p&gt;
&lt;p&gt;Ballista is a distributed compute platform primarily implemented in Rust, and powered by Apache Arrow. It is built
on an architecture that allows other programming languages (such as Python, C++, and Java) to be supported as
first-class citizens without paying a penalty for serialization costs.&lt;/p&gt;
&lt;p&gt;The foundational technologies in Ballista are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; memory model and compute kernels for efficient processing of data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion"&gt;Apache Arrow DataFusion&lt;/a&gt; query planning and 
  execution framework, extended by Ballista to provide distributed planning and execution.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/"&gt;Apache Arrow Flight Protocol&lt;/a&gt; for efficient
  data transfer between processes.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developers.google.com/protocol-buffers"&gt;Google Protocol Buffers&lt;/a&gt; for serializing query plans.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; for packaging up executors along with user-defined code.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ballista can be deployed as a standalone cluster and also supports &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt;. In either
case, the scheduler can be configured to use &lt;a href="https://etcd.io/"&gt;etcd&lt;/a&gt; as a backing store to (eventually) provide
redundancy in the case of a scheduler failing.&lt;/p&gt;
&lt;h2&gt;Status&lt;/h2&gt;
&lt;p&gt;The Ballista project is at an early stage of development. However, it is capable of running complex analytics queries 
in a distributed cluster with reasonable performance (comparable to more established distributed query frameworks).&lt;/p&gt;
&lt;p&gt;One of the benefits of Ballista being part of the Arrow codebase is that there is now an opportunity to push parts of 
the scheduler down to DataFusion so that is possible to seamlessly scale across cores in DataFusion, and across nodes 
in Ballista, using the same unified query scheduler.&lt;/p&gt;
&lt;h2&gt;Contributors Welcome!&lt;/h2&gt;
&lt;p&gt;If you are excited about being able to use Rust for distributed compute and ETL and would like to contribute to this 
work then there are many ways to get involved. The simplest way to get started is to try out Ballista against your own 
datasets and file bug reports for any issues that you find. You could also check out the current 
&lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20component%20%3D%20%22Rust%20-%20Ballista%22"&gt;list of issues&lt;/a&gt; and have a go at fixing one.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/apache/arrow/blob/master/rust/README.md#arrow-rust-community"&gt;Arrow Rust Community&lt;/a&gt;
section of the Rust README provides information on other ways to interact with the Ballista contributors and 
maintainers.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>DataFusion: A Rust-native Query Engine for Apache Arrow</title><link href="https://datafusion.apache.org/blog/2019/02/04/datafusion-donation" rel="alternate"></link><published>2019-02-04T00:00:00+00:00</published><updated>2019-02-04T00:00:00+00:00</updated><author><name>agrove</name></author><id>tag:datafusion.apache.org,2019-02-04:/blog/2019/02/04/datafusion-donation</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are excited to announce that &lt;a href="https://github.com/apache/arrow-datafusion"&gt;DataFusion&lt;/a&gt; has been donated to the Apache Arrow project. DataFusion is an in-memory query engine for the Rust implementation of Apache Arrow.&lt;/p&gt;
&lt;p&gt;Although DataFusion was started two years ago, it was recently re-implemented to be Arrow-native and currently has limited capabilities but does support …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are excited to announce that &lt;a href="https://github.com/apache/arrow-datafusion"&gt;DataFusion&lt;/a&gt; has been donated to the Apache Arrow project. DataFusion is an in-memory query engine for the Rust implementation of Apache Arrow.&lt;/p&gt;
&lt;p&gt;Although DataFusion was started two years ago, it was recently re-implemented to be Arrow-native and currently has limited capabilities but does support SQL queries against iterators of RecordBatch and has support for CSV files. There are plans to &lt;a href="https://issues.apache.org/jira/browse/ARROW-4466"&gt;add support for Parquet files&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;SQL support is limited to projection (&lt;code&gt;SELECT&lt;/code&gt;), selection (&lt;code&gt;WHERE&lt;/code&gt;), and simple aggregates (&lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt;, &lt;code&gt;SUM&lt;/code&gt;) with an optional &lt;code&gt;GROUP BY&lt;/code&gt; clause.&lt;/p&gt;
&lt;p&gt;Supported expressions are identifiers, literals, simple math operations (&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;), binary expressions (&lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;OR&lt;/code&gt;), equality and comparison operators (&lt;code&gt;=&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;), and &lt;code&gt;CAST(expr AS type)&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;The following example demonstrates running a simple aggregate SQL query against a CSV file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// create execution context
let mut ctx = ExecutionContext::new();

// define schema for data source (csv file)
let schema = Arc::new(Schema::new(vec![
    Field::new("c1", DataType::Utf8, false),
    Field::new("c2", DataType::UInt32, false),
    Field::new("c3", DataType::Int8, false),
    Field::new("c4", DataType::Int16, false),
    Field::new("c5", DataType::Int32, false),
    Field::new("c6", DataType::Int64, false),
    Field::new("c7", DataType::UInt8, false),
    Field::new("c8", DataType::UInt16, false),
    Field::new("c9", DataType::UInt32, false),
    Field::new("c10", DataType::UInt64, false),
    Field::new("c11", DataType::Float32, false),
    Field::new("c12", DataType::Float64, false),
    Field::new("c13", DataType::Utf8, false),
]));

// register csv file with the execution context
let csv_datasource =
    CsvDataSource::new("test/data/aggregate_test_100.csv", schema.clone(), 1024);
ctx.register_datasource("aggregate_test_100", Rc::new(RefCell::new(csv_datasource)));

let sql = "SELECT c1, MIN(c12), MAX(c12) FROM aggregate_test_100 WHERE c11 &amp;gt; 0.1 AND c11 &amp;lt; 0.9 GROUP BY c1";

// execute the query
let relation = ctx.sql(&amp;amp;sql).unwrap();
let mut results = relation.borrow_mut();

// iterate over the results
while let Some(batch) = results.next().unwrap() {
    println!(
        "RecordBatch has {} rows and {} columns",
        batch.num_rows(),
        batch.num_columns()
    );

    let c1 = batch
        .column(0)
        .as_any()
        .downcast_ref::&amp;lt;BinaryArray&amp;gt;()
        .unwrap();

    let min = batch
        .column(1)
        .as_any()
        .downcast_ref::&amp;lt;Float64Array&amp;gt;()
        .unwrap();

    let max = batch
        .column(2)
        .as_any()
        .downcast_ref::&amp;lt;Float64Array&amp;gt;()
        .unwrap();

    for i in 0..batch.num_rows() {
        let c1_value: String = String::from_utf8(c1.value(i).to_vec()).unwrap();
        println!("{}, Min: {}, Max: {}", c1_value, min.value(i), max.value(i),);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;The roadmap for DataFusion will depend on interest from the Rust community, but here are some of the short term items that are planned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extending test coverage of the existing functionality&lt;/li&gt;
&lt;li&gt;Adding support for Parquet data sources&lt;/li&gt;
&lt;li&gt;Implementing more SQL features such as &lt;code&gt;JOIN&lt;/code&gt;, &lt;code&gt;ORDER BY&lt;/code&gt; and &lt;code&gt;LIMIT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Implement a DataFrame API as an alternative to SQL&lt;/li&gt;
&lt;li&gt;Adding support for partitioning and parallel query execution using Rust's async and await functionality&lt;/li&gt;
&lt;li&gt;Creating a Docker image to make it easy to use DataFusion as a standalone query tool for interactive and batch queries&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contributors Welcome!&lt;/h2&gt;
&lt;p&gt;If you are excited about being able to use Rust for data science and would like to contribute to this work then there are many ways to get involved. The simplest way to get started is to try out DataFusion against your own data sources and file bug reports for any issues that you find. You could also check out the current &lt;a href="https://cwiki.apache.org/confluence/display/ARROW/Rust+JIRA+Dashboard"&gt;list of issues&lt;/a&gt; and have a go at fixing one. You can also join the &lt;a href="http://mail-archives.apache.org/mod_mbox/arrow-user/"&gt;user mailing list&lt;/a&gt; to ask questions.&lt;/p&gt;</content><category term="blog"></category></entry></feed>