<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://datafusion.apache.org/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://datafusion.apache.org/blog/" rel="alternate" type="text/html" /><updated>2024-11-20T21:23:41+00:00</updated><id>https://datafusion.apache.org/blog/feed.xml</id><title type="html">Apache DataFusion Project News &amp;amp; Blog</title><subtitle>Apache DataFusion is a very fast, extensible query engine for building high-quality  data-centric systems in Rust, using the Apache Arrow in-memory format.</subtitle><entry><title type="html">Apache DataFusion Comet 0.4.0 Release</title><link href="https://datafusion.apache.org/blog/2024/11/20/datafusion-comet-0.4.0/" rel="alternate" type="text/html" title="Apache DataFusion Comet 0.4.0 Release" /><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/11/20/datafusion-comet-0.4.0</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/11/20/datafusion-comet-0.4.0/"><![CDATA[<!--

-->

<p>The Apache DataFusion PMC is pleased to announce version 0.4.0 of the <a href="https://datafusion.apache.org/comet/">Comet</a> subproject.</p>

<p>Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.</p>

<p>Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the <a href="https://datafusion.apache.org/comet/user-guide/compatibility.html">compatibility guide</a> for more information.</p>

<p>This release covers approximately six weeks of development work and is the result of merging 51 PRs from 10
contributors. See the <a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.4.0.md">change log</a> for more information.</p>

<h2 id="release-highlights">Release Highlights</h2>

<h3 id="performance--stability">Performance &amp; Stability</h3>

<p>There are a number of performance and stability improvements in this release. Here is a summary of some of the
larger changes. Current benchmarking results can be found in the <a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html">Comet Benchmarking Guide</a>.</p>

<h4 id="unified-memory-management">Unified Memory Management</h4>

<p>Comet now uses a unified memory management approach that shares an off-heap memory pool with Apache Spark, resulting
in a much simpler configuration. Comet now requires <code class="language-plaintext highlighter-rouge">spark.memory.offHeap.enabled=true</code>. This approach provides a
holistic view of memory usage in Spark and Comet and makes it easier to optimize system performance.</p>

<h4 id="faster-joins">Faster Joins</h4>

<p>Apache Spark supports sort-merge and hash joins, which have similar performance characteristics. Spark defaults to
using sort-merge joins because they are less likely to result in OutOfMemory exceptions. In vectorized query
engines such as DataFusion, hash joins outperform sort-merge joins. Comet now has an experimental feature to
replace Spark sort-merge joins with hash joins for improved performance. This feature is experimental because
there is currently no spill-to-disk support in the hash join implementation. This feature can be enabled by
setting <code class="language-plaintext highlighter-rouge">spark.comet.exec.replaceSortMergeJoin=true</code>.</p>

<h4 id="bloom-filter-aggregates">Bloom Filter Aggregates</h4>

<p>Spark’s optimizer can insert Bloom filter aggregations and filters to prune large result sets before a shuffle. However,
Comet would fall back to Spark for the aggregation. Comet now has native support for Bloom filter aggregations
after previously supporting Bloom filter testing. Users no longer need to set
<code class="language-plaintext highlighter-rouge">spark.sql.optimizer.runtime.bloomFilter.enabled=false</code> when using Comet.</p>

<h4 id="complex-type-support">Complex Type support</h4>

<p>This release has the following improvements to complex type support:</p>

<ul>
  <li>Implemented <code class="language-plaintext highlighter-rouge">ArrayAppend</code> and <code class="language-plaintext highlighter-rouge">GetArrayStructFields</code>.</li>
  <li>Implemented native cast between structs</li>
  <li>Implemented native cast from structs to string</li>
</ul>

<h2 id="roadmap">Roadmap</h2>

<p>One of the highest priority items on the roadmap is to add support for reading complex types (maps, structs, and arrays)
from Parquet sources, both when reading Parquet directly and from Iceberg.</p>

<p>Comet currently has proprietary native code for decoding Parquet pages, native column readers for all of Spark’s
primitive types, and special handling for Spark-specific use cases such as timestamp rebasing and decimal type
promotion. This implementation does not yet support complex types. File IO, decryption, and decompression are handled
in JVM code, and Parquet pages are passed on to native code for decoding.</p>

<p>Rather than add complex type support to this existing code, we are exploring two main options to allow us to
leverage more of the upstream Arrow and DataFusion code.</p>

<h3 id="use-datafusions-parquetexec">Use DataFusion’s ParquetExec</h3>

<p>For use cases where DataFusion can support reading a Parquet source, Comet could create a native plan that uses
DataFusion’s ParquetExec. We are investigating using DataFusion’s SchemaAdapter to handle some Spark-specific
handling of timestamps and decimals.</p>

<h3 id="use-arrows-parquet-batch-reader">Use Arrow’s Parquet Batch Reader</h3>

<p>For use cases not supported by DataFusion’s ParquetExec, such as integrating with Iceberg, we are exploring
replacing our current native Parquet decoding logic with the Arrow readers provided by the Parquet crate.</p>

<p>Iceberg already provides a vectorized Spark reader for Parquet. A <a href="https://github.com/apache/iceberg/pull/9841">PR</a> is open against Iceberg for adding a native
version based on Comet, and we hope to update this to leverage the improvements outlined above.</p>

<h2 id="getting-involved">Getting Involved</h2>

<p>The Comet project welcomes new contributors. We use the same <a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord">Slack and Discord</a> channels as the main DataFusion
project and have a weekly <a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing">DataFusion video call</a>.</p>

<p>The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the <a href="https://datafusion.apache.org/comet/user-guide/installation.html">Getting Started</a> guide for instructions on downloading and installing
Comet.</p>

<p>There are also many <a href="https://github.com/apache/datafusion-comet/contribute">good first issues</a> waiting for contributions.</p>]]></content><author><name>pmc</name></author><category term="subprojects" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">Comparing approaches to User Defined Functions in Apache DataFusion using Python</title><link href="https://datafusion.apache.org/blog/2024/11/19/datafusion-python-udf-comparisons/" rel="alternate" type="text/html" title="Comparing approaches to User Defined Functions in Apache DataFusion using Python" /><published>2024-11-19T00:00:00+00:00</published><updated>2024-11-19T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/11/19/datafusion-python-udf-comparisons</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/11/19/datafusion-python-udf-comparisons/"><![CDATA[<!--

-->
<h1 id="writing-user-defined-functions-in-apache-datafusion-using-python">Writing User Defined Functions in Apache DataFusion using Python</h1>

<h2 id="personal-context">Personal Context</h2>

<p>For a few months now I’ve been working with <a href="https://datafusion.apache.org/">Apache DataFusion</a>, a
fast query engine written in Rust. From my experience the language that nearly all data scientists
are working in is Python. In general, data scientists often use <a href="https://pandas.pydata.org/">Pandas</a>
for in-memory tasks and <a href="https://spark.apache.org/">PySpark</a> for larger tasks that require
distributed processing.</p>

<p>In addition to DataFusion, there is another Rust based newcomer to the DataFrame world,
<a href="https://pola.rs/">Polars</a>. The latter is growing extremely fast, and it serves many of the same
use cases as DataFusion. For my use cases, I’m interested in DataFusion because I want to be able
to build small scale tests rapidly and then scale them up to larger distributed systems with ease.
I do recommend evaluating Polars for in-memory work.</p>

<p>Personally, I would love a single query approach that is fast for both in-memory usage and can
extend to large batch processing to exploit parallelization. I think DataFusion, coupled with
<a href="https://datafusion.apache.org/ballista/">Ballista</a> or
<a href="https://github.com/apache/datafusion-ray">DataFusion-Ray</a>, may provide this solution.</p>

<p>As I’m testing, I’m primarily limiting my work to the
<a href="https://datafusion.apache.org/python/">datafusion-python</a> project, a wrapper around the Rust
DataFusion library. This wrapper gives you the speed advantages of keeping all of the data in the
Rust implementation and the ergonomics of working in Python. Personally, I would prefer to work
purely in Rust, but I also recognize that since the industry works in Python we should meet the
people where they are.</p>

<h2 id="user-defined-functions">User-Defined Functions</h2>

<p>The focus of this post is User-Defined Functions (UDFs). The DataFusion library gives a lot of
useful functions already for doing DataFrame manipulation. These are going to be similar to those
you find in other DataFrame libraries. You’ll be able to do simple arithmetic, create substrings of
columns, or find the average value across a group of rows. These cover most of the use cases
you’ll need in a DataFrame.</p>

<p>However, there will always arise times when you want a custom function. With UDFs you open a
world of possibilities in your code. Sometimes there simply isn’t an easy way to use built-in
functions to achieve your goals.</p>

<p>In the following, I’m going to demonstrate two example use cases. These are based on real world
problems I’ve encountered. Also I want to demonstrate the approach of “make it work, make it work
well, make it work fast” that is a motto I’ve seen thrown around in data science.</p>

<p>I will demonstrate three approaches to writing UDFs. In order of increasing performance they are</p>

<ul>
  <li>Writing a pure Python function to do your computation</li>
  <li>Using the PyArrow libraries in Python to accelerate your processing</li>
  <li>Writing a UDF in Rust and exposing it to Python</li>
</ul>

<p>Additionally I will demonstrate two variants of this. The first will be nearly identical to the
PyArrow library approach to simplify understanding how to connect the Rust code to Python. In the
second version we will do the iteration through the input arrays ourselves to give even greater
flexibility to the user.</p>

<p>Here are the two example use cases, taken from my own work but generalized.</p>

<h3 id="use-case-1-scalar-function">Use Case 1: Scalar Function</h3>

<p>I have a DataFrame and a list of tuples that I’m interested in. I want to filter out the DataFrame
to only have values that match those tuples from certain columns in the DataFrame.</p>

<p>To give a concrete example, we will use data generated for the <a href="https://www.tpc.org/tpch/">TPC-H benchmarks</a>.
Suppose I have a table of sales line items. There are many columns, but I am interested in three: a
part key (<code class="language-plaintext highlighter-rouge">p_partkey</code>), supplier key (<code class="language-plaintext highlighter-rouge">p_suppkey</code>), and return status (<code class="language-plaintext highlighter-rouge">p_returnflag</code>). I want
only to return a DataFrame with a specific combination of these three values. That is, I want
to know if part number 1530 from supplier 4031 was sold (not returned), so I want a specific
combination of <code class="language-plaintext highlighter-rouge">p_partkey = 1530</code>, <code class="language-plaintext highlighter-rouge">p_suppkey = 4031</code>, and <code class="language-plaintext highlighter-rouge">p_returnflag = 'N'</code>. I have a small
handful of these combinations I want to return.</p>

<p>Probably the most ergonomic way to do this without UDF is to turn that list of tuples into a
DataFrame itself, perform a join, and select the columns from the original DataFrame. If we were
working in PySpark we would probably broadcast join the DataFrame created from the tuple list since
it is tiny. In practice, I have found that with some DataFrame libraries performing a filter rather
than a join can be significantly faster. This is worth profiling for your specific use case.</p>

<h3 id="use-case-2-aggregate-function">Use Case 2: Aggregate Function</h3>

<p>I have a DataFrame with many values that I want to aggregate. I have already analyzed it and
determined there is a noise level below which I do not want to include in my analysis. I want to
compute a sum of only values that are above my noise threshold.</p>

<p>This can be done fairly easy without leaning on a User Defined Aggegate Function (UDAF). You can
simply filter the DataFrame and then aggregate using the built-in <code class="language-plaintext highlighter-rouge">sum</code> function. Here, we
demonstrate doing this as a UDF primarily as an example of how to write UDAFs. We will use the
PyArrow compute approach.</p>

<h2 id="pure-python-approach">Pure Python approach</h2>

<p>The fastest way (developer time, not code time) for me to implement the scalar problem solution
was to do something along the lines of “for each row, check the values of interest contains that
tuple”. I’ve published this as
<a href="https://github.com/apache/datafusion-python/blob/main/examples/python-udf-comparisons.py">an example</a>
in the <a href="https://github.com/apache/datafusion-python">datafusion-python repository</a>. Here is an
example of how this can be done:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">values_of_interest</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="mi">1530</span><span class="p">,</span> <span class="mi">4031</span><span class="p">,</span> <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">6530</span><span class="p">,</span> <span class="mi">1531</span><span class="p">,</span> <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">5618</span><span class="p">,</span> <span class="mi">619</span><span class="p">,</span> <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">(</span><span class="mi">8118</span><span class="p">,</span> <span class="mi">8119</span><span class="p">,</span> <span class="sh">"</span><span class="s">N</span><span class="sh">"</span><span class="p">),</span>
<span class="p">]</span>

<span class="k">def</span> <span class="nf">is_of_interest_impl</span><span class="p">(</span>
    <span class="n">partkey_arr</span><span class="p">:</span> <span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">suppkey_arr</span><span class="p">:</span> <span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">returnflag_arr</span><span class="p">:</span> <span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">partkey</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">partkey_arr</span><span class="p">):</span>
        <span class="n">partkey</span> <span class="o">=</span> <span class="n">partkey</span><span class="p">.</span><span class="nf">as_py</span><span class="p">()</span>
        <span class="n">suppkey</span> <span class="o">=</span> <span class="n">suppkey_arr</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">as_py</span><span class="p">()</span>
        <span class="n">returnflag</span> <span class="o">=</span> <span class="n">returnflag_arr</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">as_py</span><span class="p">()</span>
        <span class="n">value</span> <span class="o">=</span> <span class="p">(</span><span class="n">partkey</span><span class="p">,</span> <span class="n">suppkey</span><span class="p">,</span> <span class="n">returnflag</span><span class="p">)</span>
        <span class="n">result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">value</span> <span class="ow">in</span> <span class="n">values_of_interest</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pa</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="c1"># Wrap our custom function with `datafusion.udf`, annotating expected 
# parameter and return types
</span><span class="n">is_of_interest</span> <span class="o">=</span> <span class="nf">udf</span><span class="p">(</span>
    <span class="n">is_of_interest_impl</span><span class="p">,</span>
    <span class="p">[</span><span class="n">pa</span><span class="p">.</span><span class="nf">int64</span><span class="p">(),</span> <span class="n">pa</span><span class="p">.</span><span class="nf">int64</span><span class="p">(),</span> <span class="n">pa</span><span class="p">.</span><span class="nf">utf8</span><span class="p">()],</span>
    <span class="n">pa</span><span class="p">.</span><span class="nf">bool_</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">stable</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">df_udf_filter</span> <span class="o">=</span> <span class="n">df_lineitem</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span>
    <span class="nf">is_of_interest</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_partkey</span><span class="sh">"</span><span class="p">),</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_suppkey</span><span class="sh">"</span><span class="p">),</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_returnflag</span><span class="sh">"</span><span class="p">))</span>
<span class="p">)</span>
</code></pre></div></div>

<p>When working with a DataFusion UDF in Python, you define your function to take in some number of
expressions. During the evaluation, these will get computed into their corresponding values and
passed to your UDF as a PyArrow Array. We must return an Array also with the same number of
elements (rows). So the UDF example just iterates through all of the arrays and checks to see if
the tuple created from these columns matches any of those that we’re looking for.</p>

<p>I’ll repeat because this is something that tripped me up the first time I wrote a UDF for
datafusion: <strong>DataFusion UDFs, even scalar UDFs, process an array of values at a time not a single
row.</strong> This is different from some other DataFrame libraries and you may need to recognize a slight
change in mentality.</p>

<p>Some important lines here are the lines like <code class="language-plaintext highlighter-rouge">partkey = partkey.as_py()</code>. When we do this, we pay a
heavy cost. Now instead of keeping the analysis in the Rust code, we have to take the values in the
array and convert them over to Python objects. In this case we end up getting two numbers and a
string as real Python objects, complete with reference counting and all. Also we are iterating
through the array in Python rather than Rust native. These will <strong>significantly</strong> slow down your
code. Any time you have to cross the barrier where you change values inside the Rust arrays into
Python objects or vice versa you will pay <strong>heavy</strong> cost in that transformation. You will want to
design your UDFs to avoid this as much as possible.</p>

<h2 id="python-approach-using-pyarrow-compute">Python approach using PyArrow compute</h2>

<p>DataFusion uses <a href="https://arrow.apache.org/">Apache Arrow</a> as its in-memory data format. This can
be seen in the way that Arrow Arrays are passed into the UDFs. We can take advantage of the fact
that <a href="https://arrow.apache.org/docs/python/">PyArrow</a>, the canonical Python Arrow implementation,
provides a variety of
useful functions. In the example below, we are only using a few of the boolean functions and the
equality function. Each of these functions takes two arrays and analyzes them row by row. In the
below example, we shift the logic around a little since we are now operating on an entire array of
values instead of checking a single row ourselves.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pyarrow.compute</span> <span class="k">as</span> <span class="n">pc</span>

<span class="k">def</span> <span class="nf">udf_using_pyarrow_compute_impl</span><span class="p">(</span>
    <span class="n">partkey_arr</span><span class="p">:</span> <span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">suppkey_arr</span><span class="p">:</span> <span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
    <span class="n">returnflag_arr</span><span class="p">:</span> <span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">partkey</span><span class="p">,</span> <span class="n">suppkey</span><span class="p">,</span> <span class="n">returnflag</span> <span class="ow">in</span> <span class="n">values_of_interest</span><span class="p">:</span>
        <span class="n">filtered_partkey_arr</span> <span class="o">=</span> <span class="n">pc</span><span class="p">.</span><span class="nf">equal</span><span class="p">(</span><span class="n">partkey_arr</span><span class="p">,</span> <span class="n">partkey</span><span class="p">)</span>
        <span class="n">filtered_suppkey_arr</span> <span class="o">=</span> <span class="n">pc</span><span class="p">.</span><span class="nf">equal</span><span class="p">(</span><span class="n">suppkey_arr</span><span class="p">,</span> <span class="n">suppkey</span><span class="p">)</span>
        <span class="n">filtered_returnflag_arr</span> <span class="o">=</span> <span class="n">pc</span><span class="p">.</span><span class="nf">equal</span><span class="p">(</span><span class="n">returnflag_arr</span><span class="p">,</span> <span class="n">returnflag</span><span class="p">)</span>

        <span class="n">resultant_arr</span> <span class="o">=</span> <span class="n">pc</span><span class="p">.</span><span class="nf">and_</span><span class="p">(</span><span class="n">filtered_partkey_arr</span><span class="p">,</span> <span class="n">filtered_suppkey_arr</span><span class="p">)</span>
        <span class="n">resultant_arr</span> <span class="o">=</span> <span class="n">pc</span><span class="p">.</span><span class="nf">and_</span><span class="p">(</span><span class="n">resultant_arr</span><span class="p">,</span> <span class="n">filtered_returnflag_arr</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">results</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">resultant_arr</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">results</span> <span class="o">=</span> <span class="n">pc</span><span class="p">.</span><span class="nf">or_</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">resultant_arr</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">results</span>


<span class="n">udf_using_pyarrow_compute</span> <span class="o">=</span> <span class="nf">udf</span><span class="p">(</span>
    <span class="n">udf_using_pyarrow_compute_impl</span><span class="p">,</span>
    <span class="p">[</span><span class="n">pa</span><span class="p">.</span><span class="nf">int64</span><span class="p">(),</span> <span class="n">pa</span><span class="p">.</span><span class="nf">int64</span><span class="p">(),</span> <span class="n">pa</span><span class="p">.</span><span class="nf">utf8</span><span class="p">()],</span>
    <span class="n">pa</span><span class="p">.</span><span class="nf">bool_</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">stable</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">df_udf_pyarrow_compute</span> <span class="o">=</span> <span class="n">df_lineitem</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span>
    <span class="nf">udf_using_pyarrow_compute</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_partkey</span><span class="sh">"</span><span class="p">),</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_suppkey</span><span class="sh">"</span><span class="p">),</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_returnflag</span><span class="sh">"</span><span class="p">))</span>
<span class="p">)</span>
</code></pre></div></div>

<p>The idea in the code above is that we will iterate through each of the values of interest, which we
expect to be small. For each of the columns, we compare the value of interest to it’s corresponding
array using <code class="language-plaintext highlighter-rouge">pyarrow.compute.equal</code>. This will give use three boolean arrays. We have a match to
the tuple if we have a row in all three arrays that is true, so we use <code class="language-plaintext highlighter-rouge">pyarrow.compute.and_</code>. Now
our return value from the UDF needs to include arrays for which any of the values of interest list
of tuples exists, so we take the result from the current loop and perform a <code class="language-plaintext highlighter-rouge">pyarrow.compute.or_</code>
on it.</p>

<p>From my benchmarking, switching from approach of converting values into Python objects to this
approach of using the PyArrow built-in functions leads to about a 10x speed improvement in this
simple problem.</p>

<p>It’s worth noting that almost all of the PyArrow compute functions expect to take one or two arrays
as their arguments. If you need to write a UDF that is evaluating three or more columns, you’ll
need to do something akin to what we’ve shown here.</p>

<h2 id="rust-udf-with-python-wrapper">Rust UDF with Python wrapper</h2>

<p>This is the most complicated approach, but has the potential to be the most performant. What we
will do here is write a Rust function to perform our computation and then expose that function to
Python. I know of two use cases where I would recommend this approach. The first is the case when
the PyArrow compute functions are insufficient for your needs. Perhaps your code is too complex or
could be greatly simplified if you pulled in some outside dependency. The second use case is when
you have written a UDF that you’re sharing across multiple projects and have hardened the approach.
It is possible that you can implement your function in Rust to give a speed improvement and then
every project that is using this shared UDF will benefit from those updates.</p>

<p>When deciding to use this approach, it’s worth considering how much you think you’ll actually
benefit from the Rust implementation to decide if it’s worth the additional effort to maintain and
deploy the Python wheels you generate. It is certainly not necessary for every use case.</p>

<p>Due to the excellent work by the Python arrow team, we can simplify our work to needing only two
dependencies on the Rust side, <a href="https://github.com/apache/arrow-rs">arrow-rs</a> and
<a href="https://pyo3.rs/">pyo3</a>. I have posted a <a href="https://github.com/timsaucer/tuple_filter_example">minimal example</a>.
You’ll need <a href="https://github.com/PyO3/maturin">maturin</a> to build the project, and you must use
release mode when building to get the expected performance.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>maturin develop <span class="nt">--release</span>
</code></pre></div></div>

<p>When you write your UDF in Rust you generally will need to take these steps</p>

<ol>
  <li>Write a function description that takes in some number of Python generic objects.</li>
  <li>Convert these objects to Arrow Arrays of the appropriate type(s).</li>
  <li>Perform your computation and create a resultant Array.</li>
  <li>Convert the array into a Python generic object.</li>
</ol>

<p>For the conversion to and from Python objects, we can take advantage of the
<code class="language-plaintext highlighter-rouge">ArrayData::from_pyarrow_bound</code> and <code class="language-plaintext highlighter-rouge">ArrayData::to_pyarrow</code> functions.  All that remains is to
perform your computation.</p>

<p>We are going to demonstrate doing this computation in two ways. The first is to mimic what we’ve
done in the above approach using PyArrow. In the second we demonstrate iterating through the three
arrays ourselves.</p>

<p>In our first approach, we can expect the performance to be nearly identical to when we used the
PyArrow compute functions. On the Rust side we will have slightly less overhead but the heavy
lifting portions of the code are essentially the same between this Rust implementation and the
PyArrow approach above.</p>

<p>The reason for demonstrating this, even though it doesn’t provide a significant speedup over
Python, is to primarily demonstrate how to make the Python to Rust with Python wrapper
transition. In the second implementation you can see how we can iterate through all of the arrays
ourselves.</p>

<p>In this first example, we are hard coding the values of interest, but in the following section
we demonstrate passing these in during initalization.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">#[pyfunction]</span>
<span class="k">pub</span> <span class="k">fn</span> <span class="nf">tuple_filter_fn</span><span class="p">(</span>
    <span class="n">py</span><span class="p">:</span> <span class="n">Python</span><span class="o">&lt;</span><span class="nv">'_</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">partkey_expr</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyAny</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">suppkey_expr</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyAny</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">returnflag_expr</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyAny</span><span class="o">&gt;</span><span class="p">,</span>
<span class="p">)</span> <span class="k">-&gt;</span> <span class="n">PyResult</span><span class="o">&lt;</span><span class="n">Py</span><span class="o">&lt;</span><span class="n">PyAny</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">partkey_arr</span><span class="p">:</span> <span class="n">PrimitiveArray</span><span class="o">&lt;</span><span class="n">Int64Type</span><span class="o">&gt;</span> <span class="o">=</span>
        <span class="nn">ArrayData</span><span class="p">::</span><span class="nf">from_pyarrow_bound</span><span class="p">(</span><span class="n">partkey_expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.into</span><span class="p">();</span>
    <span class="k">let</span> <span class="n">suppkey_arr</span><span class="p">:</span> <span class="n">PrimitiveArray</span><span class="o">&lt;</span><span class="n">Int64Type</span><span class="o">&gt;</span> <span class="o">=</span>
        <span class="nn">ArrayData</span><span class="p">::</span><span class="nf">from_pyarrow_bound</span><span class="p">(</span><span class="n">suppkey_expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.into</span><span class="p">();</span>
    <span class="k">let</span> <span class="n">returnflag_arr</span><span class="p">:</span> <span class="n">StringArray</span> <span class="o">=</span> <span class="nn">ArrayData</span><span class="p">::</span><span class="nf">from_pyarrow_bound</span><span class="p">(</span><span class="n">returnflag_expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.into</span><span class="p">();</span>

    <span class="k">let</span> <span class="n">values_of_interest</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span>
        <span class="p">(</span><span class="mi">1530</span><span class="p">,</span> <span class="mi">4031</span><span class="p">,</span> <span class="s">"N"</span><span class="nf">.to_string</span><span class="p">()),</span>
        <span class="p">(</span><span class="mi">6530</span><span class="p">,</span> <span class="mi">1531</span><span class="p">,</span> <span class="s">"N"</span><span class="nf">.to_string</span><span class="p">()),</span>
        <span class="p">(</span><span class="mi">5618</span><span class="p">,</span> <span class="mi">619</span><span class="p">,</span> <span class="s">"N"</span><span class="nf">.to_string</span><span class="p">()),</span>
        <span class="p">(</span><span class="mi">8118</span><span class="p">,</span> <span class="mi">8119</span><span class="p">,</span> <span class="s">"N"</span><span class="nf">.to_string</span><span class="p">()),</span>
    <span class="p">];</span>

    <span class="k">let</span> <span class="k">mut</span> <span class="n">res</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;</span><span class="n">BooleanArray</span><span class="o">&gt;</span> <span class="o">=</span> <span class="nb">None</span><span class="p">;</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">partkey</span><span class="p">,</span> <span class="n">suppkey</span><span class="p">,</span> <span class="n">returnflag</span><span class="p">)</span> <span class="k">in</span> <span class="o">&amp;</span><span class="n">values_of_interest</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">filtered_partkey_arr</span> <span class="o">=</span> <span class="nn">BooleanArray</span><span class="p">::</span><span class="nf">from_unary</span><span class="p">(</span><span class="o">&amp;</span><span class="n">partkey_arr</span><span class="p">,</span> <span class="p">|</span><span class="n">p</span><span class="p">|</span> <span class="n">p</span> <span class="o">==</span> <span class="o">*</span><span class="n">partkey</span><span class="p">);</span>
        <span class="k">let</span> <span class="n">filtered_suppkey_arr</span> <span class="o">=</span> <span class="nn">BooleanArray</span><span class="p">::</span><span class="nf">from_unary</span><span class="p">(</span><span class="o">&amp;</span><span class="n">suppkey_arr</span><span class="p">,</span> <span class="p">|</span><span class="n">s</span><span class="p">|</span> <span class="n">s</span> <span class="o">==</span> <span class="o">*</span><span class="n">suppkey</span><span class="p">);</span>
        <span class="k">let</span> <span class="n">filtered_returnflag_arr</span> <span class="o">=</span>
            <span class="nn">BooleanArray</span><span class="p">::</span><span class="nf">from_unary</span><span class="p">(</span><span class="o">&amp;</span><span class="n">returnflag_arr</span><span class="p">,</span> <span class="p">|</span><span class="n">s</span><span class="p">|</span> <span class="n">s</span> <span class="o">==</span> <span class="n">returnflag</span><span class="p">);</span>

        <span class="k">let</span> <span class="n">part_and_supp</span> <span class="o">=</span> <span class="nn">compute</span><span class="p">::</span><span class="nf">and</span><span class="p">(</span><span class="o">&amp;</span><span class="n">filtered_partkey_arr</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">filtered_suppkey_arr</span><span class="p">)</span>
            <span class="nf">.map_err</span><span class="p">(|</span><span class="n">e</span><span class="p">|</span> <span class="nn">PyValueError</span><span class="p">::</span><span class="nf">new_err</span><span class="p">(</span><span class="n">e</span><span class="nf">.to_string</span><span class="p">()))</span><span class="o">?</span><span class="p">;</span>
        <span class="k">let</span> <span class="n">resultant_arr</span> <span class="o">=</span> <span class="nn">compute</span><span class="p">::</span><span class="nf">and</span><span class="p">(</span><span class="o">&amp;</span><span class="n">part_and_supp</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">filtered_returnflag_arr</span><span class="p">)</span>
            <span class="nf">.map_err</span><span class="p">(|</span><span class="n">e</span><span class="p">|</span> <span class="nn">PyValueError</span><span class="p">::</span><span class="nf">new_err</span><span class="p">(</span><span class="n">e</span><span class="nf">.to_string</span><span class="p">()))</span><span class="o">?</span><span class="p">;</span>

        <span class="n">res</span> <span class="o">=</span> <span class="k">match</span> <span class="n">res</span> <span class="p">{</span>
            <span class="nf">Some</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nn">compute</span><span class="p">::</span><span class="nf">or</span><span class="p">(</span><span class="o">&amp;</span><span class="n">r</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">resultant_arr</span><span class="p">)</span><span class="nf">.ok</span><span class="p">(),</span>
            <span class="nb">None</span> <span class="k">=&gt;</span> <span class="nf">Some</span><span class="p">(</span><span class="n">resultant_arr</span><span class="p">),</span>
        <span class="p">};</span>
    <span class="p">}</span>

    <span class="n">res</span><span class="nf">.unwrap</span><span class="p">()</span><span class="nf">.into_data</span><span class="p">()</span><span class="nf">.to_pyarrow</span><span class="p">(</span><span class="n">py</span><span class="p">)</span>
<span class="p">}</span>


<span class="nd">#[pymodule]</span>
<span class="k">fn</span> <span class="nf">tuple_filter_example</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyModule</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">PyResult</span><span class="o">&lt;</span><span class="p">()</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="n">module</span><span class="nf">.add_function</span><span class="p">(</span><span class="nd">wrap_pyfunction!</span><span class="p">(</span><span class="n">tuple_filter_fn</span><span class="p">,</span> <span class="n">module</span><span class="p">)</span><span class="o">?</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>

<p>To use this we use the <code class="language-plaintext highlighter-rouge">udf</code> function in <code class="language-plaintext highlighter-rouge">datafusion-python</code> just as before.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datafusion</span> <span class="kn">import</span> <span class="n">udf</span>
<span class="kn">import</span> <span class="n">pyarrow</span> <span class="k">as</span> <span class="n">pa</span>
<span class="kn">from</span> <span class="n">tuple_filter_example</span> <span class="kn">import</span> <span class="n">tuple_filter_fn</span>

<span class="n">udf_using_custom_rust_fn</span> <span class="o">=</span> <span class="nf">udf</span><span class="p">(</span>
    <span class="n">tuple_filter_fn</span><span class="p">,</span>
    <span class="p">[</span><span class="n">pa</span><span class="p">.</span><span class="nf">int64</span><span class="p">(),</span> <span class="n">pa</span><span class="p">.</span><span class="nf">int64</span><span class="p">(),</span> <span class="n">pa</span><span class="p">.</span><span class="nf">utf8</span><span class="p">()],</span>
    <span class="n">pa</span><span class="p">.</span><span class="nf">bool_</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">stable</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>That’s it! We’ve now got a third party Rust UDF with Python wrappers working with DataFusion’s
Python bindings!</p>

<h3 id="rust-udf-with-initialization">Rust UDF with initialization</h3>

<p>Looking at the code above, you can see that it is hard coding the values we’re interested in. There
are many types of UDFs that don’t require any additional data provided to them before they start
the computation. The code above is sloppy, so let’s clean it up.</p>

<p>We want to write the function to take some additional data. A limitation of the UDFs we create is
that they expect to operate on entire arrays of data at a time. We can get around this problem by
creating an initializer for our UDF. We do this by defining a Rust struct that contains the data we
need and implement two methods on this struct, <code class="language-plaintext highlighter-rouge">new</code> and <code class="language-plaintext highlighter-rouge">__call__</code>. By doing this we will create a
Python object that is callable, so it can be the function we provide to <code class="language-plaintext highlighter-rouge">udf</code>.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">#[pyclass]</span>
<span class="k">pub</span> <span class="k">struct</span> <span class="n">TupleFilterClass</span> <span class="p">{</span>
    <span class="n">values_of_interest</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="p">(</span><span class="nb">i64</span><span class="p">,</span> <span class="nb">i64</span><span class="p">,</span> <span class="nb">String</span><span class="p">)</span><span class="o">&gt;</span><span class="p">,</span>
<span class="p">}</span>

<span class="nd">#[pymethods]</span>
<span class="k">impl</span> <span class="n">TupleFilterClass</span> <span class="p">{</span>
    <span class="nd">#[new]</span>
    <span class="k">fn</span> <span class="nf">new</span><span class="p">(</span><span class="n">values_of_interest</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="p">(</span><span class="nb">i64</span><span class="p">,</span> <span class="nb">i64</span><span class="p">,</span> <span class="nb">String</span><span class="p">)</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="k">Self</span> <span class="p">{</span>
        <span class="k">Self</span> <span class="p">{</span>
            <span class="n">values_of_interest</span><span class="p">,</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">fn</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="o">&amp;</span><span class="k">self</span><span class="p">,</span>
        <span class="n">py</span><span class="p">:</span> <span class="n">Python</span><span class="o">&lt;</span><span class="nv">'_</span><span class="o">&gt;</span><span class="p">,</span>
        <span class="n">partkey_expr</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyAny</span><span class="o">&gt;</span><span class="p">,</span>
        <span class="n">suppkey_expr</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyAny</span><span class="o">&gt;</span><span class="p">,</span>
        <span class="n">returnflag_expr</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyAny</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">-&gt;</span> <span class="n">PyResult</span><span class="o">&lt;</span><span class="n">Py</span><span class="o">&lt;</span><span class="n">PyAny</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">partkey_arr</span><span class="p">:</span> <span class="n">PrimitiveArray</span><span class="o">&lt;</span><span class="n">Int64Type</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nn">ArrayData</span><span class="p">::</span><span class="nf">from_pyarrow_bound</span><span class="p">(</span><span class="n">partkey_expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.into</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">suppkey_arr</span><span class="p">:</span> <span class="n">PrimitiveArray</span><span class="o">&lt;</span><span class="n">Int64Type</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nn">ArrayData</span><span class="p">::</span><span class="nf">from_pyarrow_bound</span><span class="p">(</span><span class="n">suppkey_expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.into</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">returnflag_arr</span><span class="p">:</span> <span class="n">StringArray</span> <span class="o">=</span> <span class="nn">ArrayData</span><span class="p">::</span><span class="nf">from_pyarrow_bound</span><span class="p">(</span><span class="n">returnflag_expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.into</span><span class="p">();</span>

        <span class="k">let</span> <span class="k">mut</span> <span class="n">res</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;</span><span class="n">BooleanArray</span><span class="o">&gt;</span> <span class="o">=</span> <span class="nb">None</span><span class="p">;</span>

        <span class="k">for</span> <span class="p">(</span><span class="n">partkey</span><span class="p">,</span> <span class="n">suppkey</span><span class="p">,</span> <span class="n">returnflag</span><span class="p">)</span> <span class="k">in</span> <span class="o">&amp;</span><span class="k">self</span><span class="py">.values_of_interest</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">filtered_partkey_arr</span> <span class="o">=</span> <span class="nn">BooleanArray</span><span class="p">::</span><span class="nf">from_unary</span><span class="p">(</span><span class="o">&amp;</span><span class="n">partkey_arr</span><span class="p">,</span> <span class="p">|</span><span class="n">p</span><span class="p">|</span> <span class="n">p</span> <span class="o">==</span> <span class="o">*</span><span class="n">partkey</span><span class="p">);</span>
            <span class="k">let</span> <span class="n">filtered_suppkey_arr</span> <span class="o">=</span> <span class="nn">BooleanArray</span><span class="p">::</span><span class="nf">from_unary</span><span class="p">(</span><span class="o">&amp;</span><span class="n">suppkey_arr</span><span class="p">,</span> <span class="p">|</span><span class="n">s</span><span class="p">|</span> <span class="n">s</span> <span class="o">==</span> <span class="o">*</span><span class="n">suppkey</span><span class="p">);</span>
            <span class="k">let</span> <span class="n">filtered_returnflag_arr</span> <span class="o">=</span>
                <span class="nn">BooleanArray</span><span class="p">::</span><span class="nf">from_unary</span><span class="p">(</span><span class="o">&amp;</span><span class="n">returnflag_arr</span><span class="p">,</span> <span class="p">|</span><span class="n">s</span><span class="p">|</span> <span class="n">s</span> <span class="o">==</span> <span class="n">returnflag</span><span class="p">);</span>

            <span class="k">let</span> <span class="n">part_and_supp</span> <span class="o">=</span> <span class="nn">compute</span><span class="p">::</span><span class="nf">and</span><span class="p">(</span><span class="o">&amp;</span><span class="n">filtered_partkey_arr</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">filtered_suppkey_arr</span><span class="p">)</span>
                <span class="nf">.map_err</span><span class="p">(|</span><span class="n">e</span><span class="p">|</span> <span class="nn">PyValueError</span><span class="p">::</span><span class="nf">new_err</span><span class="p">(</span><span class="n">e</span><span class="nf">.to_string</span><span class="p">()))</span><span class="o">?</span><span class="p">;</span>
            <span class="k">let</span> <span class="n">resultant_arr</span> <span class="o">=</span> <span class="nn">compute</span><span class="p">::</span><span class="nf">and</span><span class="p">(</span><span class="o">&amp;</span><span class="n">part_and_supp</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">filtered_returnflag_arr</span><span class="p">)</span>
                <span class="nf">.map_err</span><span class="p">(|</span><span class="n">e</span><span class="p">|</span> <span class="nn">PyValueError</span><span class="p">::</span><span class="nf">new_err</span><span class="p">(</span><span class="n">e</span><span class="nf">.to_string</span><span class="p">()))</span><span class="o">?</span><span class="p">;</span>

            <span class="n">res</span> <span class="o">=</span> <span class="k">match</span> <span class="n">res</span> <span class="p">{</span>
                <span class="nf">Some</span><span class="p">(</span><span class="n">r</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nn">compute</span><span class="p">::</span><span class="nf">or</span><span class="p">(</span><span class="o">&amp;</span><span class="n">r</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">resultant_arr</span><span class="p">)</span><span class="nf">.ok</span><span class="p">(),</span>
                <span class="nb">None</span> <span class="k">=&gt;</span> <span class="nf">Some</span><span class="p">(</span><span class="n">resultant_arr</span><span class="p">),</span>
            <span class="p">};</span>
        <span class="p">}</span>

        <span class="n">res</span><span class="nf">.unwrap</span><span class="p">()</span><span class="nf">.into_data</span><span class="p">()</span><span class="nf">.to_pyarrow</span><span class="p">(</span><span class="n">py</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="nd">#[pymodule]</span>
<span class="k">fn</span> <span class="nf">tuple_filter_example</span><span class="p">(</span><span class="n">module</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyModule</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">PyResult</span><span class="o">&lt;</span><span class="p">()</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="n">module</span><span class="py">.add_class</span><span class="p">::</span><span class="o">&lt;</span><span class="n">TupleFilterClass</span><span class="o">&gt;</span><span class="p">()</span><span class="o">?</span><span class="p">;</span>
    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>

<p>When you write this, you don’t have to call your constructor <code class="language-plaintext highlighter-rouge">new</code>. The more important part is that
you have <code class="language-plaintext highlighter-rouge">#[new]</code> designated on the function. With this you can provide any kinds of data you need
during processing. Using this initializer in Python is fairly straightforward.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datafusion</span> <span class="kn">import</span> <span class="n">udf</span>
<span class="kn">import</span> <span class="n">pyarrow</span> <span class="k">as</span> <span class="n">pa</span>
<span class="kn">from</span> <span class="n">tuple_filter_example</span> <span class="kn">import</span> <span class="n">TupleFilterClass</span>

<span class="n">tuple_filter_class</span> <span class="o">=</span> <span class="nc">TupleFilterClass</span><span class="p">(</span><span class="n">values_of_interest</span><span class="p">)</span>

<span class="n">udf_using_custom_rust_fn_with_data</span> <span class="o">=</span> <span class="nf">udf</span><span class="p">(</span>
    <span class="n">tuple_filter_class</span><span class="p">,</span>
    <span class="p">[</span><span class="n">pa</span><span class="p">.</span><span class="nf">int64</span><span class="p">(),</span> <span class="n">pa</span><span class="p">.</span><span class="nf">int64</span><span class="p">(),</span> <span class="n">pa</span><span class="p">.</span><span class="nf">utf8</span><span class="p">()],</span>
    <span class="n">pa</span><span class="p">.</span><span class="nf">bool_</span><span class="p">(),</span>
    <span class="sh">"</span><span class="s">stable</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">tuple_filter_with_data</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div>

<p>When you use this approach you will need to provide a <code class="language-plaintext highlighter-rouge">name</code> argument to <code class="language-plaintext highlighter-rouge">udf</code>. This is because our
class/struct does not get the <code class="language-plaintext highlighter-rouge">__qualname__</code> attribute that the <code class="language-plaintext highlighter-rouge">udf</code> function is looking for. You
can give this udf any name you choose.</p>

<h3 id="rust-udf-with-direct-iteration">Rust UDF with direct iteration</h3>

<p>The final version of our scalar UDF is one where we implement it in Rust and iterate through all of
the arrays ourselves. If you are iterating through more than 3 arrays at a time I recommend looking
at <a href="https://docs.rs/itertools/latest/itertools/macro.izip.html">izip</a> in the
<a href="https://crates.io/crates/itertools">itertools crate</a>. For ease of understanding and since we only
have 3 arrays here I will just explicitly create my own tuple here.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">#[pyclass]</span>
<span class="k">pub</span> <span class="k">struct</span> <span class="n">TupleFilterDirectIterationClass</span> <span class="p">{</span>
    <span class="n">values_of_interest</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="p">(</span><span class="nb">i64</span><span class="p">,</span> <span class="nb">i64</span><span class="p">,</span> <span class="nb">String</span><span class="p">)</span><span class="o">&gt;</span><span class="p">,</span>
<span class="p">}</span>

<span class="nd">#[pymethods]</span>
<span class="k">impl</span> <span class="n">TupleFilterDirectIterationClass</span> <span class="p">{</span>
    <span class="nd">#[new]</span>
    <span class="k">fn</span> <span class="nf">new</span><span class="p">(</span><span class="n">values_of_interest</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="p">(</span><span class="nb">i64</span><span class="p">,</span> <span class="nb">i64</span><span class="p">,</span> <span class="nb">String</span><span class="p">)</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="k">Self</span> <span class="p">{</span>
        <span class="k">Self</span> <span class="p">{</span> <span class="n">values_of_interest</span> <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">fn</span> <span class="nf">__call__</span><span class="p">(</span>
        <span class="o">&amp;</span><span class="k">self</span><span class="p">,</span>
        <span class="n">py</span><span class="p">:</span> <span class="n">Python</span><span class="o">&lt;</span><span class="nv">'_</span><span class="o">&gt;</span><span class="p">,</span>
        <span class="n">partkey_expr</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyAny</span><span class="o">&gt;</span><span class="p">,</span>
        <span class="n">suppkey_expr</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyAny</span><span class="o">&gt;</span><span class="p">,</span>
        <span class="n">returnflag_expr</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Bound</span><span class="o">&lt;</span><span class="nv">'_</span><span class="p">,</span> <span class="n">PyAny</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">-&gt;</span> <span class="n">PyResult</span><span class="o">&lt;</span><span class="n">Py</span><span class="o">&lt;</span><span class="n">PyAny</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">partkey_arr</span><span class="p">:</span> <span class="n">PrimitiveArray</span><span class="o">&lt;</span><span class="n">Int64Type</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nn">ArrayData</span><span class="p">::</span><span class="nf">from_pyarrow_bound</span><span class="p">(</span><span class="n">partkey_expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.into</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">suppkey_arr</span><span class="p">:</span> <span class="n">PrimitiveArray</span><span class="o">&lt;</span><span class="n">Int64Type</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nn">ArrayData</span><span class="p">::</span><span class="nf">from_pyarrow_bound</span><span class="p">(</span><span class="n">suppkey_expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.into</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">returnflag_arr</span><span class="p">:</span> <span class="n">StringArray</span> <span class="o">=</span> <span class="nn">ArrayData</span><span class="p">::</span><span class="nf">from_pyarrow_bound</span><span class="p">(</span><span class="n">returnflag_expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.into</span><span class="p">();</span>

        <span class="k">let</span> <span class="n">values_to_search</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="p">(</span><span class="o">&amp;</span><span class="nb">i64</span><span class="p">,</span> <span class="o">&amp;</span><span class="nb">i64</span><span class="p">,</span> <span class="o">&amp;</span><span class="nb">str</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">=</span> <span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="py">.values_of_interest</span><span class="p">)</span>
            <span class="nf">.iter</span><span class="p">()</span>
            <span class="nf">.map</span><span class="p">(|(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)|</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="nf">.as_str</span><span class="p">()))</span>
            <span class="nf">.collect</span><span class="p">();</span>

        <span class="k">let</span> <span class="n">values</span> <span class="o">=</span> <span class="n">partkey_arr</span>
            <span class="nf">.values</span><span class="p">()</span>
            <span class="nf">.iter</span><span class="p">()</span>
            <span class="nf">.zip</span><span class="p">(</span><span class="n">suppkey_arr</span><span class="nf">.values</span><span class="p">()</span><span class="nf">.iter</span><span class="p">())</span>
            <span class="nf">.zip</span><span class="p">(</span><span class="n">returnflag_arr</span><span class="nf">.iter</span><span class="p">())</span>
            <span class="nf">.map</span><span class="p">(|((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">c</span><span class="p">)|</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="nf">.unwrap_or_default</span><span class="p">()))</span>
            <span class="nf">.map</span><span class="p">(|</span><span class="n">v</span><span class="p">|</span> <span class="n">values_to_search</span><span class="nf">.contains</span><span class="p">(</span><span class="o">&amp;</span><span class="n">v</span><span class="p">));</span>

        <span class="k">let</span> <span class="n">res</span><span class="p">:</span> <span class="n">BooleanArray</span> <span class="o">=</span> <span class="nn">BooleanBuffer</span><span class="p">::</span><span class="nf">from_iter</span><span class="p">(</span><span class="n">values</span><span class="p">)</span><span class="nf">.into</span><span class="p">();</span>

        <span class="n">res</span><span class="nf">.into_data</span><span class="p">()</span><span class="nf">.to_pyarrow</span><span class="p">(</span><span class="n">py</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>We convert the <code class="language-plaintext highlighter-rouge">values_of_interest</code> into a vector of borrowed types so that we can do a fast search
without creating additional memory. The other option is to turn the <code class="language-plaintext highlighter-rouge">returnflag</code> into a <code class="language-plaintext highlighter-rouge">String</code>
but that memory allocation is unnecessary. After that we use two <code class="language-plaintext highlighter-rouge">zip</code> operations so that we can
iterate over all three columns in a single pass. Since each <code class="language-plaintext highlighter-rouge">zip</code> will return a tuple of two
elements, a quick <code class="language-plaintext highlighter-rouge">map</code> turns them into the tuple format we need. Also, <code class="language-plaintext highlighter-rouge">StringArray</code> is a little
different in the buffer it uses, so it is treated slightly differently from the others.</p>

<h2 id="user-defined-aggregate-function">User Defined Aggregate Function</h2>

<p>Writing a user defined aggregate function or user defined window function is slightly more complex
than scalar functions. This is because we must accumulate values and there is no guarantee that one
batch will contain all the values we are aggregating over. For this we need to define an
<code class="language-plaintext highlighter-rouge">Accumulator</code> which will do a few things.</p>

<ul>
  <li>Process a batch and compute an internal state</li>
  <li>Share the state so that we can combine multiple batches</li>
  <li>Merge the results across multiple batches</li>
  <li>Return the final result</li>
</ul>

<p>In the example below, we’re going to look at customer orders and we want to know per customer ID,
how much they have ordered total. We want to ignore small orders, which we define as anything under
5000.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">datafusion</span> <span class="kn">import</span> <span class="n">Accumulator</span><span class="p">,</span> <span class="n">udaf</span>
<span class="kn">import</span> <span class="n">pyarrow</span> <span class="k">as</span> <span class="n">pa</span>
<span class="kn">import</span> <span class="n">pyarrow.compute</span> <span class="k">as</span> <span class="n">pc</span>

<span class="n">IGNORE_THESHOLD</span> <span class="o">=</span> <span class="mf">5000.0</span>
<span class="k">class</span> <span class="nc">AboveThresholdAccum</span><span class="p">(</span><span class="n">Accumulator</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_sum</span> <span class="o">=</span> <span class="mf">0.0</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">values</span><span class="p">:</span> <span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">over_threshold</span> <span class="o">=</span> <span class="n">pc</span><span class="p">.</span><span class="nf">greater</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">pa</span><span class="p">.</span><span class="nf">scalar</span><span class="p">(</span><span class="n">IGNORE_THESHOLD</span><span class="p">))</span>
        <span class="n">sum_above</span> <span class="o">=</span> <span class="n">pc</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">values</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="n">over_threshold</span><span class="p">)).</span><span class="nf">as_py</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">sum_above</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">sum_above</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_sum</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_sum</span> <span class="o">+</span> <span class="n">sum_above</span>

    <span class="k">def</span> <span class="nf">merge</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">pa</span><span class="p">.</span><span class="n">Array</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">_sum</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">_sum</span> <span class="o">+</span> <span class="n">pc</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]).</span><span class="nf">as_py</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">state</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">pa</span><span class="p">.</span><span class="n">Scalar</span><span class="p">]:</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">pa</span><span class="p">.</span><span class="nf">scalar</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_sum</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pa</span><span class="p">.</span><span class="n">Scalar</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">pa</span><span class="p">.</span><span class="nf">scalar</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">_sum</span><span class="p">)</span>

<span class="n">sum_above_threshold</span> <span class="o">=</span> <span class="nf">udaf</span><span class="p">(</span><span class="n">AboveThresholdAccum</span><span class="p">,</span> <span class="p">[</span><span class="n">pa</span><span class="p">.</span><span class="nf">float64</span><span class="p">()],</span> <span class="n">pa</span><span class="p">.</span><span class="nf">float64</span><span class="p">(),</span> <span class="p">[</span><span class="n">pa</span><span class="p">.</span><span class="nf">float64</span><span class="p">()],</span> <span class="sh">'</span><span class="s">stable</span><span class="sh">'</span><span class="p">)</span>

<span class="n">df_orders</span><span class="p">.</span><span class="nf">aggregate</span><span class="p">([</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">o_custkey</span><span class="sh">"</span><span class="p">)],[</span><span class="nf">sum_above_threshold</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">o_totalprice</span><span class="sh">"</span><span class="p">)).</span><span class="nf">alias</span><span class="p">(</span><span class="sh">"</span><span class="s">sales</span><span class="sh">"</span><span class="p">)]).</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p>Since we are doing a <code class="language-plaintext highlighter-rouge">sum</code> we can keep a single value as our internal state. When we call <code class="language-plaintext highlighter-rouge">update()</code>
we will process a single array and update the internal state, which we share with the <code class="language-plaintext highlighter-rouge">state()</code>
function. For larger batches we may <code class="language-plaintext highlighter-rouge">merge()</code> these states. It is important to note that the
<code class="language-plaintext highlighter-rouge">states</code> in the <code class="language-plaintext highlighter-rouge">merge()</code> function are an array of the values returned from <code class="language-plaintext highlighter-rouge">state()</code>. It is
entirely possible that the <code class="language-plaintext highlighter-rouge">merge</code> function is significantly different than the <code class="language-plaintext highlighter-rouge">update</code>, though in
our example they are very similar.</p>

<p>One example of implementing a user defined aggregate function where the <code class="language-plaintext highlighter-rouge">update()</code> and <code class="language-plaintext highlighter-rouge">merge()</code>
operations are different is computing an average. In <code class="language-plaintext highlighter-rouge">update()</code> we would create a state that is both
a sum and a count. <code class="language-plaintext highlighter-rouge">state()</code> would return a list of these two values, and <code class="language-plaintext highlighter-rouge">merge()</code> would compute
the final result.</p>

<h2 id="user-defined-window-functions">User Defined Window Functions</h2>

<p>Writing a user defined window function is slightly more complex than an aggregate function due
to the variety of ways that window functions are called. I recommend reviewing the
<a href="https://datafusion.apache.org/python/user-guide/common-operations/udf-and-udfa.html">online documentation</a>
for a description of which functions need to be implemented. The details of how to implement
these generally follow the same patterns as described above for aggregate functions.</p>

<h2 id="performance-comparison">Performance Comparison</h2>

<p>For the scalar functions above, we performed a timing evaluation, repeating the operation 100
times. For this simple example these are our results.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+-----------------------------+--------------+---------+
| approach                    | Average Time | Std Dev |
+-----------------------------+--------------+---------+
| python udf                  | 4.969        | 0.062   |
| simple filter               | 1.075        | 0.022   |
| explicit filter             | 0.685        | 0.063   |
| pyarrow compute             | 0.529        | 0.017   |
| arrow rust compute          | 0.511        | 0.034   |
| arrow rust compute as class | 0.502        | 0.011   |
| rust custom iterator        | 0.478        | 0.009   |
+-----------------------------+--------------+---------+
</code></pre></div></div>

<p>As expected, the conversion to Python objects is by far the worst performance. As soon as we drop
into using any functions that keep the data entirely on the Native (Rust or C/C++) side we see a
near 10x speed improvement. Then as we increase our complexity from using PyArrow compute functions
to implementing the UDF in Rust we see incremental improvements. Our fastest approach - iterating
through the arrays ourselves does operate nearly 10% faster than the PyArrow compute approach.</p>

<h2 id="final-thoughts-and-recommendations">Final Thoughts and Recommendations</h2>

<p>For anyone who is curious about <a href="https://datafusion.apache.org/">DataFusion</a> I highly recommend
giving it a try. This post was designed to make it easier for new users to the Python implementation
to work with User Defined Functions by giving a few examples of how one might implement these.</p>

<p>When it comes to designing UDFs, I strongly recommend seeing if you can write your UDF using
<a href="https://arrow.apache.org/docs/python/api/compute.html">PyArrow functions</a> rather than pure Python
objects. As shown in the scalar example above, you can achieve a 10x speedup by using PyArrow
functions. If you must do something that isn’t well represented by the PyArrow compute functions,
then I would consider using a Rust based UDF in the manner shown above.</p>

<p>I would like to thank <a href="https://github.com/alamb">@alamb</a>, <a href="https://github.com/andygrove">@andygrove</a>, <a href="https://github.com/comphead">@comphead</a>, <a href="https://github.com/emgeee">@emgeee</a>, <a href="https://github.com/kylebarron">@kylebarron</a>, and <a href="https://github.com/Omega359">@Omega359</a>
for their helpful reviews and feedback.</p>

<p>Lastly, the Apache Arrow and DataFusion community is an active group of very helpful people working
to make a great tool. If you want to get involved, please take a look at the
<a href="https://datafusion.apache.org/python/">online documentation</a> and jump in to help with one of the
<a href="https://github.com/apache/datafusion-python/issues">open issues</a>.</p>]]></content><author><name>timsaucer</name></author><category term="tutorial" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">Apache DataFusion Comet 0.3.0 Release</title><link href="https://datafusion.apache.org/blog/2024/09/27/datafusion-comet-0.3.0/" rel="alternate" type="text/html" title="Apache DataFusion Comet 0.3.0 Release" /><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/09/27/datafusion-comet-0.3.0</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/09/27/datafusion-comet-0.3.0/"><![CDATA[<!--

-->

<p>The Apache DataFusion PMC is pleased to announce version 0.3.0 of the <a href="https://datafusion.apache.org/comet/">Comet</a> subproject.</p>

<p>Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.</p>

<p>Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the <a href="https://datafusion.apache.org/comet/user-guide/compatibility.html">compatibility guide</a> for more information.</p>

<p>This release covers approximately four weeks of development work and is the result of merging 57 PRs from 12 
contributors. See the <a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.3.0.md">change log</a> for more information.</p>

<h2 id="release-highlights">Release Highlights</h2>

<h3 id="binary-releases">Binary Releases</h3>

<p>Comet jar files are now published to Maven central for amd64 and arm64 architectures (Linux only).</p>

<p>Files can be found at https://central.sonatype.com/search?q=org.apache.datafusion</p>

<ul>
  <li>Spark versions 3.3, 3.4, and 3.5 are supported.</li>
  <li>Scala versions 2.12 and 2.13 are supported.</li>
</ul>

<h3 id="new-features">New Features</h3>

<p>The following expressions are now supported natively:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">DateAdd</code></li>
  <li><code class="language-plaintext highlighter-rouge">DateSub</code></li>
  <li><code class="language-plaintext highlighter-rouge">ElementAt</code></li>
  <li><code class="language-plaintext highlighter-rouge">GetArrayElement</code></li>
  <li><code class="language-plaintext highlighter-rouge">ToJson</code></li>
</ul>

<h3 id="performance--stability">Performance &amp; Stability</h3>

<ul>
  <li>Upgraded to DataFusion 42.0.0</li>
  <li>Reduced memory overhead due to some memory leaks being fixed</li>
  <li>Comet will now fall back to Spark for queries that use DPP, to avoid performance regressions because Comet does 
not have native support for DPP yet</li>
  <li>Improved performance when converting Spark columnar data to Arrow format</li>
  <li>Faster decimal sum and avg functions</li>
</ul>

<h3 id="documentation-updates">Documentation Updates</h3>

<ul>
  <li>Improved documentation for deploying Comet with Kubernetes and Helm in the <a href="https://datafusion.apache.org/comet/user-guide/kubernetes.html">Comet Kubernetes Guide</a></li>
  <li>More detailed architectural overview of Comet scan and execution in the <a href="https://datafusion.apache.org/comet/contributor-guide/plugin_overview.html">Comet Plugin Overview</a> in the contributor guide</li>
</ul>

<h2 id="getting-involved">Getting Involved</h2>

<p>The Comet project welcomes new contributors. We use the same <a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord">Slack and Discord</a> channels as the main DataFusion
project.</p>

<p>The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the <a href="https://datafusion.apache.org/comet/user-guide/installation.html">Getting Started</a> guide for instructions on downloading and installing
Comet.</p>

<p>There are also many <a href="https://github.com/apache/datafusion-comet/contribute">good first issues</a> waiting for contributions.</p>]]></content><author><name>pmc</name></author><category term="subprojects" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">Using StringView / German Style Strings to Make Queries Faster: Part 1- Reading Parquet</title><link href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1/" rel="alternate" type="text/html" title="Using StringView / German Style Strings to Make Queries Faster: Part 1- Reading Parquet" /><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1/"><![CDATA[<!--

-->

<p><em>Editor’s Note: This is the first of a <a href="/blog/2024/09/13/string-view-german-style-strings-part-2/">two part</a> blog series that was first published on the <a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/">InfluxData blog</a>. Thanks to InfluxData for sponsoring this work as <a href="https://haoxp.xyz/">Xiangpeng Hao</a>’s summer intern project</em></p>

<p>This blog describes our experience implementing <a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-view-layout">StringView</a> in the <a href="https://github.com/apache/arrow-rs">Rust implementation</a> of <a href="https://arrow.apache.org/">Apache Arrow</a>, and integrating it into <a href="https://datafusion.apache.org/">Apache DataFusion</a>, significantly accelerating string-intensive queries in the <a href="https://benchmark.clickhouse.com/">ClickBench</a> benchmark by 20%- 200% (Figure 1<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>).</p>

<p>Getting significant end-to-end performance improvements was non-trivial. Implementing StringView itself was only a fraction of the effort required. Among other things, we had to optimize UTF-8 validation, implement unintuitive compiler optimizations, tune block sizes, and time GC to realize the <a href="https://www.influxdata.com/blog/flight-datafusion-arrow-parquet-fdap-architecture-influxdb/">FDAP ecosystem</a>’s benefit. With other members of the open source community, we were able to overcome performance bottlenecks that could have killed the project. We would like to contribute by explaining the challenges and solutions in more detail so that more of the community can learn from our experience.</p>

<p>StringView is based on a simple idea: avoid some string copies and accelerate comparisons with inlined prefixes. Like most great ideas, it is “obvious” only after <a href="https://db.in.tum.de/~freitag/papers/p29-neumann-cidr20.pdf">someone describes it clearly</a>. Although simple, straightforward implementation actually <em>slows down performance for almost every query</em>. We must, therefore, apply astute observations and diligent engineering to realize the actual benefits from StringView.</p>

<p>Although this journey was successful, not all research ideas are as lucky. To accelerate the adoption of research into industry, it is valuable to integrate research prototypes with practical systems. Understanding the nuances of real-world systems makes it more likely that research designs<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> will lead to practical system improvements.</p>

<p>StringView support was released as part of <a href="https://crates.io/crates/arrow/52.2.0">arrow-rs v52.2.0</a> and <a href="https://crates.io/crates/datafusion/41.0.0">DataFusion v41.0.0</a>. You can try it by setting the <code class="language-plaintext highlighter-rouge">schema_force_view_types</code> <a href="https://datafusion.apache.org/user-guide/configs.html">DataFusion configuration option</a>, and we are<a href="https://github.com/apache/datafusion/issues/11682"> hard at work with the community to </a>make it the default. We invite everyone to try it out, take advantage of the effort invested so far, and contribute to making it better.</p>

<p><img src="/blog/img/string-view-1/figure1-performance.png" width="100%" class="img-responsive" alt="End to end performance improvements for ClickBench queries" /></p>

<p>Figure 1: StringView improves string-intensive ClickBench query performance by 20% - 200%</p>

<h2 id="what-is-stringview">What is StringView?</h2>

<p><img src="/blog/img/string-view-1/figure2-string-view.png" width="100%" class="img-responsive" alt="Diagram of using StringArray and StringViewArray to represent the same string content" /></p>

<p>Figure 2: Use StringArray and StringViewArray to represent the same string content.</p>

<p>The concept of inlined strings with prefixes (called “German Strings” <a href="https://x.com/andy_pavlo/status/1813258735965643203">by Andy Pavlo</a>, in homage to <a href="https://www.tum.de/">TUM</a>, where the <a href="https://db.in.tum.de/~freitag/papers/p29-neumann-cidr20.pdf">Umbra paper that describes</a> them originated) 
has been used in many recent database systems (<a href="https://engineering.fb.com/2024/02/20/developer-tools/velox-apache-arrow-15-composable-data-management/">Velox</a>, <a href="https://pola.rs/posts/polars-string-type/">Polars</a>, <a href="https://duckdb.org/2021/12/03/duck-arrow.html">DuckDB</a>, <a href="https://cedardb.com/blog/german_strings/">CedarDB</a>, etc.) 
and was introduced to Arrow as a new <a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-view-layout">StringViewArray</a><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> type. Arrow’s original <a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-layout">StringArray</a> is very memory efficient but less effective for certain operations. 
StringViewArray accelerates string-intensive operations via prefix inlining and a more flexible and compact string representation.</p>

<p>A StringViewArray consists of three components:</p>

<ol>
  <li>The <code><em>view</em></code> array</li>
  <li>The buffers</li>
  <li>The buffer pointers (IDs) that map buffer offsets to their physical locations</li>
</ol>

<p>Each <code>view</code> is 16 bytes long, and its contents differ based on the string’s length:</p>

<ul>
  <li>string length &lt; 12 bytes: the first four bytes store the string length, and the remaining 12 bytes store the inlined string.</li>
  <li>string length &gt; 12 bytes: the string is stored in a separate buffer. The length is again stored in the first 4 bytes, followed by the buffer id (4 bytes), the buffer offset (4 bytes), and the prefix (first 4 bytes) of the string.</li>
</ul>

<p>Figure 2 shows an example of the same logical content (left) using StringArray (middle) and StringViewArray (right):</p>

<ul>
  <li>The first string – <code class="language-plaintext highlighter-rouge">"Apache DataFusion"</code> – is 17 bytes long, and both StringArray and StringViewArray store the string’s bytes at the beginning of the buffer. The StringViewArray also inlines the first 4 bytes – <code class="language-plaintext highlighter-rouge">"Apac"</code> – in the view.</li>
  <li>The second string, <code class="language-plaintext highlighter-rouge">"InfluxDB"</code> is only 8 bytes long, so StringViewArray completely inlines the string content in the <code class="language-plaintext highlighter-rouge">view</code> struct while StringArray stores the string in the buffer as well.</li>
  <li>The third string <code class="language-plaintext highlighter-rouge">"Arrow Rust Impl"</code> is 15 bytes long and cannot be fully inlined. StringViewArray stores this in the same form as the first string.</li>
  <li>The last string <code class="language-plaintext highlighter-rouge">"Apache DataFusion"</code> has the same content as the first string. It’s possible to use StringViewArray to avoid this duplication and reuse the bytes by pointing the view to the previous location.</li>
</ul>

<p>StringViewArray provides three opportunities for outperforming StringArray:</p>

<ol>
  <li>Less copying via the offset + buffer format</li>
  <li>Faster comparisons using the inlined string prefix</li>
  <li>Reusing repeated string values with the flexible <code class="language-plaintext highlighter-rouge">view</code> layout</li>
</ol>

<p>The rest of this blog post discusses how to apply these opportunities in real query scenarios to improve performance, what challenges we encountered along the way, and how we solved them.</p>

<h2 id="faster-parquet-loading">Faster Parquet Loading</h2>

<p><a href="https://parquet.apache.org/">Apache Parquet</a> is the de facto format for storing large-scale analytical data commonly stored LakeHouse-style, such as <a href="https://iceberg.apache.org">Apache Iceberg</a> and <a href="https://delta.io">Delta Lake</a>. Efficiently loading data from Parquet is thus critical to query performance in many important real-world workloads.</p>

<p>Parquet encodes strings (i.e., <a href="https://docs.rs/parquet/latest/parquet/data_type/struct.ByteArray.html">byte array</a>) in a slightly different format than required for the original Arrow StringArray. The string length is encoded inline with the actual string data (as shown in Figure 4 left). As mentioned previously, StringArray requires the data buffer to be continuous and compact—the strings have to follow one after another. This requirement means that reading Parquet string data into an Arrow StringArray requires copying and consolidating the string bytes to a new buffer and tracking offsets in a separate array. Copying these strings is often wasteful. Typical queries filter out most data immediately after loading, so most of the copied data is quickly discarded.</p>

<p>On the other hand, reading Parquet data as a StringViewArray can re-use the same data buffer as storing the Parquet pages because StringViewArray does not require strings to be contiguous. For example, in Figure 4, the StringViewArray directly references the buffer with the decoded Parquet page. The string <code class="language-plaintext highlighter-rouge">"Arrow Rust Impl"</code> is represented by a <code class="language-plaintext highlighter-rouge">view</code> with offset 37 and length 15 into that buffer.</p>

<p><img src="/blog/img/string-view-1/figure4-copying.png" width="100%" class="img-responsive" alt="Diagram showing how StringViewArray can avoid copying by reusing decoded Parquet pages." /></p>

<p>Figure 4: StringViewArray avoids copying by reusing decoded Parquet pages.</p>

<p><strong>Mini benchmark</strong></p>

<p>Reusing Parquet buffers is great in theory, but how much does saving a copy actually matter? We can run the following benchmark in arrow-rs to find out:</p>

<p>Our benchmarking machine shows that loading <em>BinaryViewArray</em> is almost 2x faster than loading BinaryArray (see next section about why this isn’t <em>String</em> ViewArray).</p>

<p>You can read more on this arrow-rs issue: <a href="https://github.com/apache/arrow-rs/issues/5904">https://github.com/apache/arrow-rs/issues/5904</a></p>

<h1 id="from-binary-to-strings">From Binary to Strings</h1>

<p>You may wonder why we reported performance for BinaryViewArray when this post is about StringViewArray. Surprisingly, initially, our implementation to read StringViewArray from Parquet was much <em>slower</em> than StringArray. Why? TLDR: Although reading StringViewArray copied less data, the initial implementation also spent much more time validating <a href="https://en.wikipedia.org/wiki/UTF-8#:~:text=UTF%2D8%20is%20a%20variable,Unicode%20Standard">UTF-8</a> (as shown in Figure 5).</p>

<p>Strings are stored as byte sequences. When reading data from (potentially untrusted) Parquet files, a Parquet decoder must ensure those byte sequences are valid UTF-8 strings, and most programming languages, including Rust, include highly<a href="https://doc.rust-lang.org/std/str/fn.from_utf8.html"> optimized routines</a> for doing so.</p>

<p><img src="/blog/img/string-view-1/figure5-loading-strings.png" width="100%" class="img-responsive" alt="Figure showing time to load strings from Parquet and the effect of optimized UTF-8 validation." /></p>

<p>Figure 5: Time to load strings from Parquet. The UTF-8 validation advantage initially eliminates the advantage of reduced copying for StringViewArray.</p>

<p>A StringArray can be validated in a single call to the UTF-8 validation function as it has a continuous string buffer. As long as the underlying buffer is UTF-8<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>, all strings in the array must be UTF-8. The Rust parquet reader makes a single function call to validate the entire buffer.</p>

<p>However, validating an arbitrary StringViewArray requires validating each string with a separate call to the validation function, as the underlying buffer may also contain non-string data (for example, the lengths in Parquet pages).</p>

<p>UTF-8 validation in Rust is highly optimized and favors longer strings (as shown in Figure 6), likely because it leverages SIMD instructions to perform parallel validation. The benefit of a single function call to validate UTF-8 over a function call for each string more than eliminates the advantage of avoiding the copy for StringViewArray.</p>

<p><img src="/blog/img/string-view-1/figure6-utf8-validation.png" width="100%" class="img-responsive" alt="Figure showing UTF-8 validation throughput vs string length." /></p>

<p>Figure 6: UTF-8 validation throughput vs string length—StringArray’s contiguous buffer can be validated much faster than StringViewArray’s buffer.</p>

<p>Does this mean we should only use StringArray? No! Thankfully, there’s a clever way out. The key observation is that in many real-world datasets,<a href="https://www.vldb.org/pvldb/vol17/p148-zeng.pdf"> 99% of strings are shorter than 128 bytes</a>, meaning the encoded length values are smaller than 128, <strong>in which case the length itself is also valid UTF-8</strong> (in fact, it is <a href="https://en.wikipedia.org/wiki/ASCII">ASCII</a>).</p>

<p>This observation means we can optimize validating UTF-8 strings in Parquet pages by treating the length bytes as part of a single large string as long as the length <em>value</em> is less than 128. Put another way, prior to this optimization, the length bytes act as string boundaries, which require a UTF-8 validation on each string. After this optimization, only those strings with lengths larger than 128 bytes (less than 1% of the strings in the ClickBench dataset) are string boundaries, significantly increasing the UTF-8 validation chunk size and thus improving performance.</p>

<p>The <a href="https://github.com/apache/arrow-rs/pull/6009/files">actual implementation</a> is only nine lines of Rust (with 30 lines of comments). You can find more details in the related arrow-rs issue:<a href="https://github.com/apache/arrow-rs/issues/5995"> https://github.com/apache/arrow-rs/issues/5995</a>. As expected, with this optimization, loading StringViewArray is almost 2x faster than loading StringArray.</p>

<h1 id="be-careful-about-implicit-copies">Be Careful About Implicit Copies</h1>

<p>After all the work to avoid copying strings when loading from Parquet, performance was still not as good as expected. We tracked the problem to a few implicit data copies that we weren’t aware of, as described in<a href="https://github.com/apache/arrow-rs/issues/6033"> this issue</a>.</p>

<p>The copies we eventually identified come from the following innocent-looking line of Rust code, where <code class="language-plaintext highlighter-rouge">self.buf</code> is a <a href="https://en.wikipedia.org/wiki/Reference_counting">reference counted</a> pointer that should transform without copying into a buffer for use in StringViewArray.</p>

<p>However, Rust-type coercion rules favored a blanket implementation that <em>did</em> copy data. This implementation is shown in the following code block where the <code class="language-plaintext highlighter-rouge">impl&lt;T: AsRef&lt;[u8]&gt;&gt;</code> will accept any type that implements <code class="language-plaintext highlighter-rouge">AsRef&lt;[u8]&gt;</code> and copies the data to create a new buffer. To avoid copying, users need to explicitly call <code class="language-plaintext highlighter-rouge">from_vec</code>, which consumes the <code class="language-plaintext highlighter-rouge">Vec</code> and transforms it into a buffer.</p>

<p>Diagnosing this implicit copy was time-consuming as it relied on subtle Rust language semantics. We needed to track every step of the data flow to ensure every copy was necessary. To help other users and prevent future mistakes, we also <a href="https://github.com/apache/arrow-rs/pull/6043">removed</a> the implicit API from arrow-rs in favor of an explicit API. Using this approach, we found and fixed several <a href="https://github.com/apache/arrow-rs/pull/6039">other unintentional copies</a> in the code base—hopefully, the change will help other <a href="https://github.com/spiraldb/vortex/pull/504">downstream users</a> avoid unnecessary copies.</p>

<h1 id="help-the-compiler-by-giving-it-more-information">Help the Compiler by Giving it More Information</h1>

<p>The Rust compiler’s automatic optimizations mostly work very well for a wide variety of use cases, but sometimes, it needs additional hints to generate the most efficient code. When profiling the performance of <code class="language-plaintext highlighter-rouge">view</code> construction, we found, counterintuitively, that constructing <strong>long</strong> strings was 10x faster than constructing <strong>short</strong> strings, which made short strings slower on StringViewArray than on StringArray!</p>

<p>As described in the first section, StringViewArray treats long and short strings differently. Short strings (&lt;12 bytes) directly inline to the <code class="language-plaintext highlighter-rouge">view</code> struct, while long strings only inline the first 4 bytes. The code to construct a <code class="language-plaintext highlighter-rouge">view</code> looks something like this:</p>

<p>It appears that both branches of the code should be fast: they both involve copying at most 16 bytes of data and some memory shift/store operations. How could the branch for short strings be 10x slower?</p>

<p>Looking at the assembly code using <a href="https://godbolt.org/">Compiler Explorer</a>, we (with help from <a href="https://github.com/aoli-al">Ao Li</a>) found the compiler used CPU <strong>load instructions</strong> to copy the fixed-sized 4 bytes to the <code class="language-plaintext highlighter-rouge">view</code> for long strings, but it calls a function, <a href="https://doc.rust-lang.org/std/ptr/fn.copy_nonoverlapping.html"><code class="language-plaintext highlighter-rouge">ptr::copy_non_overlapping</code></a>, to copy the inlined bytes to the <code>view</code> for short strings. The difference is that long strings have a prefix size (4 bytes) known at compile time, so the compiler directly uses efficient CPU instructions. But, since the size of the short string is unknown to the compiler, it has to call the general-purpose function <code>ptr::copy_non_coverlapping</code>. Making a function call is significant unnecessary overhead compared to a CPU copy instruction.</p>

<p>However, we know something the compiler doesn’t know: the short string size is not arbitrary—it must be between 0 and 12 bytes, and we can leverage this information to avoid the function call. Our solution generates 13 copies of the function using generics, one for each of the possible prefix lengths. The code looks as follows, and <a href="https://godbolt.org/z/685YPsd5G">checking the assembly code</a>, we confirmed there are no calls to <code class="language-plaintext highlighter-rouge">ptr::copy_non_overlapping</code>, and only native CPU instructions are used. For more details, see <a href="https://github.com/apache/arrow-rs/issues/6034">the ticket</a>.</p>

<h1 id="end-to-end-query-performance">End-to-End Query Performance</h1>

<p>In the previous sections, we went out of our way to make sure loading StringViewArray is faster than StringArray. Before going further, we wanted to verify if obsessing about reducing copies and function calls has actually improved end-to-end performance in real-life queries. To do this, we evaluated a ClickBench query (Q20) in DataFusion that counts how many URLs contain the word <code class="language-plaintext highlighter-rouge">"google"</code>:</p>

<p>This is a relatively simple query; most of the time is spent on loading the “URL” column to find matching rows. The query plan looks like this:</p>

<p>We ran the benchmark in the DataFusion repo like this:</p>

<p>With StringViewArray we saw a 24% end-to-end performance improvement, as shown in Figure 7. With the <code class="language-plaintext highlighter-rouge">--string-view</code> argument, the end-to-end query time is <code class="language-plaintext highlighter-rouge">944.3 ms, 869.6 ms, 861.9 ms</code> (three iterations). Without <code class="language-plaintext highlighter-rouge">--string-view</code>, the end-to-end query time is <code class="language-plaintext highlighter-rouge">1186.1 ms, 1126.1 ms, 1138.3 ms</code>.</p>

<p><img src="/blog/img/string-view-1/figure7-end-to-end.png" width="100%" class="img-responsive" alt="Figure showing StringView improves end to end performance by 24 percent." /></p>

<p>Figure 7: StringView reduces end-to-end query time by 24% on ClickBench Q20.</p>

<p>We also double-checked with detailed profiling and verified that the time reduction is indeed due to faster Parquet loading.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In this first blog post, we have described what it took to improve the
performance of simply reading strings from Parquet files using StringView. While
this resulted in real end-to-end query performance improvements, in our <a href="https://datafusion.apache.org/blog/2024/09/13/using-stringview-to-make-queries-faster-part-2.html">next
post</a>, we explore additional optimizations enabled by StringView in DataFusion,
along with some of the pitfalls we encountered while implementing them.</p>

<h1 id="footnotes">Footnotes</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Benchmarked with AMD Ryzen 7600x (12 core, 24 threads, 32 MiB L3), WD Black SN770 NVMe SSD (5150MB/4950MB seq RW bandwidth) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Xiangpeng is a PhD student at the University of Wisconsin-Madison <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>There is also a corresponding <em>BinaryViewArray</em> which is similar except that the data is not constrained to be UTF-8 encoded strings. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>We also make sure that offsets do not break a UTF-8 code point, which is <a href="https://github.com/apache/arrow-rs/blob/master/parquet/src/arrow/buffer/offset_buffer.rs#L62-L71">cheaply validated</a>. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Xiangpeng Hao, Andrew Lamb</name></author><category term="performance" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">Using StringView / German Style Strings to make Queries Faster: Part 2 - String Operations</title><link href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-2/" rel="alternate" type="text/html" title="Using StringView / German Style Strings to make Queries Faster: Part 2 - String Operations" /><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-2</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-2/"><![CDATA[<!--

-->

<p><em>Editor’s Note: This blog series was first published on the <a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-two-influxdb/">InfluxData blog</a>. Thanks to InfluxData for sponsoring this work as <a href="https://haoxp.xyz/">Xiangpeng Hao</a>’s summer intern project</em></p>

<p>In the <a href="/blog/2024/09/13/string-view-german-style-strings-part-1/">first post</a>, we discussed the nuances required to accelerate Parquet loading using StringViewArray by reusing buffers and reducing copies. 
In this second part of the post, we describe the rest of the journey: implementing additional efficient operations for real query processing.</p>

<h2 id="faster-string-operations">Faster String Operations</h2>

<h1 id="faster-comparison">Faster comparison</h1>

<p>String comparison is ubiquitous; it is the core of 
<a href="https://docs.rs/arrow/latest/arrow/compute/kernels/cmp/index.html"><code class="language-plaintext highlighter-rouge">cmp</code></a>, 
<a href="https://docs.rs/arrow/latest/arrow/compute/fn.min.html"><code class="language-plaintext highlighter-rouge">min</code></a>/<a href="https://docs.rs/arrow/latest/arrow/compute/fn.max.html"><code class="language-plaintext highlighter-rouge">max</code></a>, 
and <a href="https://docs.rs/arrow/latest/arrow/compute/kernels/comparison/fn.like.html"><code class="language-plaintext highlighter-rouge">like</code></a>/<a href="https://docs.rs/arrow/latest/arrow/compute/kernels/comparison/fn.ilike.html"><code class="language-plaintext highlighter-rouge">ilike</code></a> kernels. StringViewArray is designed to accelerate such comparisons using the inlined prefix—the key observation is that, in many cases, only the first few bytes of the string determine the string comparison results.</p>

<p>For example, to compare the strings <code class="language-plaintext highlighter-rouge">InfluxDB</code> with <code class="language-plaintext highlighter-rouge">Apache DataFusion</code>, we only need to look at the first byte to determine the string ordering or equality. In this case, since <code class="language-plaintext highlighter-rouge">A</code> is earlier in the alphabet than <code class="language-plaintext highlighter-rouge">I,</code> <code class="language-plaintext highlighter-rouge">Apache DataFusion</code> sorts first, and we know the strings are not equal. Despite only needing the first byte, comparing these strings when stored as a StringArray requires two memory accesses: 1) load the string offset and 2) use the offset to locate the string bytes. For low-level operations such as <code class="language-plaintext highlighter-rouge">cmp</code> that are invoked millions of times in the very hot paths of queries, avoiding this extra memory access can make a measurable difference in query performance.</p>

<p>For StringViewArray, typically, only one memory access is needed to load the view struct. Only if the result can not be determined from the prefix is the second memory access required. For the example above, there is no need for the second access. This technique is very effective in practice: the second access is never necessary for the more than <a href="https://www.vldb.org/pvldb/vol17/p148-zeng.pdf">60% of real-world strings which are shorter than 12 bytes</a>, as they are stored completely in the prefix.</p>

<p>However, functions that operate on strings must be specialized to take advantage of the inlined prefix. In addition to low-level comparison kernels, we implemented <a href="https://github.com/apache/arrow-rs/issues/5374">a wide range</a> of other StringViewArray operations that cover the functions and operations seen in ClickBench queries. Supporting StringViewArray in all string operations takes quite a bit of effort, and thankfully the Arrow and DataFusion communities are already hard at work doing so (see <a href="https://github.com/apache/datafusion/issues/11752">https://github.com/apache/datafusion/issues/11752</a> if you want to help out).</p>

<h1 id="faster-take-and-filter">Faster <code class="language-plaintext highlighter-rouge">take </code>and<code class="language-plaintext highlighter-rouge"> filter</code></h1>

<p>After a filter operation such as <code class="language-plaintext highlighter-rouge">WHERE url &lt;&gt; ''</code> to avoid processing empty urls, DataFusion will often <em>coalesce</em> results to form a new array with only the passing elements. 
This coalescing ensures the batches are sufficiently sized to benefit from <a href="https://www.vldb.org/pvldb/vol11/p2209-kersten.pdf">vectorized processing</a> in subsequent steps.</p>

<p>The coalescing operation is implemented using the <a href="https://docs.rs/arrow/latest/arrow/compute/fn.take.html">take</a> and <a href="https://arrow.apache.org/rust/arrow/compute/kernels/filter/fn.filter.html">filter</a> kernels in arrow-rs. For StringArray, these kernels require copying the string contents to a new buffer without “holes” in between. This copy can be expensive especially when the new array is large.</p>

<p>However, <code class="language-plaintext highlighter-rouge">take</code> and <code class="language-plaintext highlighter-rouge">filter</code> for StringViewArray can avoid the copy by reusing buffers from the old array. The kernels only need to create a new list of  <code class="language-plaintext highlighter-rouge">view</code>s that point at the same strings within the old buffers. 
Figure 1 illustrates the difference between the output of both string representations. StringArray creates two new strings at offsets 0-17 and 17-32, while StringViewArray simply points to the original buffer at offsets 0 and 25.</p>

<p><img src="/blog/img/string-view-2/figure1-zero-copy-take.png" width="100%" class="img-responsive" alt="Diagram showing Zero-copy `take`/`filter` for StringViewArray" /></p>

<p>Figure 1: Zero-copy <code class="language-plaintext highlighter-rouge">take</code>/<code class="language-plaintext highlighter-rouge">filter</code> for StringViewArray</p>

<h1 id="when-to-gc">When to GC?</h1>

<p>Zero-copy <code class="language-plaintext highlighter-rouge">take/filter</code> is great for generating large arrays quickly, but it is suboptimal for highly selective filters, where most of the strings are filtered out. When the cardinality drops, StringViewArray buffers become sparse—only a small subset of the bytes in the buffer’s memory are referred to by any <code class="language-plaintext highlighter-rouge">view</code>. This leads to excessive memory usage, especially in a <a href="https://github.com/apache/datafusion/issues/11628">filter-then-coalesce scenario</a>. For example, a StringViewArray with 10M strings may only refer to 1M strings after some filter operations; however, due to zero-copy take/filter, the (reused) 10M buffers can not be released/reused.</p>

<p>To release unused memory, we implemented a <a href="https://docs.rs/arrow/latest/arrow/array/struct.GenericByteViewArray.html#method.gc">garbage collection (GC)</a> routine to consolidate the data into a new buffer to release the old sparse buffer(s). As the GC operation copies strings, similarly to StringArray, we must be careful about when to call it. If we call GC too early, we cause unnecessary copying, losing much of the benefit of StringViewArray. If we call GC too late, we hold large buffers for too long, increasing memory use and decreasing cache efficiency. The <a href="https://pola.rs/posts/polars-string-type/">Polars blog</a> on StringView also refers to the challenge presented by garbage collection timing.</p>

<p><code class="language-plaintext highlighter-rouge">arrow-rs</code> implements the GC process, but it is up to users to decide when to call it. We leverage the semantics of the query engine and observed that the <a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/coalesce_batches/struct.CoalesceBatchesExec.html"><code class="language-plaintext highlighter-rouge">CoalseceBatchesExec</code></a> operator, which merge smaller batches to a larger batch, is often used after the record cardinality is expected to shrink, which aligns perfectly with the scenario of GC in StringViewArray. 
We, therefore,<a href="https://github.com/apache/datafusion/pull/11587"> implemented the GC procedure</a> inside <code>CoalseceBatchesExec</code><sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">1</a></sup> with a heuristic that estimates when the buffers are too sparse.</p>

<h2 id="the-art-of-function-inlining-not-too-much-not-too-little">The art of function inlining: not too much, not too little</h2>

<p>Like string inlining, <em>function</em> inlining is the process of embedding a short function into the caller to avoid the overhead of function calls (caller/callee save). 
Usually, the Rust compiler does a good job of deciding when to inline. However, it is possible to override its default using the <a href="https://doc.rust-lang.org/reference/attributes/codegen.html#the-inline-attribute"><code class="language-plaintext highlighter-rouge">#[inline(always)]</code> directive</a>. 
In performance-critical code, inlined code allows us to organize large functions into smaller ones without paying the runtime cost of function invocation.</p>

<p>However, function inlining is <strong><em>not</em></strong> always better, as it leads to larger function bodies that are harder for LLVM to optimize (for example, suboptimal <a href="https://en.wikipedia.org/wiki/Register_allocation">register spilling</a>) and risk overflowing the CPU’s instruction cache. We observed several performance regressions where function inlining caused <em>slower</em> performance when implementing the StringViewArray comparison kernels. Careful inspection and tuning of the code was required to aid the compiler in generating efficient code. More details can be found in this PR: <a href="https://github.com/apache/arrow-rs/pull/5900">https://github.com/apache/arrow-rs/pull/5900</a>.</p>

<h2 id="buffer-size-tuning">Buffer size tuning</h2>

<p>StringViewArray permits multiple buffers, which enables a flexible buffer layout and potentially reduces the need to copy data. However, a large number of buffers slows down the performance of other operations. 
For example, <a href="https://docs.rs/arrow/latest/arrow/array/trait.Array.html#tymethod.get_array_memory_size"><code class="language-plaintext highlighter-rouge">get_array_memory_size</code></a> needs to sum the memory size of each buffer, which takes a long time with thousands of small buffers. 
In certain cases, we found that multiple calls to <a href="https://docs.rs/arrow/latest/arrow/compute/fn.concat_batches.html"><code class="language-plaintext highlighter-rouge">concat_batches</code></a> lead to arrays with millions of buffers, which was prohibitively expensive.</p>

<p>For example, consider a StringViewArray with the previous default buffer size of 8 KB. With this configuration, holding 4GB of string data requires almost half a million buffers! Larger buffer sizes are needed for larger arrays, but we cannot arbitrarily increase the default buffer size, as small arrays would consume too much memory (most arrays require at least one buffer). Buffer sizing is especially problematic in query processing, as we often need to construct small batches of string arrays, and the sizes are unknown at planning time.</p>

<p>To balance the buffer size trade-off, we again leverage the query processing (DataFusion) semantics to decide when to use larger buffers. While coalescing batches, we combine multiple small string arrays and set a smaller buffer size to keep the total memory consumption low. In string aggregation, we aggregate over an entire Datafusion partition, which can generate a large number of strings, so we set a larger buffer size (2MB).</p>

<p>To assist situations where the semantics are unknown, we also <a href="https://github.com/apache/arrow-rs/pull/6136">implemented</a> a classic dynamic exponential buffer size growth strategy, which starts with a small buffer size (8KB) and doubles the size of each new buffer up to 2MB. We implemented this strategy in arrow-rs and enabled it by default so that other users of StringViewArray can also benefit from this optimization. See this issue for more details: <a href="https://github.com/apache/arrow-rs/issues/6094">https://github.com/apache/arrow-rs/issues/6094</a>.</p>

<h2 id="end-to-end-query-performance">End-to-end query performance</h2>

<p>We have made significant progress in optimizing StringViewArray filtering operations. Now, let’s test it in the real world to see how it works!</p>

<p>Let’s consider ClickBench query 22, which selects multiple string fields (<code class="language-plaintext highlighter-rouge">URL</code>, <code class="language-plaintext highlighter-rouge">Title</code>, and <code class="language-plaintext highlighter-rouge">SearchPhase</code>) and applies several filters.</p>

<p>We ran the benchmark using the following command in the DataFusion repo. Again, the <code class="language-plaintext highlighter-rouge">--string-view</code> option means we use StringViewArray instead of StringArray.</p>

<p>To eliminate the impact of the faster Parquet reading using StringViewArray (see the first part of this blog), Figure 2 plots only the time spent in <code class="language-plaintext highlighter-rouge">FilterExec</code>. Without StringViewArray, the filter takes 7.17s; with StringViewArray, the filter only takes 4.86s, a 32% reduction in time. Moreover, we see a 17% improvement in end-to-end query performance.</p>

<p><img src="/blog/img/string-view-2/figure2-filter-time.png" width="100%" class="img-responsive" alt="Figure showing StringViewArray reduces the filter time by 32% on ClickBench query 22." /></p>

<p>Figure 2: StringViewArray reduces the filter time by 32% on ClickBench query 22.</p>

<h1 id="faster-string-aggregation">Faster String Aggregation</h1>

<p>So far, we have discussed how to exploit two StringViewArray features: reduced copy and faster filtering. This section focuses on reusing string bytes to repeat string values.</p>

<p>As described in part one of this blog, if two strings have identical values, StringViewArray can use two different <code class="language-plaintext highlighter-rouge">view</code>s pointing at the same buffer range, thus avoiding repeating the string bytes in the buffer. This makes StringViewArray similar to an Arrow <a href="https://docs.rs/arrow/latest/arrow/array/struct.DictionaryArray.html">DictionaryArray</a> that stores Strings—both array types work well for strings with only a few distinct values.</p>

<p>Deduplicating string values can significantly reduce memory consumption in StringViewArray. However, this process is expensive and involves hashing every string and maintaining a hash table, and so it cannot be done by default when creating a StringViewArray. We introduced an<a href="https://docs.rs/arrow/latest/arrow/array/builder/struct.GenericByteViewBuilder.html#method.with_deduplicate_strings"> opt-in string deduplication mode</a> in arrow-rs for advanced users who know their data has a small number of distinct values, and where the benefits of reduced memory consumption outweigh the additional overhead of array construction.</p>

<p>Once again, we leverage DataFusion query semantics to identify StringViewArray with duplicate values, such as aggregation queries with multiple group keys. For example, some <a href="https://github.com/apache/datafusion/blob/main/benchmarks/queries/clickbench/queries.sql">ClickBench queries</a> group by two columns:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">UserID</code> (an integer with close to 1 M distinct values)</li>
  <li><code class="language-plaintext highlighter-rouge">MobilePhoneModel</code> (a string with less than a hundred distinct values)</li>
</ul>

<p>In this case, the output row count is<code class="language-plaintext highlighter-rouge"> count(distinct UserID) * count(distinct MobilePhoneModel)</code>,  which is 100M. Each string value of  <code class="language-plaintext highlighter-rouge">MobilePhoneModel</code> is repeated 1M times. With StringViewArray, we can save space by pointing the repeating values to the same underlying buffer.</p>

<p>Faster string aggregation with StringView is part of a larger project to <a href="https://github.com/apache/datafusion/issues/7000">improve DataFusion aggregation performance</a>. We have a <a href="https://github.com/apache/datafusion/pull/11794">proof of concept implementation</a> with StringView that can improve the multi-column string aggregation by 20%. We would love your help to get it production ready!</p>

<h1 id="stringview-pitfalls">StringView Pitfalls</h1>

<p>Most existing blog posts (including this one) focus on the benefits of using StringViewArray over other string representations such as StringArray. As we have discussed, even though it requires a significant engineering investment to realize, StringViewArray is a major improvement over StringArray in many cases.</p>

<p>However, there are several cases where StringViewArray is slower than StringArray. For completeness, we have listed those instances here:</p>

<ol>
  <li><strong>Tiny strings (when strings are shorter than 8 bytes)</strong>: every element of the StringViewArray consumes at least 16 bytes of memory—the size of the <code class="language-plaintext highlighter-rouge">view</code> struct. For an array of tiny strings, StringViewArray consumes more memory than StringArray and thus can cause slower performance due to additional memory pressure on the CPU cache.</li>
  <li><strong>Many repeated short strings</strong>: Similar to the first point, StringViewArray can be slower and require more memory than a DictionaryArray because 1) it can only reuse the bytes in the buffer when the strings are longer than 12 bytes and 2) 32-bit offsets are always used, even when a smaller size (8 bit or 16 bit) could represent all the distinct values.</li>
  <li><strong>Filtering:</strong> As we mentioned above, StringViewArrays often consume more memory than the corresponding StringArray, and memory bloat quickly dominates the performance without GC. However, invoking GC also reduces the benefits of less copying so must be carefully tuned.</li>
</ol>

<h1 id="conclusion-and-takeaways">Conclusion and Takeaways</h1>

<p>In these two blog posts, we discussed what it takes to implement StringViewArray in arrow-rs and then integrate it into DataFusion. Our evaluations on ClickBench queries show that StringView can improve the performance of string-intensive workloads by up to 2x.</p>

<p>Given that DataFusion already <a href="https://benchmark.clickhouse.com/#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">performs very well on ClickBench</a>, the level of end-to-end performance improvement using StringViewArray shows the power of this technique and, of course, is a win for DataFusion and the systems that build upon it.</p>

<p>StringView is a big project that has received tremendous community support. Specifically, we would like to thank <a href="https://github.com/tustvold">@tustvold</a>, <a href="https://github.com/ariesdevil">@ariesdevil</a>, <a href="https://github.com/RinChanNOWWW">@RinChanNOWWW</a>, <a href="https://github.com/ClSlaid">@ClSlaid</a>, <a href="https://github.com/2010YOUY01">@2010YOUY01</a>, <a href="https://github.com/chloro-pn">@chloro-pn</a>, <a href="https://github.com/a10y">@a10y</a>, <a href="https://github.com/Kev1n8">@Kev1n8</a>, <a href="https://github.com/Weijun-H">@Weijun-H</a>, <a href="https://github.com/PsiACE">@PsiACE</a>, <a href="https://github.com/tshauck">@tshauck</a>, and <a href="https://github.com/xinlifoobar">@xinlifoobar</a> for their valuable contributions!</p>

<p>As the introduction states, “German Style Strings” is a relatively straightforward research idea that avoid some string copies and accelerates comparisons. However, applying this (great) idea in practice requires a significant investment in careful software engineering. Again, we encourage the research community to continue to help apply research ideas to industrial systems, such as DataFusion, as doing so provides valuable perspectives when evaluating future research questions for the greatest potential impact.</p>

<h3 id="footnotes">Footnotes</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:5" role="doc-endnote">
      <p>There are additional optimizations possible in this operation that the community is working on, such as  <a href="https://github.com/apache/datafusion/issues/7957">https://github.com/apache/datafusion/issues/7957</a>. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Xiangpeng Hao, Andrew Lamb</name></author><category term="performance" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">Apache DataFusion Comet 0.2.0 Release</title><link href="https://datafusion.apache.org/blog/2024/08/28/datafusion-comet-0.2.0/" rel="alternate" type="text/html" title="Apache DataFusion Comet 0.2.0 Release" /><published>2024-08-28T00:00:00+00:00</published><updated>2024-08-28T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/08/28/datafusion-comet-0.2.0</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/08/28/datafusion-comet-0.2.0/"><![CDATA[<!--

-->

<p>The Apache DataFusion PMC is pleased to announce version 0.2.0 of the <a href="https://datafusion.apache.org/comet/">Comet</a> subproject.</p>

<p>Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.</p>

<p>Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the <a href="https://datafusion.apache.org/comet/user-guide/compatibility.html">compatibility guide</a> for more information.</p>

<p>This release covers approximately four weeks of development work and is the result of merging 87 PRs from 14 
contributors. See the <a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.2.0.md">change log</a> for more information.</p>

<h2 id="release-highlights">Release Highlights</h2>

<h3 id="docker-images">Docker Images</h3>

<p>Docker images are now available from the <a href="https://github.com/apache/datafusion-comet/pkgs/container/datafusion-comet/265110454?tag=spark-3.4-scala-2.12-0.2.0">GitHub Container Registry</a>.</p>

<h3 id="performance-improvements">Performance improvements</h3>

<ul>
  <li>Native shuffle is now enabled by default</li>
  <li>Improved handling of decimal types</li>
  <li>Reduced some redundant copying of batches in Filter/Scan operations</li>
  <li>Optimized performance of count aggregates</li>
  <li>Optimized performance of  CASE expressions for specific uses:
    <ul>
      <li>CASE WHEN expr THEN column ELSE null END</li>
      <li>CASE WHEN expr THEN literal ELSE literal END</li>
    </ul>
  </li>
  <li>Optimized performance of IS NOT NULL</li>
</ul>

<h3 id="new-features">New Features</h3>

<ul>
  <li>Window operations now support count and sum aggregates</li>
  <li>CreateArray</li>
  <li>GetStructField</li>
  <li>Support nested types in hash join</li>
  <li>Basic implementation of RLIKE expression</li>
</ul>

<h2 id="current-performance">Current Performance</h2>

<p>We use benchmarks derived from the industry standard TPC-H and TPC-DS benchmarks for tracking progress with
performance. The following charts shows the time it takes to run the queries against 100 GB of data in
Parquet format using a single executor with eight cores. See the <a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html">Comet Benchmarking Guide</a>
for details of the environment used for these benchmarks.</p>

<h3 id="benchmark-derived-from-tpc-h">Benchmark derived from TPC-H</h3>

<p>Comet 0.2.0 provides a 62% speedup compared to Spark. This is slightly better than the Comet 0.1.0 release.</p>

<p><img src="/blog/img/comet-0.2.0/tpch_allqueries.png" width="100%" class="img-responsive" alt="Chart showing TPC-H benchmark results for Comet 0.2.0" /></p>

<h3 id="benchmark-derived-from-tpc-ds">Benchmark derived from TPC-DS</h3>

<p>Comet 0.2.0 provides a 21% speedup compared to Spark, which is a significant improvement compared to 
Comet 0.1.0, which did not provide any speedup for this benchmark.</p>

<p><img src="/blog/img/comet-0.2.0/tpcds_allqueries.png" width="100%" class="img-responsive" alt="Chart showing TPC-DS benchmark results for Comet 0.2.0" /></p>

<h2 id="getting-involved">Getting Involved</h2>

<p>The Comet project welcomes new contributors. We use the same <a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord">Slack and Discord</a> channels as the main DataFusion
project.</p>

<p>The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the <a href="https://datafusion.apache.org/comet/user-guide/installation.html">Getting Started</a> guide for instructions on downloading and installing
Comet.</p>

<p>There are also many <a href="https://github.com/apache/datafusion-comet/contribute">good first issues</a> waiting for contributions.</p>]]></content><author><name>pmc</name></author><category term="subprojects" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">Apache DataFusion Python 40.1.0 Released, Significant usability updates</title><link href="https://datafusion.apache.org/blog/2024/08/20/python-datafusion-40.0.0/" rel="alternate" type="text/html" title="Apache DataFusion Python 40.1.0 Released, Significant usability updates" /><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/08/20/python-datafusion-40.0.0</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/08/20/python-datafusion-40.0.0/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>We are happy to announce that <a href="https://pypi.org/project/datafusion/40.1.0/">DataFusion in Python 40.1.0</a> has been released. In addition to
bringing in all of the new features of the core <a href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/">DataFusion 40.0.0</a> package, this release
contains <em>significant</em> updates to the user interface and documentation. We listened to the python
user community to create a more <em>pythonic</em> experience. If you have not used the python interface to
DataFusion before, this is an excellent time to give it a try!</p>

<h2 id="background">Background</h2>

<p>Until now, the python bindings for DataFusion have primarily been a thin layer to expose the
underlying Rust functionality. This has been worked well for early adopters to use DataFusion
within their Python projects, but some users have found it difficult to work with. As compared to
other DataFrame libraries, these issues were raised:</p>

<ol>
  <li>Most of the functions had little or no documentation. Users often had to refer to the Rust
documentation or code to learn how to use DataFusion. This alienated some python users.</li>
  <li>Users could not take advantage of modern IDE features such as type hinting. These are valuable
tools for rapid testing and development.</li>
  <li>Some of the interfaces felt “clunky” to users since some Python concepts do not always map well
to their Rust counterparts.</li>
</ol>

<p>This release aims to bring a better user experience to the DataFusion Python community.</p>

<h2 id="whats-changed">What’s Changed</h2>

<p>The most significant difference is that we have added wrapper functions and classes for most of the
user facing interface. These wrappers, written in Python, contain both documentation and type
annotations.</p>

<p>This documenation is now available on the <a href="https://datafusion.apache.org/python/autoapi/datafusion/index.html">DataFusion in Python API</a> website. There you can browse
the available functions and classes to see the breadth of available functionality.</p>

<p>Modern IDEs use language servers such as
<a href="https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance">Pylance</a> or
<a href="https://jedi.readthedocs.io/en/latest/">Jedi</a> to perform analysis of python code, provide useful
hints, and identify usage errors. These are major tools in the python user community. With this
release, users can fully use these tools in their workflow.</p>

<figure style="text-align: center;">
  <img src="/blog/img/python-datafusion-40.0.0/vscode_hover_tooltip.png" width="100%" class="img-responsive" alt="Fig 1: Enhanced tooltips in an IDE." />
  <figcaption>
   <b>Figure 1</b>: With the enhanced python wrappers, users can see helpful tool tips with
   type annotations directly in modern IDEs.
</figcaption>
</figure>

<p>By having the type annotations, these IDEs can also identify quickly when a user has incorrectly
used a function’s arguments as shown in Figure 2.</p>

<figure style="text-align: center;">
  <img src="/blog/img/python-datafusion-40.0.0/pylance_error_checking.png" width="100%" class="img-responsive" alt="Fig 2: Error checking in static analysis" />
  <figcaption>
   <b>Figure 2</b>: Modern Python language servers can perform static analysis and quickly find
   errors in the arguments to functions.
</figcaption>
</figure>

<p>In addition to these wrapper libraries, we have enhancements to some of the functions to feel more
easy to use.</p>

<h3 id="improved-dataframe-filter-arguments">Improved DataFrame filter arguments</h3>

<p>You can now apply multiple <code class="language-plaintext highlighter-rouge">filter</code> statements in a single step. When using <code class="language-plaintext highlighter-rouge">DataFrame.filter</code> you
can pass in multiple arguments, separated by a comma. These will act as a logical <code class="language-plaintext highlighter-rouge">AND</code> of all of
the filter arguments. The following two statements are equivalent:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">size</span><span class="sh">"</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">max_size</span><span class="sh">"</span><span class="p">)).</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">color</span><span class="sh">"</span><span class="p">)</span> <span class="o">==</span> <span class="nf">lit</span><span class="p">(</span><span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">))</span>
<span class="n">df</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">size</span><span class="sh">"</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">max_size</span><span class="sh">"</span><span class="p">),</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">color</span><span class="sh">"</span><span class="p">)</span> <span class="o">==</span> <span class="nf">lit</span><span class="p">(</span><span class="sh">"</span><span class="s">green</span><span class="sh">"</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="comparison-against-literal-values">Comparison against literal values</h3>

<p>It is very common to write DataFrame operations that compare an expression to some fixed value.
For example, filtering a DataFrame might have an operation such as <code class="language-plaintext highlighter-rouge">df.filter(col("size") &lt; lit(16))</code>.
To make these common operations more ergonomic, you can now simply use <code class="language-plaintext highlighter-rouge">df.filter(col("size") &lt; 16)</code>.</p>

<p>For the right hand side of the comparison operator, you can now use any Python value that can be
coerced into a <code class="language-plaintext highlighter-rouge">Literal</code>. This gives an easy to ready expression. For example, consider these few
lines from one of the
<a href="https://github.com/apache/datafusion-python/tree/main/examples/tpch">TPC-H examples</a> provided in
the DataFusion Python repository.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">df_lineitem</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_shipdate</span><span class="sh">"</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nf">lit</span><span class="p">(</span><span class="n">date</span><span class="p">))</span>
    <span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_discount</span><span class="sh">"</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="nf">lit</span><span class="p">(</span><span class="n">DISCOUNT</span><span class="p">)</span> <span class="o">-</span> <span class="nf">lit</span><span class="p">(</span><span class="n">DELTA</span><span class="p">))</span>
    <span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_discount</span><span class="sh">"</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="nf">lit</span><span class="p">(</span><span class="n">DISCOUNT</span><span class="p">)</span> <span class="o">+</span> <span class="nf">lit</span><span class="p">(</span><span class="n">DELTA</span><span class="p">))</span>
    <span class="p">.</span><span class="nf">filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_quantity</span><span class="sh">"</span><span class="p">)</span> <span class="o">&lt;</span> <span class="nf">lit</span><span class="p">(</span><span class="n">QUANTITY</span><span class="p">))</span>
<span class="p">)</span>
</code></pre></div></div>

<p>The above code mirrors closely how these filters would need to be applied in rust. With this new
release, the user can simplify these lines. Also shown in the example below is that <code class="language-plaintext highlighter-rouge">filter()</code>
now accepts a variable number of arguments and filters on all such arguments (boolean AND).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span> <span class="o">=</span> <span class="n">df_lineitem</span><span class="p">.</span><span class="nf">filter</span><span class="p">(</span>
    <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_shipdate</span><span class="sh">"</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">date</span><span class="p">,</span>
    <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_discount</span><span class="sh">"</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">DISCOUNT</span> <span class="o">-</span> <span class="n">DELTA</span><span class="p">,</span>
    <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_discount</span><span class="sh">"</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">DISCOUNT</span> <span class="o">+</span> <span class="n">DELTA</span><span class="p">,</span>
    <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">l_quantity</span><span class="sh">"</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">QUANTITY</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="select-columns-by-name">Select columns by name</h3>

<p>It is very common for users to perform <code class="language-plaintext highlighter-rouge">DataFrame</code> selection where they simply want a column. For
this we have had the function <code class="language-plaintext highlighter-rouge">select_columns("a", "b")</code> or the user could perform
<code class="language-plaintext highlighter-rouge">select(col("a"), col("b"))</code>. In the new release, we accept either full expressions in <code class="language-plaintext highlighter-rouge">select()</code>
or strings of the column names. You can mix these as well.</p>

<p>Where before you may have to do an operation like</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">a</span><span class="sh">"</span><span class="p">),</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">b</span><span class="sh">"</span><span class="p">),</span> <span class="n">f</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">c</span><span class="sh">"</span><span class="p">)))</span>
</code></pre></div></div>

<p>You can now simplify this to</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="sh">"</span><span class="s">a</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">b</span><span class="sh">"</span><span class="p">,</span> <span class="n">f</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">c</span><span class="sh">"</span><span class="p">)))</span>
</code></pre></div></div>

<h3 id="creating-named-structs">Creating named structs</h3>

<p>Creating a <code class="language-plaintext highlighter-rouge">struct</code> with named fields was previously difficult to use and allowed for potential
user errors when specifying the name of each field. Now we have a cleaner interface where the
user passes a list of tuples containing the name of the field and the expression to create.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df</span><span class="p">.</span><span class="nf">select</span><span class="p">(</span><span class="n">f</span><span class="p">.</span><span class="nf">named_struct</span><span class="p">([</span>
  <span class="p">(</span><span class="sh">"</span><span class="s">a</span><span class="sh">"</span><span class="p">,</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">a</span><span class="sh">"</span><span class="p">)),</span>
  <span class="p">(</span><span class="sh">"</span><span class="s">b</span><span class="sh">"</span><span class="p">,</span> <span class="nf">col</span><span class="p">(</span><span class="sh">"</span><span class="s">b</span><span class="sh">"</span><span class="p">))</span>
<span class="p">]))</span>
</code></pre></div></div>

<h2 id="next-steps">Next Steps</h2>

<p>While most of the user facing classes and functions have been exposed, there are a few that require
exposure. Namely the classes in <code class="language-plaintext highlighter-rouge">datafusion.object_store</code> and the logical plans used by
<code class="language-plaintext highlighter-rouge">datafusion.substrait</code>. The team is working on
<a href="https://github.com/apache/datafusion-python/issues/767">these issues</a>.</p>

<p>Additionally, in the next release of DataFusion there have been improvements made to the user-defined
aggregate and window functions to make them easier to use. We plan on
<a href="https://github.com/apache/datafusion-python/issues/780">bringing these enhancements</a> to this project.</p>

<h2 id="thank-you">Thank You</h2>

<p>We would like to thank the following members for their very helpful discussions regarding these
updates: <a href="https://github.com/andygrove">@andygrove</a>, <a href="https://github.com/max-muoto">@max-muoto</a>, <a href="https://github.com/slyons">@slyons</a>, <a href="https://github.com/Throne3d">@Throne3d</a>, <a href="https://github.com/Michael-J-Ward">@Michael-J-Ward</a>, <a href="https://github.com/datapythonista">@datapythonista</a>,
<a href="https://github.com/austin362667">@austin362667</a>, <a href="https://github.com/kylebarron">@kylebarron</a>, <a href="https://github.com/simicd">@simicd</a>. The <a href="https://github.com/apache/datafusion-python/pull/750">primary PR (#750)</a> that includes these updates
had an extensive conversation, leading to a significantly improved end product. Again, thank you
to all who provided input!</p>

<p>We would like to give an special thank you to <a href="https://github.com/3ok">@3ok</a> who created the initial version of the wrapper
definitions. The work they did was time consuming and required exceptional attention to detail. It
provided enormous value to starting this project. Thank you!</p>

<h2 id="get-involved">Get Involved</h2>

<p>The DataFusion Python team is an active and engaging community and we would love
to have you join us and help the project.</p>

<p>Here are some ways to get involved:</p>

<ul>
  <li>
    <p>Learn more by visiting the <a href="https://datafusion.apache.org/python/index.html">DataFusion Python project</a>
page.</p>
  </li>
  <li>
    <p>Try out the project and provide feedback, file issues, and contribute code.</p>
  </li>
</ul>]]></content><author><name>timsaucer</name></author><category term="release" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">Apache DataFusion 40.0.0 Released</title><link href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/" rel="alternate" type="text/html" title="Apache DataFusion 40.0.0 Released" /><published>2024-07-24T00:00:00+00:00</published><updated>2024-07-24T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/"><![CDATA[<!--

-->

<!-- see https://github.com/apache/datafusion/issues/9602 for details -->

<h2 id="introduction">Introduction</h2>

<p>We are proud to announce <a href="https://crates.io/crates/datafusion/40.0.0">DataFusion 40.0.0</a>. This blog highlights some of the
many major improvements since we released <a href="https://datafusion.apache.org/blog/2024/01/19/datafusion-34.0.0/">DataFusion 34.0.0</a> and a preview of
what the community is thinking about in the next 6 months. We are hoping to make
more regular blog posts – if you are interested in helping write them, please
reach out!</p>

<p><a href="https://datafusion.apache.org/">Apache DataFusion</a> is an extensible query engine, written in <a href="https://www.rust-lang.org/">Rust</a>, that
uses <a href="https://arrow.apache.org">Apache Arrow</a> as its in-memory format. DataFusion is used by developers to
create new, fast data centric systems such as databases, dataframe libraries,
machine learning and streaming applications. While <a href="https://datafusion.apache.org/user-guide/introduction.html#project-goals">DataFusion’s primary design
goal</a> is to accelerate the creation of other data centric systems, it has a
reasonable experience directly out of the box as a <a href="https://datafusion.apache.org/python/">dataframe library</a> and
<a href="https://datafusion.apache.org/user-guide/cli/">command line SQL tool</a>.</p>

<p>DataFusion’s core thesis is that as a community, together we can build much more
advanced technology than any of us as individuals or companies could do alone. 
Without DataFusion, highly performant vectorized query engines would remain
the domain of a few large companies and world-class research institutions. 
With DataFusion, we can all build on top of a shared foundation, and focus on
what makes our projects unique.</p>

<h2 id="community-growth--">Community Growth  📈</h2>

<p>In the last 6 months, between <code class="language-plaintext highlighter-rouge">34.0.0</code> and <code class="language-plaintext highlighter-rouge">40.0.0</code>, our community continues to
grow in new and exciting ways.</p>

<ol>
  <li>DataFusion became a top level Apache Software Foundation project (read the
<a href="https://news.apache.org/foundation/entry/apache-software-foundation-announces-new-top-level-project-apache-datafusion">press release</a> and <a href="https://datafusion.apache.org/blog/2024/05/07/datafusion-tlp/">blog post</a>).</li>
  <li>We added several PMC members and new
committers: <a href="https://github.com/comphead">@comphead</a>, <a href="https://github.com/mustafasrepo">@mustafasrepo</a>, <a href="https://github.com/ozankabak">@ozankabak</a>, and <a href="https://github.com/waynexia">@waynexia</a> joined the PMC,
<a href="https://github.com/jonahgao">@jonahgao</a> and <a href="https://github.com/lewiszlw">@lewiszlw</a> joined as committers. See the <a href="https://lists.apache.org/list.html?dev@datafusion.apache.org">mailing list</a> for
more details.</li>
  <li><a href="https://datafusion.apache.org/comet/">DataFusion Comet</a> was <a href="https://arrow.apache.org/blog/2024/03/06/comet-donation/">donated</a> and is nearing its first release.</li>
  <li>In the <a href="https://github.com/apache/arrow-datafusion">core DataFusion repo</a> alone we reviewed and accepted almost 1500 PRs from 182 different
committers, created over 1000 issues and closed 781 of them 🚀. This is up
almost 50% from our last post (1000 PRs from 124 committers with 650 issues
created in our last post) 🤯. All changes are listed in the detailed
<a href="https://github.com/apache/datafusion/blob/main/datafusion/CHANGELOG.md">CHANGELOG</a>.</li>
  <li>DataFusion focused meetups happened or are happening in multiple cities 
around the world: <a href="https://github.com/apache/datafusion/discussions/8522">Austin</a>, <a href="https://github.com/apache/datafusion/discussions/10800">San Francisco</a>, <a href="https://www.huodongxing.com/event/5761971909400?td=1965290734055">Hangzhou</a>, <a href="https://github.com/apache/datafusion/discussions/11213">New York</a>, and
<a href="https://github.com/apache/datafusion/discussions/11431">Belgrade</a>.</li>
  <li>Many new projects started in the <a href="https://github.com/datafusion-contrib">datafusion-contrib</a> organization, including
<a href="https://github.com/datafusion-contrib/datafusion-table-providers">Table Providers</a>, <a href="https://github.com/datafusion-contrib/datafusion-sqlancer">SQLancer</a>, <a href="https://github.com/datafusion-contrib/datafusion-functions-variant">Open Variant</a>, <a href="https://github.com/datafusion-contrib/datafusion-functions-json">JSON</a>, and <a href="https://github.com/datafusion-contrib/datafusion-orc">ORC</a>.</li>
</ol>

<!--
$ git log --pretty=oneline 34.0.0..40.0.0 . | wc -l
     1453 (up from 1009)

$ git shortlog -sn 34.0.0..40.0.0 . | wc -l
      182 (up from 124)

https://crates.io/crates/datafusion/34.0.0
DataFusion 34 released Dec 17, 2023

https://crates.io/crates/datafusion/40.0.0
DataFusion 34 released July 12, 2024

Issues created in this time: 321 open, 781 closed (up from 214 open, 437 closed)
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+created%3A2023-12-17..2024-07-12

Issues closed: 911 (up from 517)
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+closed%3A2023-12-17..2024-07-12

PRs merged in this time 1490 (up from 908)
https://github.com/apache/arrow-datafusion/pulls?q=is%3Apr+merged%3A2023-12-17..2024-07-12

-->

<p>In addition, DataFusion has been appearing publicly more and more, both online and offline. Here are some highlights:</p>

<ol>
  <li><a href="https://dl.acm.org/doi/10.1145/3626246.3653368">Apache Arrow DataFusion: A Fast, Embeddable, Modular Analytic Query Engine</a>, was presented in <a href="https://2024.sigmod.org/">SIGMOD ‘24</a>, one of the major database conferences</li>
  <li>As part of the trend to define “the POSIX of databases” in <a href="https://db.cs.cmu.edu/papers/2024/whatgoesaround-sigmodrec2024.pdf">“What Goes Around Comes Around… And Around…”</a> from Andy Pavlo and Mike Stonebraker</li>
  <li><a href="https://www.cpard.xyz/posts/datafusion/">“Why you should keep an eye on Apache DataFusion and its community”</a></li>
  <li><a href="https://www.tisonkun.org/2024/07/15/datafusion-meetup-san-francisco/">Apache DataFusion offline meetup in the Bay Area</a></li>
</ol>

<h2 id="improved-performance-">Improved Performance 🚀</h2>

<p>Performance is a key feature of DataFusion, and the community continues to work
to keep DataFusion state of the art in this area. One major area DataFusion
improved is the time it takes to convert a SQL query into a plan that can be
executed. Planning is now almost 2x faster for TPC-DS and TPC-H queries, and
over 10x faster for some queries with many columns.</p>

<p>Here is a chart showing the improvement due to the concerted effort of many
contributors including <a href="https://github.com/jackwener">@jackwener</a>, <a href="https://github.com/alamb">@alamb</a>, <a href="https://github.com/Lordworms">@Lordworms</a>, <a href="https://github.com/dmitrybugakov">@dmitrybugakov</a>,
<a href="https://github.com/appletreeisyellow">@appletreeisyellow</a>, <a href="https://github.com/ClSlaid">@ClSlaid</a>, <a href="https://github.com/rohitrastogi">@rohitrastogi</a>, <a href="https://github.com/emgeee">@emgeee</a>, <a href="https://github.com/kevinmingtarja">@kevinmingtarja</a>,
and <a href="https://github.com/peter-toth">@peter-toth</a> over several months (see <a href="https://github.com/apache/arrow-datafusion/issues/8045">ticket</a> for more details)</p>

<p><img src="/blog/assets/datafusion-40.0.0/improved-planning-time.png" width="700" /></p>

<p>DataFusion is now up to 40% faster for queries that <code class="language-plaintext highlighter-rouge">GROUP BY</code> a single string
or binary column due to a <a href="https://github.com/apache/datafusion/pull/8827">specialization for single
Uft8/LargeUtf8/Binary/LargeBinary</a>. We are working on improving performance when
there are <a href="https://github.com/apache/datafusion/issues/9403">multiple variable length columns in the <code class="language-plaintext highlighter-rouge">GROUP BY</code> clause</a>.</p>

<p>We are also in the final phases of <a href="https://github.com/apache/datafusion/issues/10918">integrating</a> the new <a href="https://docs.rs/arrow/latest/arrow/array/struct.GenericByteViewArray.html">Arrow StringView</a>
which significantly improves performance for workloads that scan, filter and
group by variable length string and binary data. We expect the improvement to be
especially pronounced for Parquet files due to <a href="https://github.com/apache/arrow-rs/issues/5530">upstream work in the parquet
reader</a>. Kudos to <a href="https://github.com/XiangpengHong">@XiangpengHong</a>, <a href="https://github.com/AriesDevil">@AriesDevil</a>, <a href="https://github.com/PsiACE">@PsiACE</a>, <a href="https://github.com/Weijun-H">@Weijun-H</a>,
<a href="https://github.com/a10y">@a10y</a>, and <a href="https://github.com/RinChanNOWWW">@RinChanNOWWW</a> for driving this project.</p>

<h2 id="improved-quality-">Improved Quality 📋</h2>

<p>DataFusion continues to improve overall in quality. In addition to ongoing bug
fixes, one of the most exciting improvements is the addition of a new <a href="https://github.com/datafusion-contrib/datafusion-sqlancer">SQLancer</a>
based <a href="https://github.com/apache/datafusion/issues/11030">DataFusion Fuzzing</a> suite thanks to <a href="https://github.com/2010YOUY01">@2010YOUY01</a> that has already found
several bugs and thanks to <a href="https://github.com/jonahgao">@jonahgao</a>, <a href="https://github.com/tshauck">@tshauck</a>, <a href="https://github.com/xinlifoobar">@xinlifoobar</a>,
<a href="https://github.com/LorrensP-2158466">@LorrensP-2158466</a> for fixing them so fast.</p>

<h2 id="improved-documentation-">Improved Documentation 📚</h2>

<p>We continue to improve the documentation to make it easier to get started using DataFusion with
the <a href="https://datafusion.apache.org/library-user-guide/index.html">Library Users Guide</a>, <a href="https://docs.rs/datafusion/latest/datafusion/index.html">API documentation</a>, and <a href="https://github.com/apache/datafusion/tree/main/datafusion-examples">Examples</a>.</p>

<p>Some notable new examples include:</p>
<ul>
  <li><a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/sql_analysis.rs">sql_analysis.rs</a> to analyse SQL queries with DataFusion structures (thanks <a href="https://github.com/LorrensP-2158466">@LorrensP-2158466</a>)</li>
  <li><a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/function_factory.rs">function_factory.rs</a> to create custom functions via SQL (thanks <a href="https://github.com/milenkovicm">@milenkovicm</a>)</li>
  <li><a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/plan_to_sql.rs">plan_to_sql.rs</a> to generate SQL from DataFusion Expr and LogicalPlan (thanks <a href="https://github.com/edmondop">@edmondop</a>)</li>
  <li><a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs">parquet_index.rs</a> and <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs">advanced_parquet_index.rs</a> for parquet indexing, described more below (thanks <a href="https://github.com/alamb">@alamb</a>)</li>
</ul>

<h2 id="new-features-">New Features ✨</h2>

<p>There are too many new features in the last 6 months to list them all, but here
are some highlights:</p>

<h1 id="sql">SQL</h1>
<ul>
  <li>Support for <code class="language-plaintext highlighter-rouge">UNNEST</code> (thanks <a href="https://github.com/duongcongtoai">@duongcongtoai</a>, <a href="https://github.com/JasonLi-cn">@JasonLi-cn</a> and <a href="https://github.com/jayzhan211">@jayzhan211</a>)</li>
  <li>Support for <a href="https://github.com/apache/datafusion/issues/462">Recursive CTEs</a> (thanks <a href="https://github.com/jonahgao">@jonahgao</a> and <a href="https://github.com/matthewgapp">@matthewgapp</a>)</li>
  <li>Support for <code class="language-plaintext highlighter-rouge">CREATE FUNCTION</code> (see below)</li>
  <li>Many new SQL functions</li>
</ul>

<p>DataFusion now has much improved support for structured types such <code class="language-plaintext highlighter-rouge">STRUCT</code>,
<code class="language-plaintext highlighter-rouge">LIST</code>/<code class="language-plaintext highlighter-rouge">ARRAY</code> and <code class="language-plaintext highlighter-rouge">MAP</code>. For example, you can now create <code class="language-plaintext highlighter-rouge">STRUCT</code> literals 
in SQL like this:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;</span> <span class="n">select</span> <span class="p">{</span><span class="nv">'foo</span><span class="err">'</span><span class="p">:</span> <span class="p">{</span><span class="nv">'bar</span><span class="err">'</span><span class="p">:</span> <span class="mi">2</span><span class="p">}};</span>
<span class="o">+--------------------------------------------------------------+</span>
<span class="p">|</span> <span class="nf">named_struct</span><span class="p">(</span><span class="nf">Utf8</span><span class="p">(</span><span class="s">"foo"</span><span class="p">),</span><span class="nf">named_struct</span><span class="p">(</span><span class="nf">Utf8</span><span class="p">(</span><span class="s">"bar"</span><span class="p">),</span><span class="nf">Int64</span><span class="p">(</span><span class="mi">2</span><span class="p">)))</span> <span class="p">|</span>
<span class="o">+--------------------------------------------------------------+</span>
<span class="p">|</span> <span class="p">{</span><span class="n">foo</span><span class="p">:</span> <span class="p">{</span><span class="n">bar</span><span class="p">:</span> <span class="mi">2</span><span class="p">}}</span>                                              <span class="p">|</span>
<span class="o">+--------------------------------------------------------------+</span>
<span class="mi">1</span> <span class="nf">row</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="n">fetched</span><span class="py">.
Elapsed</span> <span class="mf">0.002</span> <span class="n">seconds</span><span class="err">.</span>
</code></pre></div></div>

<h1 id="sql-unparser-sql-formatter">SQL Unparser (SQL Formatter)</h1>

<p>DataFusion now supports converting <code class="language-plaintext highlighter-rouge">Expr</code>s and <code class="language-plaintext highlighter-rouge">LogicalPlan</code>s BACK to SQL text.
This can be useful in query federation to push predicates down into other
systems that only accept SQL, and for building systems that generate SQL.</p>

<p>For example, you can now convert a logical expression back to SQL text:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Form a logical expression that represents the SQL "a &lt; 5 OR a = 8"</span>
<span class="k">let</span> <span class="n">expr</span> <span class="o">=</span> <span class="nf">col</span><span class="p">(</span><span class="s">"a"</span><span class="p">)</span><span class="nf">.lt</span><span class="p">(</span><span class="nf">lit</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span><span class="nf">.or</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="s">"a"</span><span class="p">)</span><span class="nf">.eq</span><span class="p">(</span><span class="nf">lit</span><span class="p">(</span><span class="mi">8</span><span class="p">)));</span>
<span class="c1">// convert the expression back to SQL text</span>
<span class="k">let</span> <span class="n">sql</span> <span class="o">=</span> <span class="nf">expr_to_sql</span><span class="p">(</span><span class="o">&amp;</span><span class="n">expr</span><span class="p">)</span><span class="o">?</span><span class="nf">.to_string</span><span class="p">();</span>
<span class="nd">assert_eq!</span><span class="p">(</span><span class="n">sql</span><span class="p">,</span> <span class="s">"a &lt; 5 OR a = 8"</span><span class="p">);</span>
</code></pre></div></div>

<p>You can also do complex things like parsing SQL, modifying the plan, and convert
it back to SQL:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">let</span> <span class="n">df</span> <span class="o">=</span> <span class="n">ctx</span>
  <span class="c1">// Use SQL to read some data from the parquet file</span>
  <span class="nf">.sql</span><span class="p">(</span><span class="s">"SELECT int_col, double_col, CAST(date_string_col as VARCHAR) FROM alltypes_plain"</span><span class="p">)</span>
  <span class="k">.await</span><span class="o">?</span><span class="p">;</span>
<span class="c1">// Programmatically add new filters `id &gt; 1 and tinyint_col &lt; double_col`</span>
<span class="k">let</span> <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="nf">.filter</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="s">"id"</span><span class="p">)</span><span class="nf">.gt</span><span class="p">(</span><span class="nf">lit</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="nf">.and</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="s">"tinyint_col"</span><span class="p">)</span><span class="nf">.lt</span><span class="p">(</span><span class="nf">col</span><span class="p">(</span><span class="s">"double_col"</span><span class="p">))))</span><span class="o">?</span>
<span class="c1">// Convert the new logical plan back to SQL</span>
<span class="k">let</span> <span class="n">sql</span> <span class="o">=</span> <span class="nf">plan_to_sql</span><span class="p">(</span><span class="n">df</span><span class="nf">.logical_plan</span><span class="p">())</span><span class="o">?</span><span class="nf">.to_string</span><span class="p">();</span>
<span class="nd">assert_eq!</span><span class="p">(</span><span class="n">sql</span><span class="p">,</span> 
           <span class="s">"SELECT alltypes_plain.int_col, alltypes_plain.double_col, CAST(alltypes_plain.date_string_col AS VARCHAR) </span><span class="err">\</span><span class="s">
           FROM alltypes_plain WHERE ((alltypes_plain.id &gt; 1) AND (alltypes_plain.tinyint_col &lt; alltypes_plain.double_col))"</span><span class="p">)</span>
<span class="p">);</span>
</code></pre></div></div>

<p>See the <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/plan_to_sql.rs">Plan to SQL example</a> or the APIs <a href="https://docs.rs/datafusion/latest/datafusion/sql/unparser/fn.expr_to_sql.html">expr_to_sql</a> and <a href="https://docs.rs/datafusion/latest/datafusion/sql/unparser/fn.plan_to_sql.html">plan_to_sql</a> for more details.</p>

<h1 id="low-level-apis-for-fast-parquet-access-indexing">Low Level APIs for Fast Parquet Access (indexing)</h1>

<p>With their rising prevalence, supporting efficient access to Parquet files
stored remotely on object storage is important. Part of doing this efficiently
is minimizing the number of object store requests made by caching metadata and
skipping over parts of the file that are not needed (e.g. via an index).</p>

<p>DataFusion’s Parquet reader has long internally supported <a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/">advanced predicate
pushdown</a> by reading the parquet metadata from the file footer and pruning based
on row group and data page statistics. DataFusion now also supports users
supplying their own low level pruning information via the <a href="https://docs.rs/datafusion/latest/datafusion/datasource/physical_plan/parquet/struct.ParquetAccessPlan.html"><code class="language-plaintext highlighter-rouge">ParquetAccessPlan</code></a>
API.</p>

<p>This API can be used along with index information to selectively skip decoding
parts of the file. For example, Spice AI used this feature to add <a href="https://github.com/spiceai/spiceai/pull/1891">efficient
support</a> for reading from DeltaLake tables and handling <a href="https://docs.delta.io/latest/delta-deletion-vectors.html">deletion vectors</a>.</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        ┌───────────────────────┐   If the RowSelection does not include any
        │          ...          │   rows from a particular Data Page, that
        │                       │   Data Page is not fetched or decoded.
        │ ┌───────────────────┐ │   Note this requires a PageIndex
        │ │     ┌──────────┐  │ │
Row     │ │     │DataPage 0│  │ │                 ┌────────────────────┐
Groups  │ │     └──────────┘  │ │                 │                    │
        │ │     ┌──────────┐  │ │                 │    ParquetExec     │
        │ │ ... │DataPage 1│ ◀┼ ┼ ─ ─ ─           │  (Parquet Reader)  │
        │ │     └──────────┘  │ │      └ ─ ─ ─ ─ ─│                    │
        │ │     ┌──────────┐  │ │                 │ ╔═══════════════╗  │
        │ │     │DataPage 2│  │ │ If only rows    │ ║ParquetMetadata║  │
        │ │     └──────────┘  │ │ from DataPage 1 │ ╚═══════════════╝  │
        │ └───────────────────┘ │ are selected,   └────────────────────┘
        │                       │ only DataPage 1
        │          ...          │ is fetched and
        │                       │ decoded
        │ ╔═══════════════════╗ │
        │ ║  Thrift metadata  ║ │
        │ ╚═══════════════════╝ │
        └───────────────────────┘
         Parquet File
</code></pre></div></div>

<p>See the <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs">parquet_index.rs</a> and <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs">advanced_parquet_index.rs</a> examples for more details.</p>

<p>Thanks to <a href="https://github.com/alamb">@alamb</a> and <a href="https://github.com/Ted-Jiang">@Ted-Jiang</a> for this feature.</p>

<h2 id="building-systems-is-easier-with-datafusion-️">Building Systems is Easier with DataFusion 🛠️</h2>

<p>In addition to many incremental API improvements, there are several new APIs that make
it easier to build systems on top of DataFusion:</p>

<ul>
  <li>Faster and easier to use <a href="https://docs.rs/datafusion/latest/datafusion/common/tree_node/trait.TreeNode.html#overview">TreeNode API</a> for traversing and manipulating plans and expressions.</li>
  <li>All functions now use the same <a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/trait.ScalarUDFImpl.html">Scalar User Defined Function API</a>, making it easier to customize
DataFusion’s behavior without sacrificing performance. See <a href="https://github.com/apache/arrow-datafusion/issues/8045">ticket</a> for more details.</li>
  <li>DataFusion can now be compiled to <a href="https://github.com/apache/datafusion/discussions/9834">WASM</a>.</li>
</ul>

<h1 id="user-defined-sql-parsing-extensions">User Defined SQL Parsing Extensions</h1>

<p>As of DataFusion 40.0.0, you can use the <a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/planner/trait.ExprPlanner.html"><code class="language-plaintext highlighter-rouge">ExprPlanner</code></a> trait to extend
DataFusion’s SQL planner to support custom operators or syntax.</p>

<p>For example the <a href="https://github.com/datafusion-contrib/datafusion-functions-json">datafusion-functions-json</a> project uses this API to support
JSON operators in SQL queries. It provides a custom implementation for
planning JSON operators such as <code class="language-plaintext highlighter-rouge">-&gt;</code> and <code class="language-plaintext highlighter-rouge">-&gt;&gt;</code> with code like:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="n">MyCustomPlanner</span><span class="p">;</span>

<span class="k">impl</span> <span class="n">ExprPlanner</span> <span class="k">for</span> <span class="n">MyCustomPlanner</span> <span class="p">{</span>
    <span class="c1">// Provide custom implementation for planning a binary operators</span>
    <span class="c1">// such as `-&gt;` and `-&gt;&gt;`</span>
    <span class="k">fn</span> <span class="nf">plan_binary_op</span><span class="p">(</span>
        <span class="o">&amp;</span><span class="k">self</span><span class="p">,</span>
        <span class="n">expr</span><span class="p">:</span> <span class="n">RawBinaryExpr</span><span class="p">,</span>
        <span class="n">_schema</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">DFSchema</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">Result</span><span class="o">&lt;</span><span class="n">PlannerResult</span><span class="o">&lt;</span><span class="n">RawBinaryExpr</span><span class="o">&gt;&gt;</span> <span class="p">{</span>
        <span class="k">match</span> <span class="o">&amp;</span><span class="n">expr</span><span class="py">.op</span> <span class="p">{</span>
           <span class="nn">BinaryOperator</span><span class="p">::</span><span class="n">Arrow</span> <span class="k">=&gt;</span> <span class="p">{</span> <span class="cm">/* plan -&gt; operator */</span> <span class="p">}</span>
           <span class="nn">BinaryOperator</span><span class="p">::</span><span class="n">LongArrow</span> <span class="k">=&gt;</span> <span class="p">{</span> <span class="cm">/* plan -&gt;&gt; operator */</span> <span class="p">}</span>
           <span class="o">...</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>Thanks to <a href="https://github.com/samuelcolvin">@samuelcolvin</a>, <a href="https://github.com/jayzhan211">@jayzhan211</a> and <a href="https://github.com/dharanad">@dharanad</a> for helping make this
feature happen.</p>

<h1 id="pluggable-support-for-create-function">Pluggable Support for <code class="language-plaintext highlighter-rouge">CREATE FUNCTION</code></h1>

<p>DataFusion’s new <a href="https://docs.rs/datafusion/latest/datafusion/execution/context/trait.FunctionFactory.html"><code class="language-plaintext highlighter-rouge">FunctionFactory</code></a> API let’s users provide a handler for
<code class="language-plaintext highlighter-rouge">CREATE FUNCTION</code> SQL statements. This feature lets you build systems that
support defining functions in SQL such as</p>

<div class="language-sql highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">-- SQL based functions</span>
<span class="k">CREATE</span> <span class="k">FUNCTION</span> <span class="n">my_func</span><span class="p">(</span><span class="nb">DOUBLE</span><span class="p">,</span> <span class="nb">DOUBLE</span><span class="p">)</span> <span class="k">RETURNS</span> <span class="nb">DOUBLE</span>
    <span class="k">RETURN</span> <span class="err">$</span><span class="mi">1</span> <span class="o">+</span> <span class="err">$</span><span class="mi">3</span>
<span class="p">;</span>

<span class="c1">-- ML Models</span>
<span class="k">CREATE</span> <span class="k">FUNCTION</span> <span class="n">iris</span><span class="p">(</span><span class="nb">FLOAT</span><span class="p">[])</span> <span class="k">RETURNS</span> <span class="nb">FLOAT</span><span class="p">[]</span> 
<span class="k">LANGUAGE</span> <span class="n">TORCH</span> <span class="k">AS</span> <span class="s1">'models:/iris@champion'</span><span class="p">;</span>

<span class="c1">-- WebAssembly</span>
<span class="k">CREATE</span> <span class="k">FUNCTION</span> <span class="n">func</span><span class="p">(</span><span class="nb">FLOAT</span><span class="p">[])</span> <span class="k">RETURNS</span> <span class="nb">FLOAT</span><span class="p">[]</span> 
<span class="k">LANGUAGE</span> <span class="n">WASM</span> <span class="k">AS</span> <span class="s1">'func.wasm'</span>
</code></pre></div></div>

<p>Huge thanks to <a href="https://github.com/milenkovicm">@milenkovicm</a> for this feature. There is an example of how to
make macro like functions in <a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/function_factory.rs">function_factory.rs</a>. It would be
great if <a href="https://github.com/apache/datafusion/issues/9326">someone made a demo</a> showing how to create WASMs 🎣.</p>

<h2 id="looking-ahead-the-next-six-months-">Looking Ahead: The Next Six Months 🔭</h2>

<p>The community has been <a href="https://github.com/apache/datafusion/issues/11442">discussing what we will work on in the next six months</a>.
Some major initiatives from that discussion are:</p>

<ol>
  <li>
    <p><em>Performance</em>: Improve the speed of <a href="https://github.com/apache/arrow-datafusion/issues/7000">aggregating “high cardinality”</a>
  data when there are many (e.g. millions) of distinct groups as well as additional
  ideas to improve parquet performance.</p>
  </li>
  <li>
    <p><em>Modularity</em>: Make DataFusion even more modular, by completely unifying
built in and user <a href="https://github.com/apache/datafusion/issues/8708">aggregate functions</a> and <a href="https://github.com/apache/datafusion/issues/8709">window functions</a>.</p>
  </li>
  <li>
    <p><em>LogicalTypes</em>: <a href="https://github.com/apache/datafusion/issues/11513">Introduce Logical Types</a> to make it easier to use
different encodings like <code class="language-plaintext highlighter-rouge">StringView</code>, <code class="language-plaintext highlighter-rouge">RunEnd</code> and <code class="language-plaintext highlighter-rouge">Dictionary</code> arrays as well
as user defined types. Thanks <a href="https://github.com/notfilippo">@notfilippo</a> for driving this.</p>
  </li>
  <li>
    <p><em>Improved Documentation</em>: Write blog posts and videos explaining
how to use DataFusion for real-world use cases.</p>
  </li>
  <li>
    <p><em>Testing</em>: Improve CI infrastructure and test coverage, more fuzz
testing, and better functional and performance regression testing.</p>
  </li>
</ol>

<h1 id="how-to-get-involved">How to Get Involved</h1>

<p>DataFusion is not a project built or driven by a single person, company, or
foundation. Rather, our community of users and contributors work together to
build a shared technology that none of us could have built alone.</p>

<p>If you are interested in joining us we would love to have you. You can try out
DataFusion on some of your own data and projects and let us know how it goes,
contribute suggestions, documentation, bug reports, or a PR with documentation,
tests or code. A list of open issues suitable for beginners is <a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">here</a> and you
can find how to reach us on the <a href="https://datafusion.apache.org/contributor-guide/communication.html">communication doc</a>.</p>]]></content><author><name>pmc</name></author><category term="release" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">Apache DataFusion Comet 0.1.0 Release</title><link href="https://datafusion.apache.org/blog/2024/07/20/datafusion-comet-0.1.0/" rel="alternate" type="text/html" title="Apache DataFusion Comet 0.1.0 Release" /><published>2024-07-20T00:00:00+00:00</published><updated>2024-07-20T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/07/20/datafusion-comet-0.1.0</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/07/20/datafusion-comet-0.1.0/"><![CDATA[<!--

-->

<p>The Apache DataFusion PMC is pleased to announce the first official source release of the <a href="https://datafusion.apache.org/comet/">Comet</a> subproject.</p>

<p>Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.</p>

<p>Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the <a href="https://datafusion.apache.org/comet/user-guide/compatibility.html">compatibility guide</a> for more information.</p>

<p>This release covers five months of development work since the project was <a href="https://datafusion.apache.org/blog/2024/03/06/comet-donation/">donated</a> to the Apache DataFusion
project and is the result of merging 343 PRs from 41 contributors. See the <a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.1.0.md">change log</a> for more information.</p>

<p>This first release supports 15 <a href="https://datafusion.apache.org/comet/user-guide/datatypes.html#">data types</a>, 13 <a href="https://datafusion.apache.org/comet/user-guide/operators.html#">operators</a>, and 106 <a href="https://datafusion.apache.org/comet/user-guide/expressions.html#">expressions</a>. Comet is compatible with Apache
Spark versions 3.3, 3.4, and 3.5. There is also experimental support for preview versions of Spark 4.0.</p>

<h2 id="project-status">Project Status</h2>

<p>The project’s recent focus has been on fixing correctness and stability issues and implementing additional
native operators and expressions so that a broader range of queries can be executed natively.</p>

<p>Here are some of the highlights since the project was donated:</p>

<ul>
  <li>Implemented native support for:
    <ul>
      <li>SortMergeJoin</li>
      <li>HashJoin</li>
      <li>BroadcastHashJoin</li>
      <li>Columnar Shuffle</li>
      <li>More aggregate expressions</li>
      <li>Window aggregates</li>
      <li>Many Spark-compatible CAST expressions</li>
    </ul>
  </li>
  <li>Implemented a simple Spark Fuzz Testing utility to find correctness issues</li>
  <li>Published a <a href="https://datafusion.apache.org/comet/user-guide/overview.html">User Guide</a> and <a href="https://datafusion.apache.org/comet/contributor-guide/contributing.html">Contributors Guide</a></li>
  <li>Created a <a href="https://github.com/apache/datafusion-benchmarks">DataFusion Benchmarks</a> repository with scripts and documentation for running benchmarks derived<br />
from TPC-H and TPC-DS with DataFusion and Comet</li>
</ul>

<h2 id="current-performance">Current Performance</h2>

<p>Comet already delivers a modest performance speedup for many queries, enabling faster data processing and
shorter time-to-insights.</p>

<p>We use benchmarks derived from the industry standard TPC-H and TPC-DS benchmarks for tracking progress with
performance. The following chart shows the time it takes to run the 22 TPC-H queries against 100 GB of data in
Parquet format using a single executor with eight cores. See the <a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html">Comet Benchmarking Guide</a>
for details of the environment used for these benchmarks.</p>

<p><img src="/blog/img/comet-0.1.0/tpch_allqueries.png" width="100%" class="img-responsive" alt="Chart showing TPC-H benchmark results for Comet 0.1.0" /></p>

<p>Comet reduces the overall execution time from 626 seconds to 407 seconds, a 54% speedup (1.54x faster).</p>

<p>Running the same queries with DataFusion standalone using the same number of cores results in a 3.9x speedup
compared to Spark. Although this isn’t a fair comparison (DataFusion does not have shuffle or match Spark
semantics in some cases, for example), it does give some idea about the potential future performance of
Comet. Comet aims to provide a 2x-4x speedup for a wide range of queries once more operators and expressions
can run natively.</p>

<p>The following chart shows how much Comet currently accelerates each query from the benchmark.</p>

<p><img src="/blog/img/comet-0.1.0/tpch_queries_speedup.png" width="100%" class="img-responsive" alt="Chart showing TPC-H benchmark results for Comet 0.1.0" /></p>

<p>These benchmarks can be reproduced in any environment using the documentation in the <a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html">Comet Benchmarking Guide</a>. We
encourage you to run these benchmarks in your environment or, even better, try Comet out with your existing Spark jobs.</p>

<h2 id="roadmap">Roadmap</h2>

<p>Comet is an open-source project, and contributors are welcome to work on any features they are interested in, but
here are some current focus areas.</p>

<ul>
  <li>Improve Performance &amp; Reliability:
    <ul>
      <li>Implement the remaining features needed so that all TPC-H queries can run entirely natively</li>
      <li>Implement spill support in SortMergeJoin</li>
      <li>Enable columnar shuffle by default</li>
    </ul>
  </li>
  <li>Fully support Spark version 4.0.0</li>
  <li>Support more Spark operators and expressions
    <ul>
      <li>We would like to support many more expressions natively in Comet, and this is a great place to start
contributing. The contributors’ guide has a section covering <a href="https://datafusion.apache.org/comet/contributor-guide/adding_a_new_expression.html">adding support for new expressions</a>.</li>
    </ul>
  </li>
  <li>Move more Spark expressions into the <a href="https://crates.io/crates/datafusion-comet-spark-expr">datafusion-comet-spark-expr</a> crate. Although the main focus of the Comet
project is to provide an accelerator for Apache Spark, we also publish a standalone crate containing
Spark-compatible expressions that can be used by any project using DataFusion, without adding any dependencies
on JVM or Apache Spark.</li>
  <li>Release Process &amp; Documentation
    <ul>
      <li>Implement a binary release process so that we can publish JAR files to Maven for all supported platforms</li>
      <li>Add documentation for running Spark and Comet in Kubernetes, and add example Dockerfiles.</li>
    </ul>
  </li>
</ul>

<h2 id="getting-involved">Getting Involved</h2>

<p>The Comet project welcomes new contributors. We use the same <a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord">Slack and Discord</a> channels as the main DataFusion
project, and there is a Comet community video call held every four weeks on Wednesdays at 11:30 a.m. Eastern Time,
which is 16:30 UTC during Eastern Standard Time and 15:30 UTC during Eastern Daylight Time. See the
<a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing">Comet Community Meeting</a> Google Document for the next scheduled meeting date, the video call link, and
recordings of previous calls.</p>

<p>The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the <a href="https://datafusion.apache.org/comet/user-guide/installation.html">Getting Started</a> guide for instructions on downloading and installing
Comet.</p>

<p>There are also many <a href="https://github.com/apache/datafusion-comet/contribute">good first issues</a> waiting for contributions.</p>]]></content><author><name>pmc</name></author><category term="subprojects" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry><entry><title type="html">Announcing Apache Arrow DataFusion is now Apache DataFusion</title><link href="https://datafusion.apache.org/blog/2024/05/07/datafusion-tlp/" rel="alternate" type="text/html" title="Announcing Apache Arrow DataFusion is now Apache DataFusion" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://datafusion.apache.org/blog/2024/05/07/datafusion-tlp</id><content type="html" xml:base="https://datafusion.apache.org/blog/2024/05/07/datafusion-tlp/"><![CDATA[<!--

-->

<h2 id="introduction">Introduction</h2>

<p>TLDR; <a href="https://arrow.apache.org/">Apache Arrow</a> DataFusion –&gt; <a href="https://datafusion.apache.org/">Apache DataFusion</a></p>

<p>The Arrow PMC and newly created DataFusion PMC are happy to announce that as of
April 16, 2024 the Apache Arrow DataFusion subproject is now a top level
<a href="https://www.apache.org/">Apache Software Foundation</a> project.</p>

<h2 id="background">Background</h2>

<p>Apache DataFusion is a fast, extensible query engine for building high-quality
data-centric systems in Rust, using the Apache Arrow in-memory format.</p>

<p>When DataFusion was <a href="https://arrow.apache.org/blog/2019/02/04/datafusion-donation/">donated to the Apache Software Foundation</a> in 2019, the
DataFusion community was not large enough to stand on its own and the Arrow
project agreed to help support it. The community has grown significantly since
2019, benefiting immensely from being part of Arrow and following <a href="https://www.apache.org/theapacheway/">The Apache
Way</a>.</p>

<h2 id="why-now">Why now?</h2>

<p>The community <a href="https://github.com/apache/datafusion/discussions/6475">discussed graduating to a top level project publicly</a> for almost
a year, as the project seemed ready to stand on its own and would benefit from
more focused governance. For example, earlier in DataFusion’s life many
contributed to both <a href="https://github.com/apache/arrow-rs">arrow-rs</a> and DataFusion, but as DataFusion has matured many
contributors, committers and PMC members focused more and more exclusively on
DataFusion.</p>

<h2 id="looking-forward">Looking forward</h2>

<p>The future looks bright. There are now <a href="https://datafusion.apache.org/user-guide/introduction.html#known-users">10s of known projects built with
DataFusion</a>, and that number continues to grow. We recently held our <a href="https://github.com/apache/datafusion/discussions/8522">first in
person meetup</a> passed <a href="https://github.com/apache/datafusion/stargazers">5000 stars</a> on GitHub, <a href="https://github.com/apache/datafusion/issues/8373#issuecomment-2025133714">wrote a paper that was accepted
at SIGMOD 2024</a>, and began work on <a href="https://github.com/apache/datafusion-comet">Comet</a>, an <a href="https://spark.apache.org/">Apache Spark</a> accelerator
<a href="https://arrow.apache.org/blog/2024/03/06/comet-donation/">initially donated by Apple</a>.</p>

<p>Thank you to everyone in the Arrow community who helped DataFusion grow and
mature over the years, and we look forward to continuing our collaboration as
projects. All future blogs and announcements will be posted on the <a href="https://datafusion.apache.org/">Apache
DataFusion</a> website.</p>

<h2 id="get-involved">Get Involved</h2>

<p>If you are interested in joining the community, we would love to have you join
us. Get in touch using <a href="https://datafusion.apache.org/contributor-guide/communication.html">Communication Doc</a> and learn how to get involved in the
<a href="https://datafusion.apache.org/contributor-guide/index.html">Contributor Guide</a>. We welcome everyone to try DataFusion on their
own data and projects and let us know how it goes, contribute suggestions,
documentation, bug reports, or a PR with documentation, tests or code.</p>]]></content><author><name>pmc</name></author><category term="subprojects" /><summary type="html"><![CDATA[&lt;!–]]></summary></entry></feed>