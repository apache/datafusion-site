<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Apache DataFusion Blog</title><link href="https://datafusion.apache.org/blog/" rel="alternate"></link><link href="https://datafusion.apache.org/blog/feeds/all-en.atom.xml" rel="self"></link><id>https://datafusion.apache.org/blog/</id><updated>2025-09-01T00:00:00+00:00</updated><subtitle></subtitle><entry><title>Dynamic Filters: Passing Information Between Operators During Execution for 10x Faster Queries</title><link href="https://datafusion.apache.org/blog/2025/09/01/dynamic-filters" rel="alternate"></link><published>2025-09-01T00:00:00+00:00</published><updated>2025-09-01T00:00:00+00:00</updated><author><name>Adrian Garcia Badaracco (Pydantic)</name></author><id>tag:datafusion.apache.org,2025-09-01:/blog/2025/09/01/dynamic-filters</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- 
diagrams source TODO
--&gt;
&lt;p&gt;This blog post introduces a powerful query engine optimization technique called dynamic filters or sideways information passing. We implemented this optimization in DataFusion as a community effort with care to support custom operators and distributed usage. These optimizations (and related work) have resulted in order of magnitude improvements for some …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- 
diagrams source TODO
--&gt;
&lt;p&gt;This blog post introduces a powerful query engine optimization technique called dynamic filters or sideways information passing. We implemented this optimization in DataFusion as a community effort with care to support custom operators and distributed usage. These optimizations (and related work) have resulted in order of magnitude improvements for some query patterns.&lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Our main commercial product at &lt;a href="https://pydantic.dev/logfire"&gt;Pydantic&lt;/a&gt; is an observability platform built on DataFusion. One of the most common workflows / queries is "show me the last K traces" which translates to something along the lines of:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT *
FROM records
ORDER BY start_timestamp DESC
LIMIT 1000;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We noticed this was &lt;em&gt;pretty slow&lt;/em&gt; for us, and came to the finding that DataFusion runs this query by reading the &lt;em&gt;entire&lt;/em&gt; &lt;code&gt;records&lt;/code&gt; table and sorting it. It uses a specialized sort operator called a &lt;code&gt;TopK&lt;/code&gt; that only keeps ~ &lt;code&gt;K&lt;/code&gt; rows in memory: every new batch that gets read is compared against the &lt;code&gt;K&lt;/code&gt; largest (in the case of an &lt;code&gt;DESC&lt;/code&gt; order; compared by the sort key, in this case &lt;code&gt;start_timestamp&lt;/code&gt;) and then those &lt;code&gt;K&lt;/code&gt; rows possibly get replaced with any new rows that were larger.
Importantly DataFusion had no early termination here: it would read the &lt;em&gt;entire&lt;/em&gt; &lt;code&gt;records&lt;/code&gt; table even if it already had &lt;code&gt;K&lt;/code&gt; rows because it had to verify that there could not possibly be any other rows that are have a larger &lt;code&gt;start_timestamp&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You can see how this is a problem if you have 2 years worth of data: the largest &lt;code&gt;1000&lt;/code&gt; start timestamps are probably all within the first couple of files read, but even if we have 1000 timestamps on August 16th 2025 we'll keep reading files that have all of their timestamps in 2024 just to make sure.&lt;/p&gt;
&lt;p&gt;Looking through the DataFusion issues we found that Influx has a similar issue that they've solved with an operator called &lt;a href="https://github.com/apache/datafusion/issues/15191"&gt;&lt;code&gt;SortPreservingMerge&lt;/code&gt;&lt;/a&gt;, but that requires that the data is already sorted and requires some careful analysis of ordering to prove that it can be used. That is not the case for our data (and a lot of other datasets out there): data can tend to be &lt;em&gt;roughly&lt;/em&gt; sorted (e.g. if you append to files as you receive it) but that does not guarantee that it is fully sorted, including between files. We brought this up with the community which ultimately resulted in us opening &lt;a href="https://github.com/apache/datafusion/issues/15037"&gt;an issue describing a possible solution&lt;/a&gt; which we deemed "dynamic filters". The basic idea is to create a link between the state of the &lt;code&gt;TopK&lt;/code&gt; operator and a filter that is applied when opening files and during scans. For example, let's say our &lt;code&gt;TopK&lt;/code&gt; heap for an &lt;code&gt;ORDER BY start_timetsamp LIMIT 3&lt;/code&gt; has the values:&lt;/p&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;start_timestamp&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2025-08-16T20:35:15.00Z&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2025-08-16T20:35:14.00Z&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2025-08-16T20:35:13.00Z&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We'd generate a filter from these values of the form &lt;code&gt;start_timestamp &amp;gt; '2025-08-16T20:35:13.00Z'&lt;/code&gt;, if that was placed into the query would look like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT *
FROM records
WHERE start_timestamp &amp;gt; '2025-08-16T20:35:13.00Z'
ORDER BY start_timestamp DESC
LIMIT 3;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;But obviously when we start running the query we don't have the value &lt;code&gt;'2025-08-16T20:35:13.00Z'&lt;/code&gt; so what we do is put in a placeholder value, you can think of it as:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT *
FROM records
WHERE dynamic_filter()
ORDER BY start_timestamp DESC
LIMIT 3;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;dynamic_filter()&lt;/code&gt; is a structure that initially has the value &lt;code&gt;true&lt;/code&gt; but will be updated by the TopK operator as the query progresses. Although I'm showing this example as SQL for illustrative purposes these optimizations are actually done at the physical plan layer - much after SQL is parsed.&lt;/p&gt;
&lt;h2&gt;Results Summary&lt;/h2&gt;
&lt;p&gt;We've seen upwards of a 10x performance improvement for some queries and no performance regressions.
The actual numbers depend on a lot of factors which we need to dig into.
Let's look at some preliminary numbers, using &lt;a href="https://github.com/apache/datafusion/blob/main/benchmarks/queries/clickbench/queries/q23.sql"&gt;ClickBench Q23&lt;/a&gt; which is very similar to our earlier examples:&lt;/p&gt;
&lt;div class="text-center"&gt;
&lt;img alt="Q23 Performance Improvement with Dynamic Filters and Late Materialization" class="img-responsive" src="/blog/images/dynamic-filters/execution-time.svg" width="80%"/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Execution times for ClickBench Q23 with and without dynamic filters (DF) and late materialization (LM) for different partitions / core usage. Dynamic filters along show a large improvement but when combined with late materialization we can see up to a 22x improvement in execution time. See appendix for the queries used to generate these results.&lt;/p&gt;
&lt;p&gt;Let's go over some of the flags used in the benchmark:&lt;/p&gt;
&lt;h3&gt;Dynamic Filters&lt;/h3&gt;
&lt;p&gt;This is the optimization we spoke about above.
The TopK operator will generate a filter that is applied to the scan operators, which will first be used to skip rows and then as we open new files (if there are more to open) it will be used to skip entire files that do not match the filter.&lt;/p&gt;
&lt;h3&gt;Late Materialization&lt;/h3&gt;
&lt;p&gt;This optimization has been talked about in the past (see for example &lt;a href="./2025-03-21-parquet-pushdown.md"&gt;this blog post&lt;/a&gt;).
It's particularly effective when combined with dynamic filters because without them there is no time based filter to apply during the scan.
And without late materialization the filter can only be used to prune entire files, which is ineffective for large files or if the order in which files are read is not optimal.&lt;/p&gt;
&lt;h2&gt;Implementation for TopK Operator&lt;/h2&gt;
&lt;p&gt;TopK operators (a specialization of a sort operator + a limit operator) implement dynamic filter pushdown by updating a filter each time the heap / topK is updated. The filter is then used to skip rows and files during the scan operator.
At the query plan level, Q23 looks like this before it is executed:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;       SortExec(TopK)      &amp;boxv;
&amp;boxv;    --------------------   &amp;boxv;
&amp;boxv; EventTime@4 ASC NULLS LAST&amp;boxv;
&amp;boxv;                           &amp;boxv;
&amp;boxv;         limit: 10         &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;       DataSourceExec      &amp;boxv;
&amp;boxv;    --------------------   &amp;boxv;
&amp;boxv;         files: 100        &amp;boxv;
&amp;boxv;      format: parquet      &amp;boxv;
&amp;boxv;                           &amp;boxv;
&amp;boxv;         predicate:        &amp;boxv;
&amp;boxv; CAST(URL AS Utf8View) LIKE&amp;boxv;
&amp;boxv;      %google% AND true    &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the &lt;code&gt;true&lt;/code&gt; placeholder filter for the dynamic filter in the &lt;code&gt;predicate&lt;/code&gt; field of the &lt;code&gt;DataSourceExec&lt;/code&gt; operator. This will be updated by the &lt;code&gt;SortExec&lt;/code&gt; operator as it processes rows.
After running the query, the plan looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;       SortExec(TopK)      &amp;boxv;
&amp;boxv;    --------------------   &amp;boxv;
&amp;boxv; EventTime@4 ASC NULLS LAST&amp;boxv;
&amp;boxv;                           &amp;boxv;
&amp;boxv;         limit: 10         &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;       DataSourceExec      &amp;boxv;
&amp;boxv;    --------------------   &amp;boxv;
&amp;boxv;         files: 100        &amp;boxv;
&amp;boxv;      format: parquet      &amp;boxv;
&amp;boxv;                           &amp;boxv;
&amp;boxv;         predicate:        &amp;boxv;
&amp;boxv; CAST(URL AS Utf8View) LIKE&amp;boxv;
&amp;boxv;      %google% AND         &amp;boxv;
&amp;boxv; EventTime &amp;lt; 1372713773.0  &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Implementation for Hash Join Operator&lt;/h2&gt;
&lt;p&gt;We've also implemented dynamic filters for hash joins, also called "sideways information passing".
In a Hash Join the query engine picks one side of the join to be the "build" side and the other side to be the "probe" side. The build side is read first and then the probe side is read, using a hash table built from the build side to match rows from the probe side.
Dynamic filters are used to filter the probe side based on the values from the build side.
In particular, we take the min/max values from the build side and use them to create a filter that is applied to the probe side.
This is a very cheap filter to evaluate but when combined with statistics pruning, later materialization and other optimizations it can lead to significant performance improvements (we've observed up to 20x improvements in some queries).&lt;/p&gt;
&lt;p&gt;A query plan for a hash join with dynamic filters looks like this after it is executed:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;copy (select i as k from generate_series(1, 1000) t(i)) to 'small_table.parquet';
copy (select i as k, i as v from generate_series(1, 100000) t(i)) to 'large_table.parquet';
create external table small_table stored as parquet location 'small_table.parquet';
create external table large_table stored as parquet location 'large_table.parquet';
explain select * from small_table join large_table on small_table.k = large_table.k where large_table.v &amp;gt;= 50;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;    CoalesceBatchesExec    &amp;boxv;
&amp;boxv;    --------------------   &amp;boxv;
&amp;boxv;     target_batch_size:    &amp;boxv;
&amp;boxv;            8192           &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;        HashJoinExec       &amp;boxv;
&amp;boxv;    --------------------   &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;        on: (k = k)        &amp;boxv;              &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;              &amp;boxv;
&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;       DataSourceExec      &amp;boxv;&amp;boxv;    CoalesceBatchesExec    &amp;boxv;
&amp;boxv;    --------------------   &amp;boxv;&amp;boxv;    --------------------   &amp;boxv;
&amp;boxv;          files: 1         &amp;boxv;&amp;boxv;     target_batch_size:    &amp;boxv;
&amp;boxv;      format: parquet      &amp;boxv;&amp;boxv;            8192           &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
                              &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
                              &amp;boxv;         FilterExec        &amp;boxv;
                              &amp;boxv;    --------------------   &amp;boxv;
                              &amp;boxv;     predicate: v &amp;gt;= 50    &amp;boxv;
                              &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
                              &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
                              &amp;boxv;      RepartitionExec      &amp;boxv;
                              &amp;boxv;    --------------------   &amp;boxv;
                              &amp;boxv; partition_count(in-&amp;gt;out): &amp;boxv;
                              &amp;boxv;          1 -&amp;gt; 12          &amp;boxv;
                              &amp;boxv;                           &amp;boxv;
                              &amp;boxv;    partitioning_scheme:   &amp;boxv;
                              &amp;boxv;    RoundRobinBatch(12)    &amp;boxv;
                              &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
                              &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
                              &amp;boxv;       DataSourceExec      &amp;boxv;
                              &amp;boxv;    --------------------   &amp;boxv;
                              &amp;boxv;          files: 1         &amp;boxv;
                              &amp;boxv;      format: parquet      &amp;boxv;
                              &amp;boxv;                           &amp;boxv;
                              &amp;boxv;         predicate:        &amp;boxv;
                              &amp;boxv;      v &amp;gt;= 50 AND.         &amp;boxv;
                              &amp;boxv;     k &amp;gt;= 1 AND k &amp;lt;= 1000  &amp;boxv;
                              &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Implementation for Scan Operator&lt;/h2&gt;
&lt;p&gt;Scan operators do not actually know anything about dynamic filters: we were able to package up dynamic filters as an &lt;code&gt;Arc&amp;lt;dyn PhysicalExpr&amp;gt;&lt;/code&gt; which is mostly handled by scan operators like any other expression.
We did however add some new functionality to &lt;code&gt;PhysicalExpr&lt;/code&gt; to make working with dynamic filters easier:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;PhysicalExpr::generation() -&amp;gt; u64&lt;/code&gt;: used to track if a tree of filters has changed (e.g. because it has a dynamic filter that has been updated). For example, if we go from &lt;code&gt;c1 = 'a' AND DynamicFilter [ c2 &amp;gt; 1]&lt;/code&gt; to &lt;code&gt;c1 = 'a' AND DynamicFilter [ c2 &amp;gt; 2]&lt;/code&gt; the generation value will change so we know if we should re-evaluate the filter against static date like file or row group level statistics. This is used to do early termination of reading a file if the filter is updated mid scan and we can now skip the file, all without needlessly re-evaluating file level statistics all the time.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PhysicalExpr::snapshot() -&amp;gt; Arc&amp;lt;dyn PhysicalExpr&amp;gt;&lt;/code&gt;: used to create a snapshot of the filter at a given point in time. Dynamic filters use this to return the current value of their innner static filter. This can be used to serialize the filter across the wire in the case of distributed queries or to pass to systems that only support more basic filters (e.g. stats pruning rewrites).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is all encapsulated in the &lt;code&gt;DynamicFilterPhysicalExpr&lt;/code&gt; struct.&lt;/p&gt;
&lt;p&gt;One of the important design decisions was around directionality of information passing and locking: some early designs had the scan polling the source operators on every row / batch, but this causes a lot of overhead.
Instead we opted for a "push" based model where the read path has minimal locking and the write path (the TopK operator) is responsible for updating the filter.
Thus &lt;code&gt;DynamicFilterPhysicalExpr&lt;/code&gt; is essentially an &lt;code&gt;Arc&amp;lt;RwLock&amp;lt;Arc&amp;lt;dyn PhysicalExpr&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; which allows the TopK operator to update the filter while the scan operator can read it without blocking.&lt;/p&gt;
&lt;h2&gt;Custom &lt;code&gt;ExectuionPlan&lt;/code&gt; Operators&lt;/h2&gt;
&lt;p&gt;We went to great efforts to ensure that dynamic filters are not a hardcoded black box that only works for internal operators.
The DataFusion community is dynamic and the project is used in many different contexts, some with very advanced custom operators specialized for specific use cases.
To support this we made sure that dynamic filters can be used with custom &lt;code&gt;ExecutionPlan&lt;/code&gt; operators by implementing a couple of methods in the &lt;code&gt;ExecutionPlan&lt;/code&gt; trait.
We've made an extensive library of helper structs and functions that make it only 1-2 lines to implement filter pushdown support or a source of dynamic filters for custom operators.&lt;/p&gt;
&lt;p&gt;This approach has already paid off: we've had multiple community members implement support for dynamic filter pushdown in just the first few months of this feature being available.&lt;/p&gt;
&lt;h2&gt;Future Work&lt;/h2&gt;
&lt;p&gt;Although we've made great progress and DataFusion now has one of the most advanced dynamic filter / sideways information passing implementations that we know of we are not done yet!&lt;/p&gt;
&lt;p&gt;There's a multitude of areas of future improvement that we are looking into:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support for more types of joins: we only implemented support for hash inner joins so far. There's the potential to expand this to other join types both in terms of the physical implementation (nested loop joins, etc.) and join type (e.g. left outer joins, cross joins, etc.).&lt;/li&gt;
&lt;li&gt;Push down entire hash tables to the scan operator: this could potentially help a lot with join keys that are not naturally ordered or have a lot of skew.&lt;/li&gt;
&lt;li&gt;Use file level statistics to order files to match the &lt;code&gt;ORDER BY&lt;/code&gt; clause as best we can: this will help TopK dynamic filters be more effective by skipping more work earlier in the scan.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;h3&gt;Queries and Data&lt;/h3&gt;
&lt;h4&gt;Figure 1: ClickBench Q23&lt;/h4&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;-- Data was downloaded using apache/datafusion -&amp;gt; benchmarks/bench.sh -&amp;gt; ./benchmarks/bench.sh data clickbench_partitioned
create external table hits stored as parquet location 'benchmarks/data/hits_partitioned';

-- Must set for ClickBench hits_partitioned dataset. See https://github.com/apache/datafusion/issues/16591
set datafusion.execution.parquet.binary_as_string = true;
-- Only matters if pushdown_filters is enabled but they don't get enabled together sadly
set datafusion.execution.parquet.reorder_filters = true;

set datafusion.execution.target_partitions = 1;  -- or set to 12 to use multiple cores
set datafusion.optimizer.enable_dynamic_filter_pushdown = false;
set datafusion.execution.parquet.pushdown_filters = false;

explain analyze
SELECT *
FROM hits
WHERE "URL" LIKE '%google%'
ORDER BY "EventTime"
LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;dynamic filters&lt;/th&gt;
&lt;th style="text-align: left;"&gt;late materialization&lt;/th&gt;
&lt;th style="text-align: right;"&gt;cores&lt;/th&gt;
&lt;th style="text-align: right;"&gt;time (s)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;False&lt;/td&gt;
&lt;td style="text-align: left;"&gt;False&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;td style="text-align: right;"&gt;32.039&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;False&lt;/td&gt;
&lt;td style="text-align: left;"&gt;True&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;td style="text-align: right;"&gt;16.903&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;True&lt;/td&gt;
&lt;td style="text-align: left;"&gt;False&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;td style="text-align: right;"&gt;18.195&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;True&lt;/td&gt;
&lt;td style="text-align: left;"&gt;True&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1&lt;/td&gt;
&lt;td style="text-align: right;"&gt;1.42&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;False&lt;/td&gt;
&lt;td style="text-align: left;"&gt;False&lt;/td&gt;
&lt;td style="text-align: right;"&gt;12&lt;/td&gt;
&lt;td style="text-align: right;"&gt;5.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;False&lt;/td&gt;
&lt;td style="text-align: left;"&gt;True&lt;/td&gt;
&lt;td style="text-align: right;"&gt;12&lt;/td&gt;
&lt;td style="text-align: right;"&gt;2.37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;True&lt;/td&gt;
&lt;td style="text-align: left;"&gt;False&lt;/td&gt;
&lt;td style="text-align: right;"&gt;12&lt;/td&gt;
&lt;td style="text-align: right;"&gt;5.055&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;True&lt;/td&gt;
&lt;td style="text-align: left;"&gt;True&lt;/td&gt;
&lt;td style="text-align: right;"&gt;12&lt;/td&gt;
&lt;td style="text-align: right;"&gt;0.602&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;</content><category term="blog"></category></entry><entry><title>Using External Indexes, Metadata Stores, Catalogs and Caches to Accelerate Queries on Apache Parquet</title><link href="https://datafusion.apache.org/blog/2025/08/15/external-parquet-indexes" rel="alternate"></link><published>2025-08-15T00:00:00+00:00</published><updated>2025-08-15T00:00:00+00:00</updated><author><name>Andrew Lamb (InfluxData)</name></author><id>tag:datafusion.apache.org,2025-08-15:/blog/2025/08/15/external-parquet-indexes</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- diagrams source https://docs.google.com/presentation/d/1e_Z_F8nt2rcvlNvhU11khF5lzJJVqNtqtyJ-G3mp4-Q --&gt;
&lt;p&gt;It is a common misconception that &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; requires (slow) reparsing of
metadata and is limited to indexing structures provided by the format. In fact,
caching parsed metadata and using custom external indexes along with
Parquet's hierarchical data organization can significantly speed up query
processing.&lt;/p&gt;
&lt;p&gt;In this blog, I describe …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- diagrams source https://docs.google.com/presentation/d/1e_Z_F8nt2rcvlNvhU11khF5lzJJVqNtqtyJ-G3mp4-Q --&gt;
&lt;p&gt;It is a common misconception that &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; requires (slow) reparsing of
metadata and is limited to indexing structures provided by the format. In fact,
caching parsed metadata and using custom external indexes along with
Parquet's hierarchical data organization can significantly speed up query
processing.&lt;/p&gt;
&lt;p&gt;In this blog, I describe the role of external indexes, caches, and metadata
stores in high performance systems, and demonstrate how to apply these concepts
to Parquet processing using &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;. &lt;em&gt;Note this is an expanded
version of the &lt;a href="https://www.youtube.com/watch?v=74YsJT1-Rdk"&gt;companion video&lt;/a&gt; and &lt;a href="https://docs.google.com/presentation/d/1e_Z_F8nt2rcvlNvhU11khF5lzJJVqNtqtyJ-G3mp4-Q/edit"&gt;presentation&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;System designers choose between a pre-configured data system or the often
daunting task of building their own custom data platform from scratch.
For many users and use cases, one of the existing data systems will
likely be good enough. However, traditional systems such as &lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt;, &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt;,
&lt;a href="https://clickhouse.com/"&gt;ClickHouse&lt;/a&gt;, &lt;a href="https://hive.apache.org/"&gt;Hive&lt;/a&gt;, or &lt;a href="https://www.snowflake.com/"&gt;Snowflake&lt;/a&gt; are each optimized for a certain set of
tradeoffs between performance, cost, availability, interoperability, deployment
target, cloud / on-premises, operational ease and many other factors.&lt;/p&gt;
&lt;p&gt;For new, or especially demanding use cases, where no existing system makes your
optimal tradeoffs, you can build your own custom data platform. Previously this
was a long and expensive endeavor, but today, in the era of &lt;a href="https://www.vldb.org/pvldb/vol16/p2679-pedreira.pdf"&gt;Composable Data
Systems&lt;/a&gt;, it is increasingly feasible. High quality, open source building blocks
such as &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; for storage, &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; for in-memory processing,
and &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; for query execution make it possible to quickly build
custom data platforms optimized for your specific
needs&lt;sup&gt;&lt;a href="#footnote1"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction to External Indexes / Catalogs / Metadata Stores / Caches&lt;/h2&gt;
&lt;div class="text-center"&gt;
&lt;img alt="Using External Indexes to Accelerate Queries" class="img-responsive" src="/blog/images/external-parquet-indexes/external-index-overview.png" width="80%"/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Using external indexes to speed up queries in an analytic system.
Given a user's query (Step 1), the system uses an external index (one that is not
stored inline in the data files) to quickly find files that may contain
relevant data (Step 2). Then, for each file, the system uses the external index
to further narrow the required data to only those &lt;strong&gt;parts&lt;/strong&gt; of each file
(e.g. data pages) that are relevant (Step 3). Finally, the system reads only those
parts of the file and returns the results to the user (Step 4).&lt;/p&gt;
&lt;p&gt;In this blog, I use the term &lt;strong&gt;"index"&lt;/strong&gt; to mean any structure that helps
locate relevant data during processing, and a high level overview of how
external indexes are used to speed up queries is shown in Figure 1.&lt;/p&gt;
&lt;p&gt;All data systems typically store both the data itself and additional information
(metadata) to more quickly find data relevant to a query. Metadata is often
stored in structures with names like "index," "catalog" and "cache" and the
terminology varies widely across systems. &lt;/p&gt;
&lt;p&gt;There are many different types of indexes, types of content stored in indexes,
strategies to keep indexes up to date, and ways to apply indexes during query
processing. These differences each have their own set of tradeoffs, and thus
different systems understandably make different choices depending on their use
case. There is no one-size-fits-all solution for indexing. For example, Hive
uses the &lt;a href="https://cwiki.apache.org/confluence/display/Hive/Design#Design-Metastore"&gt;Hive Metastore&lt;/a&gt;, &lt;a href="https://www.vertica.com/"&gt;Vertica&lt;/a&gt; uses a purpose-built &lt;a href="https://www.vertica.com/docs/latest/HTML/Content/Authoring/AdministratorsGuide/Managing/Metadata/CatalogOverview.htm"&gt;Catalog&lt;/a&gt;, and open
data lake systems typically use a table format such as &lt;a href="https://iceberg.apache.org/"&gt;Apache Iceberg&lt;/a&gt; or &lt;a href="https://delta.io/"&gt;Delta
Lake&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;External Indexes&lt;/strong&gt; store information separately ("external") to the data
itself. External indexes are flexible and widely used, but require additional
operational overhead to keep in sync with the data files. For example, if you
add a new Parquet file to your data lake, you must also update the relevant
external index to include information about the new file. Note, you can
avoid the operational overhead of external indexes by using only the data files
themselves, including &lt;a href="https://datafusion.apache.org/blog/2025/07/14/user-defined-parquet-indexes/"&gt;Embedding User-Defined Indexes in Apache Parquet
Files&lt;/a&gt;. However, this approach comes with its own set of tradeoffs such as 
increased file sizes and the need to update the data files to update the index.&lt;/p&gt;
&lt;p&gt;Examples of information commonly stored in external indexes include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Min/Max statistics&lt;/li&gt;
&lt;li&gt;Bloom filters&lt;/li&gt;
&lt;li&gt;Inverted indexes / Full Text indexes &lt;/li&gt;
&lt;li&gt;Information needed to read the remote file (e.g the schema, or Parquet footer metadata)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Examples of locations where external indexes can be stored include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Separate files&lt;/strong&gt; such as &lt;a href="https://www.json.org/"&gt;JSON&lt;/a&gt; or Parquet files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transactional databases&lt;/strong&gt; such as &lt;a href="https://www.postgresql.org/"&gt;PostgreSQL&lt;/a&gt; tables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distributed key-value stores&lt;/strong&gt; such as &lt;a href="https://redis.io/"&gt;Redis&lt;/a&gt; or &lt;a href="https://cassandra.apache.org/"&gt;Cassandra&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Local memory&lt;/strong&gt; such as an in-memory hash map.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Using Apache Parquet for Storage&lt;/h2&gt;
&lt;p&gt;While the rest of this blog focuses on building custom external indexes using
Parquet and DataFusion, I first briefly discuss why Parquet is a good choice for
modern analytic systems. The research community frequently confuses limitations
of a particular &lt;a href="https://parquet.apache.org/docs/file-format/implementationstatus/"&gt;implementation of the Parquet format&lt;/a&gt; with the &lt;a href="https://parquet.apache.org/docs/file-format/"&gt;Parquet Format&lt;/a&gt;
itself, and this confusion often obscures capabilities that make Parquet a good
target for external indexes.&lt;/p&gt;
&lt;p&gt;Apache Parquet's combination of good compression, high-performance, high quality
open source libraries, and wide ecosystem interoperability make it a compelling
choice when building new systems. While there are some niche use cases that may
benefit from specialized formats, Parquet is typically the obvious choice.
While recent proprietary file formats differ in details, they all use the same
high level structure&lt;sup&gt;&lt;a href="#footnote2"&gt;2&lt;/a&gt;&lt;/sup&gt; as Parquet: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Metadata (typically at the end  of the file)&lt;/li&gt;
&lt;li&gt;Data divided into columns and then into horizontal slices (e.g. Parquet Row Groups and/or Data Pages). &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The structure is so widespread because it enables the hierarchical pruning
approach described in the next section. For example, the native &lt;a href="https://clickhouse.com/docs/engines/table-engines/mergetree-family/mergetree"&gt;Clickhouse
MergeTree&lt;/a&gt; format consists of &lt;em&gt;Parts&lt;/em&gt; (similar to Parquet files), and &lt;em&gt;Granules&lt;/em&gt;
(similar to Row Groups). The &lt;a href="https://clickhouse.com/docs/guides/best-practices/sparse-primary-indexes#clickhouse-index-design"&gt;Clickhouse indexing strategy&lt;/a&gt; follows a classic
hierarchical pruning approach that first locates the Parts and then the Granules
that may contain relevant data for the query. This is exactly the same pattern
as Parquet based systems, which first locate the relevant Parquet files and then
the Row Groups / Data Pages within those files.&lt;/p&gt;
&lt;p&gt;A common criticism of using Parquet is that it is not as performant as some new
proposal. These criticisms typically cherry-pick a few queries and/or datasets
and build a specialized index or data layout for that specific case. However,
as I explain in the &lt;a href="https://www.youtube.com/watch?v=74YsJT1-Rdk"&gt;companion video&lt;/a&gt; of this blog, even for
&lt;a href="https://clickbench.com/"&gt;ClickBench&lt;/a&gt;&lt;sup&gt;&lt;a href="#footnote6"&gt;6&lt;/a&gt;&lt;/sup&gt;, the current
benchmaxxing&lt;sup&gt;&lt;a href="#footnote3"&gt;3&lt;/a&gt;&lt;/sup&gt; target of analytics vendors, there is
less than a factor of two difference in performance between custom file formats
and Parquet. The difference becomes even lower when using Parquet files that
use the full range of existing Parquet features such Column and Offset
Indexes and Bloom Filters&lt;sup&gt;&lt;a href="#footnote7"&gt;7&lt;/a&gt;&lt;/sup&gt;. Compared to the low
interoperability and expensive transcoding/loading step of alternate file
formats, Parquet is hard to beat.&lt;/p&gt;
&lt;h2&gt;Hierarchical Pruning Overview&lt;/h2&gt;
&lt;p&gt;The key technique for optimizing query processing systems is skipping as
much data as possible, as quickly as possible. Analytic systems typically use a hierarchical
approach to progressively narrow the set of data that needs to be processed. 
The standard approach is shown in Figure 2:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Entire files are ruled out&lt;/li&gt;
&lt;li&gt;Within each file, large sections (e.g. Row Groups) are ruled out&lt;/li&gt;
&lt;li&gt;(Optionally) smaller sections (e.g. Data Pages) are ruled out&lt;/li&gt;
&lt;li&gt;Finally, the system reads only the relevant data pages and applies the query
   predicate to the data&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="text-center"&gt;
&lt;img alt="Standard Pruning Layers." class="img-responsive" src="/blog/images/external-parquet-indexes/processing-pipeline.png" width="80%"/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Hierarchical Pruning: The system first rules out files, then
Row Groups, then Data Pages, and finally reads only the relevant data pages.&lt;/p&gt;
&lt;p&gt;The process is hierarchical because the per-row computation required at the
earlier stages (e.g. skipping an entire file) is lower than the computation
required at later stages (apply predicates to the data). 
As mentioned before, while the details of what metadata is used and how that
metadata is managed varies substantially across query systems, they almost all
use a hierarchical pruning strategy.&lt;/p&gt;
&lt;h2&gt;Apache Parquet Overview&lt;/h2&gt;
&lt;p&gt;This section provides a brief background on the organization of Apache Parquet
files which is needed to fully understand the sections on implementing external indexes.
If you are already familiar with Parquet, you can skip this section.&lt;/p&gt;
&lt;p&gt;Logically, Parquet files are organized into  &lt;em&gt;Row Groups&lt;/em&gt; and &lt;em&gt;Column Chunks&lt;/em&gt; as
shown below.&lt;/p&gt;
&lt;div class="text-center"&gt;
&lt;img alt="Logical Parquet File layout: Row Groups and Column Chunks." class="img-responsive" src="/blog/images/external-parquet-indexes/parquet-layout.png" width="80%"/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Logical Parquet File Layout: Data is first divided in horizontal slices
called Row Groups. The data is then stored column by column in &lt;em&gt;Column Chunks&lt;/em&gt;.
This arrangement allows efficient access to only the portions of columns needed
for a query.&lt;/p&gt;
&lt;p&gt;Physically, Parquet data is stored as a series of Data Pages along with metadata
stored at the end of the file (in the footer), as shown below.&lt;/p&gt;
&lt;div class="text-center"&gt;
&lt;img alt="Physical Parquet File layout: Metadata and Footer." class="img-responsive" src="/blog/images/external-parquet-indexes/parquet-metadata.png" width="80%"/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Physical Parquet File Layout: A typical Parquet file is composed
of many data pages,  which contain the raw encoded data, and a footer that
stores metadata about the file, including the schema and the location of the
relevant data pages, and optional statistics such as min/max values for each
Column Chunk.&lt;/p&gt;
&lt;p&gt;Parquet files are organized to minimize IO and processing using two key mechanisms:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Projection Pushdown&lt;/strong&gt;: if a query needs only a subset of columns from a table, it
   only needs to read the pages for the relevant Column Chunks&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Filter Pushdown&lt;/strong&gt;: Similarly, given a query with a filter predicate such as
   &lt;code&gt;WHERE C &amp;gt; 25&lt;/code&gt;, query engines can use statistics such as (but not limited to)
   the min/max values stored in the metadata to skip reading and decoding pages that
   cannot possibly match the predicate.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The high level mechanics of Parquet predicate pushdown is shown below:&lt;/p&gt;
&lt;div class="text-center"&gt;
&lt;img alt="Parquet Filter Pushdown: use filter predicate to skip pages." class="img-responsive" src="/blog/images/external-parquet-indexes/parquet-filter-pushdown.png" width="80%"/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: Filter Pushdown in Parquet: query engines use the predicate,
&lt;code&gt;C &amp;gt; 25&lt;/code&gt;, from the query along with statistics from the metadata, to identify
pages that may match the predicate which are read for further processing. 
Please refer to the &lt;a href="https://datafusion.apache.org/blog/2025/03/21/parquet-pushdown"&gt;Efficient Filter Pushdown&lt;/a&gt; blog for more details.
&lt;strong&gt;NOTE the exact same pattern can be applied using information from external
indexes, as described in the next sections.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;Pruning Files with External Indexes&lt;/h2&gt;
&lt;p&gt;The first step in hierarchical pruning is quickly ruling out files that cannot
match the query. For example, if a system expects to see queries that
apply to a time range, it might create an external index to store the minimum
and maximum &lt;code&gt;time&lt;/code&gt; values for each file. Then, during query processing, the
system can quickly rule out files that cannot possibly contain relevant data.&lt;/p&gt;
&lt;p&gt;For example, if the user issues a query that only matches the last 7 days of
data:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;WHERE time &amp;gt; now() - interval '7 days'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;then the index can quickly rule out files that only have data older than the
most recent 7 days.&lt;/p&gt;
&lt;div class="text-center"&gt;
&lt;img alt="Data Skipping: Pruning Files." class="img-responsive" src="/blog/images/external-parquet-indexes/prune-files.png" width="80%"/&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: Step 1: File Pruning. Given a query predicate, systems use external
indexes to quickly rule out files that cannot match the query. In this case, by
consulting the index all but two files can be ruled out.&lt;/p&gt;
&lt;p&gt;External indexes offer much faster lookups and lower I/O overhead than Parquet's
built-in file-level indexes by skipping further processing for many data files.
Without an external index, systems typically fall back to reading each file's
footer to find files needed for further processing. Skipping per-file processing
is especially important when reading from remote object stores such as &lt;a href="https://aws.amazon.com/s3/"&gt;S3&lt;/a&gt;,
&lt;a href="https://cloud.google.com/storage"&gt;GCS&lt;/a&gt; or &lt;a href="https://azure.microsoft.com/en-us/services/storage/blobs/"&gt;Azure Blob Store&lt;/a&gt;, where each request adds &lt;a href="https://www.vldb.org/pvldb/vol16/p2769-durner.pdf"&gt;tens to hundreds of
milliseconds of latency&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are many different systems that use external indexes to find files such as 
&lt;a href="https://cwiki.apache.org/confluence/display/Hive/Design#Design-Metastore"&gt;Hive Metadata Store&lt;/a&gt;,
&lt;a href="https://iceberg.apache.org/"&gt;Iceberg&lt;/a&gt;, 
&lt;a href="https://delta.io/"&gt;Delta Lake&lt;/a&gt;,
&lt;a href="https://duckdb.org/2025/05/27/ducklake.html"&gt;DuckLake&lt;/a&gt;,
and &lt;a href="https://sparkbyexamples.com/apache-hive/hive-partitions-explained-with-examples/"&gt;Hive Style Partitioning&lt;/a&gt;&lt;sup&gt;&lt;a href="#footnote4"&gt;4&lt;/a&gt;&lt;/sup&gt;.
Of course, each of these systems works well for their intended use cases, but
if none meets your needs, or you want to experiment with
different strategies, you can easily build your own external index using
DataFusion.&lt;/p&gt;
&lt;h3&gt;Pruning Files with External Indexes Using DataFusion&lt;/h3&gt;
&lt;p&gt;To implement file pruning in DataFusion, you implement a custom &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/trait.TableProvider.html"&gt;TableProvider&lt;/a&gt;
with the &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/trait.TableProvider.html#method.supports_filters_pushdown"&gt;supports_filter_pushdown&lt;/a&gt; and &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/trait.TableProvider.html#tymethod.scan"&gt;scan&lt;/a&gt; methods. The
&lt;code&gt;supports_filter_pushdown&lt;/code&gt; method tells DataFusion which predicates can be used
and the &lt;code&gt;scan&lt;/code&gt; method uses those predicates with the
external index to find the files that may contain data that matches the query.&lt;/p&gt;
&lt;p&gt;The DataFusion repository contains a fully working and well-commented example,
&lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs"&gt;parquet_index.rs&lt;/a&gt;, of this technique that you can use as a starting point. 
The example creates a simple index that stores the min/max values for a column
called &lt;code&gt;value&lt;/code&gt; along with the file name. Then it runs the following query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT file_name, value FROM index_table WHERE value = 150
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The custom &lt;code&gt;IndexTableProvider&lt;/code&gt;'s &lt;code&gt;scan&lt;/code&gt; method uses the index to find files
that may contain data matching the predicate as shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;impl TableProvider for IndexTableProvider {
    async fn scan(
        &amp;amp;self,
        state: &amp;amp;dyn Session,
        projection: Option&amp;lt;&amp;amp;Vec&amp;lt;usize&amp;gt;&amp;gt;,
        filters: &amp;amp;[Expr],
        limit: Option&amp;lt;usize&amp;gt;,
    ) -&amp;gt; Result&amp;lt;Arc&amp;lt;dyn ExecutionPlan&amp;gt;&amp;gt; {
        let df_schema = DFSchema::try_from(self.schema())?;
        // Combine all the filters into a single ANDed predicate
        let predicate = conjunction(filters.to_vec());

        // Use the index to find the files that might have data that matches the
        // predicate. Any file that can not have data that matches the predicate
        // will not be returned.
        let files = self.index.get_files(predicate.clone())?;

        let object_store_url = ObjectStoreUrl::parse("file://")?;
        let source = Arc::new(ParquetSource::default().with_predicate(predicate));
        let mut file_scan_config_builder =
            FileScanConfigBuilder::new(object_store_url, self.schema(), source)
                .with_projection(projection.cloned())
                .with_limit(limit);

        // Add the files to the scan config
        for file in files {
            file_scan_config_builder = file_scan_config_builder.with_file(
                PartitionedFile::new(file.path(), file_size.size()),
            );
        }
        Ok(DataSourceExec::from_data_source(
            file_scan_config_builder.build(),
        ))
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;DataFusion handles the details of pushing down the filters to the
&lt;code&gt;TableProvider&lt;/code&gt; and the mechanics of reading the Parquet files, so you can focus
on the system specific details such as building, storing, and applying the index.
While this example uses a standard min/max index, you can implement any indexing
strategy you need, such as bloom filters, a full text index, or a more complex
multidimensional index.&lt;/p&gt;
&lt;p&gt;DataFusion also includes several libraries to help with common filtering and
pruning tasks, such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A full and well documented expression representation (&lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/enum.Expr.html"&gt;Expr&lt;/a&gt;) and &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/enum.Expr.html#visiting-and-rewriting-exprs"&gt;APIs for
  building, visiting, and rewriting&lt;/a&gt; query predicates.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Range Based Pruning (&lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/pruning/struct.PruningPredicate.html"&gt;PruningPredicate&lt;/a&gt;) for cases where your index stores
  min/max values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Expression simplification (&lt;a href="https://docs.rs/datafusion/latest/datafusion/optimizer/simplify_expressions/struct.ExprSimplifier.html#method.simplify"&gt;ExprSimplifier&lt;/a&gt;) for simplifying predicates before
  applying them to the index.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Range analysis for predicates (&lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_expr/intervals/cp_solver/index.html"&gt;cp_solver&lt;/a&gt;) for interval-based range analysis
  (e.g. &lt;code&gt;col &amp;gt; 5 AND col &amp;lt; 10&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pruning Parts of Parquet Files with External Indexes&lt;/h2&gt;
&lt;p&gt;Once the set of files to be scanned has been determined, the next step in the
hierarchical pruning process is to further narrow down the data within each file.
Similarly to the previous step, almost all advanced query processing systems use additional
metadata to prune unnecessary parts of the file, such as &lt;a href="https://clickhouse.com/docs/optimize/skipping-indexes"&gt;Data Skipping Indexes
in ClickHouse&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;For Parquet-based systems, the most common strategy is using the built-in metadata such
as &lt;a href="https://github.com/apache/parquet-format/blob/1dbc814b97c9307687a2e4bee55545ab6a2ef106/src/main/thrift/parquet.thrift#L267"&gt;min/max statistics&lt;/a&gt; and &lt;a href="https://parquet.apache.org/docs/file-format/bloomfilter/"&gt;Bloom Filters&lt;/a&gt;. However, it is also possible to use external
indexes for filtering &lt;em&gt;WITHIN&lt;/em&gt; Parquet files as shown below. &lt;/p&gt;
&lt;p&gt;&lt;img alt="Data Skipping: Pruning Row Groups and DataPages" class="img-responsive" src="/blog/images/external-parquet-indexes/prune-row-groups.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 7&lt;/strong&gt;: Step 2: Pruning Parquet Row Groups and Data Pages. Given a query predicate,
systems can use external indexes / metadata stores as well as Parquet's built-in
structures to quickly rule out Row Groups and Data Pages that cannot match the query.
In this case, the index has ruled out all but three data pages which must then be fetched
for more processing.&lt;/p&gt;
&lt;h2&gt;Pruning Parts of Parquet Files with External Indexes using DataFusion&lt;/h2&gt;
&lt;p&gt;To implement pruning within Parquet files, you use the same [&lt;code&gt;TableProvider&lt;/code&gt;] APIs
as for pruning files. For each file your provider wants to scan, you provide 
an additional &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/physical_plan/parquet/struct.ParquetAccessPlan.html"&gt;ParquetAccessPlan&lt;/a&gt; that tells DataFusion what parts of the file to read. This plan is
then &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/physical_plan/parquet/source/struct.ParquetSource.html#implementing-external-indexes"&gt;further refined by the DataFusion Parquet reader&lt;/a&gt; using the built-in
Parquet metadata to potentially prune additional row groups and data pages
during query execution. You can find a full working example in
the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs"&gt;advanced_parquet_index.rs&lt;/a&gt; example of the DataFusion repository.&lt;/p&gt;
&lt;p&gt;Here is how you build a &lt;code&gt;ParquetAccessPlan&lt;/code&gt; to scan only specific row groups
and rows within those row groups. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Default to scan all (4) row groups
let mut access_plan = ParquetAccessPlan::new_all(4);
access_plan.skip(0); // skip row group 0
// Specify scanning rows 100-200 and 350-400
// in row group 1 that has 1000 rows
let row_selection = RowSelection::from(vec![
   RowSelector::skip(100),
   RowSelector::select(100),
   RowSelector::skip(150),
   RowSelector::select(50),
   RowSelector::skip(600),  // skip last 600 rows
]);
access_plan.scan_selection(1, row_selection);
access_plan.skip(2); // skip row group 2
// all of row group 3 is scanned by default
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The rows that are selected by the resulting plan look like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;                   &amp;boxv;
&amp;boxv;                   &amp;boxv;  SKIP
&amp;boxv;                   &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
     Row Group 0

&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv; &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv;  SCAN ONLY ROWS
&amp;boxv; &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv;  100-200
&amp;boxv; &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv;  350-400
&amp;boxv; &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
     Row Group 1

&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;                   &amp;boxv;
&amp;boxv;                   &amp;boxv;  SKIP
&amp;boxv;                   &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
     Row Group 2

&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;                   &amp;boxv;
&amp;boxv;                   &amp;boxv;  SCAN ALL ROWS
&amp;boxv;                   &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
     Row Group 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the &lt;code&gt;scan&lt;/code&gt; method, you return an &lt;code&gt;ExecutionPlan&lt;/code&gt; that includes the
&lt;code&gt;ParquetAccessPlan&lt;/code&gt; for each file as shown below (again, slightly simplified for
clarity):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;impl TableProvider for IndexTableProvider {
    async fn scan(
        &amp;amp;self,
        state: &amp;amp;dyn Session,
        projection: Option&amp;lt;&amp;amp;Vec&amp;lt;usize&amp;gt;&amp;gt;,
        filters: &amp;amp;[Expr],
        limit: Option&amp;lt;usize&amp;gt;,
    ) -&amp;gt; Result&amp;lt;Arc&amp;lt;dyn ExecutionPlan&amp;gt;&amp;gt; {
        let indexed_file = &amp;amp;self.indexed_file;
        let predicate = self.filters_to_predicate(state, filters)?;

        // Use the external index to create a starting ParquetAccessPlan
        // that determines which row groups to scan based on the predicate
        let access_plan = self.create_plan(&amp;amp;predicate)?;

        let partitioned_file = indexed_file
            .partitioned_file()
            // provide the access plan to the DataSourceExec by
            // storing it as  "extensions" on PartitionedFile
            .with_extensions(Arc::new(access_plan) as _);

        let file_source = Arc::new(
            ParquetSource::default()
                // provide the predicate to the standard DataFusion source as well so
                // DataFusion's Parquet reader will apply row group pruning based on
                // the built-in Parquet metadata (min/max, bloom filters, etc) as well
                .with_predicate(predicate)
        );
        let file_scan_config =
            FileScanConfigBuilder::new(object_store_url, schema, file_source)
                .with_limit(limit)
                .with_projection(projection.cloned())
                .with_file(partitioned_file)
                .build();

        // Finally, put it all together into a DataSourceExec
        Ok(DataSourceExec::from_data_source(file_scan_config))
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Caching Parquet Metadata&lt;/h2&gt;
&lt;p&gt;It is often said that Parquet is unsuitable for low latency query systems
because the footer must be read and parsed for each query. This is simply not
true, and &lt;strong&gt;many systems use Parquet for low latency analytics and cache the parsed
metadata in memory to avoid re-reading and re-parsing the footer for each query&lt;/strong&gt;.&lt;/p&gt;
&lt;h3&gt;Caching Parquet Metadata using DataFusion&lt;/h3&gt;
&lt;p&gt;Reusing cached Parquet Metadata is also shown in the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs"&gt;advanced_parquet_index.rs&lt;/a&gt;
example. The example reads and caches the metadata for each file when the index
is first built and then uses the cached metadata when reading the files during
query execution.&lt;/p&gt;
&lt;p&gt;(Note that thanks to &lt;a href="https://nuno-faria.github.io/"&gt;Nuno Faria&lt;/a&gt;, &lt;a href="https://github.com/jonathanc-n"&gt;Jonathan Chen&lt;/a&gt;, and &lt;a href="https://github.com/shehabgamin"&gt;Shehab Amin&lt;/a&gt; the built
in &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/listing/struct.ListingTable.html"&gt;ListingTable&lt;/a&gt; &lt;code&gt;TableProvider&lt;/code&gt; included with DataFusion will cache Parquet
metadata in the next release of DataFusion (50.0.0). See the &lt;a href="https://github.com/apache/datafusion/issues/17000"&gt;mini epic&lt;/a&gt; for
details).&lt;/p&gt;
&lt;p&gt;To avoid reparsing the metadata, first implement a custom
&lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/physical_plan/trait.ParquetFileReaderFactory.html"&gt;ParquetFileReaderFactory&lt;/a&gt; as shown below, again slightly simplified for
clarity:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;impl ParquetFileReaderFactory for CachedParquetFileReaderFactory {
    fn create_reader(
        &amp;amp;self,
        _partition_index: usize,
        file_meta: FileMeta,
        metadata_size_hint: Option&amp;lt;usize&amp;gt;,
        _metrics: &amp;amp;ExecutionPlanMetricsSet,
    ) -&amp;gt; Result&amp;lt;Box&amp;lt;dyn AsyncFileReader + Send&amp;gt;&amp;gt; {
        let filename = file_meta.location();

        // Pass along the information to access the underlying storage
        // (e.g. S3, GCS, local filesystem, etc)
        let object_store = Arc::clone(&amp;amp;self.object_store);
        let mut inner =
            ParquetObjectReader::new(object_store, file_meta.object_meta.location)
                .with_file_size(file_meta.object_meta.size);

        // retrieve the pre-parsed metadata from the cache
        // (which was built when the index was built and is kept in memory)
        let metadata = self
            .metadata
            .get(&amp;amp;filename)
            .expect("metadata for file not found: {filename}");

        // Return a ParquetReader that uses the cached metadata
        Ok(Box::new(ParquetReaderWithCache {
            filename,
            metadata: Arc::clone(metadata),
            inner,
        }))
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, in your &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/trait.TableProvider.html"&gt;TableProvider&lt;/a&gt; use the factory to avoid re-reading the metadata
for each file:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;impl TableProvider for IndexTableProvider {
    async fn scan(
        &amp;amp;self,
        state: &amp;amp;dyn Session,
        projection: Option&amp;lt;&amp;amp;Vec&amp;lt;usize&amp;gt;&amp;gt;,
        filters: &amp;amp;[Expr],
        limit: Option&amp;lt;usize&amp;gt;,
    ) -&amp;gt; Result&amp;lt;Arc&amp;lt;dyn ExecutionPlan&amp;gt;&amp;gt; {
        // Configure a factory interface to avoid re-reading the metadata for each file
        let reader_factory =
            CachedParquetFileReaderFactory::new(Arc::clone(&amp;amp;self.object_store))
                .with_file(indexed_file);

        // build the partitioned file (see example above for details)
        let partitioned_file = ...; 

        // Create the ParquetSource with the predicate and the factory
        let file_source = Arc::new(
            ParquetSource::default()
                // provide the factory to create Parquet reader without re-reading metadata
                .with_parquet_file_reader_factory(Arc::new(reader_factory)),
        );

        // Pass along the information needed to read the files
        let file_scan_config =
            FileScanConfigBuilder::new(object_store_url, schema, file_source)
                .with_limit(limit)
                .with_projection(projection.cloned())
                .with_file(partitioned_file)
                .build();

        // Finally, put it all together into a DataSourceExec
        Ok(DataSourceExec::from_data_source(file_scan_config))
    }
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Parquet has the right structure for high performance analytics via hierarchical
pruning, and it is straightforward to build external indexes to speed up queries
using DataFusion without changing the file format. If you need to build a custom
data platform, it has never been easier to build it with Parquet and DataFusion.&lt;/p&gt;
&lt;p&gt;I am a firm believer that data systems of the future will be built on a
foundation of modular, high quality, open source components such as Parquet,
Arrow, and DataFusion. We should focus our efforts as a community on
improving these components rather than building new file formats that are
optimized for narrow use cases.&lt;/p&gt;
&lt;p&gt;Come Join Us! 🎣 &lt;/p&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;
&lt;img alt="https://datafusion.apache.org/" class="img-responsive" src="/blog/images/logo_original4x.png" width="20%"/&gt;
&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;About the Author&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/andrewalamb/"&gt;Andrew Lamb&lt;/a&gt; is a Staff Engineer at
&lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt;, and a member of the &lt;a href="https://datafusion.apache.org/"&gt;Apache
DataFusion&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; PMCs. He has been working on
Databases and related systems more than 20 years.&lt;/p&gt;
&lt;h2&gt;About DataFusion&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; is an extensible query engine toolkit, written
in Rust, that uses &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion and
similar technology are part of the next generation &amp;ldquo;Deconstructed Database&amp;rdquo;
architectures, where new systems are built on a foundation of fast, modular
components, rather than as a single tightly integrated system.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;DataFusion community&lt;/a&gt; is always looking for new contributors to help
improve the project. If you are interested in learning more about how query
execution works, help document or improve the DataFusion codebase, or just try
it out, we would love for you to join us.&lt;/p&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;Thank you to &lt;a href="https://github.com/zhuqi-lucas"&gt;Qi Zhu&lt;/a&gt;, &lt;a href="https://github.com/adamreeve"&gt;Adam Reeve&lt;/a&gt;, &lt;a href="https://github.com/JigaoLuo"&gt;Jigao Luo&lt;/a&gt;, &lt;a href="https://github.com/comphead"&gt;Oleks V&lt;/a&gt;, &lt;a href="https://github.com/shehabgamin"&gt;Shehab Amin&lt;/a&gt;, &lt;a href="https://nuno-faria.github.io/"&gt;Nuno Faria&lt;/a&gt;
and &lt;a href="https://github.com/Omega359"&gt;Bruce Ritchie&lt;/a&gt; for their insightful feedback on this blog post.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;&lt;a id="footnote1"&gt;&lt;/a&gt;&lt;code&gt;1&lt;/code&gt;: This trend is described in more detail in the &lt;a href="https://www.influxdata.com/blog/flight-datafusion-arrow-parquet-fdap-architecture-influxdb/"&gt;FDAP Stack&lt;/a&gt; blog&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote2"&gt;&lt;/a&gt;&lt;code&gt;2&lt;/code&gt;: This layout is referred to as &lt;a href="https://www.vldb.org/conf/2001/P169.pdf"&gt;PAX in the
database literature&lt;/a&gt; after the first research paper to describe the technique.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote3"&gt;&lt;/a&gt;&lt;code&gt;3&lt;/code&gt;: Benchmaxxing (verb): to add specific optimizations that only
impact benchmark results and are not widely applicable to real world use cases.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote4"&gt;&lt;/a&gt;&lt;code&gt;4&lt;/code&gt;: Hive Style Partitioning is a simple and widely used form of indexing based on directory paths, where the directory structure is used to
store information about the data in the files. For example, a directory structure like &lt;code&gt;year=2025/month=08/day=15/&lt;/code&gt; can be used to store data for a specific day
and the system can quickly rule out directories that do not match the query predicate.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote5"&gt;&lt;/a&gt;&lt;code&gt;5&lt;/code&gt;: I am also convinced that we can speed up the process of parsing Parquet footer
with additional engineering effort (see &lt;a href="https://xiangpeng.systems/"&gt;Xiangpeng Hao&lt;/a&gt;'s &lt;a href="https://www.influxdata.com/blog/how-good-parquet-wide-tables/"&gt;previous blog on the
topic&lt;/a&gt;). &lt;a href="https://github.com/etseidl"&gt;Ed Seidl&lt;/a&gt; is beginning this effort. See the &lt;a href="https://github.com/apache/arrow-rs/issues/5854"&gt;ticket&lt;/a&gt; for details.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote6"&gt;&lt;/a&gt;&lt;code&gt;6&lt;/code&gt;: ClickBench includes a wide variety of query patterns
such as point lookups, filters of different selectivity, and aggregations.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote7"&gt;&lt;/a&gt;&lt;code&gt;7&lt;/code&gt;: For example, &lt;a href="https://github.com/zhuqi-lucas"&gt;Qi Zhu&lt;/a&gt; was able to speed up reads by over 2x 
simply by rewriting the Parquet files with Offset Indexes and no compression (see &lt;a href="https://github.com/apache/datafusion/issues/16149#issuecomment-2918761743"&gt;issue #16149 comment&lt;/a&gt; for details).
There is likely significant additional performance available by using Bloom Filters and resorting the data
to be clustered in a more optimal way for the queries.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion 49.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/07/28/datafusion-49.0.0" rel="alternate"></link><published>2025-07-28T00:00:00+00:00</published><updated>2025-07-28T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-07-28:/blog/2025/07/28/datafusion-49.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/16347 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are proud to announce the release of &lt;a href="https://crates.io/crates/datafusion/49.0.0"&gt;DataFusion 49.0.0&lt;/a&gt;. This blog post highlights some of
the major improvements since the release of &lt;a href="https://datafusion.apache.org/blog/2025/07/18/datafusion-48.0.0/"&gt;DataFusion 48.0.0&lt;/a&gt;. The complete list of changes is available in the &lt;a href="https://github.com/apache/datafusion/blob/branch-49/dev/changelog/49.0.0.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Performance Improvements 🚀&lt;/h2&gt;
&lt;p&gt;DataFusion continues to focus on enhancing performance, as …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/16347 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are proud to announce the release of &lt;a href="https://crates.io/crates/datafusion/49.0.0"&gt;DataFusion 49.0.0&lt;/a&gt;. This blog post highlights some of
the major improvements since the release of &lt;a href="https://datafusion.apache.org/blog/2025/07/18/datafusion-48.0.0/"&gt;DataFusion 48.0.0&lt;/a&gt;. The complete list of changes is available in the &lt;a href="https://github.com/apache/datafusion/blob/branch-49/dev/changelog/49.0.0.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Performance Improvements 🚀&lt;/h2&gt;
&lt;p&gt;DataFusion continues to focus on enhancing performance, as shown in the ClickBench and other results. &lt;/p&gt;
&lt;p&gt;&lt;img alt="ClickBench performance results over time for DataFusion" class="img-responsive" src="/blog/images/datafusion-49.0.0/performance_over_time_clickbench.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: ClickBench performance improvements over time
Average and median normalized query execution times for ClickBench queries for each git revision. 
Query times are normalized using the ClickBench definition. Data and definitions on the 
&lt;a href="https://alamb.github.io/datafusion-benchmarking/"&gt;DataFusion Benchmarking Page&lt;/a&gt;. &lt;/p&gt;
&lt;!--
NOTE: Andrew is working on gathering these numbers

&lt;img
src="/blog/images/datafusion-49.0.0/performance_over_time_planning.png"
width="80%"
class="img-responsive"
alt="Planning benchmark performance results over time for DataFusion"
/&gt;

**Figure 2**: Planning benchmark performance improved XXX between DataFusion 48.0.1 and DataFusion 49.0.0. Chart source: TODO
--&gt;
&lt;p&gt;Here are some noteworthy optimizations added since DataFusion 48:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Equivalence system upgrade:&lt;/strong&gt; The lower levels of the equivalence system, which is used to implement the
  optimizations described in &lt;a href="https://datafusion.apache.org/blog/2025/03/11/ordering-analysis"&gt;Using Ordering for Better Plans&lt;/a&gt;, were rewritten, leading to
  much faster planning times, especially for queries with a &lt;a href="https://github.com/apache/datafusion/pull/16217#pullrequestreview-2891941229"&gt;large number of columns&lt;/a&gt;. This change also prepares
  the way for more sophisticated sort-based optimizations in the future. (PR &lt;a href="https://github.com/apache/datafusion/pull/16217"&gt;#16217&lt;/a&gt; by &lt;a href="https://github.com/ozankabak"&gt;ozankabak&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Filters and TopK pushdown&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;DataFusion now supports dynamic filters, which are improved during query execution,
and physical filter pushdown. Together, these features improve the performance of
queries that use &lt;code&gt;LIMIT&lt;/code&gt; and &lt;code&gt;ORDER BY&lt;/code&gt; clauses, such as the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT *
FROM data
ORDER BY timestamp DESC
LIMIT 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;While the query above is simple, without dynamic filtering or knowing that the data
is already sorted by &lt;code&gt;timestamp&lt;/code&gt;, a query engine must decode &lt;em&gt;all&lt;/em&gt; of the data to
find the top 10 values. With the dynamic filters system, DataFusion applies an
increasingly selective filter during query execution. It checks the &lt;strong&gt;current&lt;/strong&gt;
top 10 values of the &lt;code&gt;timestamp&lt;/code&gt; column &lt;strong&gt;before&lt;/strong&gt; opening files or reading
Parquet Row Groups and Data Pages, which can skip older data very quickly.&lt;/p&gt;
&lt;p&gt;Dynamic predicates are a common feature of advanced engines such as &lt;a href="https://docs.starburst.io/latest/admin/dynamic-filtering.html"&gt;Dynamic
Filters in Starburst&lt;/a&gt; and &lt;a href="https://www.snowflake.com/en/engineering-blog/optimizing-top-k-aggregation-snowflake/"&gt;Top-K Aggregation Optimization at Snowflake&lt;/a&gt;. The
technique drastically improves query performance (we've seen over a 1.5x
improvement for some TPC-H-style queries), especially in combination with late
materialization and columnar file formats such as Parquet. We &lt;a href="https://github.com/apache/datafusion/issues/15513"&gt;plan to write a
blog post&lt;/a&gt; explaining the details of this optimization in the future, and we expect to
use the same mechanism to implement additional optimizations such as &lt;a href="https://github.com/apache/datafusion/issues/7955"&gt;Sideways
Information Passing for joins&lt;/a&gt; (Issue
&lt;a href="https://github.com/apache/datafusion/issues/15037"&gt;#15037&lt;/a&gt; PR
&lt;a href="https://github.com/apache/datafusion/pull/15770"&gt;#15770&lt;/a&gt; by
&lt;a href="https://github.com/adriangb"&gt;adriangb&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;Community Growth  📈&lt;/h2&gt;
&lt;p&gt;The last few months, between &lt;code&gt;46.0.0&lt;/code&gt; and &lt;code&gt;49.0.0&lt;/code&gt;, have seen our community grow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;New PMC members and committers: &lt;a href="https://github.com/berkaysynnada"&gt;berkay&lt;/a&gt;, &lt;a href="https://github.com/xudong963"&gt;xudong963&lt;/a&gt; and &lt;a href="https://github.com/timsaucer"&gt;timsaucer&lt;/a&gt; joined the PMC.
   &lt;a href="https://github.com/blaginin"&gt;blaginin&lt;/a&gt;, &lt;a href="https://github.com/milenkovicm"&gt;milenkovicm&lt;/a&gt;, &lt;a href="https://github.com/adriangb"&gt;adriangb&lt;/a&gt; and &lt;a href="https://github.com/kosiew"&gt;kosiew&lt;/a&gt; joined as committers. See the &lt;a href="https://lists.apache.org/list.html?dev@datafusion.apache.org"&gt;mailing list&lt;/a&gt; for more details.&lt;/li&gt;
&lt;li&gt;In the &lt;a href="https://github.com/apache/arrow-datafusion"&gt;core DataFusion repo&lt;/a&gt; alone, we reviewed and accepted over 850 PRs from 172 different
   committers, created over 669 issues, and closed 379 of them 🚀. All changes are listed in the detailed
   &lt;a href="https://github.com/apache/datafusion/tree/main/dev/changelog"&gt;changelogs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;DataFusion published a number of blog posts, including &lt;a href="https://datafusion.apache.org/blog/2025/04/19/user-defined-window-functions"&gt;User defined Window Functions&lt;/a&gt;, &lt;a href="https://datafusion.apache.org/blog/2025/06/15/optimizing-sql-dataframes-part-one"&gt;Optimizing SQL (and DataFrames)
   in DataFusion part 1&lt;/a&gt;, &lt;a href="https://datafusion.apache.org/blog/2025/06/15/optimizing-sql-dataframes-part-two"&gt;part 2&lt;/a&gt;, &lt;a href="https://datafusion.apache.org/blog/2025/06/30/cancellation"&gt;Using Rust async for Query Execution and Cancelling Long-Running Queries&lt;/a&gt;, and
   &lt;a href="https://datafusion.apache.org/blog/2025/07/14/user-defined-parquet-indexes/"&gt;Embedding User-Defined Indexes in Apache Parquet Files&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
# Unique committers
$ git shortlog -sn 46.0.0..49.0.0-rc1  .| wc -l
     172
# commits
$ git log --pretty=oneline 46.0.0..49.0.0-rc1 . | wc -l
     884


https://crates.io/crates/datafusion/49.0.0
DataFusion 49 released July 25, 2025

https://crates.io/crates/datafusion/46.0.0
DataFusion 46 released March 7, 2025

Issues created in this time: 290 open, 379 closed = 669 total
https://github.com/apache/datafusion/issues?q=is%3Aissue+created%3A2025-03-07..2025-07-25

Issues closed: 508
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+closed%3A2025-03-07..2025-07-25

PRs merged in this time 874
https://github.com/apache/arrow-datafusion/pulls?q=is%3Apr+merged%3A2025-03-07..2025-07-25

--&gt;
&lt;h2&gt;New Features ✨&lt;/h2&gt;
&lt;h3&gt;Async User-Defined Functions&lt;/h3&gt;
&lt;p&gt;It is now possible to write &lt;code&gt;async&lt;/code&gt; User-Defined Functions
(UDFs) in DataFusion that perform asynchronous
operations, such as network requests or database queries, without blocking the
execution of the query. This enables new use cases, such as
integrating with large language models (LLMs) or other external services, and we can't
wait to see what the community builds with it.&lt;/p&gt;
&lt;p&gt;See the &lt;a href="https://datafusion.apache.org/library-user-guide/functions/adding-udfs.html"&gt;documentation&lt;/a&gt; for more details and the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/async_udf.rs"&gt;async UDF example&lt;/a&gt; for
working code. &lt;/p&gt;
&lt;p&gt;You could, for example, implement a function &lt;code&gt;ask_llm&lt;/code&gt; that asks a large language model
(LLM) service a question based on the content of two columns.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT * 
FROM animal a
WHERE ask_llm(a.name, 'Is this animal furry?')")
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The implementation of an async UDF is almost identical to a normal
UDF, except that it must implement the &lt;code&gt;AsyncScalarUDFImpl&lt;/code&gt; trait in addition to &lt;code&gt;ScalarUDFImpl&lt;/code&gt; and
provide an &lt;code&gt;async&lt;/code&gt; implementation via &lt;code&gt;invoke_async_with_args&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[derive(Debug)]
struct AskLLM {
    signature: Signature,
}

#[async_trait]
impl AsyncScalarUDFImpl for AskLLM {
    /// The `invoke_async_with_args` method is similar to `invoke_with_args`,
    /// but it returns a `Future` that resolves to the result.
    ///
    /// Since this signature is `async`, it can do any `async` operations, such
    /// as network requests.
    async fn invoke_async_with_args(
        &amp;amp;self,
        args: ScalarFunctionArgs,
        options: &amp;amp;ConfigOptions,
    ) -&amp;gt; Result&amp;lt;ArrayRef&amp;gt; {
        // Converts the arguments to arrays for simplicity.
        let args = ColumnarValue::values_to_arrays(&amp;amp;args.args)?;
        let [column_of_interest, question] = take_function_args(self.name(), args)?;
        let client = Client::new();

        // Make a network request to a hypothetical LLM service
        let res = client
            .post(URI)
            .headers(get_llm_headers(options))
            .json(&amp;amp;req)
            .send()
            .await?
            .json::&amp;lt;LLMResponse&amp;gt;()
            .await?;

        let results = extract_results_from_llm_response(&amp;amp;res);

        Ok(Arc::new(results))
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Issue &lt;a href="https://github.com/apache/datafusion/issues/6518"&gt;#6518&lt;/a&gt;,
&lt;a href="https://github.com/apache/datafusion/pull/14837"&gt;PR #14837&lt;/a&gt; from
&lt;a href="https://github.com/goldmedal"&gt;goldmedal&lt;/a&gt; 🏆)&lt;/p&gt;
&lt;h3&gt;Better Cancellation for Certain Long-Running Queries&lt;/h3&gt;
&lt;p&gt;In rare cases, it was previously not possible to cancel long-running queries,
leading to unresponsiveness. Other projects would likely have fixed this issue
by treating the symptom, but &lt;a href="https://github.com/pepijnve"&gt;pepijnve&lt;/a&gt; and the DataFusion community worked together to
treat the root cause. The general solution required a deep understanding of the
DataFusion execution engine, Rust &lt;code&gt;Streams&lt;/code&gt;, and the tokio cooperative
scheduling model. The &lt;a href="https://github.com/apache/datafusion/pull/16398"&gt;resulting PR&lt;/a&gt; is a model of careful
community engineering and a great example of using Rust's &lt;code&gt;async&lt;/code&gt; ecosystem
to implement complex functionality. It even resulted in a &lt;a href="https://github.com/tokio-rs/tokio/pull/7405"&gt;contribution upstream to tokio&lt;/a&gt;
(since accepted). See the &lt;a href="https://datafusion.apache.org/blog/2025/06/30/cancellation"&gt;blog post&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h3&gt;Metadata for User Defined Types such as &lt;code&gt;Variant&lt;/code&gt; and &lt;code&gt;Geometry&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;User-defined types have been &lt;a href="https://github.com/apache/datafusion/issues/12644"&gt;a long-requested feature&lt;/a&gt;, and this release provides
the low-level APIs to support them efficiently.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Metadata handling in PRs &lt;a href="https://github.com/apache/datafusion/pull/15646"&gt;#15646&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/pull/16170"&gt;#16170&lt;/a&gt; from &lt;a href="https://github.com/timsaucer"&gt;timsaucer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pushdown of filters and expressions (see "Dynamic Filters and TopK pushdown" section above)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We still have some work to do to fully support user-defined types, specifically
in documentation and testing, and we would
love your help in this area. If you are interested in contributing,
please see &lt;a href="https://github.com/apache/datafusion/issues/12644"&gt;issue #12644&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Parquet Modular Encryption&lt;/h3&gt;
&lt;p&gt;DataFusion now supports reading and writing encrypted &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; files with &lt;a href="https://parquet.apache.org/docs/file-format/data-pages/encryption/"&gt;modular
encryption&lt;/a&gt;. This allows users to encrypt specific columns in a Parquet file
using different keys, while still being able to read data without needing to
decrypt the entire file.&lt;/p&gt;
&lt;p&gt;Here is an example of how to configure DataFusion to read an encrypted Parquet
table with two columns, &lt;code&gt;double_field&lt;/code&gt; and &lt;code&gt;float_field&lt;/code&gt;, using modular
encryption:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;CREATE EXTERNAL TABLE encrypted_parquet_table
(
double_field double,
float_field float
)
STORED AS PARQUET LOCATION 'pq/' OPTIONS (
    -- encryption
    'format.crypto.file_encryption.encrypt_footer' 'true',
    'format.crypto.file_encryption.footer_key_as_hex' '30313233343536373839303132333435',  -- b"0123456789012345"
    'format.crypto.file_encryption.column_key_as_hex::double_field' '31323334353637383930313233343530', -- b"1234567890123450"
    'format.crypto.file_encryption.column_key_as_hex::float_field' '31323334353637383930313233343531', -- b"1234567890123451"
    -- decryption
    'format.crypto.file_decryption.footer_key_as_hex' '30313233343536373839303132333435', -- b"0123456789012345"
    'format.crypto.file_decryption.column_key_as_hex::double_field' '31323334353637383930313233343530', -- b"1234567890123450"
    'format.crypto.file_decryption.column_key_as_hex::float_field' '31323334353637383930313233343531', -- b"1234567890123451"
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(&lt;a href="https://github.com/apache/datafusion/issues/15216"&gt;Issue #15216&lt;/a&gt;,
&lt;a href="https://github.com/apache/datafusion/pull/16351"&gt;PR #16351&lt;/a&gt;
from &lt;a href="https://github.com/corwinjoy"&gt;corwinjoy&lt;/a&gt; and &lt;a href="https://github.com/adamreeve"&gt;adamreeve&lt;/a&gt;)&lt;/p&gt;
&lt;h3&gt;Support for &lt;code&gt;WITHIN GROUP&lt;/code&gt; for Ordered-Set Aggregate Functions&lt;/h3&gt;
&lt;p&gt;DataFusion now supports the &lt;code&gt;WITHIN GROUP&lt;/code&gt; clause for &lt;a href="https://www.postgresql.org/docs/9.4/functions-aggregate.html#FUNCTIONS-ORDEREDSET-TABLE"&gt;ordered-set aggregate
functions&lt;/a&gt; such as &lt;code&gt;approx_percentile_cont&lt;/code&gt;, &lt;code&gt;percentile_cont&lt;/code&gt;, and
&lt;code&gt;percentile_disc&lt;/code&gt;, which allows users to specify the precise order.&lt;/p&gt;
&lt;p&gt;For example, the following query computes the 50th percentile for the &lt;code&gt;temperature&lt;/code&gt; column
in the &lt;code&gt;city_data&lt;/code&gt; table, ordered by &lt;code&gt;date&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT
    percentile_disc(0.5) WITHIN GROUP (ORDER BY date) AS median_temperature
FROM city_data;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(Issue &lt;a href="https://github.com/apache/datafusion/issues/11732"&gt;#11732&lt;/a&gt;, 
PR &lt;a href="https://github.com/apache/datafusion/pull/13511"&gt;#13511&lt;/a&gt;,
by &lt;a href="https://github.com/Garamda"&gt;Garamda&lt;/a&gt;)&lt;/p&gt;
&lt;h3&gt;Compressed Spill Files&lt;/h3&gt;
&lt;p&gt;DataFusion now supports compressing the files written to disk when spilling
larger-than-memory datasets while sorting and grouping. Using compression
can significantly reduce the
size of the intermediate files and improve performance when reading them back into memory.&lt;/p&gt;
&lt;p&gt;(Issue &lt;a href="https://github.com/apache/datafusion/issues/16130"&gt;#16130&lt;/a&gt;,
PR &lt;a href="https://github.com/apache/datafusion/pull/16268"&gt;#16268&lt;/a&gt;
by &lt;a href="https://github.com/ding-young"&gt;ding-young&lt;/a&gt;)&lt;/p&gt;
&lt;h3&gt;Support for &lt;code&gt;REGEX_INSTR&lt;/code&gt; function&lt;/h3&gt;
&lt;p&gt;DataFusion now supports the [&lt;code&gt;REGEXP_INSTR&lt;/code&gt; function], which returns the position of a
regular expression match within a string.&lt;/p&gt;
&lt;p&gt;For example, to find the position of the first match of the regular expression
&lt;code&gt;C(.)(..)&lt;/code&gt; in the string &lt;code&gt;ABCDEF&lt;/code&gt;, you can use:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;&amp;gt; SELECT regexp_instr('ABCDEF', 'C(.)(..)');
+---------------------------------------------------------------+
| regexp_instr(Utf8("ABCDEF"),Utf8("C(.)(..)"))                 |
+---------------------------------------------------------------+
| 3                                                             |
+---------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(&lt;a href="https://github.com/apache/datafusion/issues/13009"&gt;Issue #13009&lt;/a&gt;,
&lt;a href="https://github.com/apache/datafusion/pull/15928"&gt;PR #15928&lt;/a&gt;
by &lt;a href="https://github.com/nirnayroy"&gt;nirnayroy&lt;/a&gt;)&lt;/p&gt;
&lt;h2&gt;Upgrade Guide and Changelog&lt;/h2&gt;
&lt;p&gt;Upgrading to 49.0.0 should be straightforward for most users. Please review the
&lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html"&gt;Upgrade Guide&lt;/a&gt;
for details on breaking changes and code snippets to help with the transition.
Recently, some users have reported success automatically upgrading DataFusion by
pairing AI tools with the upgrade guide. For a comprehensive list of all changes,
please refer to the &lt;a href="https://github.com/apache/datafusion/blob/branch-49/dev/changelog/49.0.0.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;About DataFusion&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; is an extensible query engine, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that
uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion is used by developers to
create new, fast, data-centric systems such as databases, dataframe libraries,
and machine learning and streaming applications. While &lt;a href="https://datafusion.apache.org/user-guide/introduction.html#project-goals"&gt;DataFusion&amp;rsquo;s primary design
goal&lt;/a&gt; is to accelerate the creation of other data-centric systems, it provides a
reasonable experience directly out of the box as a &lt;a href="https://datafusion.apache.org/user-guide/dataframe.html"&gt;dataframe library&lt;/a&gt;,
&lt;a href="https://datafusion.apache.org/python/"&gt;python library&lt;/a&gt;, and [command-line SQL tool].&lt;/p&gt;
&lt;p&gt;DataFusion's core thesis is that as a community, together we can build much more
advanced technology than any of us as individuals or companies could do alone.
Without DataFusion, highly performant vectorized query engines would remain
the domain of a few large companies and world-class research institutions.
With DataFusion, we can all build on top of a shared foundation and focus on
what makes our projects unique.&lt;/p&gt;
&lt;h2&gt;How to Get Involved&lt;/h2&gt;
&lt;p&gt;DataFusion is not a project built or driven by a single person, company, or
foundation. Rather, our community of users and contributors works together to
build a shared technology that none of us could have built alone.&lt;/p&gt;
&lt;p&gt;If you are interested in joining us, we would love to have you. You can try out
DataFusion on some of your own data and projects and let us know how it goes,
contribute suggestions, documentation, bug reports, or a PR with documentation,
tests, or code. A list of open issues suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;, and you
can find out how to reach us on the &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication doc&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion 48.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/07/16/datafusion-48.0.0" rel="alternate"></link><published>2025-07-16T00:00:00+00:00</published><updated>2025-07-16T00:00:00+00:00</updated><author><name>PMC</name></author><id>tag:datafusion.apache.org,2025-07-16:/blog/2025/07/16/datafusion-48.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/16347 for details --&gt;
&lt;p&gt;We&amp;rsquo;re excited to announce the release of &lt;strong&gt;Apache DataFusion 48.0.0&lt;/strong&gt;! As always, this version packs in a wide range of 
improvements and fixes. You can find the complete details in the full 
&lt;a href="https://github.com/apache/datafusion/blob/branch-48/dev/changelog/48.0.0.md"&gt;changelog&lt;/a&gt;. We&amp;rsquo;ll highlight the most
important changes below and guide you through upgrading.&lt;/p&gt;
&lt;h2&gt;Breaking …&lt;/h2&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/16347 for details --&gt;
&lt;p&gt;We&amp;rsquo;re excited to announce the release of &lt;strong&gt;Apache DataFusion 48.0.0&lt;/strong&gt;! As always, this version packs in a wide range of 
improvements and fixes. You can find the complete details in the full 
&lt;a href="https://github.com/apache/datafusion/blob/branch-48/dev/changelog/48.0.0.md"&gt;changelog&lt;/a&gt;. We&amp;rsquo;ll highlight the most
important changes below and guide you through upgrading.&lt;/p&gt;
&lt;h2&gt;Breaking Changes&lt;/h2&gt;
&lt;p&gt;DataFusion 48.0.0 brings a few &lt;strong&gt;breaking changes&lt;/strong&gt; that may require adjustments to your code as described in
the &lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html#datafusion-48-0-0"&gt;Upgrade Guide&lt;/a&gt;. Here are the most notable ones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;datafusion.execution.collect_statistics&lt;/code&gt; defaults to &lt;code&gt;true&lt;/code&gt;: In DataFusion 48.0.0, the default value of this &lt;a href="https://datafusion.apache.org/user-guide/configs.html"&gt;configuration setting&lt;/a&gt; is now true, and DataFusion will collect and store statistics when a table is first created via &lt;code&gt;CREATE EXTERNAL TABLE&lt;/code&gt; or one of the &lt;code&gt;DataFrame::register_*&lt;/code&gt; APIs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Expr::Literal&lt;/code&gt; has optional metadata: The &lt;code&gt;Expr::Literal&lt;/code&gt; variant now includes optional metadata, which allows 
  for carrying through Arrow field metadata to support extension types and other uses. This means code such as&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;match expr {
...
  Expr::Literal(scalar) =&amp;gt; ...
...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Should be updated to:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;match expr {
...
  Expr::Literal(scalar, _metadata) =&amp;gt; ...
...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Expr::WindowFunction&lt;/code&gt; is now Boxed: &lt;code&gt;Expr::WindowFunction&lt;/code&gt; is now a &lt;code&gt;Box&amp;lt;WindowFunction&amp;gt;&lt;/code&gt; instead of a &lt;code&gt;WindowFunction&lt;/code&gt; 
  directly. This change was made to reduce the size of &lt;code&gt;Expr&lt;/code&gt; and improve performance when planning queries 
  (see details on &lt;a href="https://github.com/apache/datafusion/pull/16207"&gt;#16207&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;UDFs changed to use &lt;code&gt;FieldRef&lt;/code&gt; instead of &lt;code&gt;DataType&lt;/code&gt;: To support metadata handling and 
  prepare for extension types, UDF traits now use &lt;a href="https://docs.rs/arrow/latest/arrow/datatypes/type.FieldRef.html"&gt;FieldRef&lt;/a&gt; rather than a &lt;code&gt;DataType&lt;/code&gt;
  and nullability. &lt;code&gt;FieldRef&lt;/code&gt; contains the type and nullability, and additionally allows access to 
  metadata fields, which can be used for extension types.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Physical Expression return &lt;code&gt;Field&lt;/code&gt;: Similarly to UDFs, in order to prepare for extension type support the 
  &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_expr/trait.PhysicalExpr.html"&gt;PhysicalExpr&lt;/a&gt; trait has been changed to return &lt;a href="https://docs.rs/arrow/latest/arrow/datatypes/struct.Field.html"&gt;Field&lt;/a&gt; rather than &lt;code&gt;DataType&lt;/code&gt;. To upgrade structs which 
  implement &lt;code&gt;PhysicalExpr&lt;/code&gt; you need to implement the &lt;code&gt;return_field&lt;/code&gt; function. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;FileFormat::supports_filters_pushdown&lt;/code&gt; was replaced with &lt;code&gt;FileSource::try_pushdown_filters&lt;/code&gt; to support upcoming work to push down dynamic filters and physical filter pushdown. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ParquetExec&lt;/code&gt;, &lt;code&gt;AvroExec&lt;/code&gt;, &lt;code&gt;CsvExec&lt;/code&gt;, &lt;code&gt;JsonExec&lt;/code&gt; removed: &lt;code&gt;ParquetExec&lt;/code&gt;, &lt;code&gt;AvroExec&lt;/code&gt;, &lt;code&gt;CsvExec&lt;/code&gt;, and &lt;code&gt;JsonExec&lt;/code&gt;
  were deprecated in DataFusion 46 and are removed in DataFusion 48.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Performance Improvements&lt;/h2&gt;
&lt;p&gt;DataFusion 48.0.0 comes with some noteworthy performance enhancements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Fewer unnecessary projections:&lt;/strong&gt; DataFusion now removes additional unnecessary &lt;code&gt;Projection&lt;/code&gt;s in queries. (PRs &lt;a href="https://github.com/apache/datafusion/pull/15787"&gt;#15787&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/pull/15761"&gt;#15761&lt;/a&gt;,
  and &lt;a href="https://github.com/apache/datafusion/pull/15746"&gt;#15746&lt;/a&gt; by &lt;a href="https://github.com/xudong963"&gt;xudong963&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Accelerated string functions&lt;/strong&gt;: The &lt;code&gt;ascii&lt;/code&gt; function was optimized to significantly improve its performance
  (PR &lt;a href="https://github.com/apache/datafusion/pull/16087"&gt;#16087&lt;/a&gt; by &lt;a href="https://github.com/tlm365"&gt;tlm365&lt;/a&gt;). The &lt;code&gt;character_length&lt;/code&gt; function was optimized resulting in 
  &lt;a href="https://github.com/apache/datafusion/pull/15931#issuecomment-2848561984"&gt;up to 3x&lt;/a&gt; performance improvement (PR &lt;a href="https://github.com/apache/datafusion/pull/15931"&gt;#15931&lt;/a&gt; by &lt;a href="https://github.com/Dandandan"&gt;Dandandan&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Constant aggregate window expressions:&lt;/strong&gt; For unbounded aggregate window functions the result is the 
  same for all rows within a partition. DataFusion 48.0.0 avoids unnecessary computation for such queries, resulting in &lt;a href="https://github.com/apache/datafusion/pull/16234#issuecomment-2935960865"&gt;improved performance by 5.6x&lt;/a&gt;
  (PR &lt;a href="https://github.com/apache/datafusion/pull/16234"&gt;#16234&lt;/a&gt; by &lt;a href="https://github.com/suibianwanwank"&gt;suibianwanwank&lt;/a&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Highlighted New Features&lt;/h2&gt;
&lt;h3&gt;New &lt;code&gt;datafusion-spark&lt;/code&gt; crate&lt;/h3&gt;
&lt;p&gt;The DataFusion community has requested &lt;a href="https://spark.apache.org"&gt;Apache Spark&lt;/a&gt;-compatible functions for many years, but the current builtin function library is most similar to Postgresql, which leads to friction. Unfortunately, there are even functions with the same name but different signatures and/or return types in the two systems.&lt;/p&gt;
&lt;p&gt;One of the many uses of DataFusion is to enhance (e.g. &lt;a href="https://github.com/apache/datafusion-comet"&gt;Apache DataFusion Comet&lt;/a&gt;) 
or replace (e.g. &lt;a href="https://github.com/lakehq/sail"&gt;Sail&lt;/a&gt;) &lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt;. To 
support the community requests and the use cases mentioned above, we have introduced a new
&lt;a href="https://crates.io/crates/datafusion-spark"&gt;datafusion-spark&lt;/a&gt; crate for DataFusion with spark-compatible functions so the 
community can collaborate to build this shared resource. There are several hundred functions to implement, and we are looking for help to &lt;a href="https://github.com/apache/datafusion/issues/15914"&gt;complete datafusion-spark Spark Compatible Functions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To register all functions in &lt;code&gt;datafusion-spark&lt;/code&gt; you can use:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-Rust"&gt;    // Create a new session context
    let mut ctx = SessionContext::new();
    // register all spark functions with the context
    datafusion_spark::register_all(&amp;amp;mut ctx)?;
    // run a query. Note the `sha2` function is now available which
    // has Spark semantics
    let df = ctx.sql("SELECT sha2('The input String', 256)").await?;
    ...
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or, to use an individual function, you can do:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-Rust"&gt;use datafusion_expr::{col, lit};
use datafusion_spark::expr_fn::sha2;
// Create the expression `sha2(my_data, 256)`
let expr = sha2(col("my_data"), lit(256));
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/shehabgamin"&gt;shehabgamin&lt;/a&gt; for the initial PR &lt;a href="https://github.com/apache/datafusion/pull/15168"&gt;#15168&lt;/a&gt; 
and many others for their help adding additional functions. Please consider 
helping &lt;a href="https://github.com/apache/datafusion/issues/15914"&gt;complete datafusion-spark Spark Compatible Functions&lt;/a&gt;. &lt;/p&gt;
&lt;h3&gt;&lt;code&gt;ORDER BY ALL sql&lt;/code&gt; support&lt;/h3&gt;
&lt;p&gt;Inspired by &lt;a href="https://duckdb.org/docs/stable/sql/query_syntax/orderby.html#order-by-all-examples"&gt;DuckDB&lt;/a&gt;, DataFusion 48.0.0 adds support for &lt;code&gt;ORDER BY ALL&lt;/code&gt;. This allows for easy ordering of all columns in a query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;&amp;gt; set datafusion.sql_parser.dialect = 'DuckDB';
0 row(s) fetched.
&amp;gt; CREATE OR REPLACE TABLE addresses AS
    SELECT '123 Quack Blvd' AS address, 'DuckTown' AS city, '11111' AS zip
    UNION ALL
    SELECT '111 Duck Duck Goose Ln', 'DuckTown', '11111'
    UNION ALL
    SELECT '111 Duck Duck Goose Ln', 'Duck Town', '11111'
    UNION ALL
    SELECT '111 Duck Duck Goose Ln', 'Duck Town', '11111-0001';
0 row(s) fetched.
&amp;gt; SELECT * FROM addresses ORDER BY ALL;
+------------------------+-----------+------------+
| address                | city      | zip        |
+------------------------+-----------+------------+
| 111 Duck Duck Goose Ln | Duck Town | 11111      |
| 111 Duck Duck Goose Ln | Duck Town | 11111-0001 |
| 111 Duck Duck Goose Ln | DuckTown  | 11111      |
| 123 Quack Blvd         | DuckTown  | 11111      |
+------------------------+-----------+------------+
4 row(s) fetched.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/PokIsemaine"&gt;PokIsemaine&lt;/a&gt; for PR &lt;a href="https://github.com/apache/datafusion/pull/15772"&gt;#15772&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;FFI Support for &lt;code&gt;AggregateUDF&lt;/code&gt; and &lt;code&gt;WindowUDF&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;This improvement allows for using user defined aggregate and user defined window functions across FFI boundaries, which enables shared libraries to pass functions back and forth. This feature unlocks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Modules to provide DataFusion based FFI aggregates that can be reused in projects such as &lt;a href="https://github.com/apache/datafusion-python"&gt;datafusion-python&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using the same aggregate and window functions without recompiling with different DataFusion versions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This completes the work to add support for all UDF types to DataFusion's FFI bindings. Thanks to &lt;a href="https://github.com/timsaucer"&gt;timsaucer&lt;/a&gt;
for PRs &lt;a href="https://github.com/apache/datafusion/pull/16261"&gt;#16261&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/pull/14775"&gt;#14775&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Reduced size of &lt;code&gt;Expr&lt;/code&gt; struct&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/enum.Expr.html"&gt;Expr&lt;/a&gt; struct is widely used across the DataFusion and downstream codebases. By &lt;code&gt;Box&lt;/code&gt;ing &lt;code&gt;WindowFunction&lt;/code&gt;s,  we reduced the size of &lt;code&gt;Expr&lt;/code&gt; by almost 50%, from &lt;code&gt;272&lt;/code&gt; to &lt;code&gt;144&lt;/code&gt; bytes. This reduction improved planning times between 10% and 20% and reduced memory usage. Thanks to &lt;a href="https://github.com/hendrikmakait"&gt;hendrikmakait&lt;/a&gt; for 
PR &lt;a href="https://github.com/apache/datafusion/pull/16207"&gt;#16207&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Upgrade Guide and Changelog&lt;/h2&gt;
&lt;p&gt;Upgrading to 48.0.0 should be straightforward for most users, but do review
the &lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html#datafusion-48-0-0"&gt;Upgrade Guide for DataFusion 48.0.0&lt;/a&gt; for detailed
steps and code changes. The upgrade guide covers the breaking changes mentioned above and provides code snippets to help with the
transition. For a comprehensive list of all changes, please refer to the &lt;a href="https://github.com/apache/datafusion/blob/branch-48/dev/changelog/48.0.0.md"&gt;changelog&lt;/a&gt; 
for the 48.0.0 release. The changelog enumerates every merged PR in this release, including many smaller fixes and improvements 
that we couldn&amp;rsquo;t cover in this post.&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;Apache DataFusion is an open-source project, and we welcome involvement from anyone interested. Now is a great time to
take 48.0.0 for a spin: try it out on your workloads, and let us know if you encounter any issues or have suggestions.
You can report bugs or request features on our GitHub issue tracker, or better yet, submit a pull request. Join our
community discussions &amp;ndash; whether you have questions, want to share how you&amp;rsquo;re using DataFusion, or are looking to
contribute, we&amp;rsquo;d love to hear from you. A list of open issues suitable for beginners
is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt; and you
can find how to reach us on the &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication doc&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy querying!&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Embedding User-Defined Indexes in Apache Parquet Files</title><link href="https://datafusion.apache.org/blog/2025/07/14/user-defined-parquet-indexes" rel="alternate"></link><published>2025-07-14T00:00:00+00:00</published><updated>2025-07-14T00:00:00+00:00</updated><author><name>Qi Zhu (Cloudera), Jigao Luo (Systems Group at TU Darmstadt), and Andrew Lamb (InfluxData)</name></author><id>tag:datafusion.apache.org,2025-07-14:/blog/2025/07/14/user-defined-parquet-indexes</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;It&amp;rsquo;s a common misconception that &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; files are limited to basic Min/Max/Null Count statistics and Bloom filters, and that adding more advanced indexes requires changing the specification or creating a new file format. In fact, footer metadata and offset-based addressing already provide everything needed to embed …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;It&amp;rsquo;s a common misconception that &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; files are limited to basic Min/Max/Null Count statistics and Bloom filters, and that adding more advanced indexes requires changing the specification or creating a new file format. In fact, footer metadata and offset-based addressing already provide everything needed to embed user-defined index structures within Parquet files without breaking compatibility with other Parquet readers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motivating Example:&lt;/strong&gt; Imagine your data has a &lt;code&gt;Nation&lt;/code&gt; column with dozens of distinct values across thousands of Parquet files. You execute:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;  SELECT AVG(sales_amount)
  FROM sales
  WHERE nation = 'Singapore'
  GROUP BY year;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Relying on the min/max statistics from the Parquet format will be ineffective at pruning files when &lt;code&gt;Nation&lt;/code&gt; spans "Argentina" through "Zimbabwe". Instead of relying on a Bloom Filter, you may want to store a list of every distinct &lt;code&gt;Nation&lt;/code&gt; value in the file near the end. At query time, your engine will read that tiny list and skip any file that does not contain 'Singapore'. This special distinct value index can yield dramatically better file‑pruning performance for your engine, all while preserving full compatibility with standard Parquet readers.&lt;/p&gt;
&lt;p&gt;In this post, we review how indexes are stored in the Apache Parquet format, explain the mechanism for storing user-defined indexes, and finally show how to read and write a user-defined index using &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;hr/&gt;
&lt;p&gt;Apache Parquet is a popular columnar file format with well understood and &lt;a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"&gt;production grade libraries for high‑performance analytics&lt;/a&gt;. Features like efficient encodings, column pruning, and predicate pushdown work well for many common query patterns. Apache DataFusion includes a &lt;a href="https://datafusion.apache.org/blog/2025/03/20/parquet-pruning/"&gt;highly optimized Parquet implementation&lt;/a&gt; and has excellent performance in general. However, some production query patterns require more than the statistics included in the Parquet format itself&lt;sup&gt;&lt;a href="#footnote1"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Many systems improve query performance using &lt;em&gt;external&lt;/em&gt; indexes or other metadata in addition to Parquet. For example, Apache Iceberg's &lt;a href="https://iceberg.apache.org/docs/latest/performance/#scan-planning"&gt;Scan Planning&lt;/a&gt; uses metadata stored in separate files or an in memory cache, and the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs"&gt;parquet_index.rs&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs"&gt;advanced_parquet_index.rs&lt;/a&gt; examples in the DataFusion repository use external files for Parquet pruning (skipping).&lt;/p&gt;
&lt;p&gt;External indexes are powerful and widespread, but they have some drawbacks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Increased Cost and Operational Complexity:&lt;/strong&gt; You need additional files and systems as well as the original Parquet. &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Synchronization Risks:&lt;/strong&gt; The external index may become out of sync with the Parquet data if you do not manage it carefully.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Proponents have even cited these drawbacks as justification for new file formats, such as Microsoft's &lt;a href="https://github.com/microsoft/amudai/blob/main/docs/spec/src/what_about_parquet.md"&gt;Amudai&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;However, Parquet is extensible with user-defined indexes&lt;/strong&gt;: Parquet tolerates unknown bytes within the file body and permits arbitrary key/value pairs in its footer metadata. These two features enable &lt;strong&gt;embedding&lt;/strong&gt; user-defined indexes directly in the file&amp;mdash;no extra files, no format forks, and no compatibility breakage. &lt;/p&gt;
&lt;h2&gt;Parquet File Anatomy &amp;amp; Standard Index Structures&lt;/h2&gt;
&lt;hr/&gt;
&lt;p&gt;Logically, Parquet files contain row groups, each with column chunks, which in turn contain data pages. Physically, a Parquet file is a sequence of bytes with a Thrift-encoded footer metadata containing metadata about the file structure. The footer metadata includes the schema, row groups, column chunks, and other metadata required to read the file.&lt;/p&gt;
&lt;p&gt;The Parquet format includes three main types&lt;sup&gt;&lt;a href="#footnote2"&gt;2&lt;/a&gt;&lt;/sup&gt; of optional index structures:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://github.com/apache/parquet-format/blob/819adce0ec6aa848e56c56f20b9347f4ab50857f/src/main/thrift/parquet.thrift#L263-L266"&gt;Min/Max/Null Count Statistics&lt;/a&gt;&lt;/strong&gt; for each chunk in a row group. Engines use these to quickly skip row groups that do not match a query predicate. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://parquet.apache.org/docs/file-format/pageindex/"&gt;Page Index&lt;/a&gt;&lt;/strong&gt;: Offsets, sizes, and statistics for each data page. Engines use these to quickly locate data pages without scanning all pages for a column chunk.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://parquet.apache.org/docs/file-format/bloomfilter/"&gt;Bloom Filters&lt;/a&gt;&lt;/strong&gt;: Data structure to quickly determine if a value is present in a column chunk without scanning any data pages. Particularly useful for equality and &lt;code&gt;IN&lt;/code&gt; predicates.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;!-- Source: https://docs.google.com/presentation/d/1aFjTLEDJyDqzFZHgcmRxecCvLKKXV2OvyEpTQFCNZPw --&gt;
&lt;p&gt;&lt;img alt="Parquet File layout with standard index structures." class="img-responsive" src="/blog/images/user-defined-parquet-indexes/standard_index_structures.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Parquet file layout with standard index structures (as written by arrow-rs).&lt;/p&gt;
&lt;p&gt;Only the Min/Max/Null Count Statistics are stored inline in the Parquet footer metadata. The Page Index and Bloom Filters are typically stored in the file body before the Thrift-encoded footer metadata. The locations of these index structures are recorded in the footer metadata, as shown in Figure 1. Parquet readers that do not understand these structures simply ignore them.&lt;/p&gt;
&lt;p&gt;Modern Parquet writers create these indexes automatically and provide APIs to control their generation and placement. For example, the &lt;a href="https://docs.rs/parquet/latest/parquet/"&gt;Rust Parquet Library&lt;/a&gt; provides &lt;a href="https://docs.rs/parquet/latest/parquet/file/properties/struct.WriterProperties.html"&gt;Parquet WriterProperties&lt;/a&gt;, &lt;a href="https://docs.rs/parquet/latest/parquet/file/properties/enum.EnabledStatistics.html"&gt;EnabledStatistics&lt;/a&gt;, and &lt;a href="https://docs.rs/parquet/latest/parquet/file/properties/enum.BloomFilterPosition.html"&gt;BloomFilterPosition&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Embedding User Defined Indexes in Parquet Files&lt;/h2&gt;
&lt;hr/&gt;
&lt;p&gt;Embedding user-defined indexes in Parquet files is straightforward and follows the same principles as standard index structures&lt;sup&gt;&lt;a href="#footnote6"&gt;6&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Serialize the index into a binary format and write it into the file body before the Thrift-encoded footer metadata.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Record the index location in the footer metadata as a key/value pair, such as &lt;code&gt;"my_index_offset" -&amp;gt; "&amp;lt;byte-offset&amp;gt;"&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Figure 2 shows the resulting file layout.&lt;/p&gt;
&lt;!-- Source: https://docs.google.com/presentation/d/1aFjTLEDJyDqzFZHgcmRxecCvLKKXV2OvyEpTQFCNZPw --&gt;
&lt;p&gt;&lt;img alt="Parquet File layout with custom index structures." class="img-responsive" src="/blog/images/user-defined-parquet-indexes/custom_index_structures.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Parquet file layout with user-defined indexes.&lt;/p&gt;
&lt;p&gt;Like standard index structures, user-defined indexes can be stored anywhere in the file body, such as after row group data or before the footer. There is no limit to the number of user-defined indexes, nor any restriction on their granularity: they can operate at the file, row group, page, or even row level. This flexibility enables a wide range of use cases, including:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Row group or page-level distinct sets: a finer-grained version of the file-level example in this blog.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/HyperLogLog"&gt;HyperLogLog&lt;/a&gt; sketches for distinct value estimation, addressing a common criticism&lt;sup&gt;3&lt;/sup&gt; of Parquet&amp;rsquo;s lack of cardinality estimation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Additional zone maps (&lt;a href="https://www.vldb.org/conf/1998/p476.pdf"&gt;small materialized aggregates&lt;/a&gt;) such as precomputed &lt;code&gt;sum&lt;/code&gt;s at the column chunk or data page level for faster query execution.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Histograms or samples at the row group or column chunk level for predicate selectivity estimates.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Example: Embedding a User Defined Distinct Value Index in Parquet Files&lt;/h2&gt;
&lt;hr/&gt;
&lt;p&gt;This section demonstrates how to embed a simple distinct value index in Parquet files and use it for file-level pruning (skipping) in DataFusion. The full example is available in the DataFusion repository at &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_embedded_index.rs"&gt;parquet_embedded_index.rs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Note that the example requires &lt;strong&gt;&lt;a href="https://crates.io/crates/parquet/55.2.0"&gt;arrow‑rs v55.2.0&lt;/a&gt;&lt;/strong&gt; or later, which includes the new &amp;ldquo;buffered write&amp;rdquo; API (&lt;a href="https://github.com/apache/arrow-rs/pull/7714"&gt;apache/arrow-rs#7714&lt;/a&gt;) to keep the internal byte count in sync after appending index bytes immediately after data pages.&lt;/p&gt;
&lt;p&gt;This example is intentionally simple for clarity, but you can adapt the same approach for any index type or data types. The high-level design is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Define your index payload&lt;/strong&gt; (e.g., bitmap, Bloom filter, sketch, distinct values list, etc.).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Serialize your index to bytes&lt;/strong&gt; and append them into the Parquet file body before writing the footer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Record the index location&lt;/strong&gt; by adding a key/value entry (e.g., &lt;code&gt;"my_index_offset" -&amp;gt; "&amp;lt;byte‑offset&amp;gt;"&lt;/code&gt;) in the Parquet footer metadata.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Extend DataFusion&lt;/strong&gt; with a custom &lt;code&gt;TableProvider&lt;/code&gt; (or wrap the existing Parquet provider) to use the index.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The &lt;code&gt;TableProvider&lt;/code&gt; simply reads the footer metadata to discover the index offset, seeks to that offset and deserializes the index, and then uses the index to speed up processing (e.g., skip files, row groups, data pages, etc.).&lt;/p&gt;
&lt;p&gt;The resulting Parquet files remain fully compatible with other tools such as DuckDB and Spark, which simply ignore the unknown index bytes and key/value metadata.&lt;/p&gt;
&lt;h3&gt;Introduction to Distinct Value Indexes&lt;/h3&gt;
&lt;hr/&gt;
&lt;p&gt;A &lt;strong&gt;distinct value index&lt;/strong&gt; stores the unique values of a specific column. This type of index is effective for columns with a small number of distinct values and can be used to quickly skip files that do not match the query. These indexes are popular in several engines, such as the &lt;a href="https://clickhouse.com/docs/optimize/skipping-indexes#set"&gt;"set" Skip Index in ClickHouse&lt;/a&gt; and the &lt;a href="https://docs.influxdata.com/influxdb3/enterprise/admin/distinct-value-cache/"&gt;Distinct Value Cache&lt;/a&gt; in InfluxDB 3.0.&lt;/p&gt;
&lt;p&gt;For example, if the files contain a column named &lt;code&gt;Category&lt;/code&gt; like this:&lt;/p&gt;
&lt;table class="table" style="border-collapse:collapse;"&gt;
&lt;tr&gt;
&lt;td style="border:1px solid #888;padding:2px 6px;"&gt;&lt;b&gt;&lt;code&gt;Category&lt;/code&gt;&lt;/b&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="border:1px solid #888;padding:2px 6px;"&gt;&lt;code&gt;foo&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="border:1px solid #888;padding:2px 6px;"&gt;&lt;code&gt;bar&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="border:1px solid #888;padding:2px 6px;"&gt;&lt;code&gt;...&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="border:1px solid #888;padding:2px 6px;"&gt;&lt;code&gt;baz&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="border:1px solid #888;padding:2px 6px;"&gt;&lt;code&gt;foo&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The distinct value index will contain the values &lt;code&gt;foo&lt;/code&gt;, &lt;code&gt;bar&lt;/code&gt;, and &lt;code&gt;baz&lt;/code&gt;. In contrast, traditional min/max statistics would store only the minimum (&lt;code&gt;bar&lt;/code&gt;) and maximum (&lt;code&gt;foo&lt;/code&gt;) values, so a query like&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT * FROM t WHERE Category = 'bas'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;cannot skip the file using min/max values because &lt;code&gt;bas&lt;/code&gt; falls between &lt;code&gt;bar&lt;/code&gt; and &lt;code&gt;foo&lt;/code&gt; in lexicographic order, even though &lt;code&gt;bas&lt;/code&gt; does not appear in the column.&lt;/p&gt;
&lt;p&gt;This is a key benefit of a distinct value index: accurate filtering without requiring the column to be sorted, unlike min/max-based pruning which is most effective when data is ordered.&lt;/p&gt;
&lt;p&gt;While not a traditional index structure like a B-tree, the distinct value set acts as a lightweight, embedded index that enables fast pruning and is especially effective for columns with low cardinality.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Supported Filters&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Distinct value indexes are most effective for &lt;strong&gt;equality filters&lt;/strong&gt;, such as:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;WHERE category = 'foo'
WHERE category IN ('foo', 'bar')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;They can also help with NOT IN and anti-joins, as long as the engine can evaluate them using the list of known distinct values.&lt;/p&gt;
&lt;p&gt;However, these indexes are not suitable for range predicates (e.g., category &amp;gt; 'foo'), as they do not preserve any ordering information. For such cases, other structures such as min/max statistics or sorted data layouts may be more effective.&lt;/p&gt;
&lt;p&gt;We represent a distinct value index in Rust for our example as a simple &lt;code&gt;HashSet&amp;lt;String&amp;gt;&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;/// An index of distinct values for a single column
#[derive(Debug, Clone)]
struct DistinctIndex {
   inner: HashSet&amp;lt;String&amp;gt;,
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;File Layout with Distinct Value Index&lt;/h3&gt;
&lt;hr/&gt;
&lt;p&gt;In this example, we write a distinct value index for the &lt;code&gt;Category&lt;/code&gt; column into the Parquet file body after all the data pages, and record the index location in the footer metadata. The resulting file layout looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;                  &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;                           
                  &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv;                           
                  &amp;boxv;&amp;boxv;     DataPage      &amp;boxv; &amp;boxv;                           
                  &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv;                           
 Standard Parquet &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv;                           
 Data Pages       &amp;boxv;&amp;boxv;     DataPage      &amp;boxv; &amp;boxv;                           
                  &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv;                           
                  &amp;boxv;        ...           &amp;boxv;                           
                  &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv;                           
                  &amp;boxv;&amp;boxv;     DataPage      &amp;boxv; &amp;boxv;                           
                  &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv;                           
                  &amp;boxv;┏━━━━━━━━━━━━━━━━━━━┓ &amp;boxv;                           
Non standard      &amp;boxv;┃                   ┃ &amp;boxv;                           
index (ignored by &amp;boxv;┃Custom Binary Index┃ &amp;boxv;                           
other Parquet     &amp;boxv;┃ (Distinct Values) ┃◀&amp;boxv;&amp;boxh; &amp;boxh; &amp;boxh;                      
readers)          &amp;boxv;┃                   ┃ &amp;boxv;     &amp;boxv;                     
                  &amp;boxv;┗━━━━━━━━━━━━━━━━━━━┛ &amp;boxv;                           
Standard Parquet  &amp;boxv;┏━━━━━━━━━━━━━━━━━━━┓ &amp;boxv;     &amp;boxv;  key/value metadata
Page Index        &amp;boxv;┃    Page Index     ┃ &amp;boxv;        contains location  
                  &amp;boxv;┗━━━━━━━━━━━━━━━━━━━┛ &amp;boxv;     &amp;boxv;  of special index   
                  &amp;boxv;&amp;boxDR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxDL; &amp;boxv;                           
                  &amp;boxv;&amp;boxV; Parquet Footer w/ &amp;boxV; &amp;boxv;     &amp;boxv;                     
                  &amp;boxv;&amp;boxV;     Metadata      &amp;boxV; &amp;boxvh; &amp;boxh; &amp;boxh;                       
                  &amp;boxv;&amp;boxV; (Thrift Encoded)  &amp;boxV; &amp;boxv;                           
                  &amp;boxv;&amp;boxUR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxUL; &amp;boxv;                           
                  &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;                           

&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Serializing the Distinct‑Value Index&lt;/h3&gt;
&lt;hr/&gt;
&lt;p&gt;The example uses a simple newline‑separated UTF‑8 format as the binary format. The code to serialize the distinct index is shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;/// Magic bytes to identify our custom index format
const INDEX_MAGIC: &amp;amp;[u8] = b"IDX1";

/// Serialize the distinct index to a writer as bytes
fn serialize&amp;lt;W: Write + Send&amp;gt;(
   &amp;amp;self,
   arrow_writer: &amp;amp;mut ArrowWriter&amp;lt;W&amp;gt;,
) -&amp;gt; Result&amp;lt;()&amp;gt; {
   let serialized = self
           .inner
           .iter()
           .map(|s| s.as_str())
           .collect::&amp;lt;Vec&amp;lt;_&amp;gt;&amp;gt;()
           .join("\n");
   let index_bytes = serialized.into_bytes();

   // Set the offset for the index
   let offset = arrow_writer.bytes_written();
   let index_len = index_bytes.len() as u64;

   // Write the index magic and length to the file
   arrow_writer.write_all(INDEX_MAGIC)?;
   arrow_writer.write_all(&amp;amp;index_len.to_le_bytes())?;

   // Write the index bytes
   arrow_writer.write_all(&amp;amp;index_bytes)?;

   // Append metadata about the index to the Parquet file footer metadata
   arrow_writer.append_key_value_metadata(KeyValue::new(
      "distinct_index_offset".to_string(),
      offset.to_string(),
   ));
   Ok(())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code does the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Creates a newline‑separated UTF‑8 string from the distinct values.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Writes a magic header (&lt;code&gt;IDX1&lt;/code&gt;) and the length of the index.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Writes the index bytes to the file using the &lt;a href="https://docs.rs/parquet/latest/parquet/arrow/arrow_writer/struct.ArrowWriter.html"&gt;ArrowWriter&lt;/a&gt; API.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Records the index location by adding a key/value entry (&lt;code&gt;"distinct_index_offset" -&amp;gt; &amp;lt;offset&amp;gt;&lt;/code&gt;) in the Parquet footer metadata.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note: Use the &lt;a href="https://docs.rs/parquet/latest/parquet/arrow/arrow_writer/struct.ArrowWriter.html#method.write_all"&gt;ArrowWriter::write_all&lt;/a&gt; API to ensure the offsets in the footer metadata are correctly tracked. &lt;/p&gt;
&lt;h3&gt;Reading the Index&lt;/h3&gt;
&lt;hr/&gt;
&lt;p&gt;This code reads the distinct index from a Parquet file:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;/// Read a `DistinctIndex` from a Parquet file
fn read_distinct_index(path: &amp;amp;Path) -&amp;gt; Result&amp;lt;DistinctIndex&amp;gt; {
    let file = File::open(path)?;

    let file_size = file.metadata()?.len();
    println!("Reading index from {} (size: {file_size})", path.display(), );

    let reader = SerializedFileReader::new(file.try_clone()?)?;
    let meta = reader.metadata().file_metadata();

    let offset = get_key_value(meta, "distinct_index_offset")
        .ok_or_else(|| ParquetError::General("Missing index offset".into()))?
        .parse::&amp;lt;u64&amp;gt;()
        .map_err(|e| ParquetError::General(e.to_string()))?;

    println!("Reading index at offset: {offset}, length");
    DistinctIndex::new_from_reader(file, offset)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This function:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Opens the Parquet footer metadata and extracts &lt;code&gt;distinct_index_offset&lt;/code&gt; from the metadata.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Calls &lt;code&gt;DistinctIndex::new_from_reader&lt;/code&gt; to read the index from the file at that offset.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;DistinctIndex::new_from_reader&lt;/code&gt; actually reads the index as shown below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt; /// Read the distinct values index from a reader at the given offset and length
 pub fn new_from_reader&amp;lt;R: Read + Seek&amp;gt;(mut reader: R, offset: u64) -&amp;gt; Result&amp;lt;DistinctIndex&amp;gt; {
     reader.seek(SeekFrom::Start(offset))?;

     let mut magic_buf = [0u8; 4];
     reader.read_exact(&amp;amp;mut magic_buf)?;
     if magic_buf != INDEX_MAGIC {
         return exec_err!("Invalid index magic number at offset {offset}");
     }

     let mut len_buf = [0u8; 8];
     reader.read_exact(&amp;amp;mut len_buf)?;
     let stored_len = u64::from_le_bytes(len_buf) as usize;

     let mut index_buf = vec![0u8; stored_len];
     reader.read_exact(&amp;amp;mut index_buf)?;

     let Ok(s) = String::from_utf8(index_buf) else {
         return exec_err!("Invalid UTF-8 in index data");
     };

     Ok(Self {
         inner: s.lines().map(|s| s.to_string()).collect(),
     })
 }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Seeks to the offset of the index in the file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reads the magic bytes and checks they match &lt;code&gt;IDX1&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reads the length of the index and allocates a buffer.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reads the index bytes, converts them to a &lt;code&gt;String&lt;/code&gt;, and splits into lines to populate the &lt;code&gt;HashSet&amp;lt;String&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Extending DataFusion&amp;rsquo;s &lt;code&gt;TableProvider&lt;/code&gt;&lt;/h3&gt;
&lt;hr/&gt;
&lt;p&gt;To use the distinct index for file-level pruning, extend DataFusion's &lt;code&gt;TableProvider&lt;/code&gt; to read the index and apply it during query execution:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;impl TableProvider for DistinctIndexTable {
    /* ... */

    /// Prune files before reading: only keep files whose distinct set
    /// contains the filter value
    async fn scan(
        &amp;amp;self,
        _ctx: &amp;amp;dyn Session,
        _proj: Option&amp;lt;&amp;amp;Vec&amp;lt;usize&amp;gt;&amp;gt;,
        filters: &amp;amp;[Expr],
        _limit: Option&amp;lt;usize&amp;gt;,
    ) -&amp;gt; Result&amp;lt;Arc&amp;lt;dyn ExecutionPlan&amp;gt;&amp;gt; {
        // This example only handles filters of the form
        // `category = 'X'` where X is a string literal
        //
        // You can use `PruningPredicate` for much more general range and
        // equality analysis or write your own custom logic.
        let mut target: Option&amp;lt;&amp;amp;str&amp;gt; = None;

        if filters.len() == 1 {
            if let Expr::BinaryExpr(expr) = &amp;amp;filters[0] {
                if expr.op == Operator::Eq {
                    if let (
                        Expr::Column(c),
                        Expr::Literal(ScalarValue::Utf8(Some(v)), _),
                    ) = (&amp;amp;*expr.left, &amp;amp;*expr.right)
                    {
                        if c.name == "category" {
                            println!("Filtering for category: {v}");
                            target = Some(v);
                        }
                    }
                }
            }
        }
        // Determine which files to scan
        // files_and_index is a Vec&amp;lt;(String, DistinctIndex)&amp;gt;,
        // See the full example for how this is populated.
        let files_to_scan: Vec&amp;lt;_&amp;gt; = self
            .files_and_index
            .iter()
            .filter_map(|(f, distinct_index)| {
                // keep file if no target or target is in the distinct set
                if target.is_none() || distinct_index.contains(target?) {
                    Some(f)
                } else {
                    None
                }
            })
            .collect();

        // Build ParquetSource to actually read the files
        let url = ObjectStoreUrl::parse("file://")?;
        let source = Arc::new(ParquetSource::default().with_enable_page_index(true));
        let mut builder = FileScanConfigBuilder::new(url, self.schema.clone(), source);
        for file in files_to_scan {
            let path = self.dir.join(file);
            let len = std::fs::metadata(&amp;amp;path)?.len();
           // If the index contained information about row groups or pages,
           // you could also pass that information here to further prune
           // the data read from the file.
           let partitioned_file =
                   PartitionedFile::new(path.to_str().unwrap().to_string(), len);
           builder = builder.with_file(partitioned_file);
        }
        Ok(DataSourceExec::from_data_source(builder.build()))
    }

    /// Tell DataFusion that we can handle filters on the "category" column
    fn supports_filters_pushdown(
        &amp;amp;self,
        fs: &amp;amp;[&amp;amp;Expr],
    ) -&amp;gt; Result&amp;lt;Vec&amp;lt;TableProviderFilterPushDown&amp;gt;&amp;gt; {
        // Mark as inexact since pruning is file‑granular
        Ok(vec![TableProviderFilterPushDown::Inexact; fs.len()])
    }
}

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code does the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Implements the &lt;code&gt;scan&lt;/code&gt; method to filter files based on the distinct index.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Checks if the filter is an equality predicate on the &lt;code&gt;category&lt;/code&gt; column.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If the target value is specified, checks if the distinct index contains that value.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Builds a &lt;code&gt;FileScanConfig&lt;/code&gt; with only the files that match the filter.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Putting It All Together&lt;/h3&gt;
&lt;p&gt;To use the distinct index in a DataFusion query, write sample Parquet files with the embedded index, register the &lt;code&gt;DistinctIndexTable&lt;/code&gt; provider, and run a query with a predicate that can be optimized by the index as shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Write sample files with embedded indexes
tmp_dir.iter().for_each(|(name, vals)| {
    write_file_with_index(&amp;amp;dir.join(name), vals).unwrap();
});

// Register provider and query
let provider = Arc::new(DistinctIndexTable::try_new(dir, schema.clone())?);
ctx.register_table("t", provider)?;

// Only files containing 'foo' will be scanned
let df = ctx.sql("SELECT * FROM t WHERE category = 'foo'").await?;
df.show().await?;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Verifying Compatibility with DuckDB&lt;/h3&gt;
&lt;hr/&gt;
&lt;p&gt;Even with extra bytes and unknown metadata keys, standard Parquet readers ignore the index. You can verify this using another system such as DuckDB to read the Parquet created in the example. DuckDB will read the files without any issues, ignoring the custom index and unknown footer metadata.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT * FROM read_parquet('/tmp/parquet_index_data/*');
&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv; category &amp;boxv;
&amp;boxv; varchar  &amp;boxv;
&amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl;
&amp;boxv; foo      &amp;boxv;
&amp;boxv; bar      &amp;boxv;
&amp;boxv; foo      &amp;boxv;
&amp;boxv; baz      &amp;boxv;
&amp;boxv; qux      &amp;boxv;
&amp;boxv; foo      &amp;boxv;
&amp;boxv; quux     &amp;boxv;
&amp;boxv; quux     &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we explained how index structures are stored in Apache Parquet, how to embed user-defined indexes without changing the format, and how to use user-defined indexes to speed up query processing.&lt;/p&gt;
&lt;p&gt;Parquet-based systems can achieve significant performance improvements for almost any query pattern while still retaining broad compatibility, using user-defined embedded indexes, external indexes&lt;sup&gt;&lt;a href="#footnote4"&gt;4&lt;/a&gt;&lt;/sup&gt; and rewriting files optimized for specific queries&lt;sup&gt;&lt;a href="#footnote5"&gt;5&lt;/a&gt;&lt;/sup&gt;. System designers can choose among the available options to make the appropriate trade-offs between operational complexity, performance, file size, and cost for their specific use cases.&lt;/p&gt;
&lt;p&gt;We hope this post inspires you to explore custom indexes in Parquet files, rather than proposing new file formats and reimplementing existing features. The DataFusion community is excited to see how you use this feature in your projects!&lt;/p&gt;
&lt;h2&gt;About the Authors&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/qi-zhu-862330119/"&gt;Qi Zhu&lt;/a&gt; is a Senior Engineer at &lt;a href="https://www.cloudera.com/"&gt;Cloudera&lt;/a&gt;, an active contributor to &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;, a committer on &lt;a href="https://hadoop.apache.org/"&gt;Apache Hadoop&lt;/a&gt; and &lt;a href="https://yunikorn.apache.org/"&gt;Apache YuniKorn&lt;/a&gt;. He has extensive experience in distributed systems, scheduling, and large-scale computing.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/jigao-luo/"&gt;Jigao Luo&lt;/a&gt; is a 1.5-year PhD student at
&lt;a href="https://tuda.systems"&gt;Systems Group @ TU Darmstadt&lt;/a&gt;. Regarding Parquet, he is an external 
contributor to &lt;a href="https://github.com/rapidsai/cudf"&gt;NVIDIA RAPIDS cuDF&lt;/a&gt;, focusing on the GPU Parquet reader.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/andrewalamb/"&gt;Andrew Lamb&lt;/a&gt; is a Staff Engineer at
&lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt;, and a member of the &lt;a href="https://datafusion.apache.org/"&gt;Apache
DataFusion&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; PMCs. He has been working on
Databases and related systems more than 20 years.&lt;/p&gt;
&lt;h2&gt;About DataFusion&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; is an extensible query engine toolkit, written
in Rust, that uses &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion and
similar technology are part of the next generation &amp;ldquo;Deconstructed Database&amp;rdquo;
architectures, where new systems are built on a foundation of fast, modular
components, rather than as a single tightly integrated system.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;DataFusion community&lt;/a&gt; is always looking for new contributors to help
improve the project. If you are interested in learning more about how query
execution works, help document or improve the DataFusion codebase, or just try
it out, we would love for you to join us.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;&lt;a id="footnote1"&gt;&lt;/a&gt;&lt;code&gt;1&lt;/code&gt;: A commonly cited example is highly selective predicates (e.g. &lt;code&gt;category = 'foo'&lt;/code&gt;) but for which the built in BloomFilters are not sufficient.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote2"&gt;&lt;/a&gt;&lt;code&gt;2&lt;/code&gt;: There are other index structures, but they are either 1) not widely supported (such as statistics in the page headers) or 2) not yet widely used in practice at the time of this writing (such as &lt;a href="https://github.com/apache/parquet-format/blob/819adce0ec6aa848e56c56f20b9347f4ab50857f/src/main/thrift/parquet.thrift#L256"&gt;GeospatialStatistics&lt;/a&gt; and &lt;a href="https://github.com/apache/parquet-format/blob/819adce0ec6aa848e56c56f20b9347f4ab50857f/src/main/thrift/parquet.thrift#L194-L202"&gt;SizeStatistics&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote3"&gt;&lt;/a&gt;&lt;code&gt;3&lt;/code&gt;: &lt;a href="https://dl.gi.de/items/2a8571f8-0ef2-481c-8ee9-05f82ee258c8"&gt;Seamless Integration of Parquet Files into Data Processing. / Rey, Alice; Freitag, Michael; Neumann, Thomas. / BTW 2023&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote4"&gt;&lt;/a&gt;&lt;code&gt;4&lt;/code&gt;: For more information about external indexes, see &lt;a href="https://www.youtube.com/watch?v=74YsJT1-Rdk"&gt;this talk&lt;/a&gt; and the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs"&gt;parquet_index.rs&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs"&gt;advanced_parquet_index.rs&lt;/a&gt; examples in the DataFusion repository.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote5"&gt;&lt;/a&gt;&lt;code&gt;5&lt;/code&gt;: For information about rewriting files to optimize for specific queries, such as resorting, repartitioning, and tuning data page and row group sizes, see &lt;a href="https://github.com/XiangpengHao/liquid-cache/issues/227"&gt;XiangpengHao/liquid‑cache#227&lt;/a&gt; and the conversation between &lt;a href="https://github.com/JigaoLuo"&gt;JigaoLuo&lt;/a&gt; and &lt;a href="https://github.com/XiangpengHao"&gt;XiangpengHao&lt;/a&gt; for details. We hope to make a future post about this topic.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote6"&gt;&lt;/a&gt;&lt;code&gt;6&lt;/code&gt;: An index can also be stored inline in the key-value metadata. This approach is simple to implement and ensures the index is available once the footer is read, without additional I/O. However, it requires the index to be serialized as a UTF-8 string, which may be less efficient and increases the size of the footer metadata, impacting all Parquet readers, even those that ignore the index.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion 47.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/07/11/datafusion-47.0.0" rel="alternate"></link><published>2025-07-11T00:00:00+00:00</published><updated>2025-07-11T00:00:00+00:00</updated><author><name>PMC</name></author><id>tag:datafusion.apache.org,2025-07-11:/blog/2025/07/11/datafusion-47.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/16347 for details --&gt;
&lt;p&gt;We&amp;rsquo;re excited to announce the release of &lt;strong&gt;Apache DataFusion 47.0.0&lt;/strong&gt;! This new version represents a significant
milestone for the project, packing in a wide range of improvements and fixes. You can find the complete details in the
full &lt;a href="https://github.com/apache/datafusion/blob/branch-47/dev/changelog/47.0.0.md"&gt;changelog&lt;/a&gt;. We&amp;rsquo;ll highlight the most
important changes below …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/16347 for details --&gt;
&lt;p&gt;We&amp;rsquo;re excited to announce the release of &lt;strong&gt;Apache DataFusion 47.0.0&lt;/strong&gt;! This new version represents a significant
milestone for the project, packing in a wide range of improvements and fixes. You can find the complete details in the
full &lt;a href="https://github.com/apache/datafusion/blob/branch-47/dev/changelog/47.0.0.md"&gt;changelog&lt;/a&gt;. We&amp;rsquo;ll highlight the most
important changes below and guide you through upgrading.&lt;/p&gt;
&lt;p&gt;Note that DataFusion 47.0.0 was released in April 2025, but we are only now publishing the blog post due to 
limited bandwidth in the DataFusion community. We apologize for the delay and encourage you to come help us
accelerate the next release and announcements 
by &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;joining the community&lt;/a&gt;  🎣.&lt;/p&gt;
&lt;h2&gt;Breaking Changes&lt;/h2&gt;
&lt;p&gt;DataFusion 47.0.0 brings a few &lt;strong&gt;breaking changes&lt;/strong&gt; that may require adjustments to your code as described in
the &lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html#datafusion-47-0-0"&gt;Upgrade Guide&lt;/a&gt;. Here are some notable ones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/datafusion/pull/15466"&gt;Upgrades to arrow-rs and arrow-parquet 55.0.0 and object_store 0.12.0&lt;/a&gt;:
  Several APIs changed in the underlying &lt;code&gt;arrow&lt;/code&gt;, &lt;code&gt;parquet&lt;/code&gt; and &lt;code&gt;object_store&lt;/code&gt; libraries to use a &lt;code&gt;u64&lt;/code&gt; instead of usize to better support
  WASM. This requires converting from &lt;code&gt;usize&lt;/code&gt; to &lt;code&gt;u64&lt;/code&gt; occasionally as well as changes to ObjectStore implementations such as&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class="language-Rust"&gt;impl ObjectStore {
    ...

    // The range is now a u64 instead of usize
    async fn get_range(&amp;amp;self, location: &amp;amp;Path, range: Range&amp;lt;u64&amp;gt;) -&amp;gt; ObjectStoreResult&amp;lt;Bytes&amp;gt; {
        self.inner.get_range(location, range).await
    }

    ...

    // the lifetime is now 'static instead of '_ (meaning the captured closure can't contain references)
    // (this also applies to list_with_offset)
    fn list(&amp;amp;self, prefix: Option&amp;lt;&amp;amp;Path&amp;gt;) -&amp;gt; BoxStream&amp;lt;'static, ObjectStoreResult&amp;lt;ObjectMeta&amp;gt;&amp;gt; {
        self.inner.list(prefix)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/datafusion/issues/14914"&gt;DisplayFormatType::TreeRender&lt;/a&gt;:
  Implementations of &lt;code&gt;ExecutionPlan&lt;/code&gt; must also provide a description in the &lt;code&gt;DisplayFormatType::TreeRender&lt;/code&gt; format to
  provide support for the new &lt;a href="https://datafusion.apache.org/user-guide/sql/explain.html#tree-format-default"&gt;tree style explains&lt;/a&gt;.
  This can be the same as the existing &lt;code&gt;DisplayFormatType::Default&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Performance Improvements&lt;/h2&gt;
&lt;p&gt;DataFusion 47.0.0 comes with numerous performance enhancements across the board. Here are some of the noteworthy
optimizations in this release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;FIRST_VALUE&lt;/code&gt; and &lt;code&gt;LAST_VALUE&lt;/code&gt;:&lt;/strong&gt; &lt;code&gt;FIRST_VALUE&lt;/code&gt; and &lt;code&gt;LAST_VALUE&lt;/code&gt; functions execute much faster for data with high cardinality such as those with many groups or partitions. DataFusion 47.0.0 executes the following in &lt;strong&gt;7 seconds&lt;/strong&gt; compared to &lt;strong&gt;36 seconds&lt;/strong&gt; in DataFusion 46.0.0: &lt;code&gt;select id2, id4, first_value(v1 order by id2, id4) as r2 from '~/h2o_100m.parquet' group by id2, id4&lt;/code&gt; (h2o.ai dataset). (PR's &lt;a href="https://github.com/apache/datafusion/pull/15266"&gt;#15266&lt;/a&gt;
  and &lt;a href="https://github.com/apache/datafusion/pull/15542"&gt;#15542&lt;/a&gt; by &lt;a href="https://github.com/UBarney"&gt;UBarney&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt; and &lt;code&gt;AVG&lt;/code&gt; for Durations:&lt;/strong&gt;  DataFusion executes aggregate queries up to 2.5x faster when they include &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt; and &lt;code&gt;AVG&lt;/code&gt; on &lt;code&gt;Duration&lt;/code&gt; columns. 
  (PRs &lt;a href="https://github.com/apache/datafusion/pull/15322"&gt;#15322&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/pull/15748"&gt;#15748&lt;/a&gt;
  by &lt;a href="https://github.com/shruti2522"&gt;shruti2522&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Short circuit evaluation for &lt;code&gt;AND&lt;/code&gt; and &lt;code&gt;OR&lt;/code&gt;:&lt;/strong&gt; DataFusion now eagerly skips the evaluation of
  the right operand if the left is known to be false (&lt;code&gt;AND&lt;/code&gt;) or true (&lt;code&gt;OR&lt;/code&gt;) in certain cases. For complex predicates, such as those with many &lt;code&gt;LIKE&lt;/code&gt; or &lt;code&gt;CASE&lt;/code&gt; expressions, this optimization results in
  &lt;a href="https://github.com/apache/datafusion/issues/11212#issuecomment-2753584617"&gt;significant performance improvements&lt;/a&gt; (up to 100x in extreme cases).
  (PRs &lt;a href="https://github.com/apache/datafusion/pull/15462"&gt;#15462&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/pull/15694"&gt;#15694&lt;/a&gt;
  by &lt;a href="https://github.com/acking-you"&gt;acking-you&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;TopK optimization for partially sorted input:&lt;/strong&gt; Previous versions of DataFusion implemented early termination
  optimization (TopK) for fully sorted data. DataFusion 47.0.0 extends the optimization for partially sorted data, which is common in many real-world datasets, such as time-series data sorted by day but not within each day. 
  (PR &lt;a href="https://github.com/apache/datafusion/pull/15563"&gt;#15563&lt;/a&gt; by &lt;a href="https://github.com/geoffreyclaude"&gt;geoffreyclaude&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Disable re-validation of spilled files:&lt;/strong&gt; DataFusion no longer does unnecessary re-validation of temporary spill files. The validation is unnecessary and expensive as the data is known to be valid when it was written out
  (PR &lt;a href="https://github.com/apache/datafusion/pull/15454"&gt;#15454&lt;/a&gt; by &lt;a href="https://github.com/zebsme"&gt;zebsme&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Highlighted New Features&lt;/h2&gt;
&lt;h3&gt;Tree style explains&lt;/h3&gt;
&lt;p&gt;In previous releases the &lt;a href="https://datafusion.apache.org/user-guide/sql/explain.html"&gt;EXPLAIN statement&lt;/a&gt; results in a formatted table
which is succinct and contains important details for implementers, but was often hard to read
especially with queries that included joins or unions having multiple children.&lt;/p&gt;
&lt;p&gt;DataFusion 47.0.0 includes the new &lt;code&gt;EXPLAIN FORMAT TREE&lt;/code&gt; (default in
&lt;code&gt;datafusion-cli&lt;/code&gt;) rendered in a visual tree style that is much easier to quickly
understand.&lt;/p&gt;
&lt;!-- SQL setup 
create table t1(ti int) as values (1), (2), (3);
create table t2(ti int) as values (1), (2), (3);
--&gt;
&lt;p&gt;Example of the new explain output:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;&amp;gt; explain select * from t1 inner join t2 on t1.ti=t2.ti;
+---------------+------------------------------------------------------------+
| plan_type     | plan                                                       |
+---------------+------------------------------------------------------------+
| physical_plan | &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;                              |
|               | &amp;boxv;    CoalesceBatchesExec    &amp;boxv;                              |
|               | &amp;boxv;    --------------------   &amp;boxv;                              |
|               | &amp;boxv;     target_batch_size:    &amp;boxv;                              |
|               | &amp;boxv;            8192           &amp;boxv;                              |
|               | &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;                              |
|               | &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;                              |
|               | &amp;boxv;        HashJoinExec       &amp;boxv;                              |
|               | &amp;boxv;    --------------------   &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;               |
|               | &amp;boxv;       on: (ti = ti)       &amp;boxv;              &amp;boxv;               |
|               | &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;              &amp;boxv;               |
|               | &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; |
|               | &amp;boxv;       DataSourceExec      &amp;boxv;&amp;boxv;       DataSourceExec      &amp;boxv; |
|               | &amp;boxv;    --------------------   &amp;boxv;&amp;boxv;    --------------------   &amp;boxv; |
|               | &amp;boxv;         bytes: 112        &amp;boxv;&amp;boxv;         bytes: 112        &amp;boxv; |
|               | &amp;boxv;       format: memory      &amp;boxv;&amp;boxv;       format: memory      &amp;boxv; |
|               | &amp;boxv;          rows: 1          &amp;boxv;&amp;boxv;          rows: 1          &amp;boxv; |
|               | &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; |
|               |                                                            |
+---------------+------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Example of the &lt;code&gt;EXPLAIN FORMAT INDENT&lt;/code&gt; output for the same query&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;&amp;gt; explain format indent select * from t1 inner join t2 on t1.ti=t2.ti;
+---------------+----------------------------------------------------------------------+
| plan_type     | plan                                                                 |
+---------------+----------------------------------------------------------------------+
| logical_plan  | Inner Join: t1.ti = t2.ti                                            |
|               |   TableScan: t1 projection=[ti]                                      |
|               |   TableScan: t2 projection=[ti]                                      |
| physical_plan | CoalesceBatchesExec: target_batch_size=8192                          |
|               |   HashJoinExec: mode=CollectLeft, join_type=Inner, on=[(ti@0, ti@0)] |
|               |     DataSourceExec: partitions=1, partition_sizes=[1]                |
|               |     DataSourceExec: partitions=1, partition_sizes=[1]                |
|               |                                                                      |
+---------------+----------------------------------------------------------------------+
2 row(s) fetched.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/irenjj"&gt;irenjj&lt;/a&gt; for the initial work in PR &lt;a href="https://github.com/apache/datafusion/pull/14677"&gt;#14677&lt;/a&gt;
and many others for completing the &lt;a href="https://github.com/apache/datafusion/issues/14914"&gt;followup epic&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;SQL &lt;code&gt;VARCHAR&lt;/code&gt; defaults to Utf8View&lt;/h3&gt;
&lt;p&gt;In previous releases when a column was created in SQL the column would be mapped to the &lt;a href="https://docs.rs/arrow/latest/arrow/datatypes/enum.DataType.html#variant.Utf8"&gt;Utf8 Arrow data type&lt;/a&gt;. In this release
the SQL &lt;code&gt;varchar&lt;/code&gt; columns will be mapped to the &lt;a href="https://docs.rs/arrow/latest/arrow/datatypes/enum.DataType.html#variant.Utf8View"&gt;Utf8View arrow data type&lt;/a&gt; by default, which is a more efficient representation of UTF-8 strings in Arrow.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;create table foo(x varchar);
0 row(s) fetched.

&amp;gt; describe foo;
+-------------+-----------+-------------+
| column_name | data_type | is_nullable |
+-------------+-----------+-------------+
| x           | Utf8View  | YES         |
+-------------+-----------+-------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Previous versions of DataFusion used &lt;code&gt;Utf8View&lt;/code&gt; when reading parquet files and it is faster in most cases.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/zhuqi-lucas"&gt;zhuqi-lucas&lt;/a&gt; for PR &lt;a href="https://github.com/apache/datafusion/pull/15104"&gt;#15104&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Context propagation in spawned tasks (for tracing, logging, etc.)&lt;/h3&gt;
&lt;p&gt;This release introduces an API for propagating user-defined context (such as tracing spans,
logging, or metrics) across thread boundaries without depending on any specific instrumentation library.
You can use the &lt;a href="https://docs.rs/datafusion/latest/datafusion/common/runtime/trait.JoinSetTracer.html"&gt;JoinSetTracer&lt;/a&gt; API to instrument DataFusion plans with your own tracing or logging libraries, or
use pre-integrated community crates such as the &lt;a href="https://github.com/datafusion-contrib/datafusion-tracing"&gt;datafusion-tracing&lt;/a&gt; crate.&lt;/p&gt;
&lt;div style="text-align: center;"&gt;
&lt;a href="https://github.com/datafusion-contrib/datafusion-tracing"&gt;
&lt;img alt="DataFusion telemetry project logo" class="img-responsive" src="/blog/images/datafusion-47.0.0/datafusion-telemetry.png" width="50%"/&gt;
&lt;/a&gt;
&lt;/div&gt;
&lt;p&gt;Previously, tasks spawned on new threads &amp;mdash; such as those performing
repartitioning or Parquet file reads &amp;mdash; could lose thread-local context, which is
often used in instrumentation libraries. A full example of how to use this new
API is available in the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/tracing.rs"&gt;DataFusion examples&lt;/a&gt;, and a simple example is shown below.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-Rust"&gt;/// Models a simple tracer. Calling `in_current_span()` and `in_scope()` saves thread-specific state
/// for the current span and must be called at the start of each new task or thread.
struct SpanTracer;

/// Implements the `JoinSetTracer` trait so we can inject instrumentation
/// for both async futures and blocking closures.
impl JoinSetTracer for SpanTracer {
    /// Instruments a boxed future to run in the current span. The future's
    /// return type is erased to `Box&amp;lt;dyn Any + Send&amp;gt;`, which we simply
    /// run inside the `Span::current()` context.
    fn trace_future(
        &amp;amp;self,
        fut: BoxFuture&amp;lt;'static, Box&amp;lt;dyn Any + Send&amp;gt;&amp;gt;,
    ) -&amp;gt; BoxFuture&amp;lt;'static, Box&amp;lt;dyn Any + Send&amp;gt;&amp;gt; {
        // Ensures any thread-local context is set in this future 
        fut.in_current_span().boxed()
    }

    /// Instruments a boxed blocking closure by running it inside the
    /// `Span::current()` context.
    fn trace_block(
        &amp;amp;self,
        f: Box&amp;lt;dyn FnOnce() -&amp;gt; Box&amp;lt;dyn Any + Send&amp;gt; + Send&amp;gt;,
    ) -&amp;gt; Box&amp;lt;dyn FnOnce() -&amp;gt; Box&amp;lt;dyn Any + Send&amp;gt; + Send&amp;gt; {
        let span = Span::current();
        // Ensures any thread-local context is set for this closure
        Box::new(move || span.in_scope(f))
    }
}

...
set_join_set_tracer(&amp;amp;SpanTracer).expect("Failed to set tracer");
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/geoffreyclaude"&gt;geoffreyclaude&lt;/a&gt; for PR &lt;a href="https://github.com/apache/datafusion/issues/14914"&gt;#14914&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Upgrade Guide and Changelog&lt;/h2&gt;
&lt;p&gt;Upgrading to 47.0.0 should be straightforward for most users, but do review
the &lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html#datafusion-47-0-0"&gt;Upgrade Guide for DataFusion 47.0.0&lt;/a&gt; for detailed
steps and code changes. The upgrade guide covers the breaking changes mentioned above and provides code snippets to help with the
transition. For a comprehensive list of all changes, please refer to the &lt;a href="https://github.com/apache/datafusion/blob/branch-47/dev/changelog/47.0.0.md"&gt;changelog&lt;/a&gt; for 47.0.0. The changelog
enumerates every merged PR in this release, including many smaller fixes and improvements that we couldn&amp;rsquo;t cover in this post.&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;Apache DataFusion is an open-source project, and we welcome involvement from anyone interested. Now is a great time to
take 47.0.0 for a spin: try it out on your workloads, and let us know if you encounter any issues or have suggestions.
You can report bugs or request features on our GitHub issue tracker, or better yet, submit a pull request. Join our
community discussions &amp;ndash; whether you have questions, want to share how you&amp;rsquo;re using DataFusion, or are looking to
contribute, we&amp;rsquo;d love to hear from you. A list of open issues suitable for beginners
is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt; and you
can find how to reach us on the &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication doc&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy querying!&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.9.0 Release</title><link href="https://datafusion.apache.org/blog/2025/07/01/datafusion-comet-0.9.0" rel="alternate"></link><published>2025-07-01T00:00:00+00:00</published><updated>2025-07-01T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-07-01:/blog/2025/07/01/datafusion-comet-0.9.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.9.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;This release covers approximately ten weeks of development …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.9.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;This release covers approximately ten weeks of development work and is the result of merging 139 PRs from 24
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.9.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Complex Type Support in Parquet Scans&lt;/h3&gt;
&lt;p&gt;Comet now supports complex types (Structs, Maps, and Arrays) when reading Parquet files. This functionality is not
yet available when reading Parquet files from Apache Iceberg.&lt;/p&gt;
&lt;p&gt;This functionality was only available in previous releases when manually specifying one of the new experimental
scan implementations. Comet now automatically chooses the best scan implementation based on the input schema, and no
longer requires manual configuration.&lt;/p&gt;
&lt;h3&gt;Complex Type Processing Improvements&lt;/h3&gt;
&lt;p&gt;Numerous improvements have been made to complex type support to ensure Spark-compatible behavior when casting between
structs and accessing fields within deeply nested types.&lt;/p&gt;
&lt;h3&gt;Shuffle Improvements&lt;/h3&gt;
&lt;p&gt;Comet now accelerates a broader range of shuffle operations, leading to more queries running fully natively. In
previous releases, some shuffle operations fell back to Spark to avoid some known bugs in Comet, and these bugs have
now been fixed.&lt;/p&gt;
&lt;h3&gt;New Features&lt;/h3&gt;
&lt;p&gt;Comet 0.9.0 adds support for the following Spark expressions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ArrayDistinct&lt;/li&gt;
&lt;li&gt;ArrayMax&lt;/li&gt;
&lt;li&gt;ArrayRepeat&lt;/li&gt;
&lt;li&gt;ArrayUnion&lt;/li&gt;
&lt;li&gt;BitCount&lt;/li&gt;
&lt;li&gt;BitNot&lt;/li&gt;
&lt;li&gt;Expm1&lt;/li&gt;
&lt;li&gt;MapValues&lt;/li&gt;
&lt;li&gt;Signum&lt;/li&gt;
&lt;li&gt;ToPrettyString&lt;/li&gt;
&lt;li&gt;map[]&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Improved Spark SQL Test Coverage&lt;/h3&gt;
&lt;p&gt;Comet now passes 97% of the Spark SQL test suite, with more than 24,000 tests passing (based on testing against
Spark 3.5.6). The remaining 3% of tests are ignored for various reasons, such as being too specific to Spark 
internals, or testing for features that are not relevant to Comet, such as whole-stage code generation, which
is not needed when using a vectorized execution engine.&lt;/p&gt;
&lt;p&gt;This release contains numerous bug fixes to achieve this coverage, including improved support for exchange reuse
when AQE is enabled.&lt;/p&gt;
&lt;style&gt;
  table {
    border-collapse: collapse;
    width: 100%;
    font-family: sans-serif;
  }

  th, td {
    text-align: left;
    padding: 8px 12px;
  }

  th {
    background-color: #f2f2f2;
    font-weight: bold;
  }

  td {
    border-bottom: 1px solid #ddd;
  }

  tbody tr:last-child td {
    font-weight: bold;
    border-top: 2px solid #000;
  }
&lt;/style&gt;
&lt;table class="table"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Module&lt;/th&gt;
&lt;th&gt;Passed&lt;/th&gt;
&lt;th&gt;Ignored&lt;/th&gt;
&lt;th&gt;Canceled&lt;/th&gt;
&lt;th&gt;Total&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;catalyst&lt;/td&gt;&lt;td&gt;7,232&lt;/td&gt;&lt;td&gt;5&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;7,238&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;core-1&lt;/td&gt;&lt;td&gt;9,186&lt;/td&gt;&lt;td&gt;246&lt;/td&gt;&lt;td&gt;6&lt;/td&gt;&lt;td&gt;9,438&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;core-2&lt;/td&gt;&lt;td&gt;2,649&lt;/td&gt;&lt;td&gt;393&lt;/td&gt;&lt;td&gt;0&lt;/td&gt;&lt;td&gt;3,042&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;core-3&lt;/td&gt;&lt;td&gt;1,757&lt;/td&gt;&lt;td&gt;136&lt;/td&gt;&lt;td&gt;16&lt;/td&gt;&lt;td&gt;1,909&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;hive-1&lt;/td&gt;&lt;td&gt;2,174&lt;/td&gt;&lt;td&gt;14&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;2,192&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;hive-2&lt;/td&gt;&lt;td&gt;19&lt;/td&gt;&lt;td&gt;1&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;24&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;hive-3&lt;/td&gt;&lt;td&gt;1,058&lt;/td&gt;&lt;td&gt;11&lt;/td&gt;&lt;td&gt;4&lt;/td&gt;&lt;td&gt;1,073&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;24,075&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;806&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;31&lt;/strong&gt;&lt;/td&gt;&lt;td&gt;&lt;strong&gt;24,912&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Memory &amp;amp; Performance Tracing&lt;/h3&gt;
&lt;p&gt;Comet now provides a tracing feature for analyzing performance and off-heap versus on-heap memory usage. See the
&lt;a href="https://datafusion.apache.org/comet/contributor-guide/tracing.html"&gt;Comet Tracing Guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Comet Tracing" class="img-responsive" src="/blog/images/comet-0.9.0/tracing.png" width="100%"/&gt;&lt;/p&gt;
&lt;h3&gt;Spark Compatibility&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Spark 3.4.3 with JDK 11 &amp;amp; 17, Scala 2.12 &amp;amp; 2.13&lt;/li&gt;
&lt;li&gt;Spark 3.5.4 through 3.5.6 with JDK 11 &amp;amp; 17, Scala 2.12 &amp;amp; 2.13&lt;/li&gt;
&lt;li&gt;Experimental support for Spark 4.0.0 with JDK 17, Scala 2.13&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are looking for help from the community to fully support Spark 4.0.0. See &lt;a href="https://github.com/apache/datafusion-comet/issues/1637"&gt;EPIC: Support 4.0.0&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;Note that Java 8 support was removed from this release because Apache Arrow no longer supports it.&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Using Rust async for Query Execution and Cancelling Long-Running Queries</title><link href="https://datafusion.apache.org/blog/2025/06/30/cancellation" rel="alternate"></link><published>2025-06-30T00:00:00+00:00</published><updated>2025-06-30T00:00:00+00:00</updated><author><name>Pepijn Van Eeckhoudt</name></author><id>tag:datafusion.apache.org,2025-06-30:/blog/2025/06/30/cancellation</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;style&gt;
figure {
  margin: 20px 0;
}

figure img {
  display: block;
  max-width: 80%;
  margin: auto;
}

figcaption {
  font-style: italic;
  color: #555;
  font-size: 0.9em;
  max-width: 80%;
  margin: auto;
  text-align: center;
}
&lt;/style&gt;
&lt;p&gt;Have you ever tried to cancel a query that just wouldn't stop?
In this post, we'll review how Rust's &lt;a href="https://doc.rust-lang.org/book/ch17-00-async-await.html"&gt;&lt;code&gt;async&lt;/code&gt; programming model&lt;/a&gt; works, how …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;style&gt;
figure {
  margin: 20px 0;
}

figure img {
  display: block;
  max-width: 80%;
  margin: auto;
}

figcaption {
  font-style: italic;
  color: #555;
  font-size: 0.9em;
  max-width: 80%;
  margin: auto;
  text-align: center;
}
&lt;/style&gt;
&lt;p&gt;Have you ever tried to cancel a query that just wouldn't stop?
In this post, we'll review how Rust's &lt;a href="https://doc.rust-lang.org/book/ch17-00-async-await.html"&gt;&lt;code&gt;async&lt;/code&gt; programming model&lt;/a&gt; works, how &lt;a href="https://datafusion.apache.org/"&gt;DataFusion&lt;/a&gt; uses that model for CPU intensive tasks, and how this is used to cancel queries.
Then we'll review some cases where queries could not be canceled in DataFusion and what the community did to resolve the problem.&lt;/p&gt;
&lt;h2&gt;Understanding Rust's Async Model&lt;/h2&gt;
&lt;p&gt;DataFusion, somewhat unconventionally, &lt;a href="https://docs.rs/datafusion/latest/datafusion/#thread-scheduling-cpu--io-thread-pools-and-tokio-runtimes"&gt;uses the Rust async system and the Tokio task scheduler&lt;/a&gt; for CPU intensive processing.
To really understand the cancellation problem you first need to be familiar with Rust's asynchronous programming model which is a bit different from what you might be used to from other ecosystems.
Let's go over the basics again as a refresher.
If you're familiar with the ins and outs of &lt;code&gt;Future&lt;/code&gt; and &lt;code&gt;async&lt;/code&gt; you can skip this section.&lt;/p&gt;
&lt;h3&gt;Futures Are Inert&lt;/h3&gt;
&lt;p&gt;Rust's asynchronous programming model is built around the &lt;a href="https://doc.rust-lang.org/std/future/trait.Future.html"&gt;&lt;code&gt;Future&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt; trait.
In contrast to, for instance, Javascript's &lt;a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise"&gt;&lt;code&gt;Promise&lt;/code&gt;&lt;/a&gt; or Java's &lt;a href="https://docs.oracle.com/en/java/javase/21/docs/api/java.base/java/util/concurrent/Future.html"&gt;&lt;code&gt;Future&lt;/code&gt;&lt;/a&gt; a Rust &lt;code&gt;Future&lt;/code&gt; does not necessarily represent an actively running asynchronous job.
Instead, a &lt;code&gt;Future&amp;lt;T&amp;gt;&lt;/code&gt; represents a lazy calculation that only makes progress when explicitly asked to do so.
This is done by calling the &lt;a href="https://doc.rust-lang.org/std/future/trait.Future.html#tymethod.poll"&gt;&lt;code&gt;poll&lt;/code&gt;&lt;/a&gt; method of a &lt;code&gt;Future&lt;/code&gt;.
If nobody polls a &lt;code&gt;Future&lt;/code&gt; explicitly, it is &lt;a href="https://doc.rust-lang.org/std/future/trait.Future.html#runtime-characteristics"&gt;an inert object&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Calling &lt;code&gt;Future::poll&lt;/code&gt; results in one of two options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://doc.rust-lang.org/std/task/enum.Poll.html#variant.Pending"&gt;&lt;code&gt;Poll::Pending&lt;/code&gt;&lt;/a&gt; if the evaluation is not yet complete, most often because it needs to wait for something like I/O before it can continue&lt;/li&gt;
&lt;li&gt;&lt;a href="https://doc.rust-lang.org/std/task/enum.Poll.html#variant.Ready"&gt;&lt;code&gt;Poll::Ready&amp;lt;T&amp;gt;&lt;/code&gt;&lt;/a&gt; when it has completed and produced a value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a &lt;code&gt;Future&lt;/code&gt; returns &lt;code&gt;Pending&lt;/code&gt;, it saves its internal state so it can pick up where it left off the next time you poll it.
This internal state management makes Rust's &lt;code&gt;Future&lt;/code&gt;s memory-efficient and composable.
Rather than freezing the full call stack leading to a certain point, only the relevant state to resume the future needs to be retained.&lt;/p&gt;
&lt;p&gt;Additionally, a &lt;code&gt;Future&lt;/code&gt; must set up the necessary signaling to notify the caller when it should call &lt;code&gt;poll&lt;/code&gt; again, to avoid a busy-waiting loop.
This is done using a &lt;a href="https://doc.rust-lang.org/std/task/struct.Waker.html"&gt;&lt;code&gt;Waker&lt;/code&gt;&lt;/a&gt; which the &lt;code&gt;Future&lt;/code&gt; receives via the &lt;code&gt;Context&lt;/code&gt; parameter of the &lt;code&gt;poll&lt;/code&gt; function. &lt;/p&gt;
&lt;p&gt;Manual implementations of &lt;code&gt;Future&lt;/code&gt; are most often little finite state machines.
Each state in the process of completing the calculation is modeled as a variant of an &lt;code&gt;enum&lt;/code&gt;.
Before a &lt;code&gt;Future&lt;/code&gt; returns &lt;code&gt;Pending&lt;/code&gt;, it bundles the data required to resume in an enum variant, stores that enum variant in itself, and then returns.
While compact and efficient, the resulting code is often quite verbose.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;async&lt;/code&gt; keyword was introduced to make life easier on Rust programmers.
It provides elegant syntactic sugar for the manual state machine &lt;code&gt;Future&lt;/code&gt; approach.
When you write an &lt;code&gt;async&lt;/code&gt; function or block, the compiler transforms linear code into a state machine based &lt;code&gt;Future&lt;/code&gt; similar to the one described above for you.
Since all the state management is compiler generated and hidden from sight, async code tends to be easier to write initially, more readable afterward, while maintaining the same underlying mechanics.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;await&lt;/code&gt; keyword complements &lt;code&gt;async&lt;/code&gt; pausing execution until a &lt;code&gt;Future&lt;/code&gt; completes. &lt;br/&gt;
When you &lt;code&gt;.await&lt;/code&gt; a &lt;code&gt;Future&lt;/code&gt;, you're essentially telling the compiler to generate code that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Polls the &lt;code&gt;Future&lt;/code&gt; with the current (implicit) asynchronous context&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;poll&lt;/code&gt; returns &lt;code&gt;Poll::Pending&lt;/code&gt;, save the state of the &lt;code&gt;Future&lt;/code&gt; so that it can resume at this point and return &lt;code&gt;Poll::Pending&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;If it returns &lt;code&gt;Poll::Ready(value)&lt;/code&gt;, continue execution with that value&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;From Futures to Streams&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://docs.rs/futures/latest/futures/"&gt;&lt;code&gt;futures&lt;/code&gt;&lt;/a&gt; crate extends the &lt;code&gt;Future&lt;/code&gt; model with a trait named &lt;a href="https://docs.rs/futures/latest/futures/prelude/trait.Stream.html"&gt;&lt;code&gt;Stream&lt;/code&gt;&lt;/a&gt;.
&lt;code&gt;Stream&amp;lt;Item = T&amp;gt;&lt;/code&gt; represents a sequence of values that are each produced asynchronously rather than just a single value.
It's the asynchronous equivalent of &lt;code&gt;Iterator&amp;lt;Item = T&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Stream&lt;/code&gt; trait has one method named &lt;a href="https://docs.rs/futures/latest/futures/prelude/trait.Stream.html#tymethod.poll_next"&gt;&lt;code&gt;poll_next&lt;/code&gt;&lt;/a&gt; that returns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Poll::Pending&lt;/code&gt; when the next value isn't ready yet, just like a &lt;code&gt;Future&lt;/code&gt; would&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Poll::Ready(Some(value))&lt;/code&gt; when a new value is available&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Poll::Ready(None)&lt;/code&gt; when the stream is exhausted&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Under the hood, an implementation of &lt;code&gt;Stream&lt;/code&gt; is very similar to a &lt;code&gt;Future&lt;/code&gt;.
Typically, they're also implemented as state machines, the main difference being that they produce multiple values rather than just one.
Just like &lt;code&gt;Future&lt;/code&gt;, a &lt;code&gt;Stream&lt;/code&gt; is inert unless explicitly polled.&lt;/p&gt;
&lt;p&gt;Now that we understand the basics of Rust's async model, let's see how DataFusion leverages these concepts to execute queries.&lt;/p&gt;
&lt;h2&gt;How DataFusion Executes Queries&lt;/h2&gt;
&lt;p&gt;In DataFusion, the short version of how queries are executed is as follows (you can find more in-depth coverage of this in the &lt;a href="https://docs.rs/datafusion/latest/datafusion/#streaming-execution"&gt;DataFusion documentation&lt;/a&gt;):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;First the query is compiled into a tree of &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html"&gt;&lt;code&gt;ExecutionPlan&lt;/code&gt;&lt;/a&gt; nodes&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html#tymethod.execute"&gt;&lt;code&gt;ExecutionPlan::execute&lt;/code&gt;&lt;/a&gt; is called on the root of the tree. &lt;/li&gt;
&lt;li&gt;This method returns a &lt;a href="https://docs.rs/datafusion/latest/datafusion/execution/type.SendableRecordBatchStream.html"&gt;&lt;code&gt;SendableRecordBatchStream&lt;/code&gt;&lt;/a&gt; (a pinned &lt;code&gt;Box&amp;lt;dyn Stream&amp;lt;RecordBatch&amp;gt;&amp;gt;&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Stream::poll_next&lt;/code&gt; is called in a loop to get the results&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In other words, the execution of a DataFusion query boils down to polling an asynchronous stream.
Like all &lt;code&gt;Stream&lt;/code&gt; implementations, we need to explicitly poll the stream for the query to make progress. &lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Stream&lt;/code&gt; we get in step 2 is actually the root of a tree of &lt;code&gt;Streams&lt;/code&gt; that mostly mirrors the execution plan tree.
Each stream tree node processes the record batches it gets from its children.
The leaves of the tree produce record batches themselves.&lt;/p&gt;
&lt;p&gt;Query execution progresses each time you call &lt;code&gt;poll_next&lt;/code&gt; on the root stream.
This call typically cascades down the tree, with each node calling &lt;code&gt;poll_next&lt;/code&gt; on its children to get the data it needs to process.&lt;/p&gt;
&lt;p&gt;Here's where the first signs of problems start to show up: some operations (like aggregations, sorts, or certain join phases) need to process a lot of data before producing any output.
When &lt;code&gt;poll_next&lt;/code&gt; encounters one of these operations, it might require substantial work before it can return a record batch.&lt;/p&gt;
&lt;h3&gt;Tokio and Cooperative Scheduling&lt;/h3&gt;
&lt;p&gt;We need to make a small detour now via Tokio's scheduler before we can get to the query cancellation problem.
DataFusion makes use of the &lt;a href="https://tokio.rs"&gt;Tokio asynchronous runtime&lt;/a&gt;, which uses a &lt;a href="https://docs.rs/tokio/latest/tokio/task/index.html#what-are-tasks"&gt;cooperative scheduling model&lt;/a&gt;.
This is fundamentally different from preemptive scheduling that you might be used to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In &lt;strong&gt;preemptive scheduling&lt;/strong&gt;, the system can interrupt a task at any time to run something else&lt;/li&gt;
&lt;li&gt;In &lt;strong&gt;cooperative scheduling&lt;/strong&gt;, tasks must voluntarily yield control back to the scheduler&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This distinction is crucial for understanding our cancellation problem.&lt;/p&gt;
&lt;p&gt;A task in Tokio is modeled as a &lt;code&gt;Future&lt;/code&gt; which is passed to one of the task initiation functions like &lt;a href="https://docs.rs/tokio/latest/tokio/task/fn.spawn.html"&gt;&lt;code&gt;spawn&lt;/code&gt;&lt;/a&gt;.
Tokio runs the task by calling &lt;code&gt;Future::poll&lt;/code&gt; in a loop until it returns &lt;code&gt;Poll::Ready&lt;/code&gt;.
While that &lt;code&gt;Future::poll&lt;/code&gt; call is running, Tokio has no way to forcibly interrupt it.
It must cooperate by periodically yielding control, either by returning &lt;code&gt;Poll::Pending&lt;/code&gt; or &lt;code&gt;Poll::Ready&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Similarly, when you try to abort a task by calling &lt;a href="https://docs.rs/tokio/latest/tokio/task/struct.JoinHandle.html#method.abort"&gt;&lt;code&gt;JoinHandle::abort()&lt;/code&gt;&lt;/a&gt;, the Tokio runtime can't immediately force it to stop.
You're just telling Tokio: "When this task next yields control, don't call &lt;code&gt;Future::poll&lt;/code&gt; anymore."
If the task never yields, it can't be aborted.&lt;/p&gt;
&lt;h3&gt;The Cancellation Problem&lt;/h3&gt;
&lt;p&gt;With all the necessary background in place, now let's look at how the DataFusion CLI tries to run and cancel a query.
The code below is a simplified version of &lt;a href="https://github.com/apache/datafusion/blob/db13dd93579945628cd81d534c032f5e6cc77967/datafusion-cli/src/exec.rs#L179-L186"&gt;what the CLI actually does&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;fn exec_query() {
    let runtime: tokio::runtime::Runtime = ...;
    let stream: SendableRecordBatchStream = ...;

    runtime.block_on(async {
        tokio::select! {
            next_batch = stream.next() =&amp;gt; ...
            _ = signal::ctrl_c() =&amp;gt; ...,
        }
    })
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;First the CLI sets up a Tokio runtime instance.
It then reads the query to execute from standard input or file and turns it into a &lt;code&gt;Stream&lt;/code&gt;.
Then it calls &lt;code&gt;next&lt;/code&gt; on stream which is an &lt;code&gt;async&lt;/code&gt; wrapper for &lt;code&gt;poll_next&lt;/code&gt;.
It passes this to the &lt;a href="https://docs.rs/tokio/latest/tokio/macro.select.html"&gt;&lt;code&gt;select!&lt;/code&gt;&lt;/a&gt; macro along with a ctrl-C handler.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;select!&lt;/code&gt; macro races these two &lt;code&gt;Future&lt;/code&gt;s and completes when either one finishes.
The intent is that when you press Ctrl+C, the &lt;code&gt;signal::ctrl_c()&lt;/code&gt; &lt;code&gt;Future&lt;/code&gt; should complete.
The &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html#cancellation--aborting-execution"&gt;stream is cancelled&lt;/a&gt; when it is dropped as it is inert by itself and nothing will be able to call &lt;code&gt;poll_next&lt;/code&gt; again.&lt;/p&gt;
&lt;p&gt;But there's a catch: &lt;code&gt;select!&lt;/code&gt; still follows cooperative scheduling rules.
It polls each &lt;code&gt;Future&lt;/code&gt; in sequence, and if the first one (our query) gets stuck in a long computation, it never gets around to polling the cancellation signal.&lt;/p&gt;
&lt;p&gt;Imagine a query that needs to calculate something intensive, like sorting billions of rows.
Unless the sorting Stream is written with care (which the one in DataFusion is), the &lt;code&gt;poll_next&lt;/code&gt; call may take several minutes or even longer without returning.
During this time, Tokio can't check if you've pressed Ctrl+C, and the query continues running despite your cancellation request.&lt;/p&gt;
&lt;h2&gt;A Closer Look at Blocking Operators&lt;/h2&gt;
&lt;p&gt;Let's peel back a layer of the onion and look at what's happening in a blocking &lt;code&gt;poll_next&lt;/code&gt; implementation.
Here's a drastically simplified version of a &lt;code&gt;COUNT(*)&lt;/code&gt; aggregation - something you might use in a query like &lt;code&gt;SELECT COUNT(*) FROM table&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct BlockingStream {
    // the input: an inner stream that is wrapped
    stream: SendableRecordBatchStream,
    count: usize,
    finished: bool,
}

impl Stream for BlockingStream {
    type Item = Result&amp;lt;RecordBatch&amp;gt;;
    fn poll_next(mut self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context&amp;lt;'_&amp;gt;) -&amp;gt; Poll&amp;lt;Option&amp;lt;Self::Item&amp;gt;&amp;gt; {
        if self.finished {
            // return None if we're finished
            return Poll::Ready(None);
        }

        loop {
            // poll the input stream to get the next batch if ready
            match ready!(self.stream.poll_next_unpin(cx)) {
                // increment the counter if we got a batch
                Some(Ok(batch)) =&amp;gt; self.count += batch.num_rows(),
                // on end-of-stream, create a record batch for the counter
                None =&amp;gt; {
                    self.finished = true;
                    return Poll::Ready(Some(Ok(create_record_batch(self.count))));
                }
                // pass on any errors verbatim
                Some(Err(e)) =&amp;gt; return Poll::Ready(Some(Err(e))),
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;How does this code work? Let's break it down step by step:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Initial check&lt;/strong&gt;: We first check if we've already finished processing. If so, we return &lt;code&gt;Ready(None)&lt;/code&gt; to signal the end of our stream:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;if self.finished {
    return Poll::Ready(None);
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Processing loop&lt;/strong&gt;: If we're not done yet, we enter a loop to process incoming batches from our input stream:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;loop {
    match ready!(self.stream.poll_next_unpin(cx)) {
        // Handle different cases...
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;a href="https://doc.rust-lang.org/beta/std/task/macro.ready.html"&gt;&lt;code&gt;ready!&lt;/code&gt;&lt;/a&gt; macro checks if the input stream returned &lt;code&gt;Pending&lt;/code&gt; and if so, immediately returns &lt;code&gt;Pending&lt;/code&gt; from our function as well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Processing data&lt;/strong&gt;: For each batch we receive, we simply add its row count to our running total:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;Some(Ok(batch)) =&amp;gt; self.count += batch.num_rows(),
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;4. End of input&lt;/strong&gt;: When the child stream is exhausted (returns &lt;code&gt;None&lt;/code&gt;), we calculate our final result and convert it into a record batch (omitted for brevity):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;None =&amp;gt; {
    self.finished = true;
    return Poll::Ready(Some(Ok(create_record_batch(self.count))));
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;5. Error handling&lt;/strong&gt;: If we encounter an error, we pass it along immediately:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;Some(Err(e)) =&amp;gt; return Poll::Ready(Some(Err(e))),
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This code looks perfectly reasonable at first glance.
But there's a subtle issue lurking here: what happens if the input stream &lt;em&gt;always&lt;/em&gt; returns &lt;code&gt;Ready&lt;/code&gt; and never returns &lt;code&gt;Pending&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;In that case, the processing loop will keep running without returning &lt;code&gt;Poll::Pending&lt;/code&gt; and thus never yield control back to Tokio's scheduler.
This means we could be stuck in a single &lt;code&gt;poll_next&lt;/code&gt; call for quite some time - exactly the scenario that prevents query cancellation from working!&lt;/p&gt;
&lt;p&gt;So how do we solve this problem? Let's explore some strategies to ensure our operators yield control periodically.&lt;/p&gt;
&lt;h2&gt;Unblocking Operators&lt;/h2&gt;
&lt;p&gt;Now let's look at how we can ensure we return &lt;code&gt;Pending&lt;/code&gt; every now and then.&lt;/p&gt;
&lt;h3&gt;Independent Cooperative Operators&lt;/h3&gt;
&lt;p&gt;One simple way to return &lt;code&gt;Pending&lt;/code&gt; is using a loop counter.
We do the exact same thing as before, but on each loop iteration we decrement our counter.
If the counter hits zero we return &lt;code&gt;Pending&lt;/code&gt;.
The following example ensures we iterate at most 128 times before yielding.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct CountingSourceStream {
   counter: usize
}

impl Stream for CountingSourceStream {
    type Item = Result&amp;lt;RecordBatch&amp;gt;;

    fn poll_next(mut self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context&amp;lt;'_&amp;gt;) -&amp;gt; Poll&amp;lt;Option&amp;lt;Self::Item&amp;gt;&amp;gt; {
        if self.counter &amp;gt;= 128 {
            self.counter = 0;
            cx.waker().wake_by_ref();
            return Poll::Pending;
        }

        self.counter += 1;
        let batch = ...;
        Ready(Some(Ok(batch)))
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If &lt;code&gt;CountingSourceStream&lt;/code&gt; was the input for the &lt;code&gt;BlockingStream&lt;/code&gt; example above, 
the &lt;code&gt;BlockingStream&lt;/code&gt; will receive a &lt;code&gt;Pending&lt;/code&gt; periodically causing it to yield too. 
Can we really solve the cancel problem simply by periodically yielding in source streams? &lt;/p&gt;
&lt;p&gt;Unfortunately, no.
Let's look at what happens when we start combining operators in more complex configurations.
Suppose we create a plan like this.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Diagram showing a plan that merges two branches that return Pending at different intervals." src="/blog/images/task-cancellation/merge_plan.png"/&gt;
&lt;figcaption&gt;A plan that merges two branches by alternating between them.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Each &lt;code&gt;CountingSource&lt;/code&gt; produces a &lt;code&gt;Pending&lt;/code&gt; every 128 batches.
The &lt;code&gt;Filter&lt;/code&gt; is a stream that drops a batch every 50 record batches.
Merge is a simple combining operator the uses &lt;code&gt;futures::stream::select&lt;/code&gt; to combine two stream.&lt;/p&gt;
&lt;p&gt;When we set this stream in motion, the merge operator will poll the left and right branch in a round-robin fashion.
The sources will each emit &lt;code&gt;Pending&lt;/code&gt; every 128 batches, but since the &lt;code&gt;Filter&lt;/code&gt; drops batches, they arrive out-of-phase at the merge operator.
As a consequence the merge operator will always have the opportunity of polling the other stream when one returns &lt;code&gt;Pending&lt;/code&gt;.
The &lt;code&gt;Merge&lt;/code&gt; stream thus is an always ready stream, even though the sources are yielding.
If we use &lt;code&gt;Merge&lt;/code&gt; as the input to our aggregating operator we're right back where we started.&lt;/p&gt;
&lt;h3&gt;Coordinated Cooperation&lt;/h3&gt;
&lt;p&gt;Wouldn't it be great if we could get all the operators to coordinate amongst each other?
When one of them determines that it's time to yield, all the other operators agree and start returning &lt;code&gt;Pending&lt;/code&gt; as well.
That way our task would be coaxed towards yielding even if it tried to poll many different operators.&lt;/p&gt;
&lt;p&gt;Luckily(?), the &lt;a href="https://tokio.rs/blog/2020-04-preemption"&gt;developers of Tokio ran into the exact same problem&lt;/a&gt; described above when network servers were under heavy load and came up with a solution.
Back in 2020, Tokio 0.2.14 introduced a per-task operation budget.
Rather than having individual counters littered throughout the code, the Tokio runtime itself manages a per task counter which is decremented by Tokio resources.
When the counter hits zero, all resources start returning &lt;code&gt;Pending&lt;/code&gt;.
The task will then yield, after which the Tokio runtime resets the counter.&lt;/p&gt;
&lt;p&gt;To illustrate what this process looks like, let's have a look at the execution of the following query &lt;code&gt;Stream&lt;/code&gt; tree when polled in a Tokio task.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Diagram showing a plan with a task, AggregateExec, MergeStream and Two sources." src="/blog/images/task-cancellation/tokio_budget_plan.png"&gt;
&lt;figcaption&gt;Query plan for aggregating a sorted stream from two sources. Each source reads a stream of `RecordBatch`es, which are then merged into a single Stream by the `MergeStream` operator which is then aggregated by the `AggregateExec` operator. Arrows represent the data flow direction&lt;/figcaption&gt;
&lt;/img&gt;&lt;/figure&gt;
&lt;p&gt;If we assume a task budget of 1 unit, each time Tokio schedules the task would result in the following sequence of function calls.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Sequence diagram showing how the tokio task budget is used and reset." class="img-responsive" src="/blog/images/task-cancellation/tokio_budget.png" style="width: 100%; max-width: 100%"/&gt;
&lt;figcaption&gt;Tokio task budget system, assuming the task budget is set to 1, for the plan above.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The aggregation stream would try to poll the merge stream in a loop.
The first iteration of the loop consumes the single unit of budget, and returns &lt;code&gt;Ready&lt;/code&gt;.
The second iteration polls the merge stream again which now tries to poll the second scan stream.
Since there is no budget remaining &lt;code&gt;Pending&lt;/code&gt; is returned.
The merge stream may now try to poll the first source stream again, but since the budget is still depleted &lt;code&gt;Pending&lt;/code&gt; is returned as well.
The merge stream now has no other option than to return &lt;code&gt;Pending&lt;/code&gt; itself as well, causing the aggregation to break out of its loop.
The &lt;code&gt;Pending&lt;/code&gt; result bubbles all the way up to the Tokio runtime, at which point the runtime regains control.
When the runtime reschedules the task, it resets the budget and calls &lt;code&gt;poll&lt;/code&gt; on the task &lt;code&gt;Future&lt;/code&gt; again for another round of progress.&lt;/p&gt;
&lt;p&gt;The key mechanism that makes this work well is the single task budget that's shared amongst all the scan streams.
Once the budget is depleted, no streams can make any further progress without first returning control to tokio.
This causes all possible avenues the task has to make progress to return &lt;code&gt;Pending&lt;/code&gt; which results in the task being nudged towards yielding control.&lt;/p&gt;
&lt;p&gt;As it turns out DataFusion was already using this mechanism implicitly.
Every exchange-like operator (such as &lt;code&gt;RepartitionExec&lt;/code&gt;) internally makes use of a Tokio multiple producer, single consumer &lt;a href="https://tokio.rs/tokio/tutorial/channels"&gt;&lt;code&gt;Channel&lt;/code&gt;&lt;/a&gt;.
When calling &lt;code&gt;Receiver::recv&lt;/code&gt; for one of these channels, a unit of Tokio task budget is consumed.
As a consequence, query plans that made use of exchange-like operators were
already mostly cancelable.
The plan cancellation bug only showed up when running parts of plans without such operators, such as when using a single core.&lt;/p&gt;
&lt;p&gt;Now let's see how we can explicitly implement this budget-based approach in our own operators.&lt;/p&gt;
&lt;h3&gt;Depleting The Tokio Budget&lt;/h3&gt;
&lt;p&gt;Let's revisit our original &lt;code&gt;BlockingStream&lt;/code&gt; and adapt it to use Tokio's budget system.&lt;/p&gt;
&lt;p&gt;The examples given here make use of functions from the Tokio &lt;code&gt;coop&lt;/code&gt; module that are still internal at the time of writing.
&lt;a href="https://github.com/tokio-rs/tokio/pull/7405"&gt;PR #7405&lt;/a&gt; on the Tokio project will make these accessible for external use.
The current DataFusion code emulates these functions as well as possible using &lt;a href="https://docs.rs/tokio/latest/tokio/task/coop/fn.has_budget_remaining.html"&gt;&lt;code&gt;has_budget_remaining&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://docs.rs/tokio/latest/tokio/task/coop/fn.consume_budget.html"&gt;&lt;code&gt;consume_budget&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct BudgetSourceStream {
}

impl Stream for BudgetSourceStream {
    type Item = Result&amp;lt;RecordBatch&amp;gt;;

    fn poll_next(mut self: Pin&amp;lt;&amp;amp;mut Self&amp;gt;, cx: &amp;amp;mut Context&amp;lt;'_&amp;gt;) -&amp;gt; Poll&amp;lt;Option&amp;lt;Self::Item&amp;gt;&amp;gt; {
        let coop = ready!(tokio::task::coop::poll_proceed(cx));
        let batch: Poll&amp;lt;Option&amp;lt;Self::Item&amp;gt;&amp;gt; = ...;
        if batch.is_ready() {
            coop.made_progress();
        }
        batch
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;Stream&lt;/code&gt; now goes through the following steps:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Try to consume budget&lt;/strong&gt;: the first thing the operator does is use &lt;code&gt;poll_proceed&lt;/code&gt; to try to consume a unit of budget.
If the budget is depleted, this function will return &lt;code&gt;Pending&lt;/code&gt;.
Otherwise, we consumed one budget unit and we can continue.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let coop = ready!(tokio::task::coop::poll_proceed(cx));
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;2. Try to do some work&lt;/strong&gt;: next we try to produce a record batch.
That might not be possible if we're reading from some asynchronous resource that's not ready.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let batch: Poll&amp;lt;Option&amp;lt;Self::Item&amp;gt;&amp;gt; = ...;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;3. Commit the budget consumption&lt;/strong&gt;: finally, if we did produce a batch, we need to tell Tokio that we were able to make progress.&lt;/p&gt;
&lt;p&gt;That's done by calling the &lt;code&gt;made_progress&lt;/code&gt; method on the value &lt;code&gt;poll_proceed&lt;/code&gt; returned.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;if batch.is_ready() {
   coop.made_progress();
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might be wondering why the call to &lt;code&gt;made_progress&lt;/code&gt; is necessary.
This clever construct makes it easier to manage the budget.
The value returned by &lt;code&gt;poll_proceed&lt;/code&gt; will actually restore the budget to its original value when it is dropped unless &lt;code&gt;made_progress&lt;/code&gt; is called.
This ensures that if we exit early from our &lt;code&gt;poll_next&lt;/code&gt; implementation by returning &lt;code&gt;Pending&lt;/code&gt;, that the budget we had consumed becomes available again.
The task that invoked &lt;code&gt;poll_next&lt;/code&gt; can then use that budget again to try to make some other &lt;code&gt;Stream&lt;/code&gt; (or any resource for that matter) make progress.&lt;/p&gt;
&lt;h2&gt;Automatic Cooperation For All Operators&lt;/h2&gt;
&lt;p&gt;DataFusion 49.0.0  integrates the Tokio task budget based fix in all built-in source operators.
This ensures that going forward, most queries will automatically be cancelable. 
See &lt;a href="https://github.com/apache/datafusion/pull/16398"&gt;the PR&lt;/a&gt; for more details.&lt;/p&gt;
&lt;p&gt;The design includes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A new &lt;code&gt;ExecutionPlan&lt;/code&gt; property that indicates if an operator participates in cooperative scheduling or not.&lt;/li&gt;
&lt;li&gt;A new &lt;code&gt;EnsureCooperative&lt;/code&gt; optimizer rule to inspect query plans and insert &lt;code&gt;CooperativeExec&lt;/code&gt; nodes as needed to ensure custom source operators also participate.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These two changes combined already make it very unlikely you'll encounter any query that refuses to stop, even with custom operators.
For those situations where the automatic mechanisms are still not sufficient, there's a new &lt;code&gt;datafusion::physical_plan::coop&lt;/code&gt; module
with utility functions that make it easy to adopt cooperative scheduling in your custom operators as well.  &lt;/p&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;Thank you to &lt;a href="https://datadobi.com/"&gt;Datadobi&lt;/a&gt; for sponsoring the development of this feature and to
the DataFusion community contributors including &lt;a href="https://github.com/zhuqi-lucas"&gt;Qi Zhu&lt;/a&gt; and &lt;a href="https://github.com/ozankabak"&gt;Mehmet Ozan
Kabak&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;About DataFusion&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; is an extensible query engine toolkit, written
in Rust, that uses &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion and
similar technology are part of the next generation &amp;ldquo;Deconstructed Database&amp;rdquo;
architectures, where new systems are built on a foundation of fast, modular
components, rather than as a single tightly integrated system.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;DataFusion community&lt;/a&gt; is always looking for new contributors to help
improve the project. If you are interested in learning more about how query
execution works, help document or improve the DataFusion codebase, or just try
it out, we would love for you to join us.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Optimizing SQL (and DataFrames) in DataFusion, Part 1: Query Optimization Overview</title><link href="https://datafusion.apache.org/blog/2025/06/15/optimizing-sql-dataframes-part-one" rel="alternate"></link><published>2025-06-15T00:00:00+00:00</published><updated>2025-06-15T00:00:00+00:00</updated><author><name>alamb, akurmustafa</name></author><id>tag:datafusion.apache.org,2025-06-15:/blog/2025/06/15/optimizing-sql-dataframes-part-one</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Note: this blog was originally published &lt;a href="https://www.influxdata.com/blog/optimizing-sql-dataframes-part-one/"&gt;on the InfluxData blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sometimes Query Optimizers are seen as a sort of black magic, &lt;a href="https://15799.courses.cs.cmu.edu/spring2025/"&gt;&amp;ldquo;the most
challenging problem in computer
science,&amp;rdquo;&lt;/a&gt; according to Father
Pavlo, or some behind-the-scenes player. We believe this perception is because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;One must implement the rest of a …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Note: this blog was originally published &lt;a href="https://www.influxdata.com/blog/optimizing-sql-dataframes-part-one/"&gt;on the InfluxData blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sometimes Query Optimizers are seen as a sort of black magic, &lt;a href="https://15799.courses.cs.cmu.edu/spring2025/"&gt;&amp;ldquo;the most
challenging problem in computer
science,&amp;rdquo;&lt;/a&gt; according to Father
Pavlo, or some behind-the-scenes player. We believe this perception is because:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;One must implement the rest of a database system (data storage, transactions,
   SQL parser, expression evaluation, plan execution, etc.) &lt;strong&gt;before&lt;/strong&gt; the
   optimizer becomes critical&lt;sup id="fn5"&gt;&lt;a href="#footnote5"&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some parts of the optimizer are tightly tied to the rest of the system (e.g.,
   storage or indexes), so many classic optimizers are described with
   system-specific terminology.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Some optimizer tasks, such as access path selection and join order are known
   challenges and not yet solved (practically)&amp;mdash;maybe they really do require
   black magic 🤔.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;However, Query Optimizers are no more complicated in theory or practice than other parts of a database system, as we will argue in a series of posts:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Part 1: (this post)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Review what a Query Optimizer is, what it does, and why you need one for SQL and DataFrames.&lt;/li&gt;
&lt;li&gt;Describe how industrial Query Optimizers are structured and standard optimization classes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Part 2:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Describe the optimization categories with examples and pointers to implementations.&lt;/li&gt;
&lt;li&gt;Describe &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;&amp;rsquo;s rationale and approach to query optimization, specifically for access path and join ordering.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After reading these blogs, we hope people will use DataFusion to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Build their own system specific optimizers.&lt;/li&gt;
&lt;li&gt;Perform practical academic research on optimization (especially researchers
   working on new optimizations / join ordering&amp;mdash;looking at you &lt;a href="https://15799.courses.cs.cmu.edu/spring2025/"&gt;CMU
   15-799&lt;/a&gt;, next year).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Query Optimizer Background&lt;/h2&gt;
&lt;p&gt;The key pitch for querying databases, and likely the key to the longevity of SQL
(despite people&amp;rsquo;s love/hate relationship&amp;mdash;see &lt;a href="https://db.cs.cmu.edu/seminar2025/"&gt;SQL or Death? Seminar Series &amp;ndash;
Spring 2025&lt;/a&gt;), is that it disconnects the
&lt;code&gt;WHAT&lt;/code&gt; you want to compute from the &lt;code&gt;HOW&lt;/code&gt; to do it. SQL is a &lt;em&gt;declarative&lt;/em&gt;
language&amp;mdash;it describes what answers are desired rather than an &lt;em&gt;imperative&lt;/em&gt;
language such as Python, where you describe how to do the computation as shown
in Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fig 1: Query Execution." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/query-execution.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Query Execution: Users describe the answer they want using either
SQL or a DataFrame. For SQL, a Query Planner translates the parsed query 
into an &lt;em&gt;initial plan&lt;/em&gt;. The DataFrame API creates an initial plan directly.
The initial plan is correct, but slow. Then, the Query
Optimizer rewrites the initial plan into an &lt;em&gt;optimized plan&lt;/em&gt;, which computes
the same results but faster and more efficiently. Finally, the Execution Engine
executes the optimized plan producing results.&lt;/p&gt;
&lt;h2&gt;SQL, DataFrames, LogicalPlan Equivalence&lt;/h2&gt;
&lt;p&gt;Given their name, it is not surprising that Query Optimizers can improve the
performance of SQL queries. However, it is under-appreciated that this also
applies to DataFrame style APIs.&lt;/p&gt;
&lt;p&gt;Classic DataFrame systems such as &lt;a href="https://pandas.pydata.org/"&gt;pandas&lt;/a&gt; and &lt;a href="https://pola.rs/"&gt;Polars&lt;/a&gt; (by default) execute
eagerly and thus have limited opportunities for optimization. However, more
modern APIs such as &lt;a href="https://docs.pola.rs/user-guide/lazy/using/"&gt;Polars' lazy API&lt;/a&gt;, &lt;a href="https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes),"&gt;Apache Spark's DataFrame&lt;/a&gt;. and
&lt;a href="https://datafusion.apache.org/user-guide/dataframe.html"&gt;DataFusion's DataFrame&lt;/a&gt; are much faster as they use the design shown in Figure
1 and apply many query optimization techniques.&lt;/p&gt;
&lt;h2&gt;Example of Query Optimizer&lt;/h2&gt;
&lt;p&gt;This section motivates the value of a Query Optimizer with an example. Let&amp;rsquo;s say
you have some observations of animal behavior, as illustrated in Table 1.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table 1: Observational Data." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/table1.png" width="75%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: Example observational data.&lt;/p&gt;
&lt;p&gt;If the user wants to know the average population for some species in the last
month, a user can write a SQL query or a DataFrame such as the following:&lt;/p&gt;
&lt;p&gt;SQL:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT location, AVG(population)
FROM observations
WHERE species = &amp;lsquo;contrarian spider&amp;rsquo; AND 
  observation_time &amp;gt;= now() - interval '1 month'
GROUP BY location
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;DataFrame:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;df.scan("observations")
  .filter(col("species").eq("contrarian spider"))
  .filter(col("observation_time").ge(now()).sub(interval('1 month')))
  .agg(vec![col(location)], vec![avg(col("population")])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Within DataFusion, both the SQL and DataFrame are translated into the same
&lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/enum.LogicalPlan.html"&gt;LogicalPlan&lt;/a&gt;, a &amp;ldquo;tree of relational operators.&amp;rdquo; This is a fancy way of
saying data flow graphs where the edges represent tabular data (rows + columns)
and the nodes represent a transformation (see &lt;a href="https://youtu.be/EzZTLiSJnhY"&gt;this DataFusion overview video&lt;/a&gt;
for more details). The initial &lt;code&gt;LogicalPlan&lt;/code&gt; for the queries above is shown in
Figure 2.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fig 2: Initial Logical Plan." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/initial-logical-plan.png" width="72%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Example initial &lt;code&gt;LogicalPlan&lt;/code&gt; for SQL and DataFrame query. The
plan is read from bottom to top, computing the results in each step.&lt;/p&gt;
&lt;p&gt;The optimizer's job is to take this query plan and rewrite it into an alternate
plan that computes the same results but faster, such as the one shown in Figure
3.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fig 3: Optimized Logical Plan." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/optimized-logical-plan.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: An example optimized plan that computes the same result as the
plan in Figure 2 more efficiently. The diagram highlights where the optimizer
has applied &lt;em&gt;Projection Pushdown&lt;/em&gt;, &lt;em&gt;Filter Pushdown&lt;/em&gt;, and &lt;em&gt;Constant Evaluation&lt;/em&gt;.
Note that this is a simplified example for explanatory purposes, and actual
optimizers such as the one in DataFusion perform additional tasks such as
choosing specific aggregation algorithms.&lt;/p&gt;
&lt;h2&gt;Query Optimizer Implementation&lt;/h2&gt;
&lt;p&gt;Industrial optimizers, such as 
DataFusion&amp;rsquo;s (&lt;a href="https://github.com/apache/datafusion/tree/334d6ec50f36659403c96e1bffef4228be7c458e/datafusion/optimizer/src"&gt;source&lt;/a&gt;),
ClickHouse (&lt;a href="https://github.com/ClickHouse/ClickHouse/tree/master/src/Analyzer/Passes"&gt;source&lt;/a&gt;, &lt;a href="https://github.com/ClickHouse/ClickHouse/tree/master/src/Processors/QueryPlan/Optimizations"&gt;source&lt;/a&gt;),
DuckDB (&lt;a href="https://github.com/duckdb/duckdb/tree/4afa85c6a4dacc39524d1649fd8eb8c19c28ad14/src/optimizer"&gt;source&lt;/a&gt;),
and Apache Spark (&lt;a href="https://github.com/apache/spark/tree/7bc8e99cde424c59b98fe915e3fdaaa30beadb76/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer"&gt;source&lt;/a&gt;),
are implemented as a series of passes or rules that rewrite a query plan. The
overall optimizer is composed of a sequence of these rules,&lt;sup id="fn6"&gt;&lt;a href="#footnote6"&gt;6&lt;/a&gt;&lt;/sup&gt; as shown in
Figure 4. The specific order of the rules also often matters, but we will not
discuss this detail in this post.&lt;/p&gt;
&lt;p&gt;A multi-pass design is standard because it helps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Understand, implement, and test each pass in isolation&lt;/li&gt;
&lt;li&gt;Easily extend the optimizer by adding new passes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Fig 4: Query Optimizer Passes." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/optimizer-passes.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Query Optimizers are implemented as a series of rules that each
rewrite the query plan. Each rule&amp;rsquo;s algorithm is expressed as a transformation
of a previous plan.&lt;/p&gt;
&lt;p&gt;There are three major classes of optimizations in industrial optimizers:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Always Optimizations&lt;/strong&gt;: These are always good to do and thus are always
   applied. This class of optimization includes expression simplification,
   predicate pushdown, and limit pushdown. These optimizations are typically
   simple in theory, though they require nontrivial amounts of code and tests to
   implement in practice.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Engine Specific Optimizations: &lt;/strong&gt;These optimizations take advantage of
   specific engine features, such as how expressions are evaluated or what
   particular hash or join implementations are available.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Access Path and Join Order Selection&lt;/strong&gt;: These passes choose one access
   method per table and a join order for execution, typically using heuristics
   and a cost model to make tradeoffs between the options. Databases often have
   multiple ways to access the data (e.g., index scan or full-table scan), as
   well as many potential orders to combine (join) multiple tables. These
   methods compute the same result but can vary drastically in performance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This brings us to the end of Part 1. In Part 2, we will explain these classes of
optimizations in more detail and provide examples of how they are implemented in
DataFusion and other systems.&lt;/p&gt;
&lt;h1&gt;About the Authors&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/andrewalamb/"&gt;Andrew Lamb&lt;/a&gt; is a Staff Engineer at
&lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; and an &lt;a href="https://datafusion.apache.org/"&gt;Apache
DataFusion&lt;/a&gt; PMC member. A Database Optimizer
connoisseur, he worked on the &lt;a href="https://vldb.org/pvldb/vol5/p1790_andrewlamb_vldb2012.pdf"&gt;Vertica Analytic
Database&lt;/a&gt; Query
Optimizer for six years, has several granted US patents related to query
optimization&lt;sup id="fn1"&gt;&lt;a href="#footnote1"&gt;1&lt;/a&gt;&lt;/sup&gt;, co-authored several papers&lt;sup id="fn2"&gt;&lt;a href="#footnote2"&gt;2&lt;/a&gt;&lt;/sup&gt;  about the topic (including in
VLDB 2024&lt;sup id="fn3"&gt;&lt;a href="#footnote3"&gt;3&lt;/a&gt;&lt;/sup&gt;), and spent several weeks&lt;sup id="fn4"&gt;&lt;a href="#footnote4"&gt;4&lt;/a&gt;&lt;/sup&gt; deeply geeking out about this topic
with other experts (thank you Dagstuhl).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/akurmustafa/"&gt;Mustafa Akur&lt;/a&gt; is a PhD Student at
&lt;a href="https://www.ohsu.edu/"&gt;OHSU&lt;/a&gt; Knight Cancer Institute and an &lt;a href="https://datafusion.apache.org/"&gt;Apache
DataFusion&lt;/a&gt; PMC member. He was previously a
Software Developer at &lt;a href="https://www.synnada.ai/"&gt;Synnada&lt;/a&gt; where he contributed
significant features to the DataFusion optimizer, including many &lt;a href="https://datafusion.apache.org/blog/2025/03/11/ordering-analysis/"&gt;sort-based
optimizations&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;&lt;a id="footnote1"&gt;&lt;/a&gt;&lt;sup&gt;[1]&lt;/sup&gt; &lt;em&gt;Modular Query Optimizer, US 8,312,027 &amp;middot; Issued Nov 13, 2012&lt;/em&gt;, Query Optimizer with schema conversion US 8,086,598 &amp;middot; Issued Dec 27, 2011&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote2"&gt;&lt;/a&gt;&lt;sup&gt;[2]&lt;/sup&gt; &lt;a href="https://www.researchgate.net/publication/269306314_The_Vertica_Query_Optimizer_The_case_for_specialized_query_optimizers"&gt;The Vertica Query Optimizer: The case for specialized Query Optimizers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote3"&gt;&lt;/a&gt;&lt;sup&gt;[3]&lt;/sup&gt; &lt;a href="https://www.vldb.org/pvldb/vol17/p1350-justen.pdf"&gt;https://www.vldb.org/pvldb/vol17/p1350-justen.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote4"&gt;&lt;/a&gt;&lt;sup&gt;[4]&lt;/sup&gt; &lt;a href="https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/24101"&gt;https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/24101&lt;/a&gt;, &lt;a href="https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/22111"&gt;https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/22111&lt;/a&gt;, &lt;a href="https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/12321"&gt;https://www.dagstuhl.de/en/seminars/seminar-calendar/seminar-details/12321&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote5"&gt;&lt;/a&gt;&lt;sup&gt;[5]&lt;/sup&gt;  And thus in academic classes, by the time you get around to an optimizer the semester is over and everyone is ready for the semester to be done. Once industrial systems mature to the point where the optimizer is a bottleneck, the shiny new-ness of the&lt;a href="https://en.wikipedia.org/wiki/Gartner_hype_cycle"&gt; hype cycle&lt;/a&gt; has worn off and it is likely in the trough of disappointment.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote6"&gt;&lt;/a&gt;&lt;sup&gt;[6]&lt;/sup&gt; Often systems will classify these passes into different categories, but I am simplifying here&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Optimizing SQL (and DataFrames) in DataFusion, Part 2: Optimizers in Apache DataFusion</title><link href="https://datafusion.apache.org/blog/2025/06/15/optimizing-sql-dataframes-part-two" rel="alternate"></link><published>2025-06-15T00:00:00+00:00</published><updated>2025-06-15T00:00:00+00:00</updated><author><name>alamb, akurmustafa</name></author><id>tag:datafusion.apache.org,2025-06-15:/blog/2025/06/15/optimizing-sql-dataframes-part-two</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Note, this blog was originally published &lt;a href="https://www.influxdata.com/blog/optimizing-sql-dataframes-part-two/"&gt;on the InfluxData blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2025/06/15/optimizing-sql-dataframes-part-one"&gt;first part of this post&lt;/a&gt;, we discussed what a Query Optimizer is, what
role it plays, and described how industrial optimizers are organized. In this
second post, we describe various optimizations that are found in &lt;a href="https://datafusion.apache.org/"&gt;Apache
DataFusion&lt;/a&gt; and …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Note, this blog was originally published &lt;a href="https://www.influxdata.com/blog/optimizing-sql-dataframes-part-two/"&gt;on the InfluxData blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2025/06/15/optimizing-sql-dataframes-part-one"&gt;first part of this post&lt;/a&gt;, we discussed what a Query Optimizer is, what
role it plays, and described how industrial optimizers are organized. In this
second post, we describe various optimizations that are found in &lt;a href="https://datafusion.apache.org/"&gt;Apache
DataFusion&lt;/a&gt; and other industrial systems in more
detail.&lt;/p&gt;
&lt;p&gt;DataFusion contains high quality, full-featured implementations for &lt;em&gt;Always
Optimizations&lt;/em&gt; and &lt;em&gt;Engine Specific Optimizations&lt;/em&gt; (defined in Part 1).
Optimizers are implemented as rewrites of &lt;code&gt;LogicalPlan&lt;/code&gt; in the &lt;a href="https://github.com/apache/datafusion/tree/main/datafusion/optimizer"&gt;logical
optimizer&lt;/a&gt;
or rewrites of &lt;code&gt;ExecutionPlan&lt;/code&gt; in the &lt;a href="https://github.com/apache/datafusion/tree/main/datafusion/physical-optimizer"&gt;physical
optimizer&lt;/a&gt;.
This design means the same optimizer passes are applied for SQL queries,
DataFrame queries, as well as plans for other query language frontends such as
&lt;a href="https://github.com/influxdata/influxdb3_core/tree/26a30bf8d6e2b6b3f1dd905c4ec27e3db6e20d5f/iox_query_influxql"&gt;InfluxQL&lt;/a&gt;
in InfluxDB 3.0,
&lt;a href="https://github.com/GreptimeTeam/greptimedb/blob/0bd322a078cae4f128b791475ec91149499de33a/src/query/src/promql/planner.rs#L1"&gt;PromQL&lt;/a&gt;
in &lt;a href="https://greptime.com/"&gt;Greptime&lt;/a&gt;, and
&lt;a href="https://github.com/vega/vegafusion/tree/dc15c1b9fc7d297f12bea919795d58cda1c88fcf/vegafusion-core/src/planning"&gt;vega&lt;/a&gt;
in &lt;a href="https://vegafusion.io/"&gt;VegaFusion&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Always Optimizations&lt;/h2&gt;
&lt;p&gt;Some optimizations are so important they are found in almost all query engines
and are typically the first implemented as they provide the largest cost /
benefit ratio (and performance is terrible without them).&lt;/p&gt;
&lt;h3&gt;Predicate/Filter Pushdown&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt;: Avoid carrying unneeded &lt;em&gt;rows &lt;/em&gt;as soon as possible&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Moves filters &amp;ldquo;down&amp;rdquo; in the plan so they run earlier during execution, as shown in Figure 1.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Implementations&lt;/strong&gt;: &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/optimizer/src/push_down_filter.rs"&gt;DataFusion&lt;/a&gt;, &lt;a href="https://github.com/duckdb/duckdb/blob/main/src/optimizer/filter_pushdown.cpp"&gt;DuckDB&lt;/a&gt;, &lt;a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Processors/QueryPlan/Optimizations/filterPushDown.cpp"&gt;ClickHouse&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The earlier data is filtered out in the plan, the less work the rest of the plan
has to do. Most mature databases aggressively use filter pushdown / early
filtering combined with techniques such as partition and storage pruning (e.g.
&lt;a href="https://blog.xiangpeng.systems/posts/parquet-to-arrow/"&gt;Parquet Row Group pruning&lt;/a&gt;) for performance.&lt;/p&gt;
&lt;p&gt;An extreme, and somewhat contrived, is the query&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT city, COUNT(*) FROM population GROUP BY city HAVING city = 'BOSTON';
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Semantically, &lt;code&gt;HAVING&lt;/code&gt; is &lt;a href="https://www.datacamp.com/tutorial/sql-order-of-execution"&gt;evaluated after&lt;/a&gt; &lt;code&gt;GROUP BY&lt;/code&gt; in SQL. However, computing
the population of all cities and discarding everything except Boston is much
slower than only computing the population for Boston and so most Query
Optimizers will evaluate the filter before the aggregation.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fig 1: Filter Pushdown." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/filter-pushdown.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Filter Pushdown.  In (&lt;strong&gt;A&lt;/strong&gt;) without filter pushdown, the operator
processes more rows, reducing efficiency. In (&lt;strong&gt;B&lt;/strong&gt;) with filter pushdown, the
operator receives fewer rows, resulting in less overall work and leading to a
faster and more efficient query.&lt;/p&gt;
&lt;h3&gt;Projection Pushdown&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt;: Avoid carrying unneeded &lt;em&gt;columns &lt;/em&gt;as soon as possible&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What: &lt;/strong&gt;Pushes &amp;ldquo;projection&amp;rdquo; (keeping only certain columns) earlier in the plan, as shown in Figure 2.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Implementations: &lt;/strong&gt;Implementations: &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/physical-optimizer/src/projection_pushdown.rs"&gt;DataFusion&lt;/a&gt;, &lt;a href="https://github.com/duckdb/duckdb/blob/a8a6a080c8809d5d4b3c955e9f113574f6f0bfe0/src/optimizer/pushdown/pushdown_projection.cpp"&gt;DuckDB&lt;/a&gt;, &lt;a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Processors/QueryPlan/Optimizations/optimizeUseNormalProjection.cpp"&gt;ClickHouse&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Similarly to the motivation for &lt;em&gt;Filter Pushdown&lt;/em&gt;, the earlier the plan stops
doing something, the less work it does overall and thus the faster it runs. For
Projection Pushdown, if columns are not needed later in a plan, copying the data
to the output of other operators is unnecessary and the costs of copying can add
up. For example, in Figure 3 of Part 1, the &lt;code&gt;species&lt;/code&gt; column is only needed to
evaluate the Filter within the scan and &lt;code&gt;notes&lt;/code&gt; are never used, so it is
unnecessary to copy them through the rest of the plan.&lt;/p&gt;
&lt;p&gt;Projection Pushdown is especially effective and important for column store
databases, where the storage format itself (such as &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt;) supports
efficiently reading only a subset of required columns, and is &lt;a href="https://blog.xiangpeng.systems/posts/parquet-pushdown/"&gt;especially
powerful in combination with filter pushdown&lt;/a&gt;. Projection Pushdown is still
important, but less effective for row oriented formats such as JSON or CSV where
each column in each row must be parsed even if it is not used in the plan.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fig 2: Projection Pushdown." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/projection-pushdown.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2:&lt;/strong&gt; In (&lt;strong&gt;A&lt;/strong&gt;) without projection pushdown, the operator receives more
columns, reducing efficiency. In (&lt;strong&gt;B&lt;/strong&gt;) with projection pushdown, the operator
receives fewer columns, leading to optimized execution.&lt;/p&gt;
&lt;h3&gt;Limit Pushdown&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt;: The earlier the plan stops generating data, the less overall work it
does, and some operators have more efficient limited implementations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What: &lt;/strong&gt;Pushes limits (maximum row counts) down in a plan as early as possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Implementations:&lt;/strong&gt; &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/optimizer/src/push_down_limit.rs"&gt;DataFusion&lt;/a&gt;, &lt;a href="https://github.com/duckdb/duckdb/blob/main/src/optimizer/limit_pushdown.cpp"&gt;DuckDB&lt;/a&gt;, &lt;a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Processors/QueryPlan/Optimizations/limitPushDown.cpp"&gt;ClickHouse&lt;/a&gt;, Spark (&lt;a href="https://github.com/apache/spark/blob/7bc8e99cde424c59b98fe915e3fdaaa30beadb76/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/LimitPushDownThroughWindow.scala"&gt;Window&lt;/a&gt; and &lt;a href="https://github.com/apache/spark/blob/7bc8e99cde424c59b98fe915e3fdaaa30beadb76/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/PushProjectionThroughLimit.scala"&gt;Projection&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;Often queries have a &lt;code&gt;LIMIT&lt;/code&gt; or other clause that allows them to stop generating
results early so the sooner they can stop execution, the more efficiently they
will execute.&lt;/p&gt;
&lt;p&gt;In addition, DataFusion and other systems have more efficient implementations of
some operators that can be used if there is a limit. The classic example is
replacing a full sort + limit with a &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/struct.TopK.html"&gt;TopK&lt;/a&gt; operator that only tracks the top
values using a heap. Similarly,  DataFusion&amp;rsquo;s Parquet reader stops fetching and
opening additional files once the limit has been hit.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fig 3: Limit Pushdown." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/limit-pushdown.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: In (&lt;strong&gt;A&lt;/strong&gt;), without limit pushdown all data is sorted and
everything except the first few rows are discarded. In (&lt;strong&gt;B&lt;/strong&gt;), with limit
pushdown, Sort is replaced with TopK operator which does much less work.&lt;/p&gt;
&lt;h3&gt;Expression Simplification / Constant Folding&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt;: Evaluating the same expression for each row when the value doesn&amp;rsquo;t change is wasteful.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Partially evaluates and/or algebraically simplify expressions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Implementations:&lt;/strong&gt; &lt;a href="https://github.com/apache/datafusion/tree/main/datafusion/optimizer/src/simplify_expressions"&gt;DataFusion&lt;/a&gt;, DuckDB (has several &lt;a href="https://github.com/duckdb/duckdb/tree/7b18f0f3691c1b6367cf68ed2598d7034e14f41b/src/optimizer/rule"&gt;rules&lt;/a&gt; such as &lt;a href="https://github.com/duckdb/duckdb/blob/7b18f0f3691c1b6367cf68ed2598d7034e14f41b/src/optimizer/rule/constant_folding.cpp"&gt;constant folding&lt;/a&gt;, and &lt;a href="https://github.com/duckdb/duckdb/blob/7b18f0f3691c1b6367cf68ed2598d7034e14f41b/src/optimizer/rule/comparison_simplification.cpp"&gt;comparison simplification&lt;/a&gt;), &lt;a href="https://github.com/apache/spark/blob/7bc8e99cde424c59b98fe915e3fdaaa30beadb76/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala"&gt;Spark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If an expression doesn&amp;rsquo;t change from row to row, it is better to evaluate the
expression &lt;strong&gt;once&lt;/strong&gt; during planning. This is a classic compiler technique and is
also used in database systems&lt;/p&gt;
&lt;p&gt;For example, given a query that finds all values from the current year&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT &amp;hellip; WHERE extract(year from time_column) = extract(year from now())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evaluating &lt;code&gt;extract(year from now())&lt;/code&gt; on every row is much more expensive than
evaluating it once during planning time so that the query becomes comparison to
a constant&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT &amp;hellip; WHERE extract(year from time_column) = 2025
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Furthermore, it is often possible to push such predicates &lt;strong&gt;into&lt;/strong&gt; scans.&lt;/p&gt;
&lt;h3&gt;Rewriting &lt;code&gt;OUTER JOIN&lt;/code&gt; &amp;rarr; &lt;code&gt;INNER JOIN&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why:&lt;/strong&gt; &lt;code&gt;INNER JOIN&lt;/code&gt;  implementations are almost always faster (as they are
simpler) than &lt;code&gt;OUTER JOIN&lt;/code&gt; implementations, and &lt;code&gt;INNER JOIN&lt;/code&gt; s impose fewer
restrictions on other optimizer passes (such as join reordering and additional
filter pushdown).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: In cases where it is known that NULL rows introduced by an &lt;code&gt;OUTER
JOIN&lt;/code&gt; will not appear in the results, it can be rewritten to an &lt;code&gt;INNER
JOIN&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Implementations:&lt;/strong&gt; &lt;a href="https://github.com/apache/datafusion/blob/6028474969f0bfead96eb7f413791470afb6bf82/datafusion/optimizer/src/eliminate_outer_join.rs"&gt;DataFusion&lt;/a&gt;, &lt;a href="https://github.com/apache/spark/blob/7bc8e99cde424c59b98fe915e3fdaaa30beadb76/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/joins.scala#L124-L158"&gt;Spark&lt;/a&gt;, &lt;a href="https://github.com/ClickHouse/ClickHouse/blob/master/src/Processors/QueryPlan/Optimizations/convertOuterJoinToInnerJoin.cpp"&gt;ClickHouse&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, given a query such as the following&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-SQL"&gt;SELECT &amp;hellip;
FROM orders LEFT OUTER JOIN customer ON (orders.cid = customer.id)
WHERE customer.last_name = 'Lamb'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;LEFT OUTER JOIN&lt;/code&gt; keeps all rows in &lt;code&gt;orders&lt;/code&gt;  that don&amp;rsquo;t have a matching
customer, but fills in the fields with &lt;code&gt;null&lt;/code&gt;. All such rows will be filtered
out by &lt;code&gt;customer.last_name = 'Lamb'&lt;/code&gt;, and thus an INNER JOIN produces the same
answer. This is illustrated in Figure 4.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fig 4: Join Rewrite." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/join-rewrite.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Rewriting &lt;code&gt;OUTER JOIN&lt;/code&gt; to &lt;code&gt;INNER JOIN&lt;/code&gt;. In (A) the original query
contains an &lt;code&gt;OUTER JOIN&lt;/code&gt; but also a filter on &lt;code&gt;customer.last_name&lt;/code&gt;, which
filters out all rows that might be introduced by the &lt;code&gt;OUTER JOIN&lt;/code&gt;. In (B) the
&lt;code&gt;OUTER JOIN&lt;/code&gt; is converted to inner join, a more efficient implementation can be
used.&lt;/p&gt;
&lt;h2&gt;Engine Specific Optimizations&lt;/h2&gt;
&lt;p&gt;As discussed in Part 1 of this blog, optimizers also contain a set of passes
that are still always good to do, but are closely tied to the specifics of the
query engine. This section describes some common types&lt;/p&gt;
&lt;h3&gt;Subquery Rewrites&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt;: Actually implementing subqueries by running a query for each row of the outer query is very expensive.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: It is possible to rewrite subqueries as joins which often perform much better.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Implementations:&lt;/strong&gt; DataFusion (&lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/optimizer/src/decorrelate.rs"&gt;one&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/optimizer/src/decorrelate_predicate_subquery.rs"&gt;two&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/optimizer/src/scalar_subquery_to_join.rs"&gt;three&lt;/a&gt;), &lt;a href="https://github.com/apache/spark/blob/7bc8e99cde424c59b98fe915e3fdaaa30beadb76/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/subquery.scala"&gt;Spark&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Evaluating subqueries a row at a time is so expensive that execution engines in
high performance analytic systems such as DataFusion and &lt;a href="https://vertica.com/"&gt;Vertica&lt;/a&gt; may not even
support row-at-a-time evaluation given how terrible the performance would be. 
Instead, analytic systems rewrite such queries into joins which can perform 100s
or 1000s of times faster for large datasets. However, transforming subqueries to
joins requires &amp;ldquo;exotic&amp;rdquo; join semantics such as &lt;code&gt;SEMI JOIN&lt;/code&gt;, &lt;code&gt;ANTI JOIN&lt;/code&gt;  and
variations on how to treat equality with null&lt;sup id="fn7"&gt;&lt;a href="#footnote7"&gt;7&lt;/a&gt;.&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;For a simple example, consider that a query like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT customer.name 
FROM customer 
WHERE (SELECT sum(value) 
       FROM orders WHERE
       orders.cid = customer.id) &amp;gt; 10;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Can be rewritten like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT customer.name 
FROM customer 
JOIN (
  SELECT customer.id as cid_inner, sum(value) s 
  FROM orders 
  GROUP BY customer.id
 ) ON (customer.id = cid_inner AND s &amp;gt; 10);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We don&amp;rsquo;t have space to detail this transformation or why it is so much faster to
run, but using this and many other transformations allow efficient subquery
evaluation.&lt;/p&gt;
&lt;h3&gt;Optimized Expression Evaluation&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt;: The capabilities of expression evaluation vary from system to system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Optimize expression evaluation for the particular execution environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Implementations&lt;/strong&gt;: There are many examples of this type of
optimization, including DataFusion&amp;rsquo;s &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/optimizer/src/common_subexpr_eliminate.rs"&gt;Common Subexpression
Elimination&lt;/a&gt;,
&lt;a href="https://github.com/apache/datafusion/blob/8f3f70877febaa79be3349875e979d3a6e65c30e/datafusion/optimizer/src/simplify_expressions/unwrap_cast.rs#L70"&gt;unwrap_cast&lt;/a&gt;,
and &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/optimizer/src/extract_equijoin_predicate.rs"&gt;identifying equality join
predicates&lt;/a&gt;.
DuckDB &lt;a href="https://github.com/duckdb/duckdb/blob/main/src/optimizer/in_clause_rewriter.cpp"&gt;rewrites IN
clauses&lt;/a&gt;,
and &lt;a href="https://github.com/duckdb/duckdb/blob/main/src/optimizer/sum_rewriter.cpp"&gt;SUM
expressions&lt;/a&gt;.
Spark also &lt;a href="https://github.com/apache/spark/blob/7bc8e99cde424c59b98fe915e3fdaaa30beadb76/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/UnwrapCastInBinaryComparison.scala"&gt;unwraps casts in binary
comparisons&lt;/a&gt;,
and &lt;a href="https://github.com/apache/spark/blob/7bc8e99cde424c59b98fe915e3fdaaa30beadb76/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/InjectRuntimeFilter.scala"&gt;adds special runtime
filters&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To give a specific example of what DataFusion&amp;rsquo;s common subexpression elimination
does, consider this query that refers to a complex expression multiple times:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT date_bin('1 hour', time, '1970-01-01') 
FROM table 
WHERE date_bin('1 hour', time, '1970-01-01') &amp;gt;= '2025-01-01 00:00:00'
ORDER BY date_bin('1 hour', time, '1970-01-01')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Evaluating &lt;code&gt;date_bin('1 hour', time, '1970-01-01')&lt;/code&gt;each time it is encountered
is inefficient compared to calculating its result once, and reusing that result
in when it is encountered again (similar to caching). This reuse is called
&lt;em&gt;Common Subexpression Elimination&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Some execution engines implement this optimization internally to their
expression evaluation engine, but DataFusion represents it explicitly using a
separate Projection plan node, as illustrated in Figure 5.  Effectively, the
query above is rewritten to the following&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT time_chunk 
FROM(SELECT date_bin('1 hour', time, '1970-01-01') as time_chunk 
     FROM table)
WHERE time_chunk &amp;gt;= '2025-01-01 00:00:00'
ORDER BY time_chunk
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="Fig 5: Common Subquery Elimination." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/common-subexpression-elimination.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5:&lt;/strong&gt; Adding a Projection to evaluate common complex sub expression
decreases complexity for later stages.&lt;/p&gt;
&lt;h3&gt;Algorithm Selection&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt;: Different engines have different specialized operators for certain
operations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What: &lt;/strong&gt;Selects specific implementations from the available operators, based
on properties of the query.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Implementations:&lt;/strong&gt; DataFusion&amp;rsquo;s &lt;a href="https://github.com/apache/datafusion/blob/8f3f70877febaa79be3349875e979d3a6e65c30e/datafusion/physical-optimizer/src/enforce_sorting/mod.rs"&gt;EnforceSorting&lt;/a&gt; pass uses sort optimized implementations, Spark&amp;rsquo;s &lt;a href="https://github.com/apache/spark/blob/7bc8e99cde424c59b98fe915e3fdaaa30beadb76/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteAsOfJoin.scala"&gt;rewrite to use a special operator for ASOF joins&lt;/a&gt;, and ClickHouse&amp;rsquo;s&lt;a href="https://github.com/ClickHouse/ClickHouse/blob/7d15deda4b33282f356bb3e40a190d005acf72f2/src/Interpreters/ExpressionAnalyzer.cpp#L1066-L1080"&gt; join algorithm selection &lt;/a&gt; such as &lt;a href="https://github.com/ClickHouse/ClickHouse/blob/7d15deda4b33282f356bb3e40a190d005acf72f2/src/Interpreters/ExpressionAnalyzer.cpp#L1022"&gt;when to use MergeJoin&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;For example, DataFusion uses a &lt;code&gt;TopK&lt;/code&gt; (&lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/struct.TopK.html"&gt;source&lt;/a&gt;) operator rather than a full
&lt;code&gt;Sort&lt;/code&gt; if there is also a limit on the query. Similarly, it may choose to use the
more efficient &lt;code&gt;PartialOrdered&lt;/code&gt; grouping operation when the data is sorted on
group keys or a &lt;code&gt;MergeJoin&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fig 6: Specialized Grouping." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/specialized-grouping.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6: &lt;/strong&gt;An example of specialized operation for grouping. In (&lt;strong&gt;A&lt;/strong&gt;), input data has no specified ordering and DataFusion uses a hashing-based grouping operator (&lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/physical-plan/src/aggregates/row_hash.rs"&gt;source&lt;/a&gt;) to determine distinct groups. In (&lt;strong&gt;B&lt;/strong&gt;), when the input data is ordered by the group keys, DataFusion uses a specialized grouping operator (&lt;a href="https://github.com/apache/datafusion/tree/main/datafusion/physical-plan/src/aggregates/order"&gt;source&lt;/a&gt;) to find boundaries that separate groups.&lt;/p&gt;
&lt;h3&gt;Using Statistics Directly&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt;: Using pre-computed statistics from a table, without actually reading or
opening files, is much faster than processing data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What&lt;/strong&gt;: Replace calculations on data with the value from statistics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example Implementations:&lt;/strong&gt; &lt;a href="https://github.com/apache/datafusion/blob/8f3f70877febaa79be3349875e979d3a6e65c30e/datafusion/physical-optimizer/src/aggregate_statistics.rs"&gt;DataFusion&lt;/a&gt;, &lt;a href="https://github.com/duckdb/duckdb/blob/main/src/optimizer/statistics_propagator.cpp"&gt;DuckDB&lt;/a&gt;,&lt;/p&gt;
&lt;p&gt;Some queries, such as the classic &lt;code&gt;COUNT(*) from my_table&lt;/code&gt; used for data
exploration can be answered using only statistics. Optimizers often have access
to statistics for other reasons (such as Access Path and Join Order Selection)
and statistics are commonly stored in analytic file formats. For example, the
&lt;a href="https://docs.rs/parquet/latest/parquet/file/metadata/index.html"&gt;Metadata&lt;/a&gt; of Apache Parquet files stores &lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt;, and &lt;code&gt;COUNT&lt;/code&gt; information.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Fig 7: Using Statistics." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/using-statistics.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 7: &lt;/strong&gt;When the aggregation result is already stored in the statistics,
the query can be evaluated using the values from statistics without looking at
any compressed data. The optimizer replaces the Aggregation operation with
values from statistics.&lt;/p&gt;
&lt;h2&gt;Access Path and Join Order Selection&lt;/h2&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;Last, but certainly not least, are optimizations that choose between plans with
potentially (very) different performance. The major options in this category are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Join Order:&lt;/strong&gt; In what order to combine tables using JOINs?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Access Paths:&lt;/strong&gt; Which copy of the data or index should be read to find matching tuples?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Materialized_view"&gt;Materialized View&lt;/a&gt;&lt;/strong&gt;: Can the query can be rewritten to use a materialized view (partially computed query results)? This topic deserves its own blog (or book) and we don&amp;rsquo;t discuss further here.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Fig 8: Access Path and Join Order." class="img-responsive" src="/blog/images/optimizing-sql-dataframes/access-path-and-join-order.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 8:&lt;/strong&gt; Access Path and Join Order Selection in Query Optimizers. Optimizers use heuristics to enumerate some subset of potential join orders (shape) and access paths (color). The plan with the smallest estimated cost according to some cost model is chosen. In this case, Plan 2 with a cost of 180,000 is chosen for execution as it has the lowest estimated cost.&lt;/p&gt;
&lt;p&gt;This class of optimizations is a hard problem for at least the following reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Exponential Search Space&lt;/strong&gt;: the number of potential plans increases
   exponentially as the number of joins and indexes increases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Performance Sensitivity&lt;/strong&gt;: Often different plans that are very similar in
   structure perform very differently. For example, swapping the input order to
   a hash join can result in 1000x or more (yes, a thousand-fold!) run time
   differences.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cardinality Estimation Errors&lt;/strong&gt;: Determining the optimal plan relies on
   cardinality estimates (e.g., how many rows will come out of each join). It is a
   &lt;a href="https://www.vldb.org/pvldb/vol9/p204-leis.pdf"&gt;known hard problem&lt;/a&gt; to estimate this cardinality, and in practice queries with
   as few as 3 joins often have large cardinality estimation errors.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Heuristics and Cost-Based Optimization&lt;/h3&gt;
&lt;p&gt;Industrial optimizers handle these problems using a combination of&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Heuristics:&lt;/strong&gt; to prune the search space and avoid considering plans that
   are (almost) never good. Examples include considering left-deep trees, or
   using &lt;code&gt;Foreign Key&lt;/code&gt; / &lt;code&gt;Primary Key&lt;/code&gt; relationships to pick the build size of a
   hash join.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cost Model&lt;/strong&gt;: Given the smaller set of candidate plans, the Optimizer then
   estimates their cost and picks the one using the lowest cost.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For some examples, you can read about &lt;a href="https://docs.databricks.com/aws/en/optimizations/cbo"&gt;Spark&amp;rsquo;s cost-based optimizer&lt;/a&gt; or look at
the code for &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/physical-optimizer/src/join_selection.rs"&gt;DataFusion&amp;rsquo;s join selection&lt;/a&gt; and &lt;a href="https://github.com/duckdb/duckdb/blob/main/src/optimizer/join_order/cost_model.cpp"&gt;DuckDB&amp;rsquo;s cost model&lt;/a&gt; and &lt;a href="https://github.com/duckdb/duckdb/blob/84c87b12fa9554a8775dc243b4d0afd5b407321a/src/optimizer/join_order/plan_enumerator.cpp#L469-L472"&gt;join
order enumeration&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, the use of heuristics and (imprecise) cost models means optimizers must&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Make deep assumptions about the execution environment: &lt;/strong&gt;For example the
   heuristics often include assumptions that joins implement &lt;a href="https://www.alibabacloud.com/blog/alibaba-cloud-analyticdb-for-mysql-create-ultimate-runtimefilter-capability_600228"&gt;sideways information
   passing (RuntimeFilters)&lt;/a&gt;, or that Join operators always preserve a particular
   input's order.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use one particular objective function: &lt;/strong&gt;There are almost always trade-offs
   between desirable plan properties, such as execution speed, memory use, and
   robustness in the face of cardinality estimation. Industrial optimizers
   typically have one cost function which attempts to balance between the
   properties or a series of hard to use indirect tuning knobs to control the
   behavior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Require statistics&lt;/strong&gt;: Typically cost models require up-to-date statistics,
   which can be expensive to compute, must be kept up to date as new data
   arrives, and often have trouble capturing the non-uniformity of real world
   datasets&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Join Ordering in DataFusion&lt;/h3&gt;
&lt;p&gt;DataFusion purposely does not include a sophisticated cost based optimizer.
Instead, keeping with its &lt;a href="https://docs.rs/datafusion/latest/datafusion/#design-goals"&gt;design goals&lt;/a&gt; it provides a reasonable default
implementation along with extension points to customize behavior.&lt;/p&gt;
&lt;p&gt;Specifically, DataFusion includes&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&amp;ldquo;Syntactic Optimizer&amp;rdquo; (joins in the order they are listed in the query&lt;sup id="fn8"&gt;&lt;a href="#footnote8"&gt;8&lt;/a&gt;) with basic join re-ordering (&lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/physical-optimizer/src/join_selection.rs"&gt;source&lt;/a&gt;) to prevent join disasters.&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Support for &lt;a href="https://docs.rs/datafusion/latest/datafusion/common/struct.ColumnStatistics.html"&gt;ColumnStatistics&lt;/a&gt; and &lt;a href="https://docs.rs/datafusion/latest/datafusion/common/struct.Statistics.html"&gt;Table Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The framework for &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_expr/struct.AnalysisContext.html#structfield.selectivity"&gt;filter selectivity&lt;/a&gt; + join cardinality estimation.&lt;/li&gt;
&lt;li&gt;APIs for easily rewriting plans, such as the &lt;a href="https://docs.rs/datafusion/latest/datafusion/common/tree_node/trait.TreeNode.html#overview"&gt;TreeNode API&lt;/a&gt; and &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/joins/struct.HashJoinExec.html#method.swap_inputs"&gt;reordering joins&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This combination of features along with &lt;a href="https://docs.rs/datafusion/latest/datafusion/execution/session_state/struct.SessionStateBuilder.html#method.with_physical_optimizer_rule"&gt;custom optimizer passes&lt;/a&gt; lets users
customize the behavior to their use case, such as custom indexes like &lt;a href="https://uwheel.rs/post/datafusion_uwheel/"&gt;uWheel&lt;/a&gt;
and &lt;a href="https://github.com/datafusion-contrib/datafusion-materialized-views"&gt;materialized views&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The rationale for including only a basic optimizer is that any one particular
set of heuristics and cost model is unlikely to work well for the wide variety
of DataFusion users because of the tradeoffs involved. &lt;/p&gt;
&lt;p&gt;For example, some users may always have access to adequate resources, and want
the fastest query execution, and are willing to tolerate runtime errors or a
performance cliff when there is insufficient memory. Other users, however, may
be willing to accept a slower maximum performance in return for more predictable
performance when running in a resource constrained environment. This approach is
not universally agreed. One of us has &lt;a href="https://www.researchgate.net/publication/269306314_The_Vertica_Query_Optimizer_The_case_for_specialized_query_optimizers"&gt;previously argued the case for
specialized optimizers&lt;/a&gt; in a more academic paper, and the topic comes up
regularly in the DataFusion community, (e.g. &lt;a href="https://github.com/apache/datafusion/issues/9846#issuecomment-2566568654"&gt;this recent comment&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Note: We are &lt;a href="https://github.com/apache/datafusion/issues/3929"&gt;actively improving&lt;/a&gt; this part of the code to help people write
their own optimizers (🎣 come help us define and implement it!)&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Optimizers are awesome, and we hope these two posts have demystified what they
are and how they are implemented in industrial systems. Like many modern query
engine designs, the common techniques are well known, though require substantial
effort to get right.  DataFusion&amp;rsquo;s industrial strength optimizers can and do
serve many real world systems well and we expect that number to grow over time.&lt;/p&gt;
&lt;p&gt;We also think DataFusion provides interesting opportunities for optimizer
research. As we discussed, there are still unsolved problems such as optimal
join ordering. Experiments in papers often use academic systems or modify
optimizers in tightly integrated open source systems (for example, the recent
&lt;a href="https://www.vldb.org/pvldb/vol17/p1350-justen.pdf"&gt;POLARs paper&lt;/a&gt; uses DuckDB). However, using a tightly integrated system
constrains the research to the set of heuristics and structure provided by that
system. Hopefully DataFusion&amp;rsquo;s documentation, &lt;a href="https://dl.acm.org/doi/10.1145/3626246.3653368"&gt;newly citeable SIGMOD paper&lt;/a&gt;, and
modular design will encourage more broadly applicable research in this area.&lt;/p&gt;
&lt;p&gt;And finally, as always, if you are interested in working on query engines and
learning more about how they are designed and implemented, please &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;join our
community&lt;/a&gt;. We welcome first time contributors as well as long time participants
to the fun of building a database together.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;&lt;a id="footnote7"&gt;&lt;/a&gt;&lt;sup&gt;[7]&lt;/sup&gt; See &lt;a href="https://btw-2015.informatik.uni-hamburg.de/res/proceedings/Hauptband/Wiss/Neumann-Unnesting_Arbitrary_Querie.pdf"&gt;Unnesting Arbitrary Queries&lt;/a&gt; from Neumann and Kemper for a more academic treatment.&lt;/p&gt;
&lt;p&gt;&lt;a id="footnote8"&gt;&lt;/a&gt;&lt;sup&gt;[8]&lt;/sup&gt; One of my favorite terms I learned from Andy Pavlo&amp;rsquo;s CMU online lectures&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.8.0 Release</title><link href="https://datafusion.apache.org/blog/2025/05/06/datafusion-comet-0.8.0" rel="alternate"></link><published>2025-05-06T00:00:00+00:00</published><updated>2025-05-06T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-05-06:/blog/2025/05/06/datafusion-comet-0.8.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.8.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;This release covers approximately six weeks of development …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.8.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;This release covers approximately six weeks of development work and is the result of merging 81 PRs from 11
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.8.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Performance &amp;amp; Stability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Up to 4x speedup in jobs using &lt;code&gt;dropDuplicates&lt;/code&gt;, thanks to optimizations in the &lt;code&gt;first_value&lt;/code&gt; and &lt;code&gt;last_value&lt;/code&gt;
  aggregate functions in DataFusion 47.0.0.&lt;/li&gt;
&lt;li&gt;Introduction of a global Tokio runtime, which resolves potential deadlocks in certain multi-task scenarios.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Native Shuffle Improvements&lt;/h2&gt;
&lt;p&gt;Significant enhancements to the native shuffle mechanism include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lower memory usage through using &lt;code&gt;interleave_record_batches&lt;/code&gt; instead of using array builders.&lt;/li&gt;
&lt;li&gt;Support for complex types in shuffle data (note: hash partition expressions still require primitive types).&lt;/li&gt;
&lt;li&gt;Reclaimable shuffle files, reducing disk pressure.&lt;/li&gt;
&lt;li&gt;Respects &lt;code&gt;spark.local.dir&lt;/code&gt; for temporary storage.&lt;/li&gt;
&lt;li&gt;Per-task shuffle metrics are now available, providing better visibility into execution behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Experimental Support for DataFusion&amp;rsquo;s Parquet Scan&lt;/h2&gt;
&lt;p&gt;It is now possible to configure Comet to use DataFusion&amp;rsquo;s Parquet reader instead of Comet&amp;rsquo;s current Parquet reader. This
has the advantage of supporting complex types, and also has performance optimizations that are not present in Comet's
existing reader.&lt;/p&gt;
&lt;p&gt;This release continues with the ongoing improvements and bug fixes and supports more use cases, but there are still
some known issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are schema coercion bugs for nested types containing INT96 columns, which can cause incorrect results.&lt;/li&gt;
&lt;li&gt;There are compatibility issues when reading integer values that are larger than their type annotation, such as the
  value 1024 being stored in a field annotated as int(8).&lt;/li&gt;
&lt;li&gt;A small number of Spark SQL tests remain unsupported (&lt;a href="https://github.com/apache/datafusion-comet/issues/1545"&gt;#1545&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To enable DataFusion&amp;rsquo;s Parquet reader, either set &lt;code&gt;spark.comet.scan.impl=native_datafusion&lt;/code&gt; or set the environment
variable &lt;code&gt;COMET_PARQUET_SCAN_IMPL=native_datafusion&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Updates to Supported Spark Versions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Added support for Spark 3.5.5&lt;/li&gt;
&lt;li&gt;Dropped support for Spark 3.3.x&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>User defined Window Functions in DataFusion</title><link href="https://datafusion.apache.org/blog/2025/04/19/user-defined-window-functions" rel="alternate"></link><published>2025-04-19T00:00:00+00:00</published><updated>2025-04-19T00:00:00+00:00</updated><author><name>Aditya Singh Rathore, Andrew Lamb</name></author><id>tag:datafusion.apache.org,2025-04-19:/blog/2025/04/19/user-defined-window-functions</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;Window functions are a powerful feature in SQL, allowing for complex analytical computations over a subset of data. However, efficiently implementing them, especially sliding windows, can be quite challenging. With &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;'s user-defined window functions, developers can easily take advantage of all the effort put into DataFusion's implementation.&lt;/p&gt;
&lt;p&gt;In …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;Window functions are a powerful feature in SQL, allowing for complex analytical computations over a subset of data. However, efficiently implementing them, especially sliding windows, can be quite challenging. With &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;'s user-defined window functions, developers can easily take advantage of all the effort put into DataFusion's implementation.&lt;/p&gt;
&lt;p&gt;In this post, we'll explore:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;What window functions are and why they matter&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Understanding sliding windows&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The challenges of computing window aggregates efficiently&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;How to implement user-defined window functions in DataFusion&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Understanding Window Functions in SQL&lt;/h2&gt;
&lt;p&gt;Imagine you're analyzing sales data and want insights without losing the finer details. This is where &lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Window_function_(SQL)"&gt;window functions&lt;/a&gt;&lt;/strong&gt; come into play. Unlike &lt;strong&gt;GROUP BY&lt;/strong&gt;, which condenses data, window functions let you retain each row while performing calculations over a defined &lt;strong&gt;range&lt;/strong&gt; &amp;mdash;like having a moving lens over your dataset.&lt;/p&gt;
&lt;p&gt;Picture a business tracking daily sales. They need a running total to understand cumulative revenue trends without collapsing individual transactions. SQL makes this easy:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT id, value, SUM(value) OVER (ORDER BY id) AS running_total
FROM sales;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;example:
+------------+--------+-------------------------------+
|   Date     | Sales  | Rows Considered               |
+------------+--------+-------------------------------+
| Jan 01     | 100    | [100]                         |
| Jan 02     | 120    | [100, 120]                    |
| Jan 03     | 130    | [100, 120, 130]               |
| Jan 04     | 150    | [100, 120, 130, 150]          |
| Jan 05     | 160    | [100, 120, 130, 150, 160]     |
| Jan 06     | 180    | [100, 120, 130, 150, 160, 180]|
| Jan 07     | 170    | [100, ..., 170] (7 days)      |
| Jan 08     | 175    | [120, ..., 175]               |
+------------+--------+-------------------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: A row-by-row representation of how a 7-day moving average includes the previous 6 days and the current one.&lt;/p&gt;
&lt;p&gt;This helps in analytical queries where we need cumulative sums, moving averages, or ranking without losing individual records.&lt;/p&gt;
&lt;h2&gt;User Defined Window Functions&lt;/h2&gt;
&lt;p&gt;DataFusion's &lt;a href="https://datafusion.apache.org/user-guide/sql/window_functions.html"&gt;Built-in window functions&lt;/a&gt; such as &lt;code&gt;first_value&lt;/code&gt;, &lt;code&gt;rank&lt;/code&gt; and &lt;code&gt;row_number&lt;/code&gt; serve many common use cases, but sometimes custom logic is needed&amp;mdash;for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Calculating moving averages with complex conditions (e.g. exponential averages, integrals, etc)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Implementing a custom ranking strategy&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tracking non-standard cumulative logic&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus, &lt;strong&gt;User-Defined Window Functions (UDWFs)&lt;/strong&gt; allow developers to define their own behavior while allowing DataFusion to handle the calculations of the  windows and grouping specified in the &lt;code&gt;OVER&lt;/code&gt; clause&lt;/p&gt;
&lt;p&gt;Writing a user defined window function is slightly more complex than an aggregate function due
to the variety of ways that window functions are called. I recommend reviewing the
&lt;a href="https://datafusion.apache.org/library-user-guide/adding-udfs.html#registering-a-window-udf"&gt;online documentation&lt;/a&gt;
for a description of which functions need to be implemented. &lt;/p&gt;
&lt;h2&gt;Understanding Sliding Window&lt;/h2&gt;
&lt;p&gt;Sliding windows define a &lt;strong&gt;moving range&lt;/strong&gt; of data over which aggregations are computed. Unlike simple cumulative functions, these windows are dynamically updated as new data arrives.&lt;/p&gt;
&lt;p&gt;For instance, if we want a 7-day moving average of sales:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT date, sales, 
       AVG(sales) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS moving_avg
FROM sales;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, each row&amp;rsquo;s result is computed based on the last 7 days, making it computationally intensive as data grows.&lt;/p&gt;
&lt;h2&gt;Why Computing Sliding Windows Is Hard&lt;/h2&gt;
&lt;p&gt;Imagine you&amp;rsquo;re at a caf&amp;eacute;, and the barista is preparing coffee orders. If they made each cup from scratch without using pre-prepared ingredients, the process would be painfully slow. This is exactly the problem with na&amp;iuml;ve sliding window computations.&lt;/p&gt;
&lt;p&gt;Computing sliding windows efficiently is tricky because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;High Computation Costs:&lt;/strong&gt; Just like making coffee from scratch for each customer, recalculating aggregates for every row is expensive.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Data Shuffling:&lt;/strong&gt; In large distributed systems, data must often be shuffled between nodes, causing delays&amp;mdash;like passing orders between multiple baristas who don&amp;rsquo;t communicate efficiently.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State Management:&lt;/strong&gt; Keeping track of past computations is like remembering previous orders without writing them down&amp;mdash;error-prone and inefficient.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Many traditional query engines struggle to optimize these computations effectively, leading to sluggish performance.&lt;/p&gt;
&lt;h2&gt;How DataFusion Evaluates Window Functions Quickly&lt;/h2&gt;
&lt;p&gt;In the world of big data, every millisecond counts. Imagine you&amp;rsquo;re analyzing stock market data, tracking sensor readings from millions of IoT devices, or crunching through massive customer logs&amp;mdash;speed matters. This is where &lt;a href="https://datafusion.apache.org/"&gt;DataFusion&lt;/a&gt; shines, making window function computations blazing fast. Let&amp;rsquo;s break down how it achieves this remarkable performance.&lt;/p&gt;
&lt;p&gt;DataFusion implements the battle tested sort-based approach described in &lt;a href="https://www.vldb.org/pvldb/vol8/p1058-leis.pdf"&gt;this
paper&lt;/a&gt; which is also used in systems such as Postgresql and Vertica. The input
is first sorted by both the &lt;code&gt;PARTITION BY&lt;/code&gt; and &lt;code&gt;ORDER BY&lt;/code&gt; expressions and
then the &lt;a href="https://github.com/apache/datafusion/blob/7ff6c7e68540c69b399a171654d00577e6f886bf/datafusion/physical-plan/src/windows/window_agg_exec.rs"&gt;WindowAggExec&lt;/a&gt; operator efficiently determines the partition boundaries and
creates appropriate &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/trait.PartitionEvaluator.html#background"&gt;PartitionEvaluator&lt;/a&gt; instances. &lt;/p&gt;
&lt;p&gt;The sort-based approach is well understood, scales to large data sets, and
leverages DataFusion's highly optimized sort implementation. DataFusion minimizes
resorting by leveraging the sort order tracking and optimizations described in
the &lt;a href="https://datafusion.apache.org/blog/2025/03/11/ordering-analysis/"&gt;Using Ordering for Better Plans blog&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;For example, given the query such as the following to compute the starting,
ending and average price for each stock:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT 
  FIRST_VALUE(price) OVER (PARTITION BY date_bin('1 month', time) ORDER BY time DESC) AS start_price, 
  FIRST_VALUE(price) OVER (PARTITION BY date_bin('1 month', time) ORDER BY time DESC) AS end_price,
  AVG(price)         OVER (PARTITION BY date_bin('1 month', time))                    AS avg_price
FROM quotes;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the input data is not sorted, DataFusion will first sort the data by the
&lt;code&gt;date_bin&lt;/code&gt; and &lt;code&gt;time&lt;/code&gt; and then &lt;a href="https://github.com/apache/datafusion/blob/7ff6c7e68540c69b399a171654d00577e6f886bf/datafusion/physical-plan/src/windows/window_agg_exec.rs"&gt;WindowAggExec&lt;/a&gt; computes the partition boundaries
and invokes the appropriate &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/trait.PartitionEvaluator.html#background"&gt;PartitionEvaluator&lt;/a&gt; API methods depending on the window
definition in the &lt;code&gt;OVER&lt;/code&gt; clause and the declared capabilities of the function.&lt;/p&gt;
&lt;p&gt;For example, evaluating &lt;code&gt;window_func(val) OVER (PARTITION BY col)&lt;/code&gt;
on the following data:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;col | val
--- + ----
 A  | 10
 A  | 10
 C  | 20
 D  | 30
 D  | 30
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Will instantiate three &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/trait.PartitionEvaluator.html#background"&gt;PartitionEvaluator&lt;/a&gt;s, one each for the
partitions defined by &lt;code&gt;col=A&lt;/code&gt;, &lt;code&gt;col=B&lt;/code&gt;, and &lt;code&gt;col=C&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;col | val
--- + ----
 A  | 10     &amp;lt;--- partition 1
 A  | 10

col | val
--- + ----
 C  | 20     &amp;lt;--- partition 2

col | val
--- + ----
 D  | 30     &amp;lt;--- partition 3
 D  | 30
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Creating your own Window Function&lt;/h3&gt;
&lt;p&gt;DataFusion supports &lt;a href="https://datafusion.apache.org/library-user-guide/adding-udfs.html"&gt;user-defined window aggregates (UDWAs)&lt;/a&gt;, meaning you can bring your own window function logic using the exact same APIs and performance as the built in functions.&lt;/p&gt;
&lt;p&gt;For example, we will declare a user defined window function that computes a moving average.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;use datafusion::arrow::{array::{ArrayRef, Float64Array, AsArray}, datatypes::Float64Type};
use datafusion::logical_expr::{PartitionEvaluator};
use datafusion::common::ScalarValue;
use datafusion::error::Result;
/// This implements the lowest level evaluation for a window function
///
/// It handles calculating the value of the window function for each
/// distinct values of `PARTITION BY`
#[derive(Clone, Debug)]
struct MyPartitionEvaluator {}

impl MyPartitionEvaluator {
    fn new() -&amp;gt; Self {
        Self {}
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Different evaluation methods are called depending on the various
settings of WindowUDF and the query. In the first example, we use the simplest and most
general, &lt;code&gt;evaluate&lt;/code&gt; function. We will see how to use &lt;code&gt;PartitionEvaluator&lt;/code&gt; for the other more
advanced uses later in the article.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;impl PartitionEvaluator for MyPartitionEvaluator {
    /// Tell DataFusion the window function varies based on the value
    /// of the window frame.
    fn uses_window_frame(&amp;amp;self) -&amp;gt; bool {
        true
    }

    /// This function is called once per input row.
    ///
    /// `range`specifies which indexes of `values` should be
    /// considered for the calculation.
    ///
    /// Note this is the SLOWEST, but simplest, way to evaluate a
    /// window function. It is much faster to implement
    /// evaluate_all or evaluate_all_with_rank, if possible
    fn evaluate(
        &amp;amp;mut self,
        values: &amp;amp;[ArrayRef],
        range: &amp;amp;std::ops::Range&amp;lt;usize&amp;gt;,
    ) -&amp;gt; Result&amp;lt;ScalarValue&amp;gt; {
        // Again, the input argument is an array of floating
        // point numbers to calculate a moving average
        let arr: &amp;amp;Float64Array = values[0].as_ref().as_primitive::&amp;lt;Float64Type&amp;gt;();

        let range_len = range.end - range.start;

        // our smoothing function will average all the values in the
        let output = if range_len &amp;gt; 0 {
            let sum: f64 = arr.values().iter().skip(range.start).take(range_len).sum();
            Some(sum / range_len as f64)
        } else {
            None
        };

        Ok(ScalarValue::Float64(output))
    }
}

/// Create a `PartitionEvaluator` to evaluate this function on a new
/// partition.
fn make_partition_evaluator() -&amp;gt; Result&amp;lt;Box&amp;lt;dyn PartitionEvaluator&amp;gt;&amp;gt; {
    Ok(Box::new(MyPartitionEvaluator::new()))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Registering a Window UDF&lt;/h3&gt;
&lt;p&gt;To register a Window UDF, you need to wrap the function implementation in a &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/struct.WindowUDF.html"&gt;WindowUDF&lt;/a&gt; struct and then register it with the &lt;code&gt;SessionContext&lt;/code&gt;. DataFusion provides the &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/fn.create_udwf.html"&gt;create_udwf&lt;/a&gt; helper functions to make this easier. There is a lower level API with more functionality but is more complex, that is documented in &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_udwf.rs"&gt;advanced_udwf.rs&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;use datafusion::logical_expr::{Volatility, create_udwf};
use datafusion::arrow::datatypes::DataType;
use std::sync::Arc;

// here is where we define the UDWF. We also declare its signature:
let smooth_it = create_udwf(
    "smooth_it",
    DataType::Float64,
    Arc::new(DataType::Float64),
    Volatility::Immutable,
    Arc::new(make_partition_evaluator),
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/fn.create_udwf.html"&gt;create_udwf&lt;/a&gt; functions take  five arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;first argument&lt;/strong&gt; is the name of the function. This is the name that will be used in SQL queries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;second argument&lt;/strong&gt; is the &lt;code&gt;DataType of&lt;/code&gt; input array (attention: this is not a list of arrays). I.e. in this case, the function accepts &lt;code&gt;Float64&lt;/code&gt; as argument.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;third argument&lt;/strong&gt; is the return type of the function. I.e. in this case, the function returns an &lt;code&gt;Float64&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;fourth argument&lt;/strong&gt; is the volatility of the function. In short, this is used to determine if the function&amp;rsquo;s performance can be optimized in some situations. In this case, the function is &lt;code&gt;Immutable&lt;/code&gt; because it always returns the same value for the same input. A random number generator would be &lt;code&gt;Volatile&lt;/code&gt; because it returns a different value for the same input.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;fifth argument&lt;/strong&gt; is the function implementation. This is the function that we defined above.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;That gives us a &lt;strong&gt;WindowUDF&lt;/strong&gt; that we can register with the &lt;code&gt;SessionContext&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;use datafusion::execution::context::SessionContext;

let ctx = SessionContext::new();

ctx.register_udwf(smooth_it);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For example, if we have a &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/core/tests/data/cars.csv"&gt;cars.csv&lt;/a&gt; whose contents like&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;car,speed,time
red,20.0,1996-04-12T12:05:03.000000000
red,20.3,1996-04-12T12:05:04.000000000
green,10.0,1996-04-12T12:05:03.000000000
green,10.3,1996-04-12T12:05:04.000000000
...
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, we can query like below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;use datafusion::datasource::file_format::options::CsvReadOptions;

#[tokio::main]
async fn main() -&amp;gt; Result&amp;lt;()&amp;gt; {

    let ctx = SessionContext::new();

    let smooth_it = create_udwf(
        "smooth_it",
        DataType::Float64,
        Arc::new(DataType::Float64),
        Volatility::Immutable,
        Arc::new(make_partition_evaluator),
    );
    ctx.register_udwf(smooth_it);

    // register csv table first
    let csv_path = "../../datafusion/core/tests/data/cars.csv".to_string();
    ctx.register_csv("cars", &amp;amp;csv_path, CsvReadOptions::default().has_header(true)).await?;

    // do query with smooth_it
    let df = ctx
        .sql(r#"
            SELECT
                car,
                speed,
                smooth_it(speed) OVER (PARTITION BY car ORDER BY time) as smooth_speed,
                time
            FROM cars
            ORDER BY car
        "#)
        .await?;

    // print the results
    df.show().await?;
    Ok(())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The output will be like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;+-------+-------+--------------------+---------------------+
| car   | speed | smooth_speed       | time                |
+-------+-------+--------------------+---------------------+
| green | 10.0  | 10.0               | 1996-04-12T12:05:03 |
| green | 10.3  | 10.15              | 1996-04-12T12:05:04 |
| green | 10.4  | 10.233333333333334 | 1996-04-12T12:05:05 |
| green | 10.5  | 10.3               | 1996-04-12T12:05:06 |
| green | 11.0  | 10.440000000000001 | 1996-04-12T12:05:07 |
| green | 12.0  | 10.700000000000001 | 1996-04-12T12:05:08 |
| green | 14.0  | 11.171428571428573 | 1996-04-12T12:05:09 |
| green | 15.0  | 11.65              | 1996-04-12T12:05:10 |
| green | 15.1  | 12.033333333333333 | 1996-04-12T12:05:11 |
| green | 15.2  | 12.35              | 1996-04-12T12:05:12 |
| green | 8.0   | 11.954545454545455 | 1996-04-12T12:05:13 |
| green | 2.0   | 11.125             | 1996-04-12T12:05:14 |
| red   | 20.0  | 20.0               | 1996-04-12T12:05:03 |
| red   | 20.3  | 20.15              | 1996-04-12T12:05:04 |
...
...
+-------+-------+--------------------+---------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives you full flexibility to build &lt;strong&gt;domain-specific logic&lt;/strong&gt; that plugs seamlessly into DataFusion&amp;rsquo;s engine &amp;mdash; all without sacrificing performance.&lt;/p&gt;
&lt;h2&gt;Final Thoughts and Recommendations&lt;/h2&gt;
&lt;p&gt;Window functions may be common in SQL, but &lt;em&gt;efficient and extensible&lt;/em&gt; window functions in engines are rare. 
While many databases support user defined scalar and user defined aggregate functions, user defined window functions are not as common and Datafusion making it easier for all .&lt;/p&gt;
&lt;p&gt;For anyone who is curious about &lt;a href="https://datafusion.apache.org/"&gt;DataFusion&lt;/a&gt; I highly recommend
giving it a try. This post was designed to make it easier for new users to work with User Defined Window Functions by giving a few examples of how one might implement these.&lt;/p&gt;
&lt;p&gt;When it comes to designing UDFs, I strongly recommend reviewing the 
&lt;a href="https://datafusion.apache.org/library-user-guide/adding-udfs.html"&gt;Window functions&lt;/a&gt; documentation.&lt;/p&gt;
&lt;p&gt;A heartfelt thank you to &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt; and &lt;a href="https://github.com/andygrove"&gt;@andygrove&lt;/a&gt; for their invaluable reviews and thoughtful feedback&amp;mdash;they&amp;rsquo;ve been instrumental in shaping this post.&lt;/p&gt;
&lt;p&gt;The Apache Arrow and Apache DataFusion communities are vibrant, welcoming, and full of passionate developers building something truly powerful. If you&amp;rsquo;re excited about high-performance analytics and want to be part of an open-source journey, I highly encourage you to explore the &lt;a href="(https://datafusion.apache.org/)"&gt;official documentation&lt;/a&gt; and dive into one of the many &lt;a href="https://github.com/apache/datafusion/issues"&gt;open issues&lt;/a&gt;. There&amp;rsquo;s never been a better time to get involved!&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>tpchgen-rs World’s fastest open source TPC-H data generator, written in Rust</title><link href="https://datafusion.apache.org/blog/2025/04/10/fastest-tpch-generator" rel="alternate"></link><published>2025-04-10T00:00:00+00:00</published><updated>2025-04-10T00:00:00+00:00</updated><author><name>Andrew Lamb, Achraf B, and Sean Smith</name></author><id>tag:datafusion.apache.org,2025-04-10:/blog/2025/04/10/fastest-tpch-generator</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}x
--&gt;
&lt;style&gt;
/* Table borders */
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}
th, td {
  padding: 3px;
}
&lt;/style&gt;
&lt;p&gt;&lt;strong&gt;TLDR: TPC-H SF=100 in 1min using tpchgen-rs vs 30min+ with dbgen&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;3 members of the &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; community used Rust and open source
development to build &lt;a href="https://github.com/clflushopt/tpchgen-rs"&gt;tpchgen-rs&lt;/a&gt;, a fully open TPC-H data generator over …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}x
--&gt;
&lt;style&gt;
/* Table borders */
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}
th, td {
  padding: 3px;
}
&lt;/style&gt;
&lt;p&gt;&lt;strong&gt;TLDR: TPC-H SF=100 in 1min using tpchgen-rs vs 30min+ with dbgen&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;3 members of the &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; community used Rust and open source
development to build &lt;a href="https://github.com/clflushopt/tpchgen-rs"&gt;tpchgen-rs&lt;/a&gt;, a fully open TPC-H data generator over 20x
faster than any other implementation we know of.&lt;/p&gt;
&lt;p&gt;It is now possible to create the TPC-H SF=100 dataset in 72.23 seconds (1.4 GB/s
😎) on a Macbook Air M3 with 16GB of memory, compared to the classic &lt;code&gt;dbgen&lt;/code&gt;
which takes 30 minutes&lt;sup&gt;1&lt;/sup&gt; (0.05GB/sec). On the same machine, it takes less than
2 minutes to create all 36 GB of SF=100 in &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; format, which takes 44 minutes using &lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt;.
It is finally convenient and efficient to run TPC-H queries locally when testing
analytical engines such as DataFusion.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Time to create TPC-H parquet dataset for Scale Factor  1, 10, 100 and 1000" class="img-responsive" src="/blog/images/fastest-tpch-generator/parquet-performance.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Time to create TPC-H dataset for Scale Factor (see below) 1, 10,
100 and 1000 as 8 individual SNAPPY compressed parquet files using a 22 core GCP
VM with 88GB of memory. For Scale Factor(SF) 100 &lt;code&gt;tpchgen&lt;/code&gt; takes 1 minute and 14 seconds and
&lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt; takes 17 minutes and 48 seconds. For SF=1000, &lt;code&gt;tpchgen&lt;/code&gt; takes 10
minutes and 26 and uses about 5 GB of RAM at peak, and we could not measure
DuckDB&amp;rsquo;s time as it &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html#resource-usage-of-the-data-generator"&gt;requires 647 GB of RAM&lt;/a&gt;, more than the 88 GB that was
available on our test machine. The testing methodology is in the
&lt;a href="https://github.com/clflushopt/tpchgen-rs/blob/main/benchmarks/BENCHMARKS.md"&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This blog explains what TPC-H is, how we ported the vintage C data generator to
Rust (yes, &lt;a href="https://www.reddit.com/r/rust/comments/4ri2gn/riir_rewrite_it_in_rust/"&gt;RWIR&lt;/a&gt;) and optimized its performance over the course of a few weeks
of part-time work. We began this project so we can easily generate TPC-H data in
&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; and &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Try it for yourself&lt;/h1&gt;
&lt;p&gt;The tool is entirely open source under the &lt;a href="https://www.apache.org/licenses/LICENSE-2.0"&gt;Apache 2.0 license&lt;/a&gt;. Visit the &lt;a href="https://github.com/clflushopt/tpchgen-rs"&gt;tpchgen-rs repository&lt;/a&gt; or try it for yourself by run the following commands after &lt;a href="https://www.rust-lang.org/tools/install"&gt;installing Rust&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ cargo install tpchgen-cli

# create SF=1 in classic TBL format
$ tpchgen-cli -s 1 

# create SF=10 in Parquet
$ tpchgen-cli -s 10 --format=parquet
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;What is TPC-H / dbgen?&lt;/h1&gt;
&lt;p&gt;The popular &lt;a href="https://www.tpc.org/tpch/"&gt;TPC-H&lt;/a&gt; benchmark (often referred to as TPCH) helps evaluate the
performance of database systems on &lt;a href="https://en.wikipedia.org/wiki/Online_analytical_processing"&gt;OLAP&lt;/a&gt; queries, the kind used to build BI
dashboards.&lt;/p&gt;
&lt;p&gt;TPC-H has become a de facto standard for analytic systems. While there are &lt;a href="https://www.vldb.org/pvldb/vol9/p204-leis.pdf"&gt;well
known&lt;/a&gt; limitations as the data and queries do not well represent many real world
use cases, the majority of analytic database papers and industrial systems still
use TPC-H query performance benchmarks as a baseline. You will inevitably find
multiple results for  &amp;ldquo;&lt;code&gt;TPCH Performance &amp;lt;your favorite database&amp;gt;&lt;/code&gt;&amp;rdquo; in any
search engine.&lt;/p&gt;
&lt;p&gt;The benchmark was created at a time when access to high performance analytical
systems was not widespread, so the &lt;a href="https://www.tpc.org/"&gt;Transaction Processing Performance Council&lt;/a&gt;
defined a process of formal result verification. More recently, given the broad
availability of free and open source database systems, it is common for users to
run and verify TPC-H performance themselves.&lt;/p&gt;
&lt;p&gt;TPC-H simulates a business environment with eight tables: &lt;code&gt;REGION&lt;/code&gt;, &lt;code&gt;NATION&lt;/code&gt;,
&lt;code&gt;SUPPLIER&lt;/code&gt;, &lt;code&gt;CUSTOMER&lt;/code&gt;, &lt;code&gt;PART&lt;/code&gt;, &lt;code&gt;PARTSUPP&lt;/code&gt;, &lt;code&gt;ORDERS&lt;/code&gt;, and &lt;code&gt;LINEITEM&lt;/code&gt;. These
tables are linked by foreign keys in a normalized schema representing a supply
chain with parts, suppliers, customers and orders. The benchmark itself is 22
SQL queries containing joins, aggregations, and sorting operations.&lt;/p&gt;
&lt;p&gt;The queries run against data created with &lt;code&gt;&lt;a href="https://github.com/electrum/tpch-dbgen"&gt;dbgen&lt;/a&gt;&lt;/code&gt;, a program
written in a pre &lt;a href="https://en.wikipedia.org/wiki/C99"&gt;C-99&lt;/a&gt; dialect, which generates data in a format called &lt;em&gt;TBL&lt;/em&gt;
(example in Figure 2). &lt;code&gt;dbgen&lt;/code&gt; creates data for each of the 8 tables for a
certain &lt;em&gt;Scale Factor&lt;/em&gt;, commonly abbreviated as SF. Example Scale Factors and
corresponding dataset sizes are shown in Table 1. There is no theoretical upper
bound on the Scale Factor.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;103|2844|845|3|23|40177.32|0.01|0.04|N|O|1996-09-11|1996-09-18|1996-09-26|NONE|FOB|ironic accou|
229|10540|801|6|29|42065.66|0.04|0.00|R|F|1994-01-14|1994-02-16|1994-01-22|NONE|FOB|uriously pending |
263|2396|649|1|22|28564.58|0.06|0.08|R|F|1994-08-24|1994-06-20|1994-09-09|NONE|FOB|efully express fo|
327|4172|427|2|9|9685.53|0.09|0.05|A|F|1995-05-24|1995-07-11|1995-06-05|NONE|AIR| asymptotes are fu|
450|5627|393|4|40|61304.80|0.05|0.03|R|F|1995-03-20|1995-05-25|1995-04-14|NONE|RAIL|ve. asymptote|
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Example TBL formatted output of &lt;code&gt;dbgen&lt;/code&gt; for the &lt;code&gt;LINEITEM&lt;/code&gt; table&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Data Size (TBL)&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Data Size (Parquet)&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.1
   &lt;/td&gt;
&lt;td&gt;103 Mb
   &lt;/td&gt;
&lt;td&gt;31 Mb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;1 Gb
   &lt;/td&gt;
&lt;td&gt;340 Mb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;10 Gb
   &lt;/td&gt;
&lt;td&gt;3.6 Gb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;107 Gb
   &lt;/td&gt;
&lt;td&gt;38 Gb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000
   &lt;/td&gt;
&lt;td&gt;1089 Gb
   &lt;/td&gt;
&lt;td&gt;379 Gb
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 1&lt;/strong&gt;: TPC-H data set sizes at different scale factors for both TBL and &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Why do we need a new TPC-H Data generator?&lt;/h1&gt;
&lt;p&gt;Despite the known limitations of the TPC-H benchmark, it is so well known that it
is used frequently in database performance analysis. To run TPC-H, you must first
load the data, using &lt;code&gt;dbgen&lt;/code&gt;, which is not ideal for several reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You must find and compile a copy of the 15+ year old C program (for example &lt;a href="https://github.com/electrum/tpch-dbgen"&gt;electrum/tpch-dbgen&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dbgen&lt;/code&gt; requires substantial time (Figure 3) and is not able to use more than one core.&lt;/li&gt;
&lt;li&gt;It outputs TBL format, which typically requires loading into your database (for example, &lt;a href="https://github.com/apache/datafusion/blob/507f6b6773deac69dd9d90dbe60831f5ea5abed1/datafusion/sqllogictest/test_files/tpch/create_tables.slt.part#L24-L124"&gt;here is how to do so&lt;/a&gt; in Apache DataFusion) prior to query.&lt;/li&gt;
&lt;li&gt;The implementation makes substantial assumptions about the operating environment, making it difficult to extend or embed into other systems.&lt;sup&gt;2&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="Time to generate TPC-H data in TBL format" class="img-responsive" src="/blog/images/fastest-tpch-generator/tbl-performance.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Time to generate TPC-H data in TBL format. &lt;code&gt;tpchgen&lt;/code&gt; is
shown in blue. &lt;code&gt;tpchgen&lt;/code&gt; restricted to a single core is shown in red. Unmodified
&lt;code&gt;dbgen&lt;/code&gt; is shown in green and &lt;code&gt;dbgen&lt;/code&gt; modified to use &lt;code&gt;-O3&lt;/code&gt; optimization level
is shown in yellow.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;dbgen&lt;/code&gt; is so inconvenient and takes so long that vendors often provide
preloaded TPC-H data, for example &lt;a href="https://docs.snowflake.com/en/user-guide/sample-data-tpch"&gt;Snowflake Sample Data&lt;/a&gt;, &lt;a href="https://docs.databricks.com/aws/en/discover/databricks-datasets"&gt;Databricks Sample
datasets&lt;/a&gt; and &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html#pre-generated-data-sets"&gt;DuckDB Pre-Generated Data Sets&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition to pre-generated datasets, DuckDB also provides a &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html"&gt;TPC-H extension&lt;/a&gt; 
for generating TPC-H datasets within DuckDB. This is so much easier to use than
the current alternatives that it leads many researchers and other thought
leaders to use DuckDB to evaluate new ideas. For example, &lt;a href="https://github.com/lmwnshn"&gt;Wan Shen
Lim&lt;/a&gt; explicitly &lt;a href="https://github.com/apache/datafusion/issues/14373"&gt;mentioned the ease of creating the TPC-H dataset&lt;/a&gt; as one reason
the first student project of &lt;a href="https://15799.courses.cs.cmu.edu/spring2025/"&gt;CMU-799 Spring 2025&lt;/a&gt; used DuckDB.&lt;/p&gt;
&lt;p&gt;As beneficial as the DuckDB TPC-H extension is, it is non-ideal for several reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Creates data in a proprietary format, which requires export to use in other systems.&lt;/li&gt;
&lt;li&gt;Requires significant time (e.g. 17 minutes for Scale Factor 10).&lt;/li&gt;
&lt;li&gt;Requires unnecessarily large amounts of memory (e.g. 71 GB for Scale Factor 10)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The above limitations makes it impractical to generate Scale Factor 100 and
above on laptops or standard workstations, though DuckDB offers &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html#pre-generated-data-sets"&gt;pre-computed
files&lt;/a&gt; for larger factors&lt;sup&gt;3&lt;/sup&gt;.&lt;/p&gt;
&lt;h1&gt;Why Rust?&lt;/h1&gt;
&lt;p&gt;Realistically we used Rust because we wanted to integrate the data generator
into &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; and &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt;. However, we also believe Rust is
superior to C/C++ due to its comparable performance, but much higher programmer
productivity (Figure 4). Productivity in this case refers to the ease of
optimizing and adding multithreading without introducing hard to debug memory
safety or concurrency issues.&lt;/p&gt;
&lt;p&gt;While Rust does allow unsafe access to memory (eliding bounds checking, for
example), when required for performance, our implementation is entirely memory
safe. The only &lt;a href="https://github.com/search?q=repo%3Aclflushopt%2Ftpchgen-rs%20unsafe&amp;amp;type=code"&gt;unsafe&lt;/a&gt; code is used to &lt;a href="https://github.com/clflushopt/tpchgen-rs/blob/c651da1fc309f9cb3872cbdf71e4796904dc62c6/tpchgen/src/text.rs#L72"&gt;skip&lt;/a&gt; UTF8 validation on known ASCII
strings.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Lamb Theory on Evolution of Systems Languages" class="img-responsive" src="/blog/images/fastest-tpch-generator/lamb-theory.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Lamb Theory of System Language Evolution from &lt;a href="https://midas.bu.edu/assets/slides/andrew_lamb_slides.pdf"&gt;Boston University
MiDAS Fall 2024 (Data Systems Seminar)&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=CpnxuBwHbUc"&gt;recording&lt;/a&gt;. Special
thanks to &lt;a href="https://x.com/KurtFehlhauer"&gt;@KurtFehlhauer&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;How: The Journey&lt;/h1&gt;
&lt;p&gt;We did it together as a team in the open over the course of a few weeks.
&lt;a href="https://github.com/lmwnshn"&gt;Wan Shen Lim&lt;/a&gt; inspired the project by pointing out the benefits of &lt;a href="https://github.com/apache/datafusion/issues/14373"&gt;easy TPC-H
dataset creation&lt;/a&gt;  and &lt;a href="https://github.com/apache/datafusion/issues/14608#issuecomment-2651044600"&gt;suggesting we check out a Java port on February 11,
2025&lt;/a&gt;. Achraf made &lt;a href="https://github.com/clflushopt/tpchgen-rs/commit/53d3402680422a15349ece0a7ea3c3f001018ba0"&gt;first commit a few days later&lt;/a&gt; on February 16, and &lt;a href="https://github.com/clflushopt/tpchgen-rs/commit/9bb386a4c55b8cf93ffac1b98f29b5da990ee79e"&gt;Andrew
and Sean started helping on March 8, 2025&lt;/a&gt; and we &lt;a href="https://crates.io/crates/tpchgen/0.1.0"&gt;released version 0.1&lt;/a&gt; on
March 30, 2025.&lt;/p&gt;
&lt;h2&gt;Optimizing Single Threaded Performance&lt;/h2&gt;
&lt;p&gt;Archaf &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/16"&gt;completed the end to end conformance tests&lt;/a&gt;, to ensure correctness, and
an initial &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/12"&gt;cli check in&lt;/a&gt; on March 15, 2025.&lt;/p&gt;
&lt;p&gt;On a Macbook Pro M3 (Nov 2023), the initial performance numbers were actually
slower than the original Java implementation which was ported 😭. This wasn&amp;rsquo;t
surprising since the focus of the first version was to get a byte of byte
compatible port, and knew about the performance shortcomings and how to approach
them.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;0m10.307s
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;1m26.530s
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;14m56.986s
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 2&lt;/strong&gt;: Performance of running &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/12"&gt;the initial tpchgen-cli&lt;/a&gt;, measured with
&lt;code&gt;time target/release/tpchgen-cli -s $SCALE_FACTOR&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;With this strong foundation we began optimizing the code using Rust&amp;rsquo;s low level
memory management to improve performance while retaining memory safely. We spent
several days obsessing over low level details and implemented a textbook like
list of optimizations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/19"&gt;Avoiding startup overhead&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/26"&gt;not&lt;/a&gt; &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/32"&gt;copying&lt;/a&gt; strings (many more PRs as well)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/27"&gt;Rust&amp;rsquo;s zero overhead abstractions for dates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/35"&gt;Static strings&lt;/a&gt; (entirely safely with static lifetimes)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/33"&gt;Generics to avoid virtual function call overhead&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/62"&gt;Moving lookups from runtime&lt;/a&gt; to load time&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the time of writing, single threaded performance is now 2.5x-2.7x faster than the initial version, as shown in Table 3.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Times faster&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;0m4.079s
   &lt;/td&gt;
&lt;td&gt;2.5x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;0m31.616s
   &lt;/td&gt;
&lt;td&gt;2.7x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;5m28.083s
   &lt;/td&gt;
&lt;td&gt;2.7x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 3&lt;/strong&gt;: Single threaded &lt;code&gt;tpchgen-cli&lt;/code&gt; performance, measured with &lt;code&gt;time target/release/tpchgen-cli -s $SCALE_FACTOR --num-threads=1&lt;/code&gt;&lt;/p&gt;
&lt;h2&gt;Multi-threading&lt;/h2&gt;
&lt;p&gt;Then we applied &lt;a href="https://doc.rust-lang.org/book/ch16-00-concurrency.html"&gt;Rust&amp;rsquo;s fearless concurrency&lt;/a&gt; &amp;ndash; with a single, &lt;a href="https://github.com/clflushopt/tpchgen-rs/commit/ab720a70cdc80a711f4a3dda6bac05445106f499"&gt;small PR&lt;/a&gt; (272
net new lines) we updated the same memory safe code to run with multiple threads
and consume bounded memory using &lt;a href="https://thenewstack.io/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/"&gt;tokio for the thread scheduler&lt;/a&gt;&lt;sup&gt;4&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;As shown in Table 4, with this change, tpchgen-cli generates the full SF=100
dataset in 32 seconds (which is 3.3 GB/sec 🤯). Further investigation reveals
that at SF=100 our generator is actually IO bound (which is not the case for
&lt;code&gt;dbgen&lt;/code&gt; or &lt;code&gt;duckdb&lt;/code&gt;) &amp;ndash; it creates data &lt;strong&gt;faster than can be written to an SSD&lt;/strong&gt;.
When writing to &lt;code&gt;/dev/null&lt;/code&gt; tpchgen  generates the entire dataset in 25 seconds
(4 GB/s).&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Time&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Times faster than initial implementation&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Times faster than optimized single threaded&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;0m1.369s
   &lt;/td&gt;
&lt;td&gt;7.3x
   &lt;/td&gt;
&lt;td&gt;3x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;0m3.828s
   &lt;/td&gt;
&lt;td&gt;22.6x
   &lt;/td&gt;
&lt;td&gt;8.2x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;0m32.615s
   &lt;/td&gt;
&lt;td&gt;27.5x
   &lt;/td&gt;
&lt;td&gt;10x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100 (to /dev/null)
   &lt;/td&gt;
&lt;td&gt;0m25.088s
   &lt;/td&gt;
&lt;td&gt;35.7x
   &lt;/td&gt;
&lt;td&gt;13.1x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 4&lt;/strong&gt;: tpchgen-cli (multithreaded) performance measured with &lt;code&gt;time target/release/tpchgen-cli -s $SCALE_FACTOR&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Using Rust and async streams, the data generator is also fully streaming: memory
use does not increase with increasing data size / scale factors&lt;sup&gt;5&lt;/sup&gt;. The DuckDB
generator seems to &lt;a href="https://duckdb.org/docs/stable/extensions/tpch.html#resource-usage-of-the-data-generator"&gt;require far more memory&lt;/a&gt; than is commonly available on
developer laptops and memory use increases with scale factor. With &lt;code&gt;tpchgen-cli&lt;/code&gt;
it is perfectly possible to create data for SF=10000 or larger on a machine with
16GB of memory (assuming sufficient storage capacity).&lt;/p&gt;
&lt;h2&gt;Direct to parquet&lt;/h2&gt;
&lt;p&gt;At this point, &lt;code&gt;tpchgen-cli&lt;/code&gt; could very quickly generate the TBL format.
However, as described above, the TBL is annoying to work with, because&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It has no header&lt;/li&gt;
&lt;li&gt;It is like a CSV but the delimiter is &lt;code&gt;|&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Each line ends with an extra &lt;code&gt;|&lt;/code&gt; delimiter before the newline 🙄&lt;/li&gt;
&lt;li&gt;No system that we know can read them without additional configuration.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We next &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/54"&gt;added support for CSV&lt;/a&gt; generation (special thanks &lt;a href="https://github.com/niebayes"&gt;@niebayes&lt;/a&gt; from
Datalayers for finding and &lt;a href="https://github.com/clflushopt/tpchgen-rs/issues/73"&gt;fixing&lt;/a&gt; &lt;a href="https://github.com/clflushopt/tpchgen-rs/issues/65"&gt;bugs&lt;/a&gt;) which performs at the same
speed as TBL. While CSV files are far more standard than TBL, they must still be
parsed prior to load and automatic type inference may not deduce the types
needed for the TPC-H benchmarks (e.g. floating point vs Decimal).&lt;/p&gt;
&lt;p&gt;What would be far more useful is a typed, efficient columnar format such as
Apache Parquet which is supported by all modern query engines. So we &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/71"&gt;made&lt;/a&gt; a
&lt;a href="https://crates.io/crates/tpchgen-arrow"&gt;tpchgen-arrow&lt;/a&gt; crate to create &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; arrays directly and then &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/61"&gt;a small
300 line PR&lt;/a&gt; to feed those arrays to the &lt;a href="https://crates.io/crates/parquet"&gt;Rust Parquet writer&lt;/a&gt;, again using
tokio for parallelized but memory bound work.&lt;/p&gt;
&lt;p&gt;This approach was simple, fast and scalable, as shown in Table 5. Even though
creating Parquet files is significantly more computationally expensive than TBL
or CSV, tpchgen-cli creates the full SF=100 parquet format dataset in less than
45 seconds.&lt;/p&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Scale Factor&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Time to generate Parquet&lt;/strong&gt;
&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Speed compared to tbl generation&lt;/strong&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1
   &lt;/td&gt;
&lt;td&gt;0m1.649s
   &lt;/td&gt;
&lt;td&gt;0.8x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10
   &lt;/td&gt;
&lt;td&gt;0m5.643s
   &lt;/td&gt;
&lt;td&gt;0.7x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100
   &lt;/td&gt;
&lt;td&gt;0m45.243s
   &lt;/td&gt;
&lt;td&gt;0.7x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100 (to /dev/null)
   &lt;/td&gt;
&lt;td&gt;0m45.153s
   &lt;/td&gt;
&lt;td&gt;0.5x
   &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Table 5&lt;/strong&gt;: &lt;code&gt;tpchgen-cli&lt;/code&gt; Parquet generation performance measured with  &lt;code&gt;time
target/release/tpchgen-cli -s $SCALE_FACTOR --format=parquet&lt;/code&gt;&lt;/p&gt;
&lt;h1&gt;Conclusion 👊🎤&lt;/h1&gt;
&lt;p&gt;In just a few days, with some fellow database nerds and the power of Rust, we built something 10x better than what currently exists. We hope it inspires more research
into analytical systems using the TPC-H dataset and that people build awesome
things with it. For example, Sean has already added &lt;a href="https://github.com/GlareDB/glaredb/pull/3549"&gt;on-demand generation of
tables to GlareDB&lt;/a&gt;. Please consider joining us and helping out at
&lt;a href="https://github.com/clflushopt/tpchgen-rs"&gt;https://github.com/clflushopt/tpchgen-rs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We met while working together on Apache DataFusion in various capacities. If you
are looking for a community of like minded people hacking on databases, we
welcome you to &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;come join us&lt;/a&gt;. We are in the process of integrating this into
DataFusion (see &lt;a href="https://github.com/apache/datafusion/issues/14608"&gt;apache/datafusion#14608&lt;/a&gt;) if you are interested in helping 🎣&lt;/p&gt;
&lt;h1&gt;About the Authors:&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/in/andrewalamb/"&gt;Andrew Lamb&lt;/a&gt; (&lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;) is a Staff Engineer at &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; and a PMC member of &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Achraf B (&lt;a href="https://github.com/clflushopt"&gt;@clflushopt&lt;/a&gt;) is a Software Engineer at &lt;a href="https://optable.co/"&gt;Optable&lt;/a&gt; where he works on data infrastructure.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/in/scsmithr/"&gt;Sean Smith&lt;/a&gt; (&lt;a href="https://github.com/scsmithr"&gt;@scsmithr&lt;/a&gt;) is the founder of &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt; focused on building a fast analytics database.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Footnotes themselves at the bottom. --&gt;
&lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;1&lt;/em&gt;: Actual Time: &lt;code&gt;30:35&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;2&lt;/em&gt;: It is possible to embed the dbgen code, which appears to be the approach taken by DuckDB. This approach was tried in GlareDB (&lt;a href="https://github.com/GlareDB/glaredb/pull/3313"&gt;GlareDB/glaredb#3313&lt;/a&gt;), but ultimately shelved given the amount of effort needed to adapt and isolate the dbgen code.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;3&lt;/em&gt;: It is pretty amazing to imagine the machine required to generate SF300 that had 1.8TB (!!) of RAM&lt;/p&gt;
&lt;p&gt;&lt;em&gt;4&lt;/em&gt;: We tried to &lt;a href="https://github.com/clflushopt/tpchgen-rs/pull/34"&gt;use Rayon (see discussion here)&lt;/a&gt;, but could not easily keep memory bounded.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;5&lt;/em&gt;: &lt;code&gt;tpchgen-cli&lt;/code&gt; memory usage is a function of the number of threads:  each thread needs some buffer space&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Python 46.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/03/30/datafusion-python-46.0.0" rel="alternate"></link><published>2025-03-30T00:00:00+00:00</published><updated>2025-03-30T00:00:00+00:00</updated><author><name>timsaucer</name></author><id>tag:datafusion.apache.org,2025-03-30:/blog/2025/03/30/datafusion-python-46.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/46.0.0/"&gt;datafusion-python 46.0.0&lt;/a&gt; has been released. This release
brings in all of the new features of the core &lt;a href="https://datafusion.apache.org/blog/2025/03/24/datafusion-46.0.0"&gt;DataFusion 46.0.0&lt;/a&gt; library. Since the last
blog post for &lt;a href="https://datafusion.apache.org/blog/2024/12/14/datafusion-python-43.1.0/"&gt;datafusion-python 43.1.0&lt;/a&gt;, a large number of improvements have been made
that can …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/46.0.0/"&gt;datafusion-python 46.0.0&lt;/a&gt; has been released. This release
brings in all of the new features of the core &lt;a href="https://datafusion.apache.org/blog/2025/03/24/datafusion-46.0.0"&gt;DataFusion 46.0.0&lt;/a&gt; library. Since the last
blog post for &lt;a href="https://datafusion.apache.org/blog/2024/12/14/datafusion-python-43.1.0/"&gt;datafusion-python 43.1.0&lt;/a&gt;, a large number of improvements have been made
that can be found in the &lt;a href="https://github.com/apache/datafusion-python/tree/main/dev/changelog"&gt;changelogs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We highly recommend reviewing the upstream &lt;a href="https://datafusion.apache.org/blog/2025/03/24/datafusion-46.0.0"&gt;DataFusion 46.0.0&lt;/a&gt; announcement.&lt;/p&gt;
&lt;h2&gt;Easier file reading&lt;/h2&gt;
&lt;p&gt;In these releases we have introduced two new ways to more easily read files into
DataFrames.&lt;/p&gt;
&lt;p&gt;PR &lt;a href="https://github.com/apache/datafusion-python/pull/982"&gt;#982&lt;/a&gt; introduced a series of easier read functions for Parquet, JSON, CSV, and
AVRO files. This introduces a concept of a global context that is available by
default when using these methods. Now instead of creating a default Session
Context and then calling the read methods, you can simply import these read
alternative methods and begin working with your DataFrames. Below is an example of
how easy to use this new approach is.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion.io import read_parquet
df = read_parquet(path="./examples/tpch/data/customer.parquet")
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;PR &lt;a href="https://github.com/apache/datafusion-python/pull/980"&gt;#980&lt;/a&gt; adds a method for setting up a session context to use URL tables. With
this enabled, you can use a path to a local file as a table name. An example
of how to use this is demonstrated in the following snippet.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import datafusion
ctx = datafusion.SessionContext().enable_url_table()
df = ctx.table("./examples/tpch/data/customer.parquet")
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Registering Table Views&lt;/h2&gt;
&lt;p&gt;DataFusion supports registering a logical plan as a view with a session context. This
allows creating views in one part of your work flow and passinng the session
context to other places where that logical plan can be reused. This is an useful
feature for building up complex workflows and for code clarity. PR &lt;a href="https://github.com/apache/datafusion-python/pull/1016"&gt;#1016&lt;/a&gt; enables this
feature in &lt;code&gt;datafusion-python&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For example, supposing you have a DataFrame called &lt;code&gt;df1&lt;/code&gt;, you could use this code snippet
to register the view and then use it in another place:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ctx.register_view("view1", df1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And then in another portion of your code which has access to the same session context
you can retrive the DataFrame with:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;df2 = ctx.table("view1")
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Asynchronous Iteration of Record Batches&lt;/h2&gt;
&lt;p&gt;Retrieving a &lt;code&gt;RecordBatch&lt;/code&gt; from a &lt;code&gt;RecordBatchStream&lt;/code&gt; was a synchronous call, which would
require the end user's code to wait for the data retrieval. This is described in
&lt;a href="https://github.com/apache/datafusion-python/issues/974"&gt;Issue 974&lt;/a&gt;. We continue to support this as a synchronous iterator, but we have also added
in the ability to retrieve the &lt;code&gt;RecordBatch&lt;/code&gt; using the Python asynchronous &lt;code&gt;anext&lt;/code&gt;
function.&lt;/p&gt;
&lt;h2&gt;Default ZSTD Compression for Parquet files&lt;/h2&gt;
&lt;p&gt;With PR &lt;a href="https://github.com/apache/datafusion-python/pull/981"&gt;#981&lt;/a&gt;, we change the saving of Parquet files to use zstd compression by default.
Previously the default was uncompressed, causing excessive disk storage. Zstd is an
excellent compression scheme that balances speed and compression ratio. Users can still
save their Parquet files uncompressed by passing in the appropriate value to the
&lt;code&gt;compression&lt;/code&gt; argument when calling &lt;code&gt;DataFrame.write_parquet&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;UDF Decorators&lt;/h2&gt;
&lt;p&gt;In PRs &lt;a href="https://github.com/apache/datafusion-python/pull/1040"&gt;#1040&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion-python/pull/1061"&gt;#1061&lt;/a&gt; we add methods to make creating user defined functions
easier and take advantage of Python decorators. With these PRs you can save a step
from defining a method and then defining a udf of that method. Instead you can
simply add the appropriate &lt;code&gt;udf&lt;/code&gt; decorator. Similar methods exist for aggregate
and window user defined functions.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@udf([pa.int64(), pa.int64()], pa.bool_(), "stable")
def my_custom_function(
    age: pa.Array,
    favorite_number: pa.Array,
) -&amp;gt; pa.Array:
    pass
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;&lt;code&gt;uv&lt;/code&gt; package management&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; is an extremely fast Python package manager, written in Rust. In the previous version
of &lt;code&gt;datafusion-python&lt;/code&gt; we had a combination of settings of PyPi and Conda. Instead, we
switch to using &lt;a href="https://github.com/astral-sh/uv"&gt;uv&lt;/a&gt; is our primary method for dependency management.&lt;/p&gt;
&lt;p&gt;For most users of DataFusion, this change will be transparent. You can still install
via &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt;. For developers, the instructions in the repository have been updated.&lt;/p&gt;
&lt;h2&gt;Code cleanup&lt;/h2&gt;
&lt;p&gt;In an effort to improve our code cleanliness and ensure we are following Python best
practices, we use &lt;a href="https://docs.astral.sh/ruff/"&gt;ruff&lt;/a&gt; to perform Python linting. Until now we enabled only a portion
of the available linters available. In PRs &lt;a href="https://github.com/apache/datafusion-python/pull/1055"&gt;#1055&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion-python/pull/1062"&gt;#1062&lt;/a&gt;, we enable many more
of these linters and made code improvements to ensure we are following these
recommendations.&lt;/p&gt;
&lt;h2&gt;Improved Jupyter Notebook rendering&lt;/h2&gt;
&lt;p&gt;Since PR &lt;a href="https://github.com/apache/datafusion-python/pull/839"&gt;#839&lt;/a&gt; in DataFusion 41.0.0 we have been able to render DataFrames using html in
&lt;a href="https://jupyter.org/"&gt;jupyter&lt;/a&gt; notebooks. This is a big improvement over the &lt;code&gt;show&lt;/code&gt; command when we have the
ability to render tables. In PR &lt;a href="https://github.com/apache/datafusion-python/pull/1036"&gt;#1036&lt;/a&gt; we went a step further and added in a variety
of features.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Now html tables are scrollable, vertically and horizontally.&lt;/li&gt;
&lt;li&gt;When data are truncated, we report this to the user.&lt;/li&gt;
&lt;li&gt;Instead of showing a small number of rows, we collect up to 2 megabytes of data to
display. Since we have scrollable tables, we are able to make more data available
to the user without sacrificing notebook usability.&lt;/li&gt;
&lt;li&gt;We report explicitly when the DataFrame is empty. Previously we would not output
anything for an empty table. This indicator is helpful to users to ensure their plans
are written correctly. Sometimes a non-output can be overlooked.&lt;/li&gt;
&lt;li&gt;For long output of data, we generate a collapsed view of the data with an option
for the user to click on it to expand the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the below view you can see an example of some of these features such as the
expandable text and scroll bars.&lt;/p&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Example html rendering in a jupyter notebook." class="img-responsive" src="/blog/images/python-datafusion-46.0.0/html_rendering.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 1&lt;/b&gt;: With the html rendering enhancements, tables are more easily
   viewable in jupyter notebooks.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2&gt;Extension Documentation&lt;/h2&gt;
&lt;p&gt;We have recently added &lt;a href="https://datafusion.apache.org/python/contributor-guide/ffi.html"&gt;Extension Documentation&lt;/a&gt; to the DataFusion in Python website. We
have received many requests about how to better understand how to integrate DataFusion
in Python with other Rust libraries. To address these questions we wrote an article about
some of the difficulties that we encounter when using Rust libraries in Python and our
approach to addressing them.&lt;/p&gt;
&lt;h2&gt;Migration Guide&lt;/h2&gt;
&lt;p&gt;During the upgrade from &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; to &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/44.0.0.md"&gt;DataFusion 44.0.0&lt;/a&gt; as our upstream core
dependency, we discovered a few changes were necessary within our repository and our
unit tests. These notes serve to help guide users who may encounter similar issues when
upgrading.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;RuntimeConfig&lt;/code&gt; is now deprecated in favor of &lt;code&gt;RuntimeEnvBuilder&lt;/code&gt;. The migration is
fairly straightforward, and the corresponding classes have been marked as deprecated. For
end users it should be simply a matter of changing the class name.&lt;/li&gt;
&lt;li&gt;If you perform a &lt;code&gt;concat&lt;/code&gt; of a &lt;code&gt;string_view&lt;/code&gt; and &lt;code&gt;string&lt;/code&gt;, it will now return a
&lt;code&gt;string_view&lt;/code&gt; instead of a &lt;code&gt;string&lt;/code&gt;. This likely only impacts unit tests that are validating
return types. In general, it is recommended to switch to using &lt;code&gt;string_view&lt;/code&gt; whenever 
possible. You can see the blog articles &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1/"&gt;String View Pt 1&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-2/"&gt;Pt 2&lt;/a&gt; for more information
on these performance improvements.&lt;/li&gt;
&lt;li&gt;The function &lt;code&gt;date_part&lt;/code&gt; now returns an &lt;code&gt;int32&lt;/code&gt; instead of a &lt;code&gt;float64&lt;/code&gt;. This is likely
only impactful to unit tests.&lt;/li&gt;
&lt;li&gt;We have upgraded the Python minimum version to 3.9 since 3.8 is no longer officially
supported.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Coming Soon&lt;/h2&gt;
&lt;p&gt;There is a lot of excitement around the upcoming work. This list is not comprehensive, but
a glimpse into some of the upcoming work includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reusable DataFusion UDFs: The way user defined functions are currently written in
&lt;code&gt;datafusion-python&lt;/code&gt; is slightly different from those written for the upstream Rust
&lt;code&gt;datafusion&lt;/code&gt;. The core ideas are usually the same, but it means it takes effort for users
to re-implement functions already written for Rust projects to be usable in Python. Issue
&lt;a href="https://github.com/apache/datafusion-python/issues/1017"&gt;#1017&lt;/a&gt; addresses this topic. Work is well underway to make it easier to expose these
user functions through the FFI boundary. This means that the work that already exists in
repositories such as those found in the &lt;a href="https://github.com/datafusion-contrib"&gt;datafusion-contrib&lt;/a&gt; project can be easily
re-used in Python. This will provide a low effort way to expose significant functionality
to the DataFusion in Python community.&lt;/li&gt;
&lt;li&gt;Additional table providers: We have work well underway to provide a host of table providers
to &lt;code&gt;datafusion-python&lt;/code&gt; including: sqlite, duckdb, postgres, odbc, and mysql! In
&lt;a href="https://github.com/datafusion-contrib/datafusion-table-providers/issues/279"&gt;datafusion-contrib #279&lt;/a&gt; we track the progress of this excellent work. Once complete, users
will be able to &lt;code&gt;pip install&lt;/code&gt; this library and get easy access to all of these table
providers. This is another way we are leveraging the FFI work to greatly expand the usability
of &lt;code&gt;datafusion-python&lt;/code&gt; with relatively low effort.&lt;/li&gt;
&lt;li&gt;External catalog and schema providers: For users who wish to go beyond table providers
and have an entire custom catalog with schema, Issue &lt;a href="https://github.com/apache/datafusion-python/issues/1091"&gt;#1091&lt;/a&gt; tracks the progress of exposing
this in Python. With this work, if you have already written a Rust based table catalog you
will be able to interface it in Python similar to the work described for the table
providers above.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is only a sample of the great work that is being done. If there are features you would
love to see, we encourage you to open an issue and join us as we build something wonderful.&lt;/p&gt;
&lt;h2&gt;Appreciation&lt;/h2&gt;
&lt;p&gt;We would like to thank everyone who has helped with these releases through their helpful
conversations, code review, issue descriptions, and code authoring. We would especially
like to thank the following authors of PRs who made these releases possible, listed in
alphabetical order by username: &lt;a href="https://github.com/chenkovsky"&gt;@chenkovsky&lt;/a&gt;, &lt;a href="https://github.com/CrystalZhou0529"&gt;@CrystalZhou0529&lt;/a&gt;, &lt;a href="https://github.com/ion-elgreco"&gt;@ion-elgreco&lt;/a&gt;,
&lt;a href="https://github.com/jsai28"&gt;@jsai28&lt;/a&gt;, &lt;a href="https://github.com/kevinjqliu"&gt;@kevinjqliu&lt;/a&gt;, &lt;a href="https://github.com/kylebarron"&gt;@kylebarron&lt;/a&gt;, &lt;a href="https://github.com/kosiew"&gt;@kosiew&lt;/a&gt;, &lt;a href="https://github.com/nirnayroy"&gt;@nirnayroy&lt;/a&gt;, and &lt;a href="https://github.com/Spaarsh"&gt;@Spaarsh&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you!&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;The DataFusion Python team is an active and engaging community and we would love
to have you join us and help the project.&lt;/p&gt;
&lt;p&gt;Here are some ways to get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learn more by visiting the &lt;a href="https://datafusion.apache.org/python/index.html"&gt;DataFusion Python project&lt;/a&gt; page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try out the project and provide feedback, file issues, and contribute code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Join us on &lt;a href="https://s.apache.org/slack-invite"&gt;ASF Slack&lt;/a&gt; or the &lt;a href="https://discord.gg/Qw5gKqHxUM"&gt;Arrow Rust Discord Server&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion 46.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/03/24/datafusion-46.0.0" rel="alternate"></link><published>2025-03-24T00:00:00+00:00</published><updated>2025-03-24T00:00:00+00:00</updated><author><name>Oznur Hanci and Berkay Sahin on behalf of the PMC</name></author><id>tag:datafusion.apache.org,2025-03-24:/blog/2025/03/24/datafusion-46.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We&amp;rsquo;re excited to announce the release of&amp;nbsp;&lt;strong&gt;Apache DataFusion 46.0.0&lt;/strong&gt;! This new version represents a significant milestone for the project, packing in a wide range of improvements and fixes. You can find the complete details in the&amp;nbsp;full &lt;a href="https://github.com/apache/datafusion/blob/branch-46/dev/changelog/46.0.0.md"&gt;changelog&lt;/a&gt;. We&amp;rsquo;ll highlight the most important changes below …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We&amp;rsquo;re excited to announce the release of&amp;nbsp;&lt;strong&gt;Apache DataFusion 46.0.0&lt;/strong&gt;! This new version represents a significant milestone for the project, packing in a wide range of improvements and fixes. You can find the complete details in the&amp;nbsp;full &lt;a href="https://github.com/apache/datafusion/blob/branch-46/dev/changelog/46.0.0.md"&gt;changelog&lt;/a&gt;. We&amp;rsquo;ll highlight the most important changes below and guide you through upgrading.&lt;/p&gt;
&lt;h2&gt;Breaking Changes&lt;/h2&gt;
&lt;p&gt;DataFusion 46.0.0 brings a few&amp;nbsp;&lt;strong&gt;breaking changes&lt;/strong&gt;&amp;nbsp;that may require adjustments to your code as described in the &lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html"&gt;Upgrade Guide&lt;/a&gt;. Here are the most notable ones:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/datafusion/pull/14224#"&gt;Unified &lt;code&gt;DataSourceExec&lt;/code&gt; Execution Plan&lt;/a&gt;&lt;strong&gt;:&lt;/strong&gt;&amp;nbsp;DataFusion 46.0.0 introduces a major refactor of scan operators. The separate file-format-specific execution plan nodes (&lt;code&gt;ParquetExec&lt;/code&gt;,&amp;nbsp;&lt;code&gt;CsvExec&lt;/code&gt;,&amp;nbsp;&lt;code&gt;JsonExec&lt;/code&gt;,&amp;nbsp;&lt;code&gt;AvroExec&lt;/code&gt;, etc.) have been&amp;nbsp;&lt;strong&gt;deprecated and merged into a single &lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;plan&lt;/strong&gt;. Format-specific logic is now encapsulated in new&amp;nbsp;&lt;code&gt;DataSource&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;FileSource&lt;/code&gt;&amp;nbsp;traits. This change simplifies the execution model, but if you have code that directly references the old plan nodes, you&amp;rsquo;ll need to update it to use&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;(see the&amp;nbsp;&lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html"&gt;Upgrade Guide&lt;/a&gt;&amp;nbsp;for examples of the new API).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/7360#:~:text=2"&gt;**Error Handling Improvements&lt;/a&gt; (&lt;code&gt;DataFusionError::Collection&lt;/code&gt;):**&amp;nbsp;We began overhauling DataFusion&amp;rsquo;s approach to error handling. In this release, a new error variant&amp;nbsp;&lt;code&gt;DataFusionError::Collection&lt;/code&gt;&amp;nbsp;(and related mechanisms) has been introduced to aggregate multiple errors into one. This is part of a broader effort to provide richer error context and reduce internal panics. As a result, some error types or messages have changed. Downstream code that matches on specific&amp;nbsp;&lt;code&gt;DataFusionError&lt;/code&gt;&amp;nbsp;variants might need adjustment.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Performance Improvements&lt;/h2&gt;
&lt;p&gt;DataFusion 46.0.0 comes with a slew of performance enhancements across the board. Here are some of the noteworthy optimizations in this release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faster&amp;nbsp;&lt;code&gt;median()&lt;/code&gt;&amp;nbsp;(no grouping):&lt;/strong&gt;&amp;nbsp;The&amp;nbsp;&lt;code&gt;median()&lt;/code&gt;&amp;nbsp;aggregate function got a special fast path when used without a&amp;nbsp;&lt;code&gt;GROUP BY&lt;/code&gt;. By optimizing its accumulator, median calculation is about&amp;nbsp;&lt;strong&gt;2&amp;times; faster&lt;/strong&gt;&amp;nbsp;in the single-group case. If you use&amp;nbsp;&lt;code&gt;MEDIAN()&lt;/code&gt;&amp;nbsp;on large datasets (especially as a single value), you should notice reduced query times (PR &lt;a href="https://github.com/apache/datafusion/pull/14399"&gt;#14399&lt;/a&gt; by &lt;a href="https://github.com/2010YOUY01"&gt;@2010YOUY01&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Optimized&amp;nbsp;&lt;code&gt;FIRST_VALUE&lt;/code&gt;/&lt;code&gt;LAST_VALUE&lt;/code&gt;:&lt;/strong&gt;&amp;nbsp;The&amp;nbsp;&lt;code&gt;FIRST_VALUE&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;LAST_VALUE&lt;/code&gt;&amp;nbsp;window functions have been improved by avoiding an internal sort of rows. Instead of sorting each partition, the implementation now uses a direct approach to pick the first/last element. This yields&amp;nbsp;&lt;strong&gt;10&amp;ndash;100% performance improvement&lt;/strong&gt;&amp;nbsp;for these functions, depending on the scenario. Queries using&amp;nbsp;&lt;code&gt;FIRST_VALUE(...) OVER (PARTITION BY ... ORDER BY ...)&lt;/code&gt;&amp;nbsp;will run faster, especially when partitions are large (PR &lt;a href="https://github.com/apache/datafusion/pull/14402"&gt;#14402&lt;/a&gt; by &lt;a href="https://github.com/blaginin"&gt;@blaginin&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;repeat()&lt;/code&gt;&amp;nbsp;String Function Boost:&lt;/strong&gt;&amp;nbsp;Repeating strings is now more efficient &amp;ndash; the&amp;nbsp;&lt;code&gt;repeat(text, n)&lt;/code&gt;&amp;nbsp;function was optimized by about&amp;nbsp;&lt;strong&gt;50%&lt;/strong&gt;. This was achieved by reducing allocations and using a more efficient concatenation strategy. If you generate large repeated strings in queries, this can cut the time nearly in half (PR &lt;a href="https://github.com/apache/datafusion/pull/14697"&gt;#14697&lt;/a&gt; by &lt;a href="https://github.com/zjregee"&gt;@zjregee&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Ultra-fast&amp;nbsp;&lt;code&gt;uuid()&lt;/code&gt;&amp;nbsp;UDF:&lt;/strong&gt;&amp;nbsp;The&amp;nbsp;&lt;code&gt;uuid()&lt;/code&gt;&amp;nbsp;function (which generates random UUID strings) received a major speed-up. It&amp;rsquo;s now roughly&amp;nbsp;&lt;strong&gt;40&amp;times; faster&lt;/strong&gt;&amp;nbsp;than before! The new implementation avoids unnecessary string copying and uses a more direct conversion to hex, making bulk UUID generation far more practical (PR &lt;a href="https://github.com/apache/datafusion/pull/14675"&gt;#14675&lt;/a&gt; by &lt;a href="https://github.com/simonvandel"&gt;@simonvandel&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Accelerated&amp;nbsp;&lt;code&gt;chr()&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;to_hex()&lt;/code&gt;:&lt;/strong&gt;&amp;nbsp;Several scalar functions have been micro-optimized. The&amp;nbsp;&lt;code&gt;chr()&lt;/code&gt;&amp;nbsp;function (which returns the character for a given ASCII code) is about&amp;nbsp;&lt;strong&gt;4&amp;times; faster&lt;/strong&gt;&amp;nbsp;now, and the&amp;nbsp;&lt;code&gt;to_hex()&lt;/code&gt;&amp;nbsp;function (which converts numbers to hex string) is roughly&amp;nbsp;&lt;strong&gt;2&amp;times; faster&lt;/strong&gt;. These improvements may be most noticeable in tight loops or when these functions are applied to large arrays of values (PR &lt;a href="https://github.com/apache/datafusion/pull/14700"&gt;#14700&lt;/a&gt; for&amp;nbsp;&lt;code&gt;chr&lt;/code&gt;, &lt;a href="https://github.com/apache/datafusion/pull/14686"&gt;#14686&lt;/a&gt; for&amp;nbsp;&lt;code&gt;to_hex&lt;/code&gt; by &lt;a href="https://github.com/simonvandel"&gt;@simonvandel&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;No More RowConverter in Grouped Ordering:&lt;/strong&gt;&amp;nbsp;We removed an inefficient step in the&amp;nbsp;&lt;em&gt;partial grouping&lt;/em&gt;&amp;nbsp;algorithm. The&amp;nbsp;&lt;code&gt;GroupOrderingPartial&lt;/code&gt;&amp;nbsp;operator no longer converts data to &amp;ldquo;row format&amp;rdquo; for each batch (via&amp;nbsp;&lt;code&gt;RowConverter&lt;/code&gt;). Instead, it uses a direct arrow-based approach to detect sort key changes. This eliminated overhead and yields a nice speedup for certain aggregation queries. (PR &lt;a href="https://github.com/apache/datafusion/pull/14566"&gt;#14566&lt;/a&gt; by &lt;a href="https://github.com/ctsk"&gt;@ctsk&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Predicate Pruning for&amp;nbsp;&lt;code&gt;NOT LIKE&lt;/code&gt;:&lt;/strong&gt;&amp;nbsp;DataFusion&amp;rsquo;s parquet reader can now prune row groups using&amp;nbsp;&lt;code&gt;NOT LIKE&lt;/code&gt;&amp;nbsp;filters, similar to how it handles&amp;nbsp;&lt;code&gt;LIKE&lt;/code&gt;. This means if you have a filter such as&amp;nbsp;&lt;code&gt;column NOT LIKE 'prefix%'&lt;/code&gt;, DataFusion can use min/max statistics to skip reading files/parts that can be determined to either entirely match or not match the predicate. In particular, a pattern like&amp;nbsp;&lt;code&gt;NOT LIKE 'X%'&lt;/code&gt;&amp;nbsp;can skip data ranges that definitely start with "X". While a niche case, it contributes to query efficiency in those scenarios (PR &lt;a href="https://github.com/apache/datafusion/pull/14567"&gt;#14567&lt;/a&gt; by &lt;a href="https://github.com/UBarney"&gt;@UBarney&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Google Summer of Code 2025&lt;/h2&gt;
&lt;p&gt;Another exciting development:&amp;nbsp;&lt;strong&gt;Apache DataFusion has been accepted as a mentoring organization for Google Summer of Code (GSoC) 2025&lt;/strong&gt;! 🎉 This means that this summer, students from around the world will have the opportunity to contribute to DataFusion under the guidance of our committers. We have put together &lt;a href="https://datafusion.apache.org/contributor-guide/gsoc_project_ideas.html"&gt;a list of project ideas&lt;/a&gt; that candidates can choose from.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re interested, check out our&amp;nbsp;&lt;a href="https://datafusion.apache.org/contributor-guide/gsoc_application_guidelines.html"&gt;GSoC Application Guidelines&lt;/a&gt;. We encourage students to reach out, discuss ideas with us, and apply.&lt;/p&gt;
&lt;h2&gt;Highlighted New Features&lt;/h2&gt;
&lt;h3&gt;Improved Diagnostics&lt;/h3&gt;
&lt;p&gt;DataFusion 46.0.0 introduces a new&amp;nbsp;&lt;a href="https://github.com/apache/datafusion/issues/14429"&gt;&lt;strong&gt;SQL Diagnostics framework&lt;/strong&gt;&lt;/a&gt;&amp;nbsp;to make error messages more understandable. This comes in the form of new&amp;nbsp;&lt;code&gt;Diagnostic&lt;/code&gt;&amp;nbsp;and&amp;nbsp;&lt;code&gt;DiagnosticEntry&lt;/code&gt;&amp;nbsp;types, which allow the system to attach rich context (like source query text spans) to error messages. In practical terms, certain planner errors will now point to the exact location in your SQL query that caused the issue. &lt;/p&gt;
&lt;p&gt;For example, if you reference an unknown table or miss a column in &lt;code&gt;GROUP BY&lt;/code&gt; the error message will include the query snippet causing the error. These diagnostics are meant for end-users of applications built on DataFusion, providing clearer messages instead of generic errors. Here&amp;rsquo;s an example:&lt;/p&gt;
&lt;p&gt;&lt;img alt="diagnostic-example" class="img-responsive" src="/blog/images/datafusion-46.0.0/diagnostic-example.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;Currently, diagnostics cover unresolved table/column references, missing &lt;code&gt;GROUP BY&lt;/code&gt; columns, ambiguous references, wrong number of UNION columns, type mismatches, and a few others. Future releases will extend this to more error types. This feature should greatly ease debugging of complex SQL by pinpointing errors directly in the query text. We thank &lt;a href="https://github.com/eliaperantoni"&gt;@eliaperantoni&lt;/a&gt; for his contributions in this project.&lt;/p&gt;
&lt;h3&gt;Unified&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;for Table Providers&lt;/h3&gt;
&lt;p&gt;As mentioned, DataFusion now uses a unified&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;for reading tables, which is both a breaking change and a feature.&amp;nbsp;&lt;em&gt;Why is this important?&lt;/em&gt;&amp;nbsp;The new approach simplifies how custom table providers are integrated and optimized. Namely, the optimizer can treat file scans uniformly and push down filters/limits more consistently when there is one execution plan that handles all data sources. The new&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;&amp;nbsp;is paired with a&amp;nbsp;&lt;code&gt;DataSource&lt;/code&gt;&amp;nbsp;trait that encapsulates format-specific behaviors (Parquet, CSV, JSON, Avro, etc.) in a pluggable way.&lt;/p&gt;
&lt;p&gt;All built-in sources (Parquet, CSV, Avro, Arrow, JSON, etc.) have been migrated to this framework. This unification makes the codebase cleaner and sets the stage for future enhancements (like consistent metadata handling and limit pushdown across all formats). Check out PR &lt;a href="https://github.com/apache/datafusion/pull/14224"&gt;#14224&lt;/a&gt; for design details. We thank &lt;a href="https://github.com/mertak-synnada"&gt;@mertak-synnada&lt;/a&gt; and &lt;a href="https://github.com/ozankabak"&gt;@ozankabak&lt;/a&gt; for their contributions.&lt;/p&gt;
&lt;h3&gt;FFI Support for Scalar UDFs&lt;/h3&gt;
&lt;p&gt;DataFusion&amp;rsquo;s Foreign Function Interface (FFI) has been extended to support&amp;nbsp;&lt;a href="https://github.com/apache/datafusion/pull/14579"&gt;&lt;strong&gt;user-defined scalar functions&lt;/strong&gt;&lt;/a&gt;&amp;nbsp;defined in external languages. In 46.0.0, you can now expose a custom scalar UDF through the FFI layer and use it in DataFusion as if it were built-in. This is particularly exciting for the &lt;strong&gt;Python bindings&lt;/strong&gt; and other language integrations &amp;ndash; it means you could define a function in Python (or C, etc.) and register it with DataFusion&amp;rsquo;s Rust core via the FFI crate. Thanks, &lt;a href="https://github.com/timsaucer"&gt;@timsaucer&lt;/a&gt;!&lt;/p&gt;
&lt;h3&gt;New Statistics/Distribution Framework&lt;/h3&gt;
&lt;p&gt;This release, thanks mainly to &lt;a href="https://github.com/Fly-Style"&gt;@Fly-Style&lt;/a&gt; with contributions from &lt;a href="https://github.com/ozankabak"&gt;@ozankabak&lt;/a&gt; and &lt;a href="https://github.com/berkaysynnada"&gt;@berkaysynnada&lt;/a&gt;, includes the initial pieces of a&amp;nbsp;&lt;a href="https://github.com/apache/datafusion/pull/14699"&gt;**redesigned statistics framework&lt;/a&gt;.&lt;strong&gt; DataFusion&amp;rsquo;s optimizer can now represent column data distributions using a new&amp;nbsp;&lt;code&gt;Distribution&lt;/code&gt;&amp;nbsp;enum, instead of the old precision or range estimations. The supported distribution types currently include&amp;nbsp;&lt;/strong&gt;Uniform, Gaussian (normal), Exponential, Bernoulli&lt;strong&gt;, and a&amp;nbsp;&lt;/strong&gt;Generic**&amp;nbsp;catch-all.&lt;/p&gt;
&lt;p&gt;For example, if a filter expression is applied to a column with a known uniform distribution range, the optimizer can propagate that to estimate result selectivity more accurately. Similarly, comparisons (&lt;code&gt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;, etc.) on columns yield Bernoulli distributions (with true/false probabilities) in this model.&lt;/p&gt;
&lt;p&gt;This is a foundational change with many follow-on PRs underway. Even though the immediate user-visible effect is limited (the optimizer didn't magically improve by an order of magnitude overnight), but it lays groundwork for more advanced query planning in the future. Over time, as statistics information encapsulated in &lt;code&gt;Distribution&lt;/code&gt;s get integrated, DataFusion will be able to make smarter decisions like more aggressive parquet pruning, better join orderings, and so on based on data distribution information. The core framework is now in place and is being hooked up to column and table level statistics.&lt;/p&gt;
&lt;h3&gt;Aggregate Monotonicity and Window Ordering&lt;/h3&gt;
&lt;p&gt;DataFusion 46.0.0 adds a new concept of &lt;a href="https://github.com/apache/datafusion/pull/14271"&gt;&lt;strong&gt;set-monotonicity&lt;/strong&gt;&lt;/a&gt; for certain transformations, which helps avoid unnecessary sort operations. In particular, the planner now understands when a &lt;strong&gt;window function introduces new orderings of data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;For example, DataFusion now recognizes that a window-aggregate like &lt;code&gt;MAX&lt;/code&gt; on a column can produce a result that is &lt;strong&gt;monotonically increasing&lt;/strong&gt;, even if the input column is unordered &amp;mdash; depending on the window frame used.&lt;/p&gt;
&lt;p&gt;Consider the following query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT MAX(c1) OVER (
    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
) AS max_c1
FROM c1_table
ORDER BY max_c1;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In earlier versions of DataFusion, this query would require an additional SortExec on max_c1 to satisfy the ORDER BY clause. However, with the new set-monotonicity logic, the planner knows that MAX(...) OVER (...) produces values that are not smaller than the previous row, making the extra sort redundant. This leads to more efficient query execution.&lt;/p&gt;
&lt;p&gt;PR &lt;a href="https://github.com/apache/datafusion/pull/14271"&gt;#14271&lt;/a&gt; introduced the core monotonicity tracking for aggregates and window functions.
PR &lt;a href="https://github.com/apache/datafusion/pull/14813"&gt;#14813&lt;/a&gt; improved ordering preservation within various window frame types, and brought an extensive test coverage.
Huge thanks to &lt;a href="https://github.com/berkaysynnada"&gt;@berkaysynnada&lt;/a&gt; and &lt;a href="https://github.com/mertak-synnada"&gt;@mertak-synnada&lt;/a&gt; for designing and implementing this optimizer enhancement!&lt;/p&gt;
&lt;h3&gt;UNION [ALL | DISTINCT] BY NAME Support&lt;/h3&gt;
&lt;p&gt;DataFusion now supports UNION BY NAME and UNION ALL BY NAME, which align columns by name instead of position. This matches functionality found in systems like Spark and DuckDB and simplifies combining heterogeneously ordered result sets.&lt;/p&gt;
&lt;p&gt;You no longer need to rewrite column order manually &amp;mdash; just write:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT col1, col2 FROM t1
UNION ALL BY NAME
SELECT col2, col1 FROM t2;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Under the hood, this is supported by the new union_by_name() and union_by_name_distinct() plan builder methods.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/rkrishn7"&gt;@rkrishn7&lt;/a&gt; for PR &lt;a href="https://github.com/apache/datafusion/pull/14538"&gt;#14538&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;New range() Table Function&lt;/h3&gt;
&lt;p&gt;A new table-valued function range(start, stop, step) has been added to make it easy to generate integer sequences &amp;mdash; similar to PostgreSQL&amp;rsquo;s generate_series() or Spark&amp;rsquo;s range().&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT * FROM range(1, 10, 2);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns: 1, 3, 5, 7, 9. It&amp;rsquo;s great for testing, cross joins, surrogate keys, and more.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/simonvandel"&gt;@simonvandel&lt;/a&gt; for PR &lt;a href="https://github.com/apache/datafusion/pull/14830"&gt;#14830&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Upgrade Guide and Changelog&lt;/h2&gt;
&lt;p&gt;Upgrading to 46.0.0 should be straightforward for most users, but do review the&amp;nbsp;&lt;a href="https://datafusion.apache.org/library-user-guide/upgrading.html"&gt;Upgrade Guide for DataFusion 46.0.0&lt;/a&gt;&amp;nbsp;for detailed steps and code changes. The upgrade guide covers the breaking changes mentioned (like replacing old exec nodes with&amp;nbsp;&lt;code&gt;DataSourceExec&lt;/code&gt;, updating UDF invocation to&amp;nbsp;&lt;code&gt;invoke_with_args&lt;/code&gt;, etc.) and provides code snippets to help with the transition. For a comprehensive list of all changes, please refer to the&amp;nbsp;&lt;strong&gt;changelog&lt;/strong&gt;&amp;nbsp;for 46.0.0 (linked above and in the repository). The changelog enumerates every merged PR in this release, including many smaller fixes and improvements that we couldn&amp;rsquo;t cover in this post.&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;Apache DataFusion is an open-source project, and we welcome involvement from anyone interested. Now is a great time to take 46.0.0 for a spin: try it out on your workloads, and let us know if you encounter any issues or have suggestions. You can report bugs or request features on our&amp;nbsp;GitHub issue tracker, or better yet, submit a pull request. Join our community discussions &amp;ndash; whether you have questions, want to share how you&amp;rsquo;re using DataFusion, or are looking to contribute, we&amp;rsquo;d love to hear from you. A list of open issues suitable for beginners is&amp;nbsp;&lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;&amp;nbsp;and you can find how to reach us on the&amp;nbsp;&lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication doc&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy querying!&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Efficient Filter Pushdown in Parquet</title><link href="https://datafusion.apache.org/blog/2025/03/21/parquet-pushdown" rel="alternate"></link><published>2025-03-21T00:00:00+00:00</published><updated>2025-03-21T00:00:00+00:00</updated><author><name>Xiangpeng Hao</name></author><id>tag:datafusion.apache.org,2025-03-21:/blog/2025/03/21/parquet-pushdown</id><summary type="html">&lt;style&gt;
figure {
  margin: 20px 0;
}

figure img {
  display: block;
  max-width: 80%;
}

figcaption {
  font-style: italic;
  margin-top: 10px;
  color: #555;
  font-size: 0.9em;
  max-width: 80%;
}
&lt;/style&gt;
&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog was first published on &lt;a href="https://blog.xiangpeng.systems/posts/parquet-pushdown/"&gt;Xiangpeng Hao's blog&lt;/a&gt;. Thanks to &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; for sponsoring this work as part of his PhD funding.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2025/03/20/parquet-pruning"&gt;previous post …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;style&gt;
figure {
  margin: 20px 0;
}

figure img {
  display: block;
  max-width: 80%;
}

figcaption {
  font-style: italic;
  margin-top: 10px;
  color: #555;
  font-size: 0.9em;
  max-width: 80%;
}
&lt;/style&gt;
&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog was first published on &lt;a href="https://blog.xiangpeng.systems/posts/parquet-pushdown/"&gt;Xiangpeng Hao's blog&lt;/a&gt;. Thanks to &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; for sponsoring this work as part of his PhD funding.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2025/03/20/parquet-pruning"&gt;previous post&lt;/a&gt;, we discussed how &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; prunes &lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; files to skip irrelevant &lt;strong&gt;files/row_groups&lt;/strong&gt; (sometimes also &lt;a href="https://parquet.apache.org/docs/file-format/pageindex/"&gt;pages&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This post discusses how Parquet readers skip irrelevant &lt;strong&gt;rows&lt;/strong&gt; while scanning data,
leveraging Parquet's columnar layout by first reading only filter columns,
and then selectively reading other columns only for matching rows.&lt;/p&gt;
&lt;h2&gt;Why filter pushdown in Parquet?&lt;/h2&gt;
&lt;p&gt;Below is an example query that reads sensor data with filters on &lt;code&gt;date_time&lt;/code&gt; and &lt;code&gt;location&lt;/code&gt;.
Without filter pushdown, all rows from &lt;code&gt;location&lt;/code&gt;, &lt;code&gt;val&lt;/code&gt;, and &lt;code&gt;date_time&lt;/code&gt; columns are decoded before &lt;code&gt;location='office'&lt;/code&gt; is evaluated. Filter pushdown is especially useful when the filter is selective, i.e., removes many rows.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT val, location 
FROM sensor_data 
WHERE date_time &amp;gt; '2025-03-11' AND location = 'office';
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img alt="Parquet pruning skips irrelevant files/row_groups, while filter pushdown skips irrelevant rows. Without filter pushdown, all rows from location, val, and date_time columns are decoded before `location='office'` is evaluated. Filter pushdown is especially useful when the filter is selective, i.e., removes many rows." class="img-responsive" src="/blog/images/parquet-pushdown/pushdown-vs-no-pushdown.jpg" width="80%"/&gt;
&lt;figcaption&gt;
    Parquet pruning skips irrelevant files/row_groups, while filter pushdown skips irrelevant rows. Without filter pushdown, all rows from location, val, and date_time columns are decoded before `location='office'` is evaluated. Filter pushdown is especially useful when the filter is selective, i.e., removes many rows.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In our setup, sensor data is aggregated by date &amp;mdash; each day has its own Parquet file.
At planning time, DataFusion prunes the unneeded Parquet files, i.e., &lt;code&gt;2025-03-10.parquet&lt;/code&gt; and &lt;code&gt;2025-03-11.parquet&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once the files to read are located, the &lt;a href="https://github.com/apache/datafusion/issues/3463"&gt;&lt;em&gt;DataFusion's current default implementation&lt;/em&gt;&lt;/a&gt; reads all the projected columns (&lt;code&gt;sensor_id&lt;/code&gt;, &lt;code&gt;val&lt;/code&gt;, and &lt;code&gt;location&lt;/code&gt;) into Arrow RecordBatches, then applies the filters over &lt;code&gt;location&lt;/code&gt; to get the final set of rows.&lt;/p&gt;
&lt;p&gt;A better approach is called &lt;strong&gt;filter pushdown&lt;/strong&gt; with &lt;strong&gt;late materialization&lt;/strong&gt;, which evaluates filter conditions first and only decodes data that passes these conditions.
In practice, this works by first processing only the filter columns (&lt;code&gt;date_time&lt;/code&gt; and &lt;code&gt;location&lt;/code&gt;), building a boolean mask of rows that satisfy our conditions, then using this mask to selectively decode only the relevant rows from other columns (&lt;code&gt;sensor_id&lt;/code&gt;, &lt;code&gt;val&lt;/code&gt;). 
This eliminates the waste of decoding rows that will be immediately filtered out.&lt;/p&gt;
&lt;p&gt;While simple in theory, practical implementations often make performance worse.&lt;/p&gt;
&lt;h2&gt;How can filter pushdown be slower?&lt;/h2&gt;
&lt;p&gt;At a high level, the Parquet reader first builds a filter mask -- essentially a boolean array indicating which rows meet the filter criteria -- and then uses this mask to selectively decode only the needed rows from the remaining columns in the projection.&lt;/p&gt;
&lt;p&gt;Let's dig into details of &lt;a href="https://github.com/apache/arrow-rs/blob/d5339f31a60a4bd8a4256e7120fe32603249d88e/parquet/src/arrow/async_reader/mod.rs#L618-L712"&gt;how filter pushdown is implemented&lt;/a&gt; in the current Rust Parquet reader implementation, illustrated in the following figure.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Implementation of filter pushdown in Rust Parquet readers" class="img-responsive" src="/blog/images/parquet-pushdown/baseline-impl.jpg" with="70%"/&gt;
&lt;figcaption&gt;
    Implementation of filter pushdown in Rust Parquet readers -- the first phase builds the filter mask, the second phase applies the filter mask to the other columns
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The filter pushdown has two phases:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Build the filter mask (steps 1-3)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use the filter mask to selectively decode other columns (steps 4-7), e.g., output step 3 is used as input for step 5 and 7.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Within each phase, it takes three steps from Parquet to Arrow:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Decompress the Parquet pages using generic decompression algorithms like LZ4, Zstd, etc. (steps 1, 4, 6)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decode the page content into Arrow format (steps 2, 5, 7)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Evaluate the filter over Arrow data (step 3)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the figure above, we can see that &lt;code&gt;location&lt;/code&gt; is &lt;strong&gt;decompressed and decoded twice&lt;/strong&gt;, first when building the filter mask (steps 1, 2), and second when building the output (steps 4, 5).
This happens for all columns that appear both in the filter and output.&lt;/p&gt;
&lt;p&gt;The table below shows the corresponding CPU time on the &lt;a href="https://github.com/apache/datafusion/blob/main/benchmarks/queries/clickbench/queries.sql#L23"&gt;ClickBench query 22&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+------------+--------+-------------+--------+
| Decompress | Decode | Apply filter| Others |
+------------+--------+-------------+--------+
| 206 ms     | 117 ms | 22 ms       | 48 ms  |
+------------+--------+-------------+--------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Clearly, decompress/decode operations dominate the time spent. With filter pushdown, it needs to decompress/decode twice; but without filter pushdown, it only needs to do this once.
This explains why filter pushdown is slower in some cases.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Highly selective filters may skip the entire page; but as long as it reads one row from the page, it needs to decompress and often decode the entire page.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Attempt: cache filter columns&lt;/h2&gt;
&lt;p&gt;Intuitively, caching the filter columns and reusing them later could help.&lt;/p&gt;
&lt;p&gt;But naively caching decoded pages consumes prohibitively high memory:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It needs to cache Arrow arrays, which are on average &lt;a href="https://github.com/XiangpengHao/liquid-cache/blob/main/dev/doc/liquid-cache-vldb.pdf"&gt;4x larger than Parquet data&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;It needs to cache the &lt;strong&gt;entire column chunk in memory&lt;/strong&gt;, because in Phase 1 it builds filters over the column chunk, and only use it in Phase 2.  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The memory usage is proportional to the number of filter columns, which can be unboundedly high. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Worse, caching filter columns means it needs to read partially from Parquet and partially from cache, which is complex to implement, likely requiring a substantial change to the current implementation. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Feel the complexity:&lt;/strong&gt; consider building a cache that properly handles nested columns, multiple filters, and filters with multiple columns.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Real solution&lt;/h2&gt;
&lt;p&gt;We need a solution that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Is simple to implement, i.e., doesn't require thousands of lines of code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Incurs minimal memory overhead.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This section describes my &lt;a href="https://github.com/apache/arrow-rs/pull/6921#issuecomment-2718792433"&gt;&amp;lt;700 LOC PR (with lots of comments and tests)&lt;/a&gt; that &lt;strong&gt;reduces total ClickBench time by 15%, with up to 2x lower latency for some queries, no obvious regression on other queries, and caches at most 2 pages (~2MB) per column in memory&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="New decoding pipeline, building filter mask and output columns are interleaved in a single pass, allowing us to cache minimal pages for minimal amount of time" class="img-responsive" src="/blog/images/parquet-pushdown/new-pipeline.jpg" width="80%"/&gt;
&lt;figcaption&gt;
    New decoding pipeline, building filter mask and output columns are interleaved in a single pass, allowing us to cache minimal pages for minimal amount of time
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The new pipeline interleaves the previous two phases into a single pass, so that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The page being decompressed is immediately used to build filter masks and output columns.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Decompressed pages are cached for minimal time; after one pass (steps 1-6), the cache memory is released for the next pass. &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This allows the cache to only hold 1 page at a time, and to immediately discard the previous page after it's used, significantly reducing the memory requirement for caching.&lt;/p&gt;
&lt;h3&gt;What pages are cached?&lt;/h3&gt;
&lt;p&gt;You may have noticed that only &lt;code&gt;location&lt;/code&gt; is cached, not &lt;code&gt;val&lt;/code&gt;, because &lt;code&gt;val&lt;/code&gt; is only used for output.
More generally, only columns that appear both in the filter and output are cached, and at most 1 page is cached for each such column.&lt;/p&gt;
&lt;p&gt;More examples:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT val 
FROM sensor_data 
WHERE date_time &amp;gt; '2025-03-11' AND location = 'office';
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, no columns are cached, because &lt;code&gt;val&lt;/code&gt; is not used for filtering.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT COUNT(*) 
FROM sensor_data 
WHERE date_time &amp;gt; '2025-03-11' AND location = 'office';
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case, again, no columns are cached, because the output projection is empty after query plan optimization.&lt;/p&gt;
&lt;h3&gt;Then why cache 2 pages per column instead of 1?&lt;/h3&gt;
&lt;p&gt;This is another real-world nuance regarding how Parquet layouts the pages.&lt;/p&gt;
&lt;p&gt;Parquet by default encodes data using &lt;a href="https://parquet.apache.org/docs/file-format/data-pages/encodings/"&gt;dictionary encoding&lt;/a&gt;, which writes a dictionary page as the first page of a column chunk, followed by the keys referencing the dictionary.&lt;/p&gt;
&lt;p&gt;You can see this in action using &lt;a href="https://parquet-viewer.xiangpeng.systems"&gt;parquet-viewer&lt;/a&gt;:&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Parquet viewer shows the page layout of a column chunk" class="img-responsive" src="/blog/images/parquet-pushdown/parquet-viewer.jpg" width="80%"/&gt;
&lt;figcaption&gt;
    Parquet viewer shows the page layout of a column chunk
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;This means that to decode a page of data, it actually references two pages: the dictionary page and the data page.&lt;/p&gt;
&lt;p&gt;This is why it caches 2 pages per column: one dictionary page and one data page.
The data page slot will move forward as it reads the data; but the dictionary page slot always references the first page.&lt;/p&gt;
&lt;figure&gt;
&lt;img alt="Cached two pages, one for dictionary (pinned), one for data (moves as it reads the data)" class="img-responsive" src="/blog/images/parquet-pushdown/cached-pages.jpg" width="80%"/&gt;
&lt;figcaption&gt;
    Cached two pages, one for dictionary (pinned), one for data (moves as it reads the data)
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2&gt;How does it perform?&lt;/h2&gt;
&lt;p&gt;Here are my results on &lt;a href="https://github.com/apache/datafusion/tree/main/benchmarks#clickbench"&gt;ClickBench&lt;/a&gt; on my AMD 9900X machine. The total time is reduced by 15%, with Q23 being 2.24x faster,
and queries that get slower are likely due to noise.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Query        ┃ no-pushdown ┃ new-pushdown ┃        Change ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
&amp;boxv; QQuery 0     &amp;boxv;      0.47ms &amp;boxv;       0.43ms &amp;boxv; +1.10x faster &amp;boxv;
&amp;boxv; QQuery 1     &amp;boxv;     51.10ms &amp;boxv;      50.10ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 2     &amp;boxv;     68.23ms &amp;boxv;      64.49ms &amp;boxv; +1.06x faster &amp;boxv;
&amp;boxv; QQuery 3     &amp;boxv;     90.68ms &amp;boxv;      86.73ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 4     &amp;boxv;    458.93ms &amp;boxv;     458.59ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 5     &amp;boxv;    522.06ms &amp;boxv;     478.50ms &amp;boxv; +1.09x faster &amp;boxv;
&amp;boxv; QQuery 6     &amp;boxv;     49.84ms &amp;boxv;      49.94ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 7     &amp;boxv;     55.09ms &amp;boxv;      55.77ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 8     &amp;boxv;    565.26ms &amp;boxv;     556.95ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 9     &amp;boxv;    575.83ms &amp;boxv;     575.05ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 10    &amp;boxv;    164.56ms &amp;boxv;     178.23ms &amp;boxv;  1.08x slower &amp;boxv;
&amp;boxv; QQuery 11    &amp;boxv;    177.20ms &amp;boxv;     191.32ms &amp;boxv;  1.08x slower &amp;boxv;
&amp;boxv; QQuery 12    &amp;boxv;    591.05ms &amp;boxv;     569.92ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 13    &amp;boxv;    861.06ms &amp;boxv;     848.59ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 14    &amp;boxv;    596.20ms &amp;boxv;     580.73ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 15    &amp;boxv;    554.96ms &amp;boxv;     548.77ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 16    &amp;boxv;   1175.08ms &amp;boxv;    1146.07ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 17    &amp;boxv;   1150.45ms &amp;boxv;    1121.49ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 18    &amp;boxv;   2634.75ms &amp;boxv;    2494.07ms &amp;boxv; +1.06x faster &amp;boxv;
&amp;boxv; QQuery 19    &amp;boxv;     90.15ms &amp;boxv;      89.24ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 20    &amp;boxv;    620.15ms &amp;boxv;     591.67ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 21    &amp;boxv;    782.38ms &amp;boxv;     703.15ms &amp;boxv; +1.11x faster &amp;boxv;
&amp;boxv; QQuery 22    &amp;boxv;   1927.94ms &amp;boxv;    1404.35ms &amp;boxv; +1.37x faster &amp;boxv;
&amp;boxv; QQuery 23    &amp;boxv;   8104.11ms &amp;boxv;    3610.76ms &amp;boxv; +2.24x faster &amp;boxv;
&amp;boxv; QQuery 24    &amp;boxv;    360.79ms &amp;boxv;     330.55ms &amp;boxv; +1.09x faster &amp;boxv;
&amp;boxv; QQuery 25    &amp;boxv;    290.61ms &amp;boxv;     252.54ms &amp;boxv; +1.15x faster &amp;boxv;
&amp;boxv; QQuery 26    &amp;boxv;    395.18ms &amp;boxv;     362.72ms &amp;boxv; +1.09x faster &amp;boxv;
&amp;boxv; QQuery 27    &amp;boxv;    891.76ms &amp;boxv;     959.39ms &amp;boxv;  1.08x slower &amp;boxv;
&amp;boxv; QQuery 28    &amp;boxv;   4059.54ms &amp;boxv;    4137.37ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 29    &amp;boxv;    235.88ms &amp;boxv;     228.99ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 30    &amp;boxv;    564.22ms &amp;boxv;     584.65ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 31    &amp;boxv;    741.20ms &amp;boxv;     757.87ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 32    &amp;boxv;   2652.48ms &amp;boxv;    2574.19ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 33    &amp;boxv;   2373.71ms &amp;boxv;    2327.10ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 34    &amp;boxv;   2391.00ms &amp;boxv;    2342.15ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 35    &amp;boxv;    700.79ms &amp;boxv;     694.51ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 36    &amp;boxv;    151.51ms &amp;boxv;     152.93ms &amp;boxv;     no change &amp;boxv;
&amp;boxv; QQuery 37    &amp;boxv;    108.18ms &amp;boxv;      86.03ms &amp;boxv; +1.26x faster &amp;boxv;
&amp;boxv; QQuery 38    &amp;boxv;    114.64ms &amp;boxv;     106.22ms &amp;boxv; +1.08x faster &amp;boxv;
&amp;boxv; QQuery 39    &amp;boxv;    260.80ms &amp;boxv;     239.13ms &amp;boxv; +1.09x faster &amp;boxv;
&amp;boxv; QQuery 40    &amp;boxv;     60.74ms &amp;boxv;      73.29ms &amp;boxv;  1.21x slower &amp;boxv;
&amp;boxv; QQuery 41    &amp;boxv;     58.75ms &amp;boxv;      67.85ms &amp;boxv;  1.15x slower &amp;boxv;
&amp;boxv; QQuery 42    &amp;boxv;     65.49ms &amp;boxv;      68.11ms &amp;boxv;     no change &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓
┃ Benchmark Summary           ┃            ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩
&amp;boxv; Total Time (no-pushdown)    &amp;boxv; 38344.79ms &amp;boxv;
&amp;boxv; Total Time (new-pushdown)   &amp;boxv; 32800.50ms &amp;boxv;
&amp;boxv; Average Time (no-pushdown)  &amp;boxv;   891.74ms &amp;boxv;
&amp;boxv; Average Time (new-pushdown) &amp;boxv;   762.80ms &amp;boxv;
&amp;boxv; Queries Faster              &amp;boxv;         13 &amp;boxv;
&amp;boxv; Queries Slower              &amp;boxv;          5 &amp;boxv;
&amp;boxv; Queries with No Change      &amp;boxv;         25 &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Despite being simple in theory, filter pushdown in Parquet is non-trivial to implement.
It requires understanding both the Parquet format and reader implementation details. 
The challenge lies in efficiently navigating through the dynamics of decoding, filter evaluation, and memory management.&lt;/p&gt;
&lt;p&gt;If you are interested in this level of optimization and want to help test, document and implement this type of optimization, come find us in the &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;DataFusion Community&lt;/a&gt;. We would love to have you. &lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.7.0 Release</title><link href="https://datafusion.apache.org/blog/2025/03/20/datafusion-comet-0.7.0" rel="alternate"></link><published>2025-03-20T00:00:00+00:00</published><updated>2025-03-20T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-03-20:/blog/2025/03/20/datafusion-comet-0.7.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.7.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.7.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately four weeks of development work and is the result of merging 46 PRs from 11
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.7.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Performance&lt;/h3&gt;
&lt;p&gt;Comet 0.7.0 has improved performance compared to the previous release due to improvements in the native shuffle 
implementation and performance improvements in DataFusion 46.&lt;/p&gt;
&lt;p&gt;For single-node TPC-H at 100 GB, Comet now delivers a &lt;strong&gt;greater than 2x speedup&lt;/strong&gt; compared to Spark using the same 
CPU and RAM. Even with &lt;strong&gt;half the resources&lt;/strong&gt;, Comet still provides a measurable performance improvement.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.7.0" class="img-responsive" src="/blog/images/comet-0.7.0/performance.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;These benchmarks were performed on a Linux workstation with PCIe 5, AMD 7950X CPU (16 cores), 128 GB RAM, and data 
stored locally in Parquet format on NVMe storage. Spark was running in Kubernetes with hard memory limits.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Shuffle Improvements&lt;/h2&gt;
&lt;p&gt;There are several improvements to shuffle in this release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When running in off-heap mode (which is the recommended approach), Comet was using the wrong memory allocator 
  implementation for some types of shuffle operation, which could result in OOM rather than spilling to disk.&lt;/li&gt;
&lt;li&gt;The number of spill files is drastically reduced. In previous releases, each instance of ShuffleMapTask could 
  potentially create a new spill file for each output partition each time that spill was invoked. Comet now creates 
  a maximum of one spill file per output partition per instance of ShuffleMapTask, which is appended to in subsequent 
  spills.&lt;/li&gt;
&lt;li&gt;There was a flaw in the memory accounting which resulted in Comet requesting approximately twice the amount of 
  memory that was needed, resulting in premature spilling. This is now resolved.&lt;/li&gt;
&lt;li&gt;The metric for number of spilled bytes is now accurate. It was previously reporting invalid information.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Improved Hash Join Performance&lt;/h2&gt;
&lt;p&gt;When using the &lt;code&gt;spark.comet.exec.replaceSortMergeJoin&lt;/code&gt; setting to replace sort-merge joins with hash joins, Comet 
will now do a better job of picking the optimal build side. Thanks to &lt;a href="https://github.com/hayman42"&gt;@hayman42&lt;/a&gt; for suggesting this, and thanks to the 
&lt;a href="https://github.com/apache/incubator-gluten/"&gt;Apache Gluten(incubating)&lt;/a&gt; project for the inspiration in implementing this feature.&lt;/p&gt;
&lt;h2&gt;Experimental Support for DataFusion&amp;rsquo;s Parquet Scan&lt;/h2&gt;
&lt;p&gt;It is now possible to configure Comet to use DataFusion&amp;rsquo;s Parquet reader instead of Comet&amp;rsquo;s current Parquet reader. This 
has the advantage of supporting complex types, and also has performance optimizations that are not present in Comet's 
existing reader.&lt;/p&gt;
&lt;p&gt;Support should still be considered experimental, but most of Comet&amp;rsquo;s unit tests are now passing with the new reader. 
Known issues include handling of &lt;code&gt;INT96&lt;/code&gt; timestamps and unsigned bytes and shorts.&lt;/p&gt;
&lt;p&gt;To enable DataFusion&amp;rsquo;s Parquet reader, either set &lt;code&gt;spark.comet.scan.impl=native_datafusion&lt;/code&gt; or set the environment 
variable &lt;code&gt;COMET_PARQUET_SCAN_IMPL=native_datafusion&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Complex Type Support&lt;/h2&gt;
&lt;p&gt;With DataFusion&amp;rsquo;s Parquet reader enabled, there is now some early support for reading structs from Parquet. This is 
not thoroughly tested yet. We would welcome additional testing from the community to help determine what is and isn&amp;rsquo;t 
working, as well as contributions to improve support for structs and other complex types. The tracking issue is 
&lt;a href="https://github.com/apache/datafusion-comet/issues/1043"&gt;https://github.com/apache/datafusion-comet/issues/1043&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Updates to supported Spark versions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Comet 0.7.0 is now tested against Spark 3.5.4 rather than 3.5.1&lt;/li&gt;
&lt;li&gt;This will be the last Comet release to support Spark 3.3.x&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Improved Tuning Guide&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://datafusion.apache.org/comet/user-guide/tuning.html"&gt;Comet Tuning Guide&lt;/a&gt; has been improved and now provides guidance on determining how much memory to allocate to 
Comet.&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Parquet Pruning in DataFusion: Read Only What Matters</title><link href="https://datafusion.apache.org/blog/2025/03/20/parquet-pruning" rel="alternate"></link><published>2025-03-20T00:00:00+00:00</published><updated>2025-03-20T00:00:00+00:00</updated><author><name>Xiangpeng Hao</name></author><id>tag:datafusion.apache.org,2025-03-20:/blog/2025/03/20/parquet-pruning</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog was first published on &lt;a href="https://blog.xiangpeng.systems/posts/parquet-to-arrow/"&gt;Xiangpeng Hao's blog&lt;/a&gt;. Thanks to &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; for sponsoring this work as part of his PhD funding.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; has become the industry standard for storing columnar data, and reading Parquet efficiently -- especially from remote storage -- is crucial for query performance.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog was first published on &lt;a href="https://blog.xiangpeng.systems/posts/parquet-to-arrow/"&gt;Xiangpeng Hao's blog&lt;/a&gt;. Thanks to &lt;a href="https://www.influxdata.com/"&gt;InfluxData&lt;/a&gt; for sponsoring this work as part of his PhD funding.&lt;/em&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;&lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; has become the industry standard for storing columnar data, and reading Parquet efficiently -- especially from remote storage -- is crucial for query performance.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; implements advanced Parquet pruning techniques to effectively read only the data that matters for a given query.&lt;/p&gt;
&lt;p&gt;Achieving high performance adds complexity.
This post provides an overview of the techniques used in DataFusion to selectively read Parquet files.&lt;/p&gt;
&lt;h3&gt;The pipeline&lt;/h3&gt;
&lt;p&gt;The diagram below illustrates the &lt;a href="https://docs.rs/datafusion/46.0.0/datafusion/datasource/physical_plan/parquet/source/struct.ParquetSource.html"&gt;Parquet reading pipeline&lt;/a&gt; in DataFusion, highlighting how data flows through various pruning stages before being converted to Arrow format:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Parquet pruning pipeline in DataFusion" class="img-responsive" src="/blog/images/parquet-pruning/read-parquet.jpg" width="100%"/&gt;&lt;/p&gt;
&lt;h4&gt;Background: Parquet file structure&lt;/h4&gt;
&lt;p&gt;As shown in the figure above, each Parquet file has multiple row groups. Each row group contains a set of columns, and each column contains a set of pages.&lt;/p&gt;
&lt;p&gt;Pages are the smallest units of data in Parquet files and typically contain compressed and encoded values for a specific column. This hierarchical structure enables efficient columnar access and forms the foundation for the pruning techniques we'll discuss.&lt;/p&gt;
&lt;p&gt;Check out &lt;a href="https://www.influxdata.com/blog/querying-parquet-millisecond-latency/"&gt;Querying Parquet with Millisecond Latency&lt;/a&gt; for more details on the Parquet file structure.&lt;/p&gt;
&lt;h4&gt;1. Read metadata&lt;/h4&gt;
&lt;p&gt;DataFusion first reads the &lt;a href="https://parquet.apache.org/docs/file-format/metadata/"&gt;Parquet metadata&lt;/a&gt; to understand the data in the file. 
Metadata often includes data schema, the exact location of each row group and column chunk, and their corresponding statistics (e.g., min/max values).
It also optionally includes &lt;a href="https://parquet.apache.org/docs/file-format/pageindex/"&gt;page-level stats&lt;/a&gt; and &lt;a href="https://www.influxdata.com/blog/using-parquets-bloom-filters/"&gt;Bloom filters&lt;/a&gt;.
This information is used to prune the file before reading the actual data.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/reader.rs#L118"&gt;Fetching metadata&lt;/a&gt; requires up to two network requests: one to read the footer size from the end of the file, and another to read the footer itself. &lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.influxdata.com/blog/how-good-parquet-wide-tables/"&gt;Decoding metadata&lt;/a&gt; is generally fast since it only requires parsing a small amount of data. However, for tables with hundreds or thousands of columns, the metadata can become quite large and decoding it can become a bottleneck. This is particularly noticeable when scanning many small files.&lt;/p&gt;
&lt;p&gt;Reading metadata is latency-critical, so DataFusion allows users to cache metadata through the &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/reader.rs#L39"&gt;ParquetFileReaderFactory&lt;/a&gt; trait.&lt;/p&gt;
&lt;h4&gt;2. Prune by projection&lt;/h4&gt;
&lt;p&gt;The simplest yet perhaps most effective pruning is to read only the columns that are needed.
This is because queries usually don't select all columns, e.g., &lt;code&gt;SELECT a FROM table&lt;/code&gt; only reads column &lt;code&gt;a&lt;/code&gt;.
As a &lt;strong&gt;columnar&lt;/strong&gt; format, Parquet allows DataFusion to &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/mod.rs#L778"&gt;only read&lt;/a&gt; the &lt;strong&gt;columns&lt;/strong&gt; that are needed.&lt;/p&gt;
&lt;p&gt;This projection pruning happens at the column level and can dramatically reduce I/O when working with wide tables where queries typically access only a small subset of columns.&lt;/p&gt;
&lt;h4&gt;3. Prune by row group stats and Bloom filters&lt;/h4&gt;
&lt;p&gt;Each row group has &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/physical_optimizer/pruning.rs#L81"&gt;basic stats&lt;/a&gt; like min/max values for each column.
DataFusion applies the query predicates to these stats to prune row groups, e.g., &lt;code&gt;SELECT * FROM table WHERE a &amp;gt; 10&lt;/code&gt; will only read row groups where &lt;code&gt;a&lt;/code&gt; has a max value greater than 10.&lt;/p&gt;
&lt;p&gt;Sometimes min/max stats are too simple to prune effectively, so Parquet also supports &lt;a href="https://www.influxdata.com/blog/using-parquets-bloom-filters/"&gt;Bloom filters&lt;/a&gt;. DataFusion &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/opener.rs#L202"&gt;uses Bloom filters when available&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Bloom filters are particularly effective for equality predicates (&lt;code&gt;WHERE a = 10&lt;/code&gt;) and can significantly reduce the number of row groups that need to be read for point queries or queries with highly selective predicates.&lt;/p&gt;
&lt;h4&gt;4. Prune by page stats&lt;/h4&gt;
&lt;p&gt;Parquet optionally supports &lt;a href="https://github.com/apache/parquet-format/blob/master/PageIndex.md"&gt;page-level stats&lt;/a&gt; -- similar to row group stats but more fine-grained.
DataFusion implements &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/opener.rs#L219"&gt;page pruning&lt;/a&gt; when the stats are present.&lt;/p&gt;
&lt;p&gt;Page-level pruning provides an additional layer of filtering after row group pruning. It allows DataFusion to skip individual pages within a row group, further reducing the amount of data that needs to be read and decoded.&lt;/p&gt;
&lt;h4&gt;5. Read from storage&lt;/h4&gt;
&lt;p&gt;Now we (hopefully) have pruned the Parquet file into small ranges of bytes, i.e., the &lt;a href="https://github.com/apache/datafusion/blob/76a7789ace33ced54c973fa0d5fc9d1866e1bf19/datafusion/datasource-parquet/src/access_plan.rs#L86"&gt;Access Plan&lt;/a&gt;.
The last step is to &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/reader.rs#L103"&gt;make requests&lt;/a&gt; to fetch those bytes and decode them into Arrow RecordBatch. &lt;/p&gt;
&lt;h3&gt;Preview of coming attractions: filter pushdown&lt;/h3&gt;
&lt;p&gt;So far we have discussed techniques that prune the Parquet file using only the metadata, i.e., before reading the actual data.&lt;/p&gt;
&lt;p&gt;Filter pushdown, also known as predicate pushdown or late materialization, is a technique that prunes data during scanning, with filters being generated and applied in the Parquet reader.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Filter pushdown in DataFusion" class="img-responsive" src="/blog/images/parquet-pruning/filter-pushdown.jpg" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Unlike metadata-based pruning which works at the row group or page level, filter pushdown operates at the row level, allowing DataFusion to filter out individual rows that don't match the query predicates during the decoding process.&lt;/p&gt;
&lt;p&gt;DataFusion &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/core/src/datasource/physical_plan/parquet/row_filter.rs#L154"&gt;implements filter pushdown&lt;/a&gt; but has &lt;a href="https://github.com/apache/datafusion/blob/31701b8dc9c6486856c06a29a32107d9f4549cec/datafusion/common/src/config.rs#L382"&gt;not enabled it by default&lt;/a&gt; due to &lt;a href="https://github.com/apache/datafusion/issues/3463"&gt;some performance regressions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are working to remove the &lt;a href="https://github.com/apache/arrow-rs/issues/5523#issuecomment-2429470872"&gt;remaining performance issues&lt;/a&gt; and enable it by default, which we will discuss in the next blog post.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;DataFusion employs a multi-step approach to Parquet pruning, from column projection to row group stats, page stats, and potentially row-level filtering. 
Each step may reduce the amount of data to be read and processed, significantly improving query performance.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Using Ordering for Better Plans in Apache DataFusion</title><link href="https://datafusion.apache.org/blog/2025/03/11/ordering-analysis" rel="alternate"></link><published>2025-03-11T00:00:00+00:00</published><updated>2025-03-11T00:00:00+00:00</updated><author><name>Mustafa Akur, Andrew Lamb</name></author><id>tag:datafusion.apache.org,2025-03-11:/blog/2025/03/11/ordering-analysis</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/11631 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this blog post, we explain when an ordering requirement of an operator is satisfied by its input data. This analysis is essential for order-based optimizations and is often more complex than one might initially think.&lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Ordering Requirement&lt;/strong&gt; for an operator describes how the input data to that operator …&lt;/blockquote&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/11631 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this blog post, we explain when an ordering requirement of an operator is satisfied by its input data. This analysis is essential for order-based optimizations and is often more complex than one might initially think.&lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Ordering Requirement&lt;/strong&gt; for an operator describes how the input data to that operator must be sorted for the operator to compute the correct result. It is the job of the planner to make sure that these requirements are satisfied during execution (See DataFusion &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/enforce_sorting/struct.EnforceSorting.html" target="_blank"&gt;EnforceSorting&lt;/a&gt; for an implementation of such a rule).
&lt;/blockquote&gt;
&lt;p&gt;There are various use cases where this type of analysis can be useful such as the following examples.&lt;/p&gt;
&lt;h3&gt;Removing Unnecessary Sorts&lt;/h3&gt;
&lt;p&gt;Imagine a user wants to execute the following query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-SQL"&gt;SELECT hostname, log_line 
FROM telemetry ORDER BY time ASC limit 10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we don't know anything about the &lt;code&gt;telemetry&lt;/code&gt; table we need to sort it by &lt;code&gt;time ASC&lt;/code&gt; and then retrieve the first 10 rows to get the correct result. However, if the table is already ordered by &lt;code&gt;time ASC&lt;/code&gt;, we can simply retrieve the first 10 rows. This approach executes much faster and uses less memory compared to resorting the entire table, even when the &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/struct.TopK.html"&gt;TopK&lt;/a&gt; operator is used. &lt;/p&gt;
&lt;p&gt;In order to avoid the sort the query optimizer must determine the data is already sorted. For simple queries the analysis is straightforward however it gets complicated fast. For example, what if your data is sorted by &lt;code&gt;[hostname, time ASC]&lt;/code&gt; and your query is&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT hostname, log_line 
FROM telemetry WHERE hostname = 'app.example.com' ORDER BY time ASC;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In this case a sort still isn't needed, but the analysis must reason about the sortedness of the stream when it knows &lt;code&gt;hostname&lt;/code&gt; has a single value.&lt;/p&gt;
&lt;h3&gt;Optimized Operator Implementations&lt;/h3&gt;
&lt;p&gt;As another use case, some operators can utilize the ordering information to change its underlying algorithm to execute more efficiently. Consider the following query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-SQL"&gt;SELECT COUNT(log_line) 
FROM telemetry GROUP BY hostname;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most analytic systems, including DataFusion, by default implement such a query using a hash table keyed on values of &lt;code&gt;hostname&lt;/code&gt; to store the counts. However, if the &lt;code&gt;telemetry&lt;/code&gt; table is sorted by &lt;code&gt;hostname&lt;/code&gt;,  there are much more efficient algorithms for grouping on &lt;code&gt;hostname&lt;/code&gt; values than hashing every value and storing it in memory. However, the more efficient algorithm can only be used when the input is sorted correctly. To see this in practice, check out the &lt;a href="https://github.com/apache/datafusion/tree/main/datafusion/physical-plan/src/aggregates/order"&gt;source&lt;/a&gt; for ordered variant of the &lt;code&gt;Aggregation&lt;/code&gt; in &lt;code&gt;DataFusion&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Streaming-Friendly Execution&lt;/h3&gt;
&lt;p&gt;Stream processing aims to produce results immediately as they become available ensuring minimal latency for real-time workloads. However, some operators need to consume all input data before producing any output. Consider the &lt;code&gt;Sort&lt;/code&gt; operation: before it can start generating output, the algorithm must first process all input data. As a result, data flow halts whenever such an operator is encountered until all input is consumed. When a physical query plan contains such an operator (&lt;code&gt;Sort&lt;/code&gt;, &lt;code&gt;CrossJoin&lt;/code&gt;, ..) we refer to this as pipeline breaking, meaning the query cannot be executed in a streaming fashion.&lt;/p&gt;
&lt;p&gt;For a query to be executed in a streaming fashion we need to satisfy 2 conditions:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Logically Streamable&lt;/strong&gt;&lt;br/&gt;
It should be possible to generate what user wants in streaming fashion. Consider following query:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-SQL"&gt;SELECT SUM(amount)  
FROM orders  
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here, the user wants to compute the sum of all amounts in the orders table. By the nature of the query this requires scanning the entire table to generate a result making it impossible to execute in a streaming fashion.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Streaming Aware Planner&lt;/strong&gt;&lt;br/&gt;
Being logically streamable does not guarantee that a query will execute in a streaming fashion. SQL is a declarative language, meaning it specifies 'WHAT' user wants. It is up to the planner 'HOW' to generate the result. In most cases there are many ways to compute the correct result for a given query. The query planner is responsible for choosing "a way" (ideally the best&lt;sup id="optimal1"&gt;&lt;a href="#optimal"&gt;*&lt;/a&gt;&lt;/sup&gt; one) among the all alternatives to generate what user asks for. If a plan contains a pipeline-breaking operator the execution will not be streaming&amp;mdash;even if the query is logically streamable. To generate truly streaming plans from logically streamable queries the planner must carefully analyze the existing orderings in the source tables to ensure that the final plan does not contain any pipeline-breaking operators.&lt;/p&gt;
&lt;h2&gt;Analysis&lt;/h2&gt;
&lt;p&gt;Let's start by creating an example table that we will refer throughout the post. This table models the input data of an operator for the analysis:&lt;/p&gt;
&lt;h3&gt;Example Virtual Table&lt;/h3&gt;
&lt;style&gt;
  table {
    border-collapse: collapse;
    width: 80%;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
  }
  th, td {
    padding: 12px 16px;
    text-align: left;
    border-bottom: 1px solid #e0e0e0;
  }
  th {
    background-color: #f9f9f9;
    font-weight: 600;
  }
  tr:hover {
    background-color: #f1f1f1;
  }
&lt;/style&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;amount&lt;/th&gt; &lt;th&gt;price&lt;/th&gt; &lt;th&gt;hostname&lt;/th&gt;&lt;th&gt;currency&lt;/th&gt;&lt;th&gt;time_bin&lt;/th&gt; &lt;th&gt;time&lt;/th&gt; &lt;th&gt;price_cloned&lt;/th&gt; &lt;th&gt;time_cloned&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:01:30&lt;/td&gt; &lt;td&gt;25&lt;/td&gt; &lt;td&gt;08:01:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt; &lt;td&gt;26&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:11:30&lt;/td&gt; &lt;td&gt;26&lt;/td&gt; &lt;td&gt;08:11:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:41:30&lt;/td&gt; &lt;td&gt;30&lt;/td&gt; &lt;td&gt;08:41:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:55:15&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;08:55:15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;35&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:10:23&lt;/td&gt; &lt;td&gt;35&lt;/td&gt; &lt;td&gt;09:10:23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:20:33&lt;/td&gt; &lt;td&gt;18&lt;/td&gt; &lt;td&gt;09:20:33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;app.example.com&lt;/td&gt; &lt;td&gt;USD&lt;/td&gt; &lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:40:15&lt;/td&gt; &lt;td&gt;22&lt;/td&gt; &lt;td&gt;09:40:15&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br/&gt;&lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;How can a table have multiple orderings?&lt;/strong&gt; At first glance it may seem counterintuitive for a table to have more than one valid ordering. However, during query execution such scenarios can arise.

For example consider the following query:


&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT time, date_bin('1 hour', time, '1970-01-01') as time_bin  
FROM table;
&lt;/code&gt;&lt;/pre&gt;

If we know that the table is ordered by &lt;code&gt;time ASC&lt;/code&gt; we can infer that &lt;code&gt;time_bin ASC&lt;/code&gt; is also a valid ordering. This is because the &lt;code&gt;date_bin&lt;/code&gt; function is monotonic, meaning it preserves the order of its input.

DataFusion leverages these functional dependencies to infer new orderings as data flows through different query operators. For details on the implementation see the &lt;a ,="" href="https://github.com/apache/datafusion/blob/main/datafusion/common/src/functional_dependencies.rs" target="_blank"&gt;source&lt;/a&gt; code.
&lt;/blockquote&gt;
&lt;p&gt;By inspection, you can see this table is sorted by the &lt;code&gt;amount&lt;/code&gt; column, but It is also sorted by &lt;code&gt;time&lt;/code&gt; and &lt;code&gt;time_bin&lt;/code&gt; as well as the compound &lt;code&gt;(time_bin, amount)&lt;/code&gt; and many other variations. While this example is an extreme case, real-world data often has multiple sort orders. &lt;/p&gt;
&lt;p&gt;A naive approach for analyzing whether the ordering requirement of an operator is satisfied by its input would be:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Store all the valid ordering expressions that the tables satisfies  &lt;/li&gt;
&lt;li&gt;Check whether the ordering requirement by the operator is among valid orderings.  &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This naive algorithm works and correct. However, listing all valid orderings can be quite lengthy and is of exponential complexity as the number of orderings grows. For the example table here is a (small) subset of the valid orderings:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;[amount ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[amount ASC, price_cloned ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[hostname ASC, amount ASC, price_cloned ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[amount ASC, hostname ASC,  price_cloned ASC]&lt;/code&gt;&lt;br/&gt;
&lt;code&gt;[amount ASC, price_cloned ASC, hostname ASC]&lt;/code&gt;&lt;br/&gt;
.&lt;br/&gt;
.&lt;br/&gt;
.  &lt;/p&gt;
&lt;p&gt;As can be seen from the listing above storing all valid orderings is wasteful and contains significant redundancy. Here are some observations which suggest that we can do much better:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Storing a prefix of another valid ordering is redundant. If the table satisfies the lexicographic ordering&lt;sup id="fn1"&gt;&lt;a href="#footnote1"&gt;1&lt;/a&gt;&lt;/sup&gt;: &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, it already satisfies ordering &lt;code&gt;[amount ASC]&lt;/code&gt; trivially. Hence, once we store &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt; storing &lt;code&gt;[amount ASC]&lt;/code&gt; is redundant.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using all columns that are equal to each other in the listings is redundant. If we know the table is ordered by &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, it is also ordered by &lt;code&gt;[amount ASC, price_cloned ASC]&lt;/code&gt; since &lt;code&gt;price&lt;/code&gt; and &lt;code&gt;price_cloned&lt;/code&gt; are copy of each other. It is enough to use just one expression among the expressions that exact copy of each other.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Constant expressions can be inserted anywhere in a valid ordering with an arbitrary direction (e.g. &lt;code&gt;ASC&lt;/code&gt;, &lt;code&gt;DESC&lt;/code&gt;). Hence, if the table is ordered by &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, it is also ordered by: &lt;br/&gt;
&lt;code&gt;[hostname ASC, amount ASC, price ASC]&lt;/code&gt;,&lt;br/&gt;
&lt;code&gt;[hostname DESC, amount ASC, price ASC]&lt;/code&gt;,&lt;br/&gt;
&lt;code&gt;[amount ASC, hostname ASC, price ASC]&lt;/code&gt;,&lt;br/&gt;
   .&lt;br/&gt;
   .    &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is clearly redundant. For this reason, it is better to avoid explicitly encoding constant expressions in valid sort orders.&lt;/p&gt;
&lt;p&gt;In summary,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We should store only the longest lexicographic ordering (shouldn't use any prefix of it)&lt;/li&gt;
&lt;li&gt;Using expressions that are exact copies of each other is redundant.&lt;/li&gt;
&lt;li&gt;Ordering expressions shouldn't contain any constant expression.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Key Concepts for Analyzing Orderings&lt;/h2&gt;
&lt;p&gt;To solve the shortcomings above DataFusion needs to track of following properties for the table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Constant Expressions  &lt;/li&gt;
&lt;li&gt;Equivalent Expression Groups (will be explained shortly)&lt;/li&gt;
&lt;li&gt;Succinct Valid Orderings (will be explained shortly)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Note:&lt;/strong&gt; These properties are implemented in the &lt;code&gt;EquivalenceProperties&lt;/code&gt; structure in &lt;code&gt;DataFusion&lt;/code&gt;, please see the &lt;a href="https://github.com/apache/datafusion/blob/f47ea73b87eec4af044f9b9923baf042682615b2/datafusion/physical-expr/src/equivalence/properties/mod.rs#L134" target="_blank"&gt;source&lt;/a&gt; for more details&lt;br/&gt;
&lt;/blockquote&gt;
&lt;p&gt;These properties allow us to analyze whether the ordering requirement is satisfied by the data already.&lt;/p&gt;
&lt;h3&gt;1. Constant Expressions&lt;/h3&gt;
&lt;p&gt;Constant expressions are those where each row in the expression has the same value across all rows. Although constant expressions may seem odd in a table they can arise after operations like &lt;code&gt;Filter&lt;/code&gt; or &lt;code&gt;Join&lt;/code&gt; occur. &lt;/p&gt;
&lt;p&gt;For instance in the example table:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Columns &lt;code&gt;hostname&lt;/code&gt; and &lt;code&gt;currency&lt;/code&gt; are constant because every row in the table has the same value (&lt;code&gt;'app.example.com'&lt;/code&gt; for &lt;code&gt;hostname&lt;/code&gt;, and &lt;code&gt;'USD'&lt;/code&gt; for &lt;code&gt;currency&lt;/code&gt;) for these columns.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Note:&lt;/strong&gt; Constant expressions can arise during query execution. For example, in following query:&lt;br/&gt;
&lt;code&gt;SELECT hostname FROM logs&lt;/code&gt;&lt;br/&gt;&lt;code&gt;WHERE hostname='app.example.com'&lt;/code&gt; &lt;br/&gt;
    after filtering is done, for subsequent operators the &lt;code&gt;hostname&lt;/code&gt; column will be constant.
&lt;/blockquote&gt;
&lt;h3&gt;2. Equivalent Expression Groups&lt;/h3&gt;
&lt;p&gt;Equivalent expression groups are expressions that always hold the same value across rows. These expressions can be thought of as clones of each other and may arise from operations like &lt;code&gt;Filter&lt;/code&gt;, &lt;code&gt;Join&lt;/code&gt;, or &lt;code&gt;Projection&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In the example table, the expressions &lt;code&gt;price&lt;/code&gt; and &lt;code&gt;price_cloned&lt;/code&gt; form one equivalence group, and &lt;code&gt;time&lt;/code&gt; and &lt;code&gt;time_cloned&lt;/code&gt; form another equivalence group.&lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;strong&gt;Note:&lt;/strong&gt; Equivalent expression groups can arise during the query execution. For example, in the following query:&lt;br/&gt;
&lt;code&gt;SELECT time, time as time_cloned FROM logs&lt;/code&gt; &lt;br/&gt;
    after the projection is done, for subsequent operators &lt;code&gt;time&lt;/code&gt; and &lt;code&gt;time_cloned&lt;/code&gt; will form an equivalence group. As another example, in the following query:&lt;br/&gt;
&lt;code&gt;SELECT employees.id, employees.name, departments.department_name&lt;/code&gt;
&lt;code&gt;FROM employees&lt;/code&gt;
&lt;code&gt;JOIN departments ON employees.department_id = departments.id;&lt;/code&gt; &lt;br/&gt;
after joining, &lt;code&gt;employees.department_id&lt;/code&gt; and &lt;code&gt;departments.id&lt;/code&gt; will form an equivalence group.
&lt;/blockquote&gt;
&lt;h3&gt;3. Succinct Encoding of Valid Orderings&lt;/h3&gt;
&lt;p&gt;Valid orderings are the orderings that the table already satisfies. However, naively listing them requires exponential space as the number of columns grows as discussed before. Instead, we list all valid orderings after following constraints are applied:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do not use any constant expressions in the valid ordering construction&lt;/li&gt;
&lt;li&gt;Use only one entry (by convention the first entry) in the equivalent expression group.&lt;/li&gt;
&lt;li&gt;Lexicographic ordering shouldn't contain any leading ordering&lt;sup id="fn2"&gt;&lt;a href="#footnote2"&gt;2&lt;/a&gt;&lt;/sup&gt;except the first position &lt;sup id="fn3"&gt;&lt;a href="#footnote3"&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;Do not use any prefix of a valid lexicographic ordering&lt;sup id="fn4"&gt;&lt;a href="#footnote4"&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After applying the first and second constraint, the example table simplifies to &lt;/p&gt;
&lt;style&gt;
  table {
    border-collapse: collapse;
    width: 80%;
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
  }
  th, td {
    padding: 12px 16px;
    text-align: left;
    border-bottom: 1px solid #e0e0e0;
  }
  th {
    background-color: #f9f9f9;
    font-weight: 600;
  }
  tr:hover {
    background-color: #f1f1f1;
  }
&lt;/style&gt;
&lt;table class="table"&gt;
&lt;tr&gt;
&lt;th&gt;amount&lt;/th&gt; &lt;th&gt;price&lt;/th&gt;&lt;th&gt;time_bin&lt;/th&gt; &lt;th&gt;time&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt; &lt;td&gt;25&lt;/td&gt;&lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:01:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt; &lt;td&gt;26&lt;/td&gt;&lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:11:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;30&lt;/td&gt;&lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:41:30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;32&lt;/td&gt;&lt;td&gt;08:00:00&lt;/td&gt; &lt;td&gt;08:55:15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt; &lt;td&gt;35&lt;/td&gt;&lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:10:23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt; &lt;td&gt;18&lt;/td&gt;&lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:20:33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt; &lt;td&gt;22&lt;/td&gt;&lt;td&gt;09:00:00&lt;/td&gt; &lt;td&gt;09:40:15&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br/&gt;
Following third and fourth constraints for the simplified table, the succinct valid orderings are:&lt;br/&gt;
&lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, &lt;br/&gt;
&lt;code&gt;[time_bin ASC]&lt;/code&gt;,&lt;br/&gt;
&lt;code&gt;[time ASC]&lt;/code&gt; &lt;/p&gt;
&lt;blockquote style="border-left: 4px solid #007bff; padding: 10px; background-color: #f8f9fa;"&gt;
&lt;p&gt;&lt;strong&gt;How can DataFusion find orderings?&lt;/strong&gt;&lt;/p&gt; 
DataFusion's &lt;code&gt;CREATE EXTERNAL TABLE&lt;/code&gt; has a &lt;code&gt;WITH ORDER&lt;/code&gt; clause (see &lt;a href="https://datafusion.apache.org/user-guide/sql/ddl.html#create-external-table"&gt;docs&lt;/a&gt;) to specify the known orderings of the table during table creation. For example the following query:&lt;br/&gt;
&lt;pre&gt;&lt;code&gt;
CREATE EXTERNAL TABLE source (
    amount INT NOT NULL,
    price DOUBLE NOT NULL,
    time TIMESTAMP NOT NULL,
    ...
)
STORED AS CSV
WITH ORDER (time ASC)
WITH ORDER (amount ASC, price ASC)
LOCATION '/path/to/FILE_NAME.csv'
OPTIONS ('has_header' 'true');
&lt;/code&gt;&lt;/pre&gt;
communicates that &lt;code&gt;source&lt;/code&gt; table has the orderings: &lt;code&gt;[time ASC]&lt;/code&gt; and &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;.&lt;br/&gt;
When orderings are communicated from the source, DataFusion tracks the orderings through each operator while optimizing the plan.&lt;br/&gt;
&lt;ul&gt;
&lt;li&gt;add new orderings (such as when "date_bin" function is applied to the "time" column)&lt;/li&gt;
&lt;li&gt;Remove orderings, if operation doesn't preserve the ordering of the data at its input&lt;/li&gt;
&lt;li&gt;Update equivalent groups&lt;/li&gt;
&lt;li&gt;Update constant expressions&lt;/li&gt;
&lt;/ul&gt;

Figure 1 shows an example how DataFusion generates an efficient plan for the query:
&lt;pre&gt;&lt;code&gt;
SELECT 
  row_number() OVER (ORDER BY time) as rn,
  time
FROM events
ORDER BY rn, time
&lt;/code&gt;&lt;/pre&gt;
using the orderings of the query intermediates.&lt;br/&gt;
&lt;br/&gt;
&lt;figure&gt;
&lt;img alt="Window Query Datafusion Optimization" class="img-responsive" src="/blog/images/ordering_analysis/query_window_plan.png" width="80%"/&gt;
&lt;figcaption&gt;&lt;strong&gt;Figure 1:&lt;/strong&gt; DataFusion analyzes orderings of the sources and query intermediates to generate efficient plans&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Table Properties&lt;/h3&gt;
&lt;p&gt;In summary, for the example table, the following properties correctly describe the sort properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Constant Expressions&lt;/strong&gt; = &lt;code&gt;hostname, currency&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Equivalent Expression Groups&lt;/strong&gt; = &lt;code&gt;[price, price_cloned], [time, time_cloned]&lt;/code&gt; &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Valid Orderings&lt;/strong&gt; = &lt;code&gt;[amount ASC, price ASC], [time_bin ASC], [time ASC]&lt;/code&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Algorithm for Analyzing Ordering Requirements&lt;/h3&gt;
&lt;p&gt;After deriving these properties for the data, following algorithm can be used to check whether an ordering requirement is satisfied by the table:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Prune constant expressions&lt;/strong&gt;: Remove any constant expressions from the ordering requirement.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Normalize the requirement&lt;/strong&gt;: Replace each expression in the ordering requirement with the first entry from its equivalence group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;De-duplicate expressions&lt;/strong&gt;: If an expression appears more than once, remove duplicates, keeping only the first occurrence.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Match leading orderings&lt;/strong&gt;: Check whether the leading ordering requirement&lt;sup id="fn5"&gt;&lt;a href="#footnote5"&gt;5&lt;/a&gt;&lt;/sup&gt; matches the leading valid orderings&lt;sup id="fn6"&gt;&lt;a href="#footnote6"&gt;6&lt;/a&gt;&lt;/sup&gt; of table. If so:&lt;ul&gt;
&lt;li&gt;Remove the leading ordering requirement from the ordering requirement &lt;/li&gt;
&lt;li&gt;Remove the matching leading valid ordering from the valid orderings of table. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterate through the remaining expressions&lt;/strong&gt;: Go back to step 4 until ordering requirement is empty or leading ordering requirement is not found among the leading valid orderings of table.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If, at the end of the procedure above, the ordering requirement is an empty list, we can conclude that the requirement is satisfied by the table.&lt;/p&gt;
&lt;h3&gt;Example Walkthrough&lt;/h3&gt;
&lt;p&gt;Let's say the user provided a query such as the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT * FROM table
ORDER BY hostname DESC, amount ASC, time_bin ASC, price_cloned ASC, time ASC, currency ASC, price DESC;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the input has the same properties explained above&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Constant Expressions&lt;/strong&gt; = &lt;code&gt;hostname, currency&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Equivalent Expressions Groups&lt;/strong&gt; = &lt;code&gt;[price, price_cloned], [time, time_cloned]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Succinct Valid Orderings&lt;/strong&gt; = &lt;code&gt;[amount ASC, price ASC], [time_bin ASC], [time ASC]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to remove a sort the optimizer must check if the ordering requirement &lt;code&gt;[hostname DESC, amount ASC, time_bin ASC, price_cloned ASC, time ASC, currency ASC, price DESC]&lt;/code&gt; is satisfied by the properties.&lt;/p&gt;
&lt;h3&gt;Algorithm Steps&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prune constant expressions&lt;/strong&gt;:&lt;br/&gt;
   Remove &lt;code&gt;hostname&lt;/code&gt; and &lt;code&gt;currency&lt;/code&gt; from the requirement. The requirement becomes:&lt;br/&gt;
&lt;code&gt;[amount ASC, time_bin ASC, price_cloned ASC, time ASC, price DESC]&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Normalize using equivalent groups&lt;/strong&gt;:&lt;br/&gt;
   Replace &lt;code&gt;price_cloned&lt;/code&gt; with &lt;code&gt;price&lt;/code&gt; and &lt;code&gt;time_cloned&lt;/code&gt; with &lt;code&gt;time&lt;/code&gt;. The requirement becomes:&lt;br/&gt;
&lt;code&gt;[amount ASC, time_bin ASC, price ASC, time ASC, price DESC]&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;De-duplicate expressions&lt;/strong&gt;:&lt;br/&gt;
   Since &lt;code&gt;price&lt;/code&gt; appears twice, we simplify the requirement to:&lt;br/&gt;
&lt;code&gt;[amount ASC, time_bin ASC, price ASC, time ASC]&lt;/code&gt; (keeping the first occurrence from the left side).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Match leading orderings&lt;/strong&gt;:&lt;br/&gt;
  Check if leading ordering requirement &lt;code&gt;amount ASC&lt;/code&gt; is among the leading valid orderings: &lt;code&gt;amount ASC, time_bin ASC, time ASC&lt;/code&gt;. Since this is the case, we remove &lt;code&gt;amount ASC&lt;/code&gt; from both the ordering requirement and the valid orderings of the table.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Iterate through the remaining expressions&lt;/strong&gt;:
Now, the problem is converted from&lt;br/&gt;
&lt;em&gt;"whether the requirement: &lt;code&gt;[amount ASC, time_bin ASC, price ASC, time ASC]&lt;/code&gt; is satisfied by valid orderings:  &lt;code&gt;[amount ASC, price ASC], [time_bin ASC], [time ASC]&lt;/code&gt;"&lt;/em&gt;&lt;br/&gt;
into&lt;br/&gt;
&lt;em&gt;"whether the requirement: &lt;code&gt;[time_bin ASC, price ASC, time ASC]&lt;/code&gt; is satisfied by valid orderings:  &lt;code&gt;[price ASC], [time_bin ASC], [time ASC]&lt;/code&gt;"&lt;/em&gt;&lt;br/&gt;
We go back to step 4 until the ordering requirement list is exhausted or its length no longer decreases.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At the end of stages above we end up with an empty ordering requirement list. Given this, we can conclude that the table satisfies the ordering requirement and thus no sort is required. &lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we described the conditions under which an ordering requirement is satisfied based on the properties of a table. We introduced key concepts such as constant expressions, equivalence groups, and valid orderings, and used them to determine whether a given ordering requirement are satisfied by an input table.&lt;/p&gt;
&lt;p&gt;This analysis plays a crucial role in:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choosing more efficient algorithm variants&lt;/li&gt;
&lt;li&gt;Generating streaming-friendly plans&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;DataFusion&lt;/code&gt; query engine employs this analysis (and many more) during its planning stage to ensure correct and efficient query execution. We &lt;a href="https://datafusion.apache.org/contributor-guide/index.html"&gt;welcome you&lt;/a&gt; to come and join the project.&lt;/p&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;!--
&lt;p id="footnote1"&gt;&lt;sup&gt;[1]&lt;/sup&gt;The ordering requirement refers to the condition that input data must be sorted in a certain way for a specific operator to function as intended.&lt;/p&gt;
--&gt;
&lt;p id="footnote1"&gt;&lt;sup&gt;[1]&lt;/sup&gt;Lexicographic order is a way of ordering sequences (like strings, list of expressions) based on the order of their components, similar to how words are ordered in a dictionary. It compares each element of the sequences one by one, from left to right.&lt;/p&gt;
&lt;p id="footnote2"&gt;&lt;sup&gt;[2]&lt;/sup&gt;Leading ordering is the first ordering in a lexicographic ordering list. As an example, for the ordering: &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;, leading ordering will be: &lt;code&gt;amount ASC&lt;/code&gt;. &lt;/p&gt;
&lt;p id="footnote3"&gt;&lt;sup&gt;[3]&lt;/sup&gt;This means that, if we know that &lt;code&gt;[amount ASC]&lt;/code&gt; and &lt;code&gt;[time ASC]&lt;/code&gt; are both valid orderings for the table. We shouldn't enlist &lt;code&gt;[amount ASC, time ASC]&lt;/code&gt; or &lt;code&gt;[time ASC, amount ASC]&lt;/code&gt; as valid orderings. These orderings can be deduced if we know that table satisfies the ordering &lt;code&gt;[amount ASC]&lt;/code&gt; and &lt;code&gt;[time ASC]&lt;/code&gt;.&lt;/p&gt;
&lt;p id="footnote4"&gt;&lt;sup&gt;[4]&lt;/sup&gt;This means that, if ordering &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt; is a valid ordering for the table. We shouldn't enlist &lt;code&gt;[amount ASC]&lt;/code&gt; as valid ordering. Validity of it can be deduced from the ordering: &lt;code&gt;[amount ASC, price ASC]&lt;/code&gt;
&lt;p id="footnote5"&gt;&lt;sup&gt;[5]&lt;/sup&gt;Leading ordering requirement is the first ordering requirement in the list of lexicographic ordering requirement expression. As an example for the requirement: &lt;code&gt;[amount ASC, time_bin ASC, prices ASC, time ASC]&lt;/code&gt;, leading ordering requirement is: &lt;code&gt;amount ASC&lt;/code&gt;.&lt;/p&gt;
&lt;p id="footnote6"&gt;&lt;sup&gt;[6]&lt;/sup&gt;Leading valid orderings are the first ordering for each valid ordering list in the table. As an example, for the valid orderings: &lt;code&gt;[amount ASC, prices ASC], [time_bin ASC], [time ASC]&lt;/code&gt;, leading valid orderings will be: &lt;code&gt;amount ASC, time_bin ASC, time ASC&lt;/code&gt;. &lt;/p&gt;
&lt;p id="optimal"&gt;&lt;sup&gt;*&lt;/sup&gt;Best depends on the use case, &lt;code&gt;DataFusion&lt;/code&gt; has many various flags to communicate what user thinks the best plan is (e.g. streamable, fastest, lowest memory, etc.). See &lt;a href="https://datafusion.apache.org/user-guide/configs.html" target="_blank"&gt;configurations&lt;/a&gt; for detail.&lt;/p&gt;&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion 45.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/02/20/datafusion-45.0.0" rel="alternate"></link><published>2025-02-20T00:00:00+00:00</published><updated>2025-02-20T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-02-20:/blog/2025/02/20/datafusion-45.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/11631 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are very proud to announce &lt;a href="https://crates.io/crates/datafusion/45.0.0"&gt;DataFusion 45.0.0&lt;/a&gt;. This blog highlights some of the
many major improvements since we released &lt;a href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/"&gt;DataFusion 40.0.0&lt;/a&gt; and a preview of
what the community is thinking about in the next 6 months. It has been an exciting
period of development …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/11631 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are very proud to announce &lt;a href="https://crates.io/crates/datafusion/45.0.0"&gt;DataFusion 45.0.0&lt;/a&gt;. This blog highlights some of the
many major improvements since we released &lt;a href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/"&gt;DataFusion 40.0.0&lt;/a&gt; and a preview of
what the community is thinking about in the next 6 months. It has been an exciting
period of development for DataFusion!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; is an extensible query engine, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that
uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion is used by developers to
create new, fast data centric systems such as databases, dataframe libraries,
machine learning and streaming applications. While &lt;a href="https://datafusion.apache.org/user-guide/introduction.html#project-goals"&gt;DataFusion&amp;rsquo;s primary design
goal&lt;/a&gt; is to accelerate the creation of other data centric systems, it has a
reasonable experience directly out of the box as a &lt;a href="https://datafusion.apache.org/user-guide/dataframe.html"&gt;dataframe library&lt;/a&gt;,
&lt;a href="https://datafusion.apache.org/python/"&gt;python library&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/user-guide/cli/"&gt;command line SQL tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion's core thesis is that as a community, together we can build much more
advanced technology than any of us as individuals or companies could do alone. 
Without DataFusion, highly performant vectorized query engines would remain
the domain of a few large companies and world-class research institutions. 
With DataFusion, we can all build on top of a shared foundation, and focus on
what makes our projects unique.&lt;/p&gt;
&lt;h2&gt;Community Growth  📈&lt;/h2&gt;
&lt;p&gt;In the last 6 months, between &lt;code&gt;40.0.0&lt;/code&gt; and &lt;code&gt;45.0.0&lt;/code&gt;, our community continues to
grow in new and exciting ways.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We added several PMC members and new committers: &lt;a href="https://github.com/jayzhan211"&gt;@jayzhan211&lt;/a&gt; and &lt;a href="https://github.com/jonahgao"&gt;@jonahgao&lt;/a&gt; joined the PMC,
   &lt;a href="https://github.com/2010YOUY01"&gt;@2010YOUY01&lt;/a&gt;, &lt;a href="https://github.com/rachelint"&gt;@rachelint&lt;/a&gt;, &lt;a href="https://github.com/findepi/"&gt;@findpi&lt;/a&gt;, &lt;a href="https://github.com/iffyio"&gt;@iffyio&lt;/a&gt;, &lt;a href="https://github.com/goldmedal"&gt;@goldmedal&lt;/a&gt;, &lt;a href="https://github.com/Weijun-H"&gt;@Weijun-H&lt;/a&gt;, &lt;a href="https://github.com/Michael-J-Ward"&gt;@Michael-J-Ward&lt;/a&gt; and &lt;a href="https://github.com/korowa"&gt;@korowa&lt;/a&gt;
   joined as committers. See the &lt;a href="https://lists.apache.org/list.html?dev@datafusion.apache.org"&gt;mailing list&lt;/a&gt; for more details.&lt;/li&gt;
&lt;li&gt;In the &lt;a href="https://github.com/apache/arrow-datafusion"&gt;core DataFusion repo&lt;/a&gt; alone we reviewed and accepted almost 1600 PRs from 206 different
   committers, created over 1100 issues and closed 751 of them 🚀. All changes are listed in the detailed
   &lt;a href="https://github.com/apache/datafusion/tree/main/dev/changelog"&gt;changelogs&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;DataFusion focused meetups happened in multiple cities around the world: &lt;a href="https://github.com/apache/datafusion/discussions/10341#discussioncomment-10110273"&gt;Hangzhou&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/11431"&gt;Belgrade&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/11213"&gt;New York&lt;/a&gt;, 
   &lt;a href="https://github.com/apache/datafusion/discussions/10348"&gt;Seattle&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/12894"&gt;Chicago&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/13165"&gt;Boston&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/discussions/12988"&gt;Amsterdam&lt;/a&gt; as well as a Rust NYC meetup in NYC focused on DataFusion.&lt;/li&gt;
&lt;/ol&gt;
&lt;!--
$ git log --pretty=oneline 40.0.0..45.0.0 . | wc -l
     1532 (up from 1453)

$ git shortlog -sn 40.0.0..45.0.0 . | wc -l
     206 (up from 182)

https://crates.io/crates/datafusion/45.0.0
DataFusion 45 released Feb 7, 2025

https://crates.io/crates/datafusion/40.0.0
DataFusion 40 released July 12, 2024

Issues created in this time: 375 open, 751 closed (from 321 open, 781 closed)
https://github.com/apache/datafusion/issues?q=is%3Aissue+created%3A2024-07-12..2025-02-07

Issues closed: 956 (up from 911)
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+closed%3A2024-07-12..2025-02-07

PRs merged in this time 1597 (up from 1490)
https://github.com/apache/arrow-datafusion/pulls?q=is%3Apr+merged%3A2024-07-12..2025-02-07

--&gt;
&lt;p&gt;DataFusion has put in an application to be part of &lt;a href="https://summerofcode.withgoogle.com/"&gt;Google Summer of Code&lt;/a&gt; with a 
&lt;a href="https://github.com/apache/datafusion/issues/14478"&gt;number of ideas&lt;/a&gt; for projects with mentors already selected. Additionally, &lt;a href="https://github.com/apache/datafusion/issues/14373"&gt;some ideas&lt;/a&gt; on
how to make DataFusion an ideal selection for university database projects such as the 
&lt;a href="https://15445.courses.cs.cmu.edu/spring2025/"&gt;CMU database classes&lt;/a&gt; have been put forward.&lt;/p&gt;
&lt;p&gt;In addition, DataFusion has been appearing publicly more and more, both online and offline. Here are some highlights:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;a href="https://uwheel.rs/post/datafusion_uwheel/"&gt;demonstration of how uwheel&lt;/a&gt; is integrated into DataFusion&lt;/li&gt;
&lt;li&gt;Integrating StringView into DataFusion - &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;part 1&lt;/a&gt; and &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-two-influxdb/"&gt;part 2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://techontherocks.show/3"&gt;Building streams&lt;/a&gt; with DataFusion&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.haoxp.xyz/posts/caching-datafusion"&gt;Caching in DataFusion&lt;/a&gt;: Don't read twice&lt;/li&gt;
&lt;li&gt;&lt;a href="https://blog.haoxp.xyz/posts/parquet-to-arrow/"&gt;Parquet pruning in DataFusion&lt;/a&gt;: Read no more than you need&lt;/li&gt;
&lt;li&gt;DataFusion is one of &lt;a href="https://www.crn.com/news/software/2024/the-10-coolest-open-source-software-tools-of-2024?page=3"&gt;The 10 coolest open source software tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.denormalized.io/blog/building-databases"&gt;Building databases over a weekend&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Improved Performance 🚀&lt;/h2&gt;
&lt;p&gt;DataFusion hit a milestone in its development by becoming &lt;a href="https://datafusion.apache.org/blog/2024/11/18/datafusion-fastest-single-node-parquet-clickbench/"&gt;the fastest single node engine&lt;/a&gt; 
for querying Apache Parquet files in &lt;a href="https://benchmark.clickhouse.com/"&gt;clickbench&lt;/a&gt; benchmark for the 43.0.0 release. A &lt;a href="https://github.com/apache/datafusion/issues/12821"&gt;lot 
of work&lt;/a&gt; went into making this happen! While other engines have subsequently gotten faster,
displacing DataFusion from the top spot, DataFusion still remains near the top and we &lt;a href="https://github.com/apache/datafusion/issues/14586"&gt;are planning
more improvements&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ClickBench performance results over time for DataFusion" class="img-responsive" src="/blog/images/datafusion-45.0.0/performance_over_time.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: ClickBench performance improved over 33% between DataFusion 33
(released Nov. 2023) and DataFusion 45 (released Feb. 2025). &lt;/p&gt;
&lt;p&gt;The task of &lt;a href="https://github.com/apache/datafusion/issues/10918"&gt;integrating&lt;/a&gt; the new &lt;a href="https://docs.rs/arrow/latest/arrow/array/struct.GenericByteViewArray.html"&gt;Arrow StringView&lt;/a&gt; which significantly improves performance 
for workloads that scan, filter and group by variable length string and binary data was completed 
and enabled by default in the past 6 months. The improvement is especially pronounced for Parquet 
files due to &lt;a href="https://github.com/apache/arrow-rs/issues/5530"&gt;upstream work in the parquet reader&lt;/a&gt;. Kudos to &lt;a href="https://github.com/XiangpengHong"&gt;@XiangpengHong&lt;/a&gt;, &lt;a href="https://github.com/AriesDevil"&gt;@AriesDevil&lt;/a&gt;, 
&lt;a href="https://github.com/PsiACE"&gt;@PsiACE&lt;/a&gt;, &lt;a href="https://github.com/Weijun-H"&gt;@Weijun-H&lt;/a&gt;, &lt;a href="https://github.com/a10y"&gt;@a10y&lt;/a&gt;, and &lt;a href="https://github.com/RinChanNOWWW"&gt;@RinChanNOWWW&lt;/a&gt; for driving this project.&lt;/p&gt;
&lt;h2&gt;Improved Quality 📋&lt;/h2&gt;
&lt;p&gt;DataFusion continues to improve overall in quality. In addition to ongoing bug
fixes, one of the most exciting improvements in the last 6 months was the addition of the 
&lt;a href="https://github.com/apache/datafusion/pull/13936"&gt;SQLite sqllogictest suite&lt;/a&gt; thanks to &lt;a href="https://github.com/Omega359"&gt;@Omega359&lt;/a&gt;. These tests run over 5 million 
sql statements on every push to the main branch.&lt;/p&gt;
&lt;p&gt;Support for &lt;a href="https://github.com/apache/datafusion/pull/13651"&gt;explicitly checking logical plan invariants&lt;/a&gt; was added by &lt;a href="https://github.com/wiedld"&gt;@wiedld&lt;/a&gt; which 
can help catch implicit changes that might cause problems during upgrades.&lt;/p&gt;
&lt;p&gt;We have also started other quality initiatives to make it &lt;a href="https://github.com/apache/datafusion/issues/13525"&gt;easier to use DataFusion&lt;/a&gt; 
based on &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt;'s experience along with more &lt;a href="https://github.com/apache/datafusion/issues/13661"&gt;extensive prerelease testing&lt;/a&gt;.  &lt;/p&gt;
&lt;h2&gt;Improved Documentation 📚&lt;/h2&gt;
&lt;p&gt;We continue to improve the documentation to make it easier to get started using DataFusion. 
During the last 6 months two projects were initiated to migrate the function documentation
from strictly static markdown files. First, &lt;a href="https://github.com/apache/datafusion/pull/12668"&gt;@Omega359&lt;/a&gt; to allow function
documentation to be generated from code and &lt;a href="https://github.com/jonathanc-n"&gt;@jonathanc-n&lt;/a&gt; and others helped with the migration,
then &lt;a href="https://github.com/comphead"&gt;@comphead&lt;/a&gt; lead a project to &lt;a href="https://github.com/apache/datafusion/pull/12822"&gt;create a doc macro&lt;/a&gt; to allow for an even easier way to write 
function documentation. A special thanks to &lt;a href="https://github.com/Chen-Yuan-Lai"&gt;@Chen-Yuan-Lai&lt;/a&gt; for migrating many functions to 
the new syntax.&lt;/p&gt;
&lt;p&gt;Additionally, the &lt;a href="https://github.com/apache/datafusion/pull/13877"&gt;examples&lt;/a&gt; were &lt;a href="https://github.com/apache/datafusion/pull/13905"&gt;refactored&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/pull/13950"&gt;cleaned up&lt;/a&gt; to improve their usefulness.&lt;/p&gt;
&lt;h2&gt;New Features ✨&lt;/h2&gt;
&lt;p&gt;There are too many new features in the last 6 months to list them all, but here
are some highlights:&lt;/p&gt;
&lt;h3&gt;Functions&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Uniform Window Functions:  &lt;code&gt;BuiltInWindowFunctions&lt;/code&gt; was removed and all now use UDFs (&lt;a href="https://github.com/jcsherin"&gt;@jcsherin&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Uniform Aggregate Functions: &lt;code&gt;BuiltInAggregateFunctions&lt;/code&gt; was removed and all now use UDFs&lt;/li&gt;
&lt;li&gt;As mentioned above function documentation was extracted from the markdown files&lt;/li&gt;
&lt;li&gt;Some new functions and sql support were added including '&lt;a href="https://github.com/apache/datafusion/pull/13799"&gt;show functions&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/11347"&gt;to_local_time&lt;/a&gt;',
  '&lt;a href="https://github.com/apache/datafusion/pull/12970"&gt;regexp_count&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/11969"&gt;map_extract&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/12211"&gt;array_distance&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/12329"&gt;array_any_value&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/12474"&gt;greatest&lt;/a&gt;',
  '&lt;a href="https://github.com/apache/datafusion/pull/13786"&gt;least&lt;/a&gt;', '&lt;a href="https://github.com/apache/datafusion/pull/14217"&gt;arrays_overlap&lt;/a&gt;'&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;FFI&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Foreign Function Interface work has started. This should allow for 
  &lt;a href="https://github.com/apache/datafusion/pull/12920"&gt;using table providers&lt;/a&gt; across languages and versions of DataFusion. This 
  is especially pertinent for integration with &lt;a href="https://delta-io.github.io/delta-rs/"&gt;delta-rs&lt;/a&gt; and other table formats.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Materialized Views&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/suremarc"&gt;@suremarc&lt;/a&gt; has added a &lt;a href="https://github.com/datafusion-contrib/datafusion-materialized-views"&gt;materialized view implementation&lt;/a&gt; in datafusion-contrib 🚀&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Substrait&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;A lot of work was put into improving and enhancing substrait support (&lt;a href="https://github.com/Blizzara"&gt;@Blizzara&lt;/a&gt;, &lt;a href="https://github.com/westonpace"&gt;@westonpace&lt;/a&gt;, &lt;a href="https://github.com/tokoko"&gt;@tokoko&lt;/a&gt;, &lt;a href="https://github.com/vbarua"&gt;@vbarua&lt;/a&gt;, &lt;a href="https://github.com/LatrecheYasser"&gt;@LatrecheYasser&lt;/a&gt;, &lt;a href="https://github.com/notfilippo"&gt;@notfilippo&lt;/a&gt; and others)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Looking Ahead: The Next Six Months 🔭&lt;/h2&gt;
&lt;p&gt;One of the long term goals of &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;, DataFusion's PMC chair, has been to have 
&lt;a href="https://www.influxdata.com/blog/datafusion-2025-influxdb/"&gt;1000 DataFusion based projects&lt;/a&gt;. This may be the year that happens!&lt;/p&gt;
&lt;p&gt;The community has been &lt;a href="https://github.com/apache/datafusion/issues/14580"&gt;discussing what we will work on in the next six months&lt;/a&gt;.
Some major initiatives are likely to be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;em&gt;Performance&lt;/em&gt;: A &lt;a href="https://github.com/apache/datafusion/issues/14482"&gt;number of items have been identified&lt;/a&gt; as areas that could use additional work&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Memory usage&lt;/em&gt;: Tracking and improving memory usage, statistics and spilling to disk &lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;a href="https://summerofcode.withgoogle.com/"&gt;Google Summer of Code&lt;/a&gt; (GSOC)&lt;/em&gt;: DataFusion is hopefully selected as a project and we start accepting and supporting student projects &lt;/li&gt;
&lt;li&gt;&lt;em&gt;FFI&lt;/em&gt;: Extending the FFI implementation to support to all types of UDF's and SessionContext&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Spark Functions&lt;/em&gt;: A &lt;a href="https://github.com/apache/datafusion/issues/5600"&gt;proposal has been made to add a crate&lt;/a&gt; covering spark compatible builtin functions &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;How to Get Involved&lt;/h2&gt;
&lt;p&gt;DataFusion is not a project built or driven by a single person, company, or
foundation. Rather, our community of users and contributors work together to
build a shared technology that none of us could have built alone.&lt;/p&gt;
&lt;p&gt;If you are interested in joining us we would love to have you. You can try out
DataFusion on some of your own data and projects and let us know how it goes,
contribute suggestions, documentation, bug reports, or a PR with documentation,
tests or code. A list of open issues suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt; and you
can find how to reach us on the &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication doc&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.6.0 Release</title><link href="https://datafusion.apache.org/blog/2025/02/17/datafusion-comet-0.6.0" rel="alternate"></link><published>2025-02-17T00:00:00+00:00</published><updated>2025-02-17T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-02-17:/blog/2025/02/17/datafusion-comet-0.6.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.6.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.6.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately four weeks of development work and is the result of merging 39 PRs from 12
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.6.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;Starting with this release, we now plan on releasing new versions of Comet more frequently, typically within 1-2 weeks
of each major DataFusion release. The main motivation for this change is to better support downstream Rust projects 
that depend on the &lt;a href="https://docs.rs/datafusion-comet-spark-expr/latest/datafusion_comet_spark_expr/"&gt;datafusion_comet_spark_expr&lt;/a&gt; crate.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;DataFusion Upgrade&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Comet 0.6.0 uses DataFusion 45.0.0&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;New Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Comet now supports &lt;code&gt;array_join&lt;/code&gt;, &lt;code&gt;array_intersect&lt;/code&gt;, and &lt;code&gt;arrays_overlap&lt;/code&gt;. Note that these expressions are not 
  yet guaranteed to be 100% compatible with Spark for all input data types, so these expressions are only enabled 
  with the configuration setting &lt;code&gt;spark.comet.expression.allowIncompatible=true&lt;/code&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Performance &amp;amp; Stability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Metrics from native execution are now updated in Spark every 3 seconds by default, rather than for each
  batch being processed. The mechanism for passing the metrics via JNI is also more efficient.&lt;/li&gt;
&lt;li&gt;New memory pool options "fair unified" and "unbounded" have been added. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/tuning.html"&gt;Comet Tuning Guide&lt;/a&gt; for more information.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Bug Fixes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Hashing of decimal values with precision &amp;lt;= 18 is now compatible with Spark&lt;/li&gt;
&lt;li&gt;Comet falls back to Spark when hashing decimals with precision &amp;gt; 18&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Ballista 43.0.0 Released</title><link href="https://datafusion.apache.org/blog/2025/02/02/datafusion-ballista-43.0.0" rel="alternate"></link><published>2025-02-02T00:00:00+00:00</published><updated>2025-02-02T00:00:00+00:00</updated><author><name>milenkovicm</name></author><id>tag:datafusion.apache.org,2025-02-02:/blog/2025/02/02/datafusion-ballista-43.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are  pleased to announce version &lt;a href="https://github.com/apache/datafusion-ballista/blob/main/CHANGELOG.md#4300-2025-01-07"&gt;43.0.0&lt;/a&gt; of the &lt;a href="https://datafusion.apache.org/ballista/"&gt;DataFusion Ballista&lt;/a&gt;. Ballista allows existing &lt;a href="https://datafusion.apache.org"&gt;DataFusion&lt;/a&gt; applications to be scaled out on a cluster for use cases that are not practical to run on a single node.&lt;/p&gt;
&lt;h2&gt;Highlights of this release&lt;/h2&gt;
&lt;h3&gt;Seamless Integration with DataFusion&lt;/h3&gt;
&lt;p&gt;The primary objective of …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are  pleased to announce version &lt;a href="https://github.com/apache/datafusion-ballista/blob/main/CHANGELOG.md#4300-2025-01-07"&gt;43.0.0&lt;/a&gt; of the &lt;a href="https://datafusion.apache.org/ballista/"&gt;DataFusion Ballista&lt;/a&gt;. Ballista allows existing &lt;a href="https://datafusion.apache.org"&gt;DataFusion&lt;/a&gt; applications to be scaled out on a cluster for use cases that are not practical to run on a single node.&lt;/p&gt;
&lt;h2&gt;Highlights of this release&lt;/h2&gt;
&lt;h3&gt;Seamless Integration with DataFusion&lt;/h3&gt;
&lt;p&gt;The primary objective of this release has been to achieve a more seamless integration with the DataFusion ecosystem and try to achieve the same level of flexibility as DataFusion.&lt;/p&gt;
&lt;p&gt;In recent months, our development efforts have been directed toward providing a robust and extensible Ballista API. This new API empowers end-users to tailor Ballista's core functionality to their specific use cases. As a result, we have deprecated several experimental features from the Ballista core, allowing users to reintroduce them as custom extensions outside the core framework. This shift reduces the maintenance burden on Ballista's core maintainers and paves the way for optional features, such as &lt;a href="https://github.com/delta-io/delta-rs"&gt;delta-rs&lt;/a&gt; support, to be added externally when needed.&lt;/p&gt;
&lt;p&gt;The most significant enhancement in this release is the deprecation of &lt;code&gt;BallistaContext&lt;/code&gt;, which has been superseded by the DataFusion &lt;code&gt;SessionContext&lt;/code&gt;. This change enables DataFusion applications written in Rust to execute on a Ballista cluster with minimal modifications. Beyond simplifying migration and reducing maintenance overhead, this update introduces distributed write functionality to Ballista for the first time, significantly enhancing its capabilities.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;use ballista::prelude::*;
use datafusion::prelude::*;

#[tokio::main]
async fn main() -&amp;gt; datafusion::error::Result&amp;lt;()&amp;gt; {

  // Instead of creating classic SessionContext
  // let ctx = SessionContext::new();

  // create DataFusion SessionContext with ballista standalone cluster started
  // let ctx = SessionContext::standalone().await;

  // create DataFusion SessionContext with ballista remote cluster started
  let ctx = SessionContext::remote("df://localhost:50050").await;

  // register the table
  ctx.register_csv("example", "tests/data/example.csv", CsvReadOptions::new()).await?;

  // create a plan to run a SQL query
  let df = ctx.sql("SELECT a, MIN(b) FROM example WHERE a &amp;lt;= b GROUP BY a LIMIT 100").await?;

  // execute and print results
  df.show().await?;
  Ok(())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Additionally, Ballista&amp;rsquo;s versioning scheme has been aligned with that of DataFusion, ensuring that Ballista's version number reflects the compatible DataFusion version.&lt;/p&gt;
&lt;p&gt;At the moment there is a gap between DataFusion and Ballista, which we will try to bridge in the future.&lt;/p&gt;
&lt;h3&gt;Removal of Experimental Features&lt;/h3&gt;
&lt;p&gt;Ballista had grown in scope to include several experimental features in various states of completeness. Some features have been removed from this release in an effort to strip Ballista back to its core and make it easier to maintain and extend.&lt;/p&gt;
&lt;p&gt;Specifically, the caching subsystem, predefined object store registry, plugin subsystem, key-value stores for persistent scheduler state, and the UI have been removed.&lt;/p&gt;
&lt;h3&gt;Performance &amp;amp; Scalability&lt;/h3&gt;
&lt;p&gt;Ballista has significantly leveraged the advancements made in the DataFusion project over the past year. Benchmark results demonstrate notable improvements in performance, highlighting the impact of these enhancements:&lt;/p&gt;
&lt;p&gt;Per query comparison:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Per query comparison" class="img-responsive" src="/blog/images/datafusion-ballista-43.0.0/tpch_queries_compare.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Relative speedup:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Relative speedup graph" class="img-responsive" src="/blog/images/datafusion-ballista-43.0.0/tpch_queries_speedup_rel.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;The overall speedup is 2.9x&lt;/p&gt;
&lt;p&gt;&lt;img alt="Overall speedup" class="img-responsive" src="/blog/images/datafusion-ballista-43.0.0/tpch_allqueries.png" width="50%"/&gt;&lt;/p&gt;
&lt;h3&gt;New Logo&lt;/h3&gt;
&lt;p&gt;Ballista now has a new logo, which is visually similar to other DataFusion projects.  &lt;/p&gt;
&lt;p&gt;&lt;img alt="New logo" class="img-responsive" src="/blog/images/datafusion-ballista-43.0.0/ballista-logo.png" width="50%"/&gt;&lt;/p&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;Moving forward, Ballista will adopt the same release cadence as DataFusion, providing synchronized updates across the ecosystem.
Currently, there is no established long-term roadmap for Ballista. A plan will be formulated in the coming months based on community feedback and the availability of additional maintainers.&lt;/p&gt;
&lt;p&gt;In the short term, development efforts will concentrate on closing the feature gap between DataFusion and Ballista. Key priorities include implementing support for &lt;code&gt;INSERT INTO&lt;/code&gt;, enabling table &lt;code&gt;URL&lt;/code&gt; functionality, and achieving deeper integration with the Python ecosystem.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.5.0 Release</title><link href="https://datafusion.apache.org/blog/2025/01/17/datafusion-comet-0.5.0" rel="alternate"></link><published>2025-01-17T00:00:00+00:00</published><updated>2025-01-17T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2025-01-17:/blog/2025/01/17/datafusion-comet-0.5.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.5.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.5.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately 8 weeks of development work and is the result of merging 69 PRs from 15
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.5.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Performance&lt;/h3&gt;
&lt;p&gt;Comet 0.5.0 achieves a 1.9x speedup for single-node TPC-H @ 100 GB, an improvement from 1.7x in the previous release.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.5.0" class="img-responsive" src="/blog/images/comet-0.5.0/tpch_allqueries.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.5.0" class="img-responsive" src="/blog/images/comet-0.5.0/tpch_queries_compare.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;More benchmarking results can be found in the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Shuffle Improvements&lt;/h3&gt;
&lt;p&gt;Comet now supports multiple compression algorithms for compressing shuffle files. Previously, only ZSTD was supported
but Comet now also supports LZ4 and Snappy. The default is now LZ4, which matches the default in Spark. ZSTD may be
a better choice when the compression ratio is more important than CPU overhead.&lt;/p&gt;
&lt;p&gt;Previously, Comet used Arrow IPC to encode record batches into shuffle files. Although Arrow IPC is a good
general-purpose framework for serializing Arrow record batches, we found that we could get better performance using
a custom serialization approach optimized for Comet. One optimization is that the schema is encoded once per shuffle
operation rather than once per batch. There are some planned performance improvements in the Rust implementation of
Arrow IPC and Comet may switch back to Arrow IPC in the future.&lt;/p&gt;
&lt;p&gt;Comet provides two shuffle implementations. Comet native shuffle is the fastest and performs repartitioning in
native code. Comet columnar shuffle delegates to Spark to perform repartitioning and is used in cases where native
shuffle is not supported, such as with &lt;code&gt;RangePartitioning&lt;/code&gt;. Comet generally tries to use native shuffle first, then
columnar shuffle, and finally falls back to Spark if neither is supported. There was a bug in previous releases
where Comet would sometimes fall back to Spark shuffle if native shuffle was not supported and missed opportunities
to use columnar shuffle. This bug was fixed in this release but currently requires the configuration setting
&lt;code&gt;spark.comet.exec.shuffle.fallbackToColumnar=true&lt;/code&gt;. This will be enabled by default in the next release.&lt;/p&gt;
&lt;h3&gt;Memory Management&lt;/h3&gt;
&lt;p&gt;Comet 0.4.0 required Spark to be configured to use off-heap memory. In this release it is no longer required and
there are multiple options for configuring Comet to use on-heap memory instead. More details are available in the
&lt;a href="https://datafusion.apache.org/comet/user-guide/tuning.html"&gt;Comet Tuning Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Spark SQL Metrics&lt;/h3&gt;
&lt;p&gt;Comet now provides detailed metrics for native shuffle, showing time for repartitioning, encoding and compressing,
and writing to disk.&lt;/p&gt;
&lt;h3&gt;Crate Reorganization&lt;/h3&gt;
&lt;p&gt;One of the goals of the Comet project is to make Spark-compatible functionality available to other projects that
are based on DataFusion. In this release, many implementations of Spark-compatible expressions were moved from the
unpublished &lt;code&gt;datafusion-comet&lt;/code&gt; crate, which provides the native part of the Spark plugin, into the
&lt;code&gt;datafusion-comet-spark-expr&lt;/code&gt; crate. There is also ongoing work to reorganize this crate to move expressions into
subfolders named after the group name that Spark uses to organize expressions. For example, there are now subfolders
named &lt;code&gt;agg_funcs&lt;/code&gt;, &lt;code&gt;datetime_funcs&lt;/code&gt;, &lt;code&gt;hash_funcs&lt;/code&gt;, and so on.&lt;/p&gt;
&lt;h2&gt;Update on Complex Type Support&lt;/h2&gt;
&lt;p&gt;Good progress has been made with proof-of-concept work using DataFusion&amp;rsquo;s &lt;code&gt;ParquetExec&lt;/code&gt;, which has the advantage of
supporting complex types. This work is available on the &lt;code&gt;comet-parquet-exec&lt;/code&gt; branch, and the current focus is on
fixing test regressions, particularly regarding timestamp conversion issues.&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Python 43.1.0 Released</title><link href="https://datafusion.apache.org/blog/2024/12/14/datafusion-python-43.1.0" rel="alternate"></link><published>2024-12-14T00:00:00+00:00</published><updated>2024-12-14T00:00:00+00:00</updated><author><name>timsaucer</name></author><id>tag:datafusion.apache.org,2024-12-14:/blog/2024/12/14/datafusion-python-43.1.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/43.1.0/"&gt;datafusion-python 43.1.0&lt;/a&gt; has been released. This release
brings in all of the new features of the core &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; library. Since the last
blog post for &lt;a href="https://datafusion.apache.org/blog/2024/08/20/python-datafusion-40.0.0/"&gt;datafusion-python 40.1.0&lt;/a&gt;, a large number of improvements have been made
that can …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/43.1.0/"&gt;datafusion-python 43.1.0&lt;/a&gt; has been released. This release
brings in all of the new features of the core &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; library. Since the last
blog post for &lt;a href="https://datafusion.apache.org/blog/2024/08/20/python-datafusion-40.0.0/"&gt;datafusion-python 40.1.0&lt;/a&gt;, a large number of improvements have been made
that can be found in the &lt;a href="https://github.com/apache/datafusion-python/tree/main/dev/changelog"&gt;changelogs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We would like to point out four features that are particularly noteworthy.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Arrow PyCapsule import and export&lt;/li&gt;
&lt;li&gt;User-Defined Window Functions&lt;/li&gt;
&lt;li&gt;Foreign Table Providers&lt;/li&gt;
&lt;li&gt;String View performance enhancements&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Arrow PyCapsule import and export&lt;/h2&gt;
&lt;p&gt;Arrow has stable C interface for moving data between different libraries, but difficulties
sometimes arise when different Python libraries expose this interface through different
methods, requiring developers to write function calls for each library they are attempting
to work with. A better approach is to use the &lt;a href="https://arrow.apache.org/docs/format/CDataInterface/PyCapsuleInterface.html"&gt;Arrow PyCapsule Interface&lt;/a&gt; which gives a
consistent method for exposing these data structures across libraries.&lt;/p&gt;
&lt;p&gt;In &lt;a href="https://github.com/apache/datafusion-python/pull/825"&gt;PR #825&lt;/a&gt;, we introduced support for both importing and exporting Arrow data in
&lt;code&gt;datafusion-python&lt;/code&gt;. With this improvement, you can now use a single function call to import
a table from &lt;strong&gt;any&lt;/strong&gt; Python library that implements the &lt;a href="https://arrow.apache.org/docs/format/CDataInterface/PyCapsuleInterface.html"&gt;Arrow PyCapsule Interface&lt;/a&gt;.
Many popular libaries, such as &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt; and &lt;a href="https://pola.rs/"&gt;Polars&lt;/a&gt;
already support these interfaces.&lt;/p&gt;
&lt;p&gt;Suppose you have a Pandas and Polars DataFrames named &lt;code&gt;df_pandas&lt;/code&gt; or &lt;code&gt;df_polars&lt;/code&gt;, respectively:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ctx = SessionContext()
df_dfn1 = ctx.from_arrow(df_pandas)
df_dfn1.show()

df_dfn2 = ctx.from_arrow(df_polars)
df_dfn2.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One great thing about using this interface is that as any new library is developed and
uses these stable interfaces, they will work out of the box with DataFusion!&lt;/p&gt;
&lt;p&gt;Additionally, DataFusion DataFrames allow for exporting via the PyCapsule interface. For example,
to convert a DataFrame to a PyArrow table, it is simply&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pyarrow as pa
table = pa.table(df)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;User-Defined Window Functions&lt;/h2&gt;
&lt;p&gt;In &lt;code&gt;datafusion-python 42.0.0&lt;/code&gt; we released User-Defined Window Support in &lt;a href="https://github.com/apache/datafusion-python/pull/880"&gt;PR #880&lt;/a&gt;.
For a detailed description of how these work please see the online documentation for
all &lt;a href="https://datafusion.apache.org/python/user-guide/common-operations/udf-and-udfa.html"&gt;user-defined functions&lt;/a&gt;. Additionally the &lt;a href="https://github.com/apache/datafusion-python/tree/main/examples"&gt;examples folder&lt;/a&gt; contains a complete
example demonstrating the four different modes of operation of window functions
within DataFusion.&lt;/p&gt;
&lt;h2&gt;Foreign Table Providers&lt;/h2&gt;
&lt;p&gt;In the core &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; release, support was added for a Foreign Function
Interface to table providers. This creates a stable way for sharing functionality
across different libraries, similar to the &lt;a href="https://arrow.apache.org/docs/format/CDataInterface.html"&gt;Arrow C data interface&lt;/a&gt; operates. This
enables libraries, such as &lt;a href="https://delta.io/docs/"&gt;delta lake&lt;/a&gt; and &lt;a href="https://github.com/datafusion-contrib/datafusion-table-providers"&gt;datafusion-contrib&lt;/a&gt; to write their own
table providers in Rust and expose them in Python without requiring a Rust dependency
on &lt;code&gt;datafusion-python&lt;/code&gt;. This is important because it allows these libraries to
operate with &lt;code&gt;datafusion-python&lt;/code&gt; regardless of which version of &lt;code&gt;datafusion&lt;/code&gt; they
were built against.&lt;/p&gt;
&lt;p&gt;To implement this feature in a table provider is quite simple. There is a complete
example in the &lt;a href="https://github.com/apache/datafusion-python/tree/main/examples"&gt;examples folder&lt;/a&gt;, but the relevant code is here, exposed as a
Python function via &lt;a href="https://pyo3.rs/"&gt;pyo3&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;    fn __datafusion_table_provider__&amp;lt;'py&amp;gt;(
        &amp;amp;self,
        py: Python&amp;lt;'py&amp;gt;,
    ) -&amp;gt; PyResult&amp;lt;Bound&amp;lt;'py, PyCapsule&amp;gt;&amp;gt; {
        let name = CString::new("datafusion_table_provider").unwrap();

        let provider = self
            .create_table()
            .map_err(|e| PyRuntimeError::new_err(e.to_string()))?;
        let provider = FFI_TableProvider::new(Arc::new(provider), false);

        PyCapsule::new_bound(py, provider, Some(name.clone()))
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's it! All of the work of converting the table provider to use the FFI interface
is performed by the core library.&lt;/p&gt;
&lt;h2&gt;String View performance enhancements&lt;/h2&gt;
&lt;p&gt;In the core &lt;a href="https://github.com/apache/datafusion/blob/main/dev/changelog/43.0.0.md"&gt;DataFusion 43.0.0&lt;/a&gt; release, the option to enable StringView by default
was turned on. This leads to some significant performance enhancements, but it &lt;em&gt;may&lt;/em&gt;
require some changes to users of &lt;code&gt;datafusion-python&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To learn more about the excellent work on this feature please read &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1/"&gt;part 1&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-2/"&gt;part 2&lt;/a&gt;
of the blog post describing how these enhancements can lead to 20-200% performance
gains in some tests.&lt;/p&gt;
&lt;p&gt;During our testing we identified some cases where we needed to adjust workflows to
account for the fact that StringView is now the default type for string based operations.
First, when performing manipulations on string objects there is a perfomance loss when
needing to cast from string to string view or vice versa. To reap the best performance,
ideally all of your string type data will use StringView. For most users this should be
transparent. However if you specify a schema for reading or creating data, then you
likely need to change from &lt;code&gt;pa.string()&lt;/code&gt; to &lt;code&gt;pa.string_view()&lt;/code&gt;. For our testing, this
primarily happens during data loading operations and in unit tests.&lt;/p&gt;
&lt;p&gt;If you wish to disable StringView as the default type to retain the old approach,
you can do so following this example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion import SessionContext
from datafusion import SessionConfig
config = SessionConfig({"datafusion.execution.parquet.schema_force_view_types": "false"})
ctx = SessionContext(config=config)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Appreciation&lt;/h2&gt;
&lt;p&gt;We would like to thank everyone who has helped with these releases through their helpful
conversations, code review, issue descriptions, and code authoring. We would especially
like to thank the following authors of PRs who made these releases possible, listed in
alphabetical order by username: &lt;a href="https://github.com/andygrove"&gt;@andygrove&lt;/a&gt;, &lt;a href="https://github.com/drauschenbach"&gt;@drauschenbach&lt;/a&gt;, &lt;a href="https://github.com/emgeee"&gt;@emgeee&lt;/a&gt;, &lt;a href="https://github.com/ion-elgreco"&gt;@ion-elgreco&lt;/a&gt;,
&lt;a href="https://github.com/jcrist"&gt;@jcrist&lt;/a&gt;, &lt;a href="https://github.com/kosiew"&gt;@kosiew&lt;/a&gt;, &lt;a href="https://github.com/mesejo"&gt;@mesejo&lt;/a&gt;, &lt;a href="https://github.com/Michael-J-Ward"&gt;@Michael-J-Ward&lt;/a&gt;, and &lt;a href="https://github.com/sir-sigurd"&gt;@sir-sigurd&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you!&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;The DataFusion Python team is an active and engaging community and we would love
to have you join us and help the project.&lt;/p&gt;
&lt;p&gt;Here are some ways to get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learn more by visiting the &lt;a href="https://datafusion.apache.org/python/index.html"&gt;DataFusion Python project&lt;/a&gt;
page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try out the project and provide feedback, file issues, and contribute code.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.4.0 Release</title><link href="https://datafusion.apache.org/blog/2024/11/20/datafusion-comet-0.4.0" rel="alternate"></link><published>2024-11-20T00:00:00+00:00</published><updated>2024-11-20T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-11-20:/blog/2024/11/20/datafusion-comet-0.4.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.4.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.4.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately six weeks of development work and is the result of merging 51 PRs from 10
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.4.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Performance &amp;amp; Stability&lt;/h3&gt;
&lt;p&gt;There are a number of performance and stability improvements in this release. Here is a summary of some of the
larger changes. Current benchmarking results can be found in the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Unified Memory Management&lt;/h4&gt;
&lt;p&gt;Comet now uses a unified memory management approach that shares an off-heap memory pool with Apache Spark, resulting
in a much simpler configuration. Comet now requires &lt;code&gt;spark.memory.offHeap.enabled=true&lt;/code&gt;. This approach provides a
holistic view of memory usage in Spark and Comet and makes it easier to optimize system performance.&lt;/p&gt;
&lt;h4&gt;Faster Joins&lt;/h4&gt;
&lt;p&gt;Apache Spark supports sort-merge and hash joins, which have similar performance characteristics. Spark defaults to
using sort-merge joins because they are less likely to result in OutOfMemory exceptions. In vectorized query
engines such as DataFusion, hash joins outperform sort-merge joins. Comet now has an experimental feature to
replace Spark sort-merge joins with hash joins for improved performance. This feature is experimental because
there is currently no spill-to-disk support in the hash join implementation. This feature can be enabled by
setting &lt;code&gt;spark.comet.exec.replaceSortMergeJoin=true&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Bloom Filter Aggregates&lt;/h4&gt;
&lt;p&gt;Spark&amp;rsquo;s optimizer can insert Bloom filter aggregations and filters to prune large result sets before a shuffle. However,
Comet would fall back to Spark for the aggregation. Comet now has native support for Bloom filter aggregations
after previously supporting Bloom filter testing. Users no longer need to set
&lt;code&gt;spark.sql.optimizer.runtime.bloomFilter.enabled=false&lt;/code&gt; when using Comet.&lt;/p&gt;
&lt;h4&gt;Complex Type support&lt;/h4&gt;
&lt;p&gt;This release has the following improvements to complex type support:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implemented &lt;code&gt;ArrayAppend&lt;/code&gt; and &lt;code&gt;GetArrayStructFields&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Implemented native cast between structs&lt;/li&gt;
&lt;li&gt;Implemented native cast from structs to string&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;One of the highest priority items on the roadmap is to add support for reading complex types (maps, structs, and arrays)
from Parquet sources, both when reading Parquet directly and from Iceberg.&lt;/p&gt;
&lt;p&gt;Comet currently has proprietary native code for decoding Parquet pages, native column readers for all of Spark&amp;rsquo;s
primitive types, and special handling for Spark-specific use cases such as timestamp rebasing and decimal type
promotion. This implementation does not yet support complex types. File IO, decryption, and decompression are handled
in JVM code, and Parquet pages are passed on to native code for decoding.&lt;/p&gt;
&lt;p&gt;Rather than add complex type support to this existing code, we are exploring two main options to allow us to
leverage more of the upstream Arrow and DataFusion code.&lt;/p&gt;
&lt;h3&gt;Use DataFusion&amp;rsquo;s ParquetExec&lt;/h3&gt;
&lt;p&gt;For use cases where DataFusion can support reading a Parquet source, Comet could create a native plan that uses
DataFusion&amp;rsquo;s ParquetExec. We are investigating using DataFusion&amp;rsquo;s SchemaAdapter to handle some Spark-specific
handling of timestamps and decimals.&lt;/p&gt;
&lt;h3&gt;Use Arrow&amp;rsquo;s Parquet Batch Reader&lt;/h3&gt;
&lt;p&gt;For use cases not supported by DataFusion&amp;rsquo;s ParquetExec, such as integrating with Iceberg, we are exploring
replacing our current native Parquet decoding logic with the Arrow readers provided by the Parquet crate.&lt;/p&gt;
&lt;p&gt;Iceberg already provides a vectorized Spark reader for Parquet. A &lt;a href="https://github.com/apache/iceberg/pull/9841"&gt;PR&lt;/a&gt; is open against Iceberg for adding a native
version based on Comet, and we hope to update this to leverage the improvements outlined above.&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project and have a weekly &lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;DataFusion video call&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Comparing approaches to User Defined Functions in Apache DataFusion using Python</title><link href="https://datafusion.apache.org/blog/2024/11/19/datafusion-python-udf-comparisons" rel="alternate"></link><published>2024-11-19T00:00:00+00:00</published><updated>2024-11-19T00:00:00+00:00</updated><author><name>timsaucer</name></author><id>tag:datafusion.apache.org,2024-11-19:/blog/2024/11/19/datafusion-python-udf-comparisons</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Personal Context&lt;/h2&gt;
&lt;p&gt;For a few months now I&amp;rsquo;ve been working with &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;, a
fast query engine written in Rust. From my experience the language that nearly all data scientists
are working in is Python. In general, data scientists often use &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;
for in-memory tasks and &lt;a href="https://spark.apache.org/"&gt;PySpark&lt;/a&gt; for larger …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Personal Context&lt;/h2&gt;
&lt;p&gt;For a few months now I&amp;rsquo;ve been working with &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;, a
fast query engine written in Rust. From my experience the language that nearly all data scientists
are working in is Python. In general, data scientists often use &lt;a href="https://pandas.pydata.org/"&gt;Pandas&lt;/a&gt;
for in-memory tasks and &lt;a href="https://spark.apache.org/"&gt;PySpark&lt;/a&gt; for larger tasks that require
distributed processing.&lt;/p&gt;
&lt;p&gt;In addition to DataFusion, there is another Rust based newcomer to the DataFrame world,
&lt;a href="https://pola.rs/"&gt;Polars&lt;/a&gt;. The latter is growing extremely fast, and it serves many of the same
use cases as DataFusion. For my use cases, I'm interested in DataFusion because I want to be able
to build small scale tests rapidly and then scale them up to larger distributed systems with ease.
I do recommend evaluating Polars for in-memory work.&lt;/p&gt;
&lt;p&gt;Personally, I would love a single query approach that is fast for both in-memory usage and can
extend to large batch processing to exploit parallelization. I think DataFusion, coupled with
&lt;a href="https://datafusion.apache.org/ballista/"&gt;Ballista&lt;/a&gt; or
&lt;a href="https://github.com/apache/datafusion-ray"&gt;DataFusion-Ray&lt;/a&gt;, may provide this solution.&lt;/p&gt;
&lt;p&gt;As I&amp;rsquo;m testing, I&amp;rsquo;m primarily limiting my work to the
&lt;a href="https://datafusion.apache.org/python/"&gt;datafusion-python&lt;/a&gt; project, a wrapper around the Rust
DataFusion library. This wrapper gives you the speed advantages of keeping all of the data in the
Rust implementation and the ergonomics of working in Python. Personally, I would prefer to work
purely in Rust, but I also recognize that since the industry works in Python we should meet the
people where they are.&lt;/p&gt;
&lt;h2&gt;User-Defined Functions&lt;/h2&gt;
&lt;p&gt;The focus of this post is User-Defined Functions (UDFs). The DataFusion library gives a lot of
useful functions already for doing DataFrame manipulation. These are going to be similar to those
you find in other DataFrame libraries. You&amp;rsquo;ll be able to do simple arithmetic, create substrings of
columns, or find the average value across a group of rows. These cover most of the use cases
you&amp;rsquo;ll need in a DataFrame.&lt;/p&gt;
&lt;p&gt;However, there will always arise times when you want a custom function. With UDFs you open a
world of possibilities in your code. Sometimes there simply isn&amp;rsquo;t an easy way to use built-in
functions to achieve your goals.&lt;/p&gt;
&lt;p&gt;In the following, I&amp;rsquo;m going to demonstrate two example use cases. These are based on real world
problems I&amp;rsquo;ve encountered. Also I want to demonstrate the approach of &amp;ldquo;make it work, make it work
well, make it work fast&amp;rdquo; that is a motto I&amp;rsquo;ve seen thrown around in data science.&lt;/p&gt;
&lt;p&gt;I will demonstrate three approaches to writing UDFs. In order of increasing performance they are&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Writing a pure Python function to do your computation&lt;/li&gt;
&lt;li&gt;Using the PyArrow libraries in Python to accelerate your processing&lt;/li&gt;
&lt;li&gt;Writing a UDF in Rust and exposing it to Python&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Additionally I will demonstrate two variants of this. The first will be nearly identical to the
PyArrow library approach to simplify understanding how to connect the Rust code to Python. In the
second version we will do the iteration through the input arrays ourselves to give even greater
flexibility to the user.&lt;/p&gt;
&lt;p&gt;Here are the two example use cases, taken from my own work but generalized.&lt;/p&gt;
&lt;h3&gt;Use Case 1: Scalar Function&lt;/h3&gt;
&lt;p&gt;I have a DataFrame and a list of tuples that I&amp;rsquo;m interested in. I want to filter out the DataFrame
to only have values that match those tuples from certain columns in the DataFrame.&lt;/p&gt;
&lt;p&gt;To give a concrete example, we will use data generated for the &lt;a href="https://www.tpc.org/tpch/"&gt;TPC-H benchmarks&lt;/a&gt;.
Suppose I have a table of sales line items. There are many columns, but I am interested in three: a
part key (&lt;code&gt;p_partkey&lt;/code&gt;), supplier key (&lt;code&gt;p_suppkey&lt;/code&gt;), and return status (&lt;code&gt;p_returnflag&lt;/code&gt;). I want
only to return a DataFrame with a specific combination of these three values. That is, I want
to know if part number 1530 from supplier 4031 was sold (not returned), so I want a specific
combination of &lt;code&gt;p_partkey = 1530&lt;/code&gt;, &lt;code&gt;p_suppkey = 4031&lt;/code&gt;, and &lt;code&gt;p_returnflag = 'N'&lt;/code&gt;. I have a small
handful of these combinations I want to return.&lt;/p&gt;
&lt;p&gt;Probably the most ergonomic way to do this without UDF is to turn that list of tuples into a
DataFrame itself, perform a join, and select the columns from the original DataFrame. If we were
working in PySpark we would probably broadcast join the DataFrame created from the tuple list since
it is tiny. In practice, I have found that with some DataFrame libraries performing a filter rather
than a join can be significantly faster. This is worth profiling for your specific use case.&lt;/p&gt;
&lt;h3&gt;Use Case 2: Aggregate Function&lt;/h3&gt;
&lt;p&gt;I have a DataFrame with many values that I want to aggregate. I have already analyzed it and
determined there is a noise level below which I do not want to include in my analysis. I want to
compute a sum of only values that are above my noise threshold.&lt;/p&gt;
&lt;p&gt;This can be done fairly easy without leaning on a User Defined Aggegate Function (UDAF). You can
simply filter the DataFrame and then aggregate using the built-in &lt;code&gt;sum&lt;/code&gt; function. Here, we
demonstrate doing this as a UDF primarily as an example of how to write UDAFs. We will use the
PyArrow compute approach.&lt;/p&gt;
&lt;h2&gt;Pure Python approach&lt;/h2&gt;
&lt;p&gt;The fastest way (developer time, not code time) for me to implement the scalar problem solution
was to do something along the lines of &amp;ldquo;for each row, check the values of interest contains that
tuple&amp;rdquo;. I&amp;rsquo;ve published this as
&lt;a href="https://github.com/apache/datafusion-python/blob/main/examples/python-udf-comparisons.py"&gt;an example&lt;/a&gt;
in the &lt;a href="https://github.com/apache/datafusion-python"&gt;datafusion-python repository&lt;/a&gt;. Here is an
example of how this can be done:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;values_of_interest = [
    (1530, 4031, "N"),
    (6530, 1531, "N"),
    (5618, 619, "N"),
    (8118, 8119, "N"),
]

def is_of_interest_impl(
    partkey_arr: pa.Array,
    suppkey_arr: pa.Array,
    returnflag_arr: pa.Array,
) -&amp;gt; pa.Array:
    result = []
    for idx, partkey in enumerate(partkey_arr):
        partkey = partkey.as_py()
        suppkey = suppkey_arr[idx].as_py()
        returnflag = returnflag_arr[idx].as_py()
        value = (partkey, suppkey, returnflag)
        result.append(value in values_of_interest)

    return pa.array(result)

# Wrap our custom function with `datafusion.udf`, annotating expected 
# parameter and return types
is_of_interest = udf(
    is_of_interest_impl,
    [pa.int64(), pa.int64(), pa.utf8()],
    pa.bool_(),
    "stable",
)

df_udf_filter = df_lineitem.filter(
    is_of_interest(col("l_partkey"), col("l_suppkey"), col("l_returnflag"))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When working with a DataFusion UDF in Python, you define your function to take in some number of
expressions. During the evaluation, these will get computed into their corresponding values and
passed to your UDF as a PyArrow Array. We must return an Array also with the same number of
elements (rows). So the UDF example just iterates through all of the arrays and checks to see if
the tuple created from these columns matches any of those that we&amp;rsquo;re looking for.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll repeat because this is something that tripped me up the first time I wrote a UDF for
datafusion: &lt;strong&gt;DataFusion UDFs, even scalar UDFs, process an array of values at a time not a single
row.&lt;/strong&gt; This is different from some other DataFrame libraries and you may need to recognize a slight
change in mentality.&lt;/p&gt;
&lt;p&gt;Some important lines here are the lines like &lt;code&gt;partkey = partkey.as_py()&lt;/code&gt;. When we do this, we pay a
heavy cost. Now instead of keeping the analysis in the Rust code, we have to take the values in the
array and convert them over to Python objects. In this case we end up getting two numbers and a
string as real Python objects, complete with reference counting and all. Also we are iterating
through the array in Python rather than Rust native. These will &lt;strong&gt;significantly&lt;/strong&gt; slow down your
code. Any time you have to cross the barrier where you change values inside the Rust arrays into
Python objects or vice versa you will pay &lt;strong&gt;heavy&lt;/strong&gt; cost in that transformation. You will want to
design your UDFs to avoid this as much as possible.&lt;/p&gt;
&lt;h2&gt;Python approach using PyArrow compute&lt;/h2&gt;
&lt;p&gt;DataFusion uses &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; as its in-memory data format. This can
be seen in the way that Arrow Arrays are passed into the UDFs. We can take advantage of the fact
that &lt;a href="https://arrow.apache.org/docs/python/"&gt;PyArrow&lt;/a&gt;, the canonical Python Arrow implementation,
provides a variety of
useful functions. In the example below, we are only using a few of the boolean functions and the
equality function. Each of these functions takes two arrays and analyzes them row by row. In the
below example, we shift the logic around a little since we are now operating on an entire array of
values instead of checking a single row ourselves.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pyarrow.compute as pc

def udf_using_pyarrow_compute_impl(
    partkey_arr: pa.Array,
    suppkey_arr: pa.Array,
    returnflag_arr: pa.Array,
) -&amp;gt; pa.Array:
    results = None
    for partkey, suppkey, returnflag in values_of_interest:
        filtered_partkey_arr = pc.equal(partkey_arr, partkey)
        filtered_suppkey_arr = pc.equal(suppkey_arr, suppkey)
        filtered_returnflag_arr = pc.equal(returnflag_arr, returnflag)

        resultant_arr = pc.and_(filtered_partkey_arr, filtered_suppkey_arr)
        resultant_arr = pc.and_(resultant_arr, filtered_returnflag_arr)

        if results is None:
            results = resultant_arr
        else:
            results = pc.or_(results, resultant_arr)

    return results


udf_using_pyarrow_compute = udf(
    udf_using_pyarrow_compute_impl,
    [pa.int64(), pa.int64(), pa.utf8()],
    pa.bool_(),
    "stable",
)

df_udf_pyarrow_compute = df_lineitem.filter(
    udf_using_pyarrow_compute(col("l_partkey"), col("l_suppkey"), col("l_returnflag"))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The idea in the code above is that we will iterate through each of the values of interest, which we
expect to be small. For each of the columns, we compare the value of interest to it&amp;rsquo;s corresponding
array using &lt;code&gt;pyarrow.compute.equal&lt;/code&gt;. This will give use three boolean arrays. We have a match to
the tuple if we have a row in all three arrays that is true, so we use &lt;code&gt;pyarrow.compute.and_&lt;/code&gt;. Now
our return value from the UDF needs to include arrays for which any of the values of interest list
of tuples exists, so we take the result from the current loop and perform a &lt;code&gt;pyarrow.compute.or_&lt;/code&gt;
on it.&lt;/p&gt;
&lt;p&gt;From my benchmarking, switching from approach of converting values into Python objects to this
approach of using the PyArrow built-in functions leads to about a 10x speed improvement in this
simple problem.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s worth noting that almost all of the PyArrow compute functions expect to take one or two arrays
as their arguments. If you need to write a UDF that is evaluating three or more columns, you&amp;rsquo;ll
need to do something akin to what we&amp;rsquo;ve shown here.&lt;/p&gt;
&lt;h2&gt;Rust UDF with Python wrapper&lt;/h2&gt;
&lt;p&gt;This is the most complicated approach, but has the potential to be the most performant. What we
will do here is write a Rust function to perform our computation and then expose that function to
Python. I know of two use cases where I would recommend this approach. The first is the case when
the PyArrow compute functions are insufficient for your needs. Perhaps your code is too complex or
could be greatly simplified if you pulled in some outside dependency. The second use case is when
you have written a UDF that you&amp;rsquo;re sharing across multiple projects and have hardened the approach.
It is possible that you can implement your function in Rust to give a speed improvement and then
every project that is using this shared UDF will benefit from those updates.&lt;/p&gt;
&lt;p&gt;When deciding to use this approach, it&amp;rsquo;s worth considering how much you think you&amp;rsquo;ll actually
benefit from the Rust implementation to decide if it&amp;rsquo;s worth the additional effort to maintain and
deploy the Python wheels you generate. It is certainly not necessary for every use case.&lt;/p&gt;
&lt;p&gt;Due to the excellent work by the Python arrow team, we can simplify our work to needing only two
dependencies on the Rust side, &lt;a href="https://github.com/apache/arrow-rs"&gt;arrow-rs&lt;/a&gt; and
&lt;a href="https://pyo3.rs/"&gt;pyo3&lt;/a&gt;. I have posted a &lt;a href="https://github.com/timsaucer/tuple_filter_example"&gt;minimal example&lt;/a&gt;.
You&amp;rsquo;ll need &lt;a href="https://github.com/PyO3/maturin"&gt;maturin&lt;/a&gt; to build the project, and you must use
release mode when building to get the expected performance.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;maturin develop --release
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you write your UDF in Rust you generally will need to take these steps&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Write a function description that takes in some number of Python generic objects.&lt;/li&gt;
&lt;li&gt;Convert these objects to Arrow Arrays of the appropriate type(s).&lt;/li&gt;
&lt;li&gt;Perform your computation and create a resultant Array.&lt;/li&gt;
&lt;li&gt;Convert the array into a Python generic object.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For the conversion to and from Python objects, we can take advantage of the
&lt;code&gt;ArrayData::from_pyarrow_bound&lt;/code&gt; and &lt;code&gt;ArrayData::to_pyarrow&lt;/code&gt; functions.  All that remains is to
perform your computation.&lt;/p&gt;
&lt;p&gt;We are going to demonstrate doing this computation in two ways. The first is to mimic what we&amp;rsquo;ve
done in the above approach using PyArrow. In the second we demonstrate iterating through the three
arrays ourselves.&lt;/p&gt;
&lt;p&gt;In our first approach, we can expect the performance to be nearly identical to when we used the
PyArrow compute functions. On the Rust side we will have slightly less overhead but the heavy
lifting portions of the code are essentially the same between this Rust implementation and the
PyArrow approach above.&lt;/p&gt;
&lt;p&gt;The reason for demonstrating this, even though it doesn&amp;rsquo;t provide a significant speedup over
Python, is to primarily demonstrate how to make the Python to Rust with Python wrapper
transition. In the second implementation you can see how we can iterate through all of the arrays
ourselves.&lt;/p&gt;
&lt;p&gt;In this first example, we are hard coding the values of interest, but in the following section
we demonstrate passing these in during initalization.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[pyfunction]
pub fn tuple_filter_fn(
    py: Python&amp;lt;'_&amp;gt;,
    partkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
    suppkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
    returnflag_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
) -&amp;gt; PyResult&amp;lt;Py&amp;lt;PyAny&amp;gt;&amp;gt; {
    let partkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
        ArrayData::from_pyarrow_bound(partkey_expr)?.into();
    let suppkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
        ArrayData::from_pyarrow_bound(suppkey_expr)?.into();
    let returnflag_arr: StringArray = ArrayData::from_pyarrow_bound(returnflag_expr)?.into();

    let values_of_interest = vec![
        (1530, 4031, "N".to_string()),
        (6530, 1531, "N".to_string()),
        (5618, 619, "N".to_string()),
        (8118, 8119, "N".to_string()),
    ];

    let mut res: Option&amp;lt;BooleanArray&amp;gt; = None;

    for (partkey, suppkey, returnflag) in &amp;amp;values_of_interest {
        let filtered_partkey_arr = BooleanArray::from_unary(&amp;amp;partkey_arr, |p| p == *partkey);
        let filtered_suppkey_arr = BooleanArray::from_unary(&amp;amp;suppkey_arr, |s| s == *suppkey);
        let filtered_returnflag_arr =
            BooleanArray::from_unary(&amp;amp;returnflag_arr, |s| s == returnflag);

        let part_and_supp = compute::and(&amp;amp;filtered_partkey_arr, &amp;amp;filtered_suppkey_arr)
            .map_err(|e| PyValueError::new_err(e.to_string()))?;
        let resultant_arr = compute::and(&amp;amp;part_and_supp, &amp;amp;filtered_returnflag_arr)
            .map_err(|e| PyValueError::new_err(e.to_string()))?;

        res = match res {
            Some(r) =&amp;gt; compute::or(&amp;amp;r, &amp;amp;resultant_arr).ok(),
            None =&amp;gt; Some(resultant_arr),
        };
    }

    res.unwrap().into_data().to_pyarrow(py)
}


#[pymodule]
fn tuple_filter_example(module: &amp;amp;Bound&amp;lt;'_, PyModule&amp;gt;) -&amp;gt; PyResult&amp;lt;()&amp;gt; {
    module.add_function(wrap_pyfunction!(tuple_filter_fn, module)?)?;
    Ok(())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To use this we use the &lt;code&gt;udf&lt;/code&gt; function in &lt;code&gt;datafusion-python&lt;/code&gt; just as before.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion import udf
import pyarrow as pa
from tuple_filter_example import tuple_filter_fn

udf_using_custom_rust_fn = udf(
    tuple_filter_fn,
    [pa.int64(), pa.int64(), pa.utf8()],
    pa.bool_(),
    "stable",
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That's it! We've now got a third party Rust UDF with Python wrappers working with DataFusion's
Python bindings!&lt;/p&gt;
&lt;h3&gt;Rust UDF with initialization&lt;/h3&gt;
&lt;p&gt;Looking at the code above, you can see that it is hard coding the values we're interested in. There
are many types of UDFs that don't require any additional data provided to them before they start
the computation. The code above is sloppy, so let's clean it up.&lt;/p&gt;
&lt;p&gt;We want to write the function to take some additional data. A limitation of the UDFs we create is
that they expect to operate on entire arrays of data at a time. We can get around this problem by
creating an initializer for our UDF. We do this by defining a Rust struct that contains the data we
need and implement two methods on this struct, &lt;code&gt;new&lt;/code&gt; and &lt;code&gt;__call__&lt;/code&gt;. By doing this we will create a
Python object that is callable, so it can be the function we provide to &lt;code&gt;udf&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[pyclass]
pub struct TupleFilterClass {
    values_of_interest: Vec&amp;lt;(i64, i64, String)&amp;gt;,
}

#[pymethods]
impl TupleFilterClass {
    #[new]
    fn new(values_of_interest: Vec&amp;lt;(i64, i64, String)&amp;gt;) -&amp;gt; Self {
        Self {
            values_of_interest,
        }
    }

    fn __call__(
        &amp;amp;self,
        py: Python&amp;lt;'_&amp;gt;,
        partkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
        suppkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
        returnflag_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
    ) -&amp;gt; PyResult&amp;lt;Py&amp;lt;PyAny&amp;gt;&amp;gt; {
        let partkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
            ArrayData::from_pyarrow_bound(partkey_expr)?.into();
        let suppkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
            ArrayData::from_pyarrow_bound(suppkey_expr)?.into();
        let returnflag_arr: StringArray = ArrayData::from_pyarrow_bound(returnflag_expr)?.into();

        let mut res: Option&amp;lt;BooleanArray&amp;gt; = None;

        for (partkey, suppkey, returnflag) in &amp;amp;self.values_of_interest {
            let filtered_partkey_arr = BooleanArray::from_unary(&amp;amp;partkey_arr, |p| p == *partkey);
            let filtered_suppkey_arr = BooleanArray::from_unary(&amp;amp;suppkey_arr, |s| s == *suppkey);
            let filtered_returnflag_arr =
                BooleanArray::from_unary(&amp;amp;returnflag_arr, |s| s == returnflag);

            let part_and_supp = compute::and(&amp;amp;filtered_partkey_arr, &amp;amp;filtered_suppkey_arr)
                .map_err(|e| PyValueError::new_err(e.to_string()))?;
            let resultant_arr = compute::and(&amp;amp;part_and_supp, &amp;amp;filtered_returnflag_arr)
                .map_err(|e| PyValueError::new_err(e.to_string()))?;

            res = match res {
                Some(r) =&amp;gt; compute::or(&amp;amp;r, &amp;amp;resultant_arr).ok(),
                None =&amp;gt; Some(resultant_arr),
            };
        }

        res.unwrap().into_data().to_pyarrow(py)
    }
}

#[pymodule]
fn tuple_filter_example(module: &amp;amp;Bound&amp;lt;'_, PyModule&amp;gt;) -&amp;gt; PyResult&amp;lt;()&amp;gt; {
    module.add_class::&amp;lt;TupleFilterClass&amp;gt;()?;
    Ok(())
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you write this, you don't have to call your constructor &lt;code&gt;new&lt;/code&gt;. The more important part is that
you have &lt;code&gt;#[new]&lt;/code&gt; designated on the function. With this you can provide any kinds of data you need
during processing. Using this initializer in Python is fairly straightforward.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion import udf
import pyarrow as pa
from tuple_filter_example import TupleFilterClass

tuple_filter_class = TupleFilterClass(values_of_interest)

udf_using_custom_rust_fn_with_data = udf(
    tuple_filter_class,
    [pa.int64(), pa.int64(), pa.utf8()],
    pa.bool_(),
    "stable",
    name="tuple_filter_with_data"
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When you use this approach you will need to provide a &lt;code&gt;name&lt;/code&gt; argument to &lt;code&gt;udf&lt;/code&gt;. This is because our
class/struct does not get the &lt;code&gt;__qualname__&lt;/code&gt; attribute that the &lt;code&gt;udf&lt;/code&gt; function is looking for. You
can give this udf any name you choose.&lt;/p&gt;
&lt;h3&gt;Rust UDF with direct iteration&lt;/h3&gt;
&lt;p&gt;The final version of our scalar UDF is one where we implement it in Rust and iterate through all of
the arrays ourselves. If you are iterating through more than 3 arrays at a time I recommend looking
at &lt;a href="https://docs.rs/itertools/latest/itertools/macro.izip.html"&gt;izip&lt;/a&gt; in the
&lt;a href="https://crates.io/crates/itertools"&gt;itertools crate&lt;/a&gt;. For ease of understanding and since we only
have 3 arrays here I will just explicitly create my own tuple here.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;#[pyclass]
pub struct TupleFilterDirectIterationClass {
    values_of_interest: Vec&amp;lt;(i64, i64, String)&amp;gt;,
}

#[pymethods]
impl TupleFilterDirectIterationClass {
    #[new]
    fn new(values_of_interest: Vec&amp;lt;(i64, i64, String)&amp;gt;) -&amp;gt; Self {
        Self { values_of_interest }
    }

    fn __call__(
        &amp;amp;self,
        py: Python&amp;lt;'_&amp;gt;,
        partkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
        suppkey_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
        returnflag_expr: &amp;amp;Bound&amp;lt;'_, PyAny&amp;gt;,
    ) -&amp;gt; PyResult&amp;lt;Py&amp;lt;PyAny&amp;gt;&amp;gt; {
        let partkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
            ArrayData::from_pyarrow_bound(partkey_expr)?.into();
        let suppkey_arr: PrimitiveArray&amp;lt;Int64Type&amp;gt; =
            ArrayData::from_pyarrow_bound(suppkey_expr)?.into();
        let returnflag_arr: StringArray = ArrayData::from_pyarrow_bound(returnflag_expr)?.into();

        let values_to_search: Vec&amp;lt;(&amp;amp;i64, &amp;amp;i64, &amp;amp;str)&amp;gt; = (&amp;amp;self.values_of_interest)
            .iter()
            .map(|(a, b, c)| (a, b, c.as_str()))
            .collect();

        let values = partkey_arr
            .values()
            .iter()
            .zip(suppkey_arr.values().iter())
            .zip(returnflag_arr.iter())
            .map(|((a, b), c)| (a, b, c.unwrap_or_default()))
            .map(|v| values_to_search.contains(&amp;amp;v));

        let res: BooleanArray = BooleanBuffer::from_iter(values).into();

        res.into_data().to_pyarrow(py)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We convert the &lt;code&gt;values_of_interest&lt;/code&gt; into a vector of borrowed types so that we can do a fast search
without creating additional memory. The other option is to turn the &lt;code&gt;returnflag&lt;/code&gt; into a &lt;code&gt;String&lt;/code&gt;
but that memory allocation is unnecessary. After that we use two &lt;code&gt;zip&lt;/code&gt; operations so that we can
iterate over all three columns in a single pass. Since each &lt;code&gt;zip&lt;/code&gt; will return a tuple of two
elements, a quick &lt;code&gt;map&lt;/code&gt; turns them into the tuple format we need. Also, &lt;code&gt;StringArray&lt;/code&gt; is a little
different in the buffer it uses, so it is treated slightly differently from the others.&lt;/p&gt;
&lt;h2&gt;User Defined Aggregate Function&lt;/h2&gt;
&lt;p&gt;Writing a user defined aggregate function or user defined window function is slightly more complex
than scalar functions. This is because we must accumulate values and there is no guarantee that one
batch will contain all the values we are aggregating over. For this we need to define an
&lt;code&gt;Accumulator&lt;/code&gt; which will do a few things.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Process a batch and compute an internal state&lt;/li&gt;
&lt;li&gt;Share the state so that we can combine multiple batches&lt;/li&gt;
&lt;li&gt;Merge the results across multiple batches&lt;/li&gt;
&lt;li&gt;Return the final result&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the example below, we're going to look at customer orders and we want to know per customer ID,
how much they have ordered total. We want to ignore small orders, which we define as anything under
5000.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datafusion import Accumulator, udaf
import pyarrow as pa
import pyarrow.compute as pc

IGNORE_THESHOLD = 5000.0
class AboveThresholdAccum(Accumulator):
    def __init__(self) -&amp;gt; None:
        self._sum = 0.0

    def update(self, values: pa.Array) -&amp;gt; None:
        over_threshold = pc.greater(values, pa.scalar(IGNORE_THESHOLD))
        sum_above = pc.sum(values.filter(over_threshold)).as_py()
        if sum_above is None:
            sum_above = 0.0
        self._sum = self._sum + sum_above

    def merge(self, states: List[pa.Array]) -&amp;gt; None:
        self._sum = self._sum + pc.sum(states[0]).as_py()

    def state(self) -&amp;gt; List[pa.Scalar]:
        return [pa.scalar(self._sum)]

    def evaluate(self) -&amp;gt; pa.Scalar:
        return pa.scalar(self._sum)

sum_above_threshold = udaf(AboveThresholdAccum, [pa.float64()], pa.float64(), [pa.float64()], 'stable')

df_orders.aggregate([col("o_custkey")],[sum_above_threshold(col("o_totalprice")).alias("sales")]).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since we are doing a &lt;code&gt;sum&lt;/code&gt; we can keep a single value as our internal state. When we call &lt;code&gt;update()&lt;/code&gt;
we will process a single array and update the internal state, which we share with the &lt;code&gt;state()&lt;/code&gt;
function. For larger batches we may &lt;code&gt;merge()&lt;/code&gt; these states. It is important to note that the
&lt;code&gt;states&lt;/code&gt; in the &lt;code&gt;merge()&lt;/code&gt; function are an array of the values returned from &lt;code&gt;state()&lt;/code&gt;. It is
entirely possible that the &lt;code&gt;merge&lt;/code&gt; function is significantly different than the &lt;code&gt;update&lt;/code&gt;, though in
our example they are very similar.&lt;/p&gt;
&lt;p&gt;One example of implementing a user defined aggregate function where the &lt;code&gt;update()&lt;/code&gt; and &lt;code&gt;merge()&lt;/code&gt;
operations are different is computing an average. In &lt;code&gt;update()&lt;/code&gt; we would create a state that is both
a sum and a count. &lt;code&gt;state()&lt;/code&gt; would return a list of these two values, and &lt;code&gt;merge()&lt;/code&gt; would compute
the final result.&lt;/p&gt;
&lt;h2&gt;User Defined Window Functions&lt;/h2&gt;
&lt;p&gt;Writing a user defined window function is slightly more complex than an aggregate function due
to the variety of ways that window functions are called. I recommend reviewing the
&lt;a href="https://datafusion.apache.org/python/user-guide/common-operations/udf-and-udfa.html"&gt;online documentation&lt;/a&gt;
for a description of which functions need to be implemented. The details of how to implement
these generally follow the same patterns as described above for aggregate functions.&lt;/p&gt;
&lt;h2&gt;Performance Comparison&lt;/h2&gt;
&lt;p&gt;For the scalar functions above, we performed a timing evaluation, repeating the operation 100
times. For this simple example these are our results.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;+-----------------------------+--------------+---------+
| approach                    | Average Time | Std Dev |
+-----------------------------+--------------+---------+
| python udf                  | 4.969        | 0.062   |
| simple filter               | 1.075        | 0.022   |
| explicit filter             | 0.685        | 0.063   |
| pyarrow compute             | 0.529        | 0.017   |
| arrow rust compute          | 0.511        | 0.034   |
| arrow rust compute as class | 0.502        | 0.011   |
| rust custom iterator        | 0.478        | 0.009   |
+-----------------------------+--------------+---------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As expected, the conversion to Python objects is by far the worst performance. As soon as we drop
into using any functions that keep the data entirely on the Native (Rust or C/C++) side we see a
near 10x speed improvement. Then as we increase our complexity from using PyArrow compute functions
to implementing the UDF in Rust we see incremental improvements. Our fastest approach - iterating
through the arrays ourselves does operate nearly 10% faster than the PyArrow compute approach.&lt;/p&gt;
&lt;h2&gt;Final Thoughts and Recommendations&lt;/h2&gt;
&lt;p&gt;For anyone who is curious about &lt;a href="https://datafusion.apache.org/"&gt;DataFusion&lt;/a&gt; I highly recommend
giving it a try. This post was designed to make it easier for new users to the Python implementation
to work with User Defined Functions by giving a few examples of how one might implement these.&lt;/p&gt;
&lt;p&gt;When it comes to designing UDFs, I strongly recommend seeing if you can write your UDF using
&lt;a href="https://arrow.apache.org/docs/python/api/compute.html"&gt;PyArrow functions&lt;/a&gt; rather than pure Python
objects. As shown in the scalar example above, you can achieve a 10x speedup by using PyArrow
functions. If you must do something that isn't well represented by the PyArrow compute functions,
then I would consider using a Rust based UDF in the manner shown above.&lt;/p&gt;
&lt;p&gt;I would like to thank &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;, &lt;a href="https://github.com/andygrove"&gt;@andygrove&lt;/a&gt;, &lt;a href="https://github.com/comphead"&gt;@comphead&lt;/a&gt;, &lt;a href="https://github.com/emgeee"&gt;@emgeee&lt;/a&gt;, &lt;a href="https://github.com/kylebarron"&gt;@kylebarron&lt;/a&gt;, and &lt;a href="https://github.com/Omega359"&gt;@Omega359&lt;/a&gt;
for their helpful reviews and feedback.&lt;/p&gt;
&lt;p&gt;Lastly, the Apache Arrow and DataFusion community is an active group of very helpful people working
to make a great tool. If you want to get involved, please take a look at the
&lt;a href="https://datafusion.apache.org/python/"&gt;online documentation&lt;/a&gt; and jump in to help with one of the
&lt;a href="https://github.com/apache/datafusion-python/issues"&gt;open issues&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion is now the fastest single node engine for querying Apache Parquet files</title><link href="https://datafusion.apache.org/blog/2024/11/18/datafusion-fastest-single-node-parquet-clickbench" rel="alternate"></link><published>2024-11-18T00:00:00+00:00</published><updated>2024-11-18T00:00:00+00:00</updated><author><name>Andrew Lamb, Staff Engineer at InfluxData</name></author><id>tag:datafusion.apache.org,2024-11-18:/blog/2024/11/18/datafusion-fastest-single-node-parquet-clickbench</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;I am extremely excited to announce that &lt;a href="https://crates.io/crates/datafusion"&gt;Apache DataFusion&lt;/a&gt;  is the
fastest engine for querying Apache Parquet files in &lt;a href="https://benchmark.clickhouse.com/"&gt;ClickBench&lt;/a&gt;. It is faster
than &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt;, &lt;a href="https://clickhouse.com/chdb"&gt;chDB&lt;/a&gt; and &lt;a href="https://clickhouse.com/"&gt;Clickhouse&lt;/a&gt; using the same hardware. It also marks
the first time a &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;-based engine holds the top spot, which has previously
been …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;I am extremely excited to announce that &lt;a href="https://crates.io/crates/datafusion"&gt;Apache DataFusion&lt;/a&gt;  is the
fastest engine for querying Apache Parquet files in &lt;a href="https://benchmark.clickhouse.com/"&gt;ClickBench&lt;/a&gt;. It is faster
than &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt;, &lt;a href="https://clickhouse.com/chdb"&gt;chDB&lt;/a&gt; and &lt;a href="https://clickhouse.com/"&gt;Clickhouse&lt;/a&gt; using the same hardware. It also marks
the first time a &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;-based engine holds the top spot, which has previously
been held by traditional C/C++-based engines.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Apache DataFusion Logo" class="img-responsive" src="/blog/images/2x_bgwhite_original.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="ClickBench performance for DataFusion 43.0.0" class="img-responsive" src="/blog/images/clickbench-datafusion-43/perf.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: 2024-11-16 &lt;a href="https://benchmark.clickhouse.com/#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"&gt;ClickBench Results&lt;/a&gt; for the  &amp;lsquo;hot&amp;rsquo;[^1] run against the
partitioned 14 GB Parquet dataset (100 files, each ~140MB) on a &lt;code&gt;c6a.4xlarge&lt;/code&gt; (16
CPU / 32 GB  RAM) VM. Measurements are relative (&lt;code&gt;1.x&lt;/code&gt;) to results using
different hardware.&lt;/p&gt;
&lt;p&gt;Best in class performance on Parquet is now available to anyone. DataFusion&amp;rsquo;s
open design lets you start quickly with a full featured Query Engine, including
SQL, data formats, catalogs, and more, and then customize any behavior you need.
I predict the continued emergence of new classes of data systems now that
creators can focus the bulk of their innovation on areas such as query
languages, system integrations, and data formats rather than trying to play
catchup with core engine performance.&lt;/p&gt;
&lt;p&gt;ClickBench also includes results for proprietary storage formats, which require
costly load / export steps, making them useful in fewer use cases and thus much
less important than open formats (though the idea of use case specific formats
is interesting[^2]).&lt;/p&gt;
&lt;p&gt;This blog post highlights some of the techniques we used to achieve this
performance, and celebrates the teamwork involved.&lt;/p&gt;
&lt;h1&gt;A Strong History of Performance Improvements&lt;/h1&gt;
&lt;p&gt;Performance has long been a core focus for DataFusion's community, and 
speed attracts users and contributors. Recently, we seem to have been
even more focused on performance, including in July, 2024 when &lt;a href="https://www.linkedin.com/in/mehmet-ozan-kabak/"&gt;Mehmet Ozan
Kabak&lt;/a&gt;, CEO of &lt;a href="https://www.synnada.ai/"&gt;Synnada&lt;/a&gt;, again &lt;a href="https://github.com/apache/datafusion/issues/11442#issuecomment-2226834443"&gt;suggested focusing on performance&lt;/a&gt;. This
got many of us excited (who doesn&amp;rsquo;t love a challenge!), and we have subsequently
rallied to steadily improve the performance release on release as shown in
Figure 2.&lt;/p&gt;
&lt;p&gt;&lt;img alt="ClickBench performance results over time for DataFusion" class="img-responsive" src="/blog/images/clickbench-datafusion-43/perf-over-time.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: ClickBench performance improved over 30% between DataFusion 34
(released Dec. 2023) and DataFusion 43 (released Nov. 2024).&lt;/p&gt;
&lt;p&gt;Like all good optimization efforts, ours took sustained effort as DataFusion ran
out of &lt;a href="https://www.influxdata.com/blog/aggregating-millions-groups-fast-apache-arrow-datafusion"&gt;single 2x performance improvements&lt;/a&gt; several years ago. Working together our
community of engineers from around the world[^3] and all experience levels[^4]
pulled it off (check out &lt;a href="https://github.com/apache/datafusion/issues/12821"&gt;this discussion&lt;/a&gt; to get a sense). It may be a "&lt;a href="https://db.cs.cmu.edu/seminar2024/"&gt;hobo
sandwich&lt;/a&gt;" [^5], but it is a tasty one!&lt;/p&gt;
&lt;p&gt;Of course, most of these techniques have been implemented and described before,
but until now they were only available in proprietary systems such as
&lt;a href="https://www.vertica.com/"&gt;Vertica&lt;/a&gt;, &lt;a href="https://www.databricks.com/product/photon"&gt;DataBricks
Photon&lt;/a&gt;, or
&lt;a href="https://www.snowflake.com/en/"&gt;Snowflake&lt;/a&gt; or in tightly integrated open source
systems such as &lt;a href="https://duckdb.org/"&gt;DuckDB&lt;/a&gt; or
&lt;a href="https://clickhouse.com/"&gt;ClickHouse&lt;/a&gt; which were not designed to be extended.&lt;/p&gt;
&lt;h2&gt;StringView&lt;/h2&gt;
&lt;p&gt;Performance improved for all queries when DataFusion switched to using Arrow
&lt;code&gt;StringView&lt;/code&gt;. Using &lt;code&gt;StringView&lt;/code&gt; &amp;ldquo;just&amp;rdquo; saves some copies and avoids one memory
access for certain comparisons. However, these copies and comparisons happen to
occur in many of the hottest loops during query processing, so optimizing them
resulted in measurable performance improvements.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Illustration of how take works with StringView" class="img-responsive" src="/blog/images/clickbench-datafusion-43/string-view-take.png" width="80%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 3:&lt;/strong&gt; Figure from &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;Using StringView / German Style Strings to Make
Queries Faster: Part 1&lt;/a&gt; showing how &lt;code&gt;StringView&lt;/code&gt; saves copying data in many cases.&lt;/p&gt;
&lt;p&gt;Using StringView to make DataFusion faster for ClickBench required substantial
careful, low level optimization work described in &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;Using StringView / German
Style Strings to Make Queries Faster: Part 1&lt;/a&gt; and &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-two-influxdb/"&gt;Part 2&lt;/a&gt;. However, it &lt;em&gt;also&lt;/em&gt;
required extending the rest of DataFusion&amp;rsquo;s operations to support the new type.
You can get a sense of the magnitude of the work required by looking at the 100+
pull requests linked to the epic in arrow-rs
(&lt;a href="https://github.com/apache/arrow-rs/issues/5374"&gt;here&lt;/a&gt;) and three major epics
(&lt;a href="https://github.com/apache/datafusion/issues/10918"&gt;here&lt;/a&gt;,
&lt;a href="https://github.com/apache/datafusion/issues/11790"&gt;here&lt;/a&gt; and
&lt;a href="https://github.com/apache/datafusion/issues/11752"&gt;here&lt;/a&gt;) in DataFusion.&lt;/p&gt;
&lt;p&gt;Here is a partial list of people involved in the project (I am sorry to those whom I forgot)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Arrow&lt;/strong&gt;:  &lt;a href="https://github.com/XiangpengHao"&gt;Xiangpeng Hao&lt;/a&gt; (InfluxData&amp;rsquo;s amazing 2024 summer intern and UW Madison PhD), &lt;a href="https://github.com/ariesdevil"&gt;Yijun Zhao&lt;/a&gt; from DataBend Labs, and &lt;a href="https://github.com/tustvold"&gt;Raphael Taylor-Davies&lt;/a&gt; laid the foundation.  &lt;a href="https://github.com/RinChanNOWWW"&gt;RinChanNOW&lt;/a&gt; from Tencent and &lt;a href="https://github.com/a10y"&gt;Andrew Duffy&lt;/a&gt; from SpiralDB helped push it along in the early days, and &lt;a href="https://github.com/viirya"&gt;Liang-Chi Hsieh&lt;/a&gt;, &lt;a href="https://github.com/Dandandan"&gt;Dani&amp;euml;l Heres&lt;/a&gt; reviewed and provided guidance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DataFusion&lt;/strong&gt;:  &lt;a href="https://github.com/XiangpengHao"&gt;Xiangpeng Hao&lt;/a&gt;, again charted the initial path and &lt;a href="https://github.com/Weijun-H"&gt;Weijun Huang&lt;/a&gt;, &lt;a href="https://github.com/dharanad"&gt;Dharan Aditya&lt;/a&gt; &lt;a href="https://github.com/Lordworms"&gt;Lordworms&lt;/a&gt;, &lt;a href="https://github.com/goldmedal"&gt;Jax Liu&lt;/a&gt;,  &lt;a href="https://github.com/wiedld"&gt;wiedld&lt;/a&gt;, &lt;a href="https://github.com/tlm365"&gt;Tai Le Manh&lt;/a&gt;, &lt;a href="https://github.com/my-vegetable-has-exploded"&gt;yi wang&lt;/a&gt;, &lt;a href="https://github.com/doupache"&gt;doupache&lt;/a&gt;, &lt;a href="https://github.com/jayzhan211"&gt;Jay Zhan&lt;/a&gt; , &lt;a href="https://github.com/xinlifoobar"&gt;Xin Li&lt;/a&gt;  and &lt;a href="https://github.com/Kev1n8"&gt;Kaifeng Zheng&lt;/a&gt; made it real.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DataFusion String Function Migration&lt;/strong&gt;:  &lt;a href="https://github.com/tshauck"&gt;Trent Hauck&lt;/a&gt; organized the effort and set the patterns, &lt;a href="https://github.com/goldmedal"&gt;Jax Liu&lt;/a&gt; made a clever testing framework, and &lt;a href="https://github.com/austin362667"&gt;Austin Liu&lt;/a&gt;, &lt;a href="https://github.com/demetribu"&gt;Dmitrii Bu&lt;/a&gt;, &lt;a href="https://github.com/tlm365"&gt;Tai Le Manh&lt;/a&gt;, &lt;a href="https://github.com/PsiACE"&gt;Chojan Shang&lt;/a&gt;, &lt;a href="https://github.com/devanbenz"&gt;WeblWabl&lt;/a&gt;, &lt;a href="https://github.com/Lordworms"&gt;Lordworms&lt;/a&gt;, &lt;a href="https://github.com/thinh2"&gt;iamthinh&lt;/a&gt;, &lt;a href="https://github.com/Omega359"&gt;Bruce Ritchie&lt;/a&gt;, &lt;a href="https://github.com/Kev1n8"&gt;Kaifeng Zheng&lt;/a&gt;, and &lt;a href="https://github.com/xinlifoobar"&gt;Xin Li&lt;/a&gt; bashed out the conversions.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Parquet&lt;/h2&gt;
&lt;p&gt;Part of the reason for DataFusion's speed in ClickBench is reading Parquet files (really) quickly,
which reflects invested effort in the Parquet reading system (see &lt;a href="https://www.influxdata.com/blog/querying-parquet-millisecond-latency/"&gt;Querying
Parquet with Millisecond Latency&lt;/a&gt; )&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/physical_plan/parquet/struct.ParquetExec.html"&gt;DataFusion ParquetExec&lt;/a&gt; (built on the &lt;a href="https://crates.io/crates/parquet"&gt;Rust Parquet Implementation&lt;/a&gt;) is now the most
sophisticated open source Parquet reader I know of. It has every optimization we
can think of for reading Parquet, including projection pushdown, predicate
pushdown (row group metadata, page index, and bloom filters), limit pushdown,
parallel reading, interleaved I/O, and late materialized filtering (coming soon &amp;trade;️
by default). Some recent work from &lt;a href="https://github.com/itsjunetime"&gt;June&lt;/a&gt;
&lt;a href="https://github.com/apache/datafusion/pull/12135"&gt;recently unblocked a remaining hurdle&lt;/a&gt; for enabling late materialized
filtering, and conveniently &lt;a href="https://github.com/XiangpengHao"&gt;Xiangpeng Hao&lt;/a&gt; is
working on the &lt;a href="https://github.com/apache/arrow-datafusion/issues/3463"&gt;final piece&lt;/a&gt; (no pressure😅)&lt;/p&gt;
&lt;h2&gt;Skipping Partial Aggregation When It Doesn't Help&lt;/h2&gt;
&lt;p&gt;Many ClickBench queries are aggregations that summarize millions of rows, a
common task for reporting and dashboarding. DataFusion uses state of the art
&lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.Accumulator.html#tymethod.state"&gt;two phase aggregation&lt;/a&gt; plans. Normally, two phase aggregation works well as the
first phase consolidates many rows immediately after reading, while the data is
still in cache. However, for certain &amp;ldquo;high cardinality&amp;rdquo; aggregate queries (that
have large numbers of groups), &lt;a href="https://github.com/apache/datafusion/issues/6937"&gt;the two phase aggregation strategy used in
DataFusion was inefficient&lt;/a&gt;,
manifesting in relatively slower performance compared to other engines for
ClickBench queries such as&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT "WatchID", "ClientIP", COUNT(*) AS c, ... 
FROM hits 
GROUP BY "WatchID", "ClientIP" /* &amp;lt;----- 13M Distinct Groups!!! */
ORDER BY c DESC 
LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For such queries, the first aggregation phase does not significantly
reduce the number of rows, which wastes significant effort. &lt;a href="https://github.com/korowa"&gt;Eduard
Karacharov&lt;/a&gt; contributed a &lt;a href="https://github.com/apache/datafusion/pull/11627"&gt;dynamic strategy&lt;/a&gt; to
bypass the first phase when it is not working efficiently, shown in Figure 4.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Two phase aggregation diagram from DataFusion API docs annotated to show first phase not helping" class="img-responsive" src="/blog/images/clickbench-datafusion-43/skipping-partial-aggregation.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Diagram from &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.Accumulator.html#tymethod.state"&gt;DataFusion API docs&lt;/a&gt; showing when the multi-phase
grouping is not effective&lt;/p&gt;
&lt;h2&gt;Optimized Multi-Column Grouping&lt;/h2&gt;
&lt;p&gt;Another method for improving analytic database performance is specialized (aka
highly optimized) versions of operations for different data types, which the
system picks at runtime based on the query. Like other systems, DataFusion has
specialized code for handling different types of group columns. For example,
there is &lt;a href="https://github.com/apache/datafusion/blob/73507c307487708deb321e1ba4e0d302084ca27e/datafusion/physical-plan/src/aggregates/group_values/single_group_by/primitive.rs"&gt;special code&lt;/a&gt; that handles &lt;code&gt;GROUP BY int_id&lt;/code&gt;  and &lt;a href="https://github.com/apache/datafusion/blob/73507c307487708deb321e1ba4e0d302084ca27e/datafusion/physical-plan/src/aggregates/group_values/single_group_by/bytes.rs"&gt;different special
code&lt;/a&gt; that handles &lt;code&gt;GROUP BY string_id&lt;/code&gt; .&lt;/p&gt;
&lt;p&gt;When a query groups by multiple columns, it is tricker to apply this technique.
For example &lt;code&gt;GROUP BY string_id, int_id&lt;/code&gt; and &lt;code&gt;GROUP BY int_id, string_id&lt;/code&gt; have
different optimal structures, but it is not possible to include specialized
versions for all possible combinations of group column types.&lt;/p&gt;
&lt;p&gt;DataFusion includes &lt;a href="https://github.com/apache/datafusion/blob/73507c307487708deb321e1ba4e0d302084ca27e/datafusion/physical-plan/src/aggregates/group_values/row.rs#L33-L39"&gt;a general Row based mechanism&lt;/a&gt; that works for any
combination of column types, but this general mechanism copies each value twice
as shown in Figure 5. The cost of this copy &lt;a href="https://github.com/apache/datafusion/issues/9403"&gt;is especially high for variable
length strings and binary data&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Row based storage for multiple group columns" class="img-responsive" src="/blog/images/clickbench-datafusion-43/row-based-storage.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: Prior to DataFusion 43.0.0, queries with multiple group columns
used Row based group storage and copied each group value twice. This copy
consumes a substantial amount of the query time for queries with many distinct
groups, such as several of the queries in ClickBench.&lt;/p&gt;
&lt;p&gt;Many optimizations in Databases boil down to simply avoiding copies, and this
was no exception. The trick was to figure out how to avoid copies without
causing per-column comparison overhead to dominate or complexity to get out of
hand. In a great example of diligent and disciplined engineering, &lt;a href="https://github.com/jayzhan211"&gt;Jay
Zhan&lt;/a&gt; tried &lt;a href="https://github.com/apache/datafusion/pull/10937"&gt;several&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/pull/10976"&gt;different&lt;/a&gt; approaches until arriving
at the [one shipped in DataFusion &lt;code&gt;43.0.0&lt;/code&gt;], shown in Figure 6.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Column based storage for multiple group columns" class="img-responsive" src="/blog/images/clickbench-datafusion-43/column-based-storage.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: DataFusion 43.0.0&amp;rsquo;s new columnar group storage copies each group
value exactly once, which is significantly faster when grouping by multiple
columns.&lt;/p&gt;
&lt;p&gt;Huge thanks as well to &lt;a href="https://github.com/eejbyfeldt"&gt;Emil Ejbyfeldt&lt;/a&gt; and
&lt;a href="https://github.com/Dandandan"&gt;Dani&amp;euml;l Heres&lt;/a&gt; for their help reviewing and to
&lt;a href="https://github.com/Rachelint"&gt;Rachelint (kamille&lt;/a&gt;) for reviewing and
contributing a faster &lt;a href="https://github.com/apache/datafusion/pull/12996"&gt;vectorized append and compare for multiple groups&lt;/a&gt; which
will be released in DataFusion 44. The discussion on &lt;a href="https://github.com/apache/datafusion/issues/9403"&gt;the ticket&lt;/a&gt; is another
great example of the power of the DataFusion community working together to build
great software.&lt;/p&gt;
&lt;h1&gt;What&amp;rsquo;s Next 🚀&lt;/h1&gt;
&lt;p&gt;Just as I expect the performance of other engines to improve, DataFusion has
several more performance improvements lined up itself:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/datafusion/pull/11943#top"&gt;Intermediate results blocked management&lt;/a&gt; (thanks again &lt;a href="https://github.com/Rachelint"&gt;Rachelint (kamille&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/datafusion/issues/3463"&gt;Enable parquet filter pushdown by default&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We are also talking about what to focus on over the &lt;a href="https://github.com/apache/datafusion/issues/13274"&gt;next three
months&lt;/a&gt; and are always
looking for people to help! If you want to geek out (obsess??) about performance
and other features with engineers from around the world, &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;we would love you to
join us&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Additional Thanks&lt;/h1&gt;
&lt;p&gt;In addition to the people called out above, thanks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/pmcgleenon"&gt;Patrick McGleenon&lt;/a&gt; for running ClickBench and gathering this data (&lt;a href="https://github.com/apache/datafusion/issues/13099#issuecomment-2478314793"&gt;source&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Everyone I missed in the shoutouts &amp;ndash; there are so many of you. We appreciate everyone.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I have dreamed about DataFusion being on top of the ClickBench leaderboard for
several years. I often watched with envy improvements in systems backed by large
VC investments, internet companies, or world class research institutions, and
doubted that we could pull off something similar in an open source project with
always limited time.&lt;/p&gt;
&lt;p&gt;The fact that we have now surpassed those other systems in query performance I
think speaks to the power and possibility of focusing on community and aligning
our collective enthusiasm and skills towards a common goal. Of course, being on
the top in any particular benchmark is likely fleeting as other engines will
improve, but so will DataFusion!&lt;/p&gt;
&lt;p&gt;I love working on DataFusion &amp;ndash; the people, the quality of the code, my
interactions and the results we have achieved together far surpass my
expectations as well as most of my other software development experiences. I
can&amp;rsquo;t wait to see what people will build next, and hope to &lt;a href="https://github.com/apache/datafusion"&gt;see you
online&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;[^1]: Note that DuckDB is slightly faster on the &amp;lsquo;cold&amp;rsquo; run.&lt;/p&gt;
&lt;p&gt;[^2]: Want to try your hand at a custom format for ClickBench fame / glory?: &lt;a href="https://github.com/apache/datafusion/issues/13448"&gt;Make DataFusion the fastest engine in ClickBench with custom file format&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[^3]: We have contributors from North America, South American, Europe, Asia, Africa and Australia&lt;/p&gt;
&lt;p&gt;[^4]: Undergraduates, PhD, Junior engineers, and getting-kind-of-crotchety experienced engineers&lt;/p&gt;
&lt;p&gt;[^5]: Thanks to Andy Pavlo, I love that nomenclature&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.3.0 Release</title><link href="https://datafusion.apache.org/blog/2024/09/27/datafusion-comet-0.3.0" rel="alternate"></link><published>2024-09-27T00:00:00+00:00</published><updated>2024-09-27T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-09-27:/blog/2024/09/27/datafusion-comet-0.3.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.3.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.3.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately four weeks of development work and is the result of merging 57 PRs from 12 
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.3.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Binary Releases&lt;/h3&gt;
&lt;p&gt;Comet jar files are now published to Maven central for amd64 and arm64 architectures (Linux only).&lt;/p&gt;
&lt;p&gt;Files can be found at https://central.sonatype.com/search?q=org.apache.datafusion&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spark versions 3.3, 3.4, and 3.5 are supported.&lt;/li&gt;
&lt;li&gt;Scala versions 2.12 and 2.13 are supported.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;New Features&lt;/h3&gt;
&lt;p&gt;The following expressions are now supported natively:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;DateAdd&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DateSub&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ElementAt&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GetArrayElement&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ToJson&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Performance &amp;amp; Stability&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Upgraded to DataFusion 42.0.0&lt;/li&gt;
&lt;li&gt;Reduced memory overhead due to some memory leaks being fixed&lt;/li&gt;
&lt;li&gt;Comet will now fall back to Spark for queries that use DPP, to avoid performance regressions because Comet does 
  not have native support for DPP yet&lt;/li&gt;
&lt;li&gt;Improved performance when converting Spark columnar data to Arrow format&lt;/li&gt;
&lt;li&gt;Faster decimal sum and avg functions &lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Documentation Updates&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Improved documentation for deploying Comet with Kubernetes and Helm in the &lt;a href="https://datafusion.apache.org/comet/user-guide/kubernetes.html"&gt;Comet Kubernetes Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More detailed architectural overview of Comet scan and execution in the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/plugin_overview.html"&gt;Comet Plugin Overview&lt;/a&gt; in the contributor guide&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Using StringView / German Style Strings to Make Queries Faster: Part 1- Reading Parquet</title><link href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1" rel="alternate"></link><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><author><name>Xiangpeng Hao, Andrew Lamb</name></author><id>tag:datafusion.apache.org,2024-09-13:/blog/2024/09/13/string-view-german-style-strings-part-1</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This is the first of a &lt;a href="../2024/09/13/string-view-german-style-strings-part-2/"&gt;two part&lt;/a&gt; blog series that was first published on the &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;InfluxData blog&lt;/a&gt;. Thanks to InfluxData for sponsoring this work as &lt;a href="https://haoxp.xyz/"&gt;Xiangpeng Hao&lt;/a&gt;'s summer intern project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This blog describes our experience implementing &lt;a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-view-layout"&gt;StringView&lt;/a&gt; in the &lt;a href="https://github.com/apache/arrow-rs"&gt;Rust implementation&lt;/a&gt; of &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;, and integrating …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This is the first of a &lt;a href="../2024/09/13/string-view-german-style-strings-part-2/"&gt;two part&lt;/a&gt; blog series that was first published on the &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-one-influxdb/"&gt;InfluxData blog&lt;/a&gt;. Thanks to InfluxData for sponsoring this work as &lt;a href="https://haoxp.xyz/"&gt;Xiangpeng Hao&lt;/a&gt;'s summer intern project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This blog describes our experience implementing &lt;a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-view-layout"&gt;StringView&lt;/a&gt; in the &lt;a href="https://github.com/apache/arrow-rs"&gt;Rust implementation&lt;/a&gt; of &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt;, and integrating it into &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;, significantly accelerating string-intensive queries in the &lt;a href="https://benchmark.clickhouse.com/"&gt;ClickBench&lt;/a&gt; benchmark by 20%- 200% (Figure 1[^1]).&lt;/p&gt;
&lt;p&gt;Getting significant end-to-end performance improvements was non-trivial. Implementing StringView itself was only a fraction of the effort required. Among other things, we had to optimize UTF-8 validation, implement unintuitive compiler optimizations, tune block sizes, and time GC to realize the &lt;a href="https://www.influxdata.com/blog/flight-datafusion-arrow-parquet-fdap-architecture-influxdb/"&gt;FDAP ecosystem&lt;/a&gt;&amp;rsquo;s benefit. With other members of the open source community, we were able to overcome performance bottlenecks that could have killed the project. We would like to contribute by explaining the challenges and solutions in more detail so that more of the community can learn from our experience.&lt;/p&gt;
&lt;p&gt;StringView is based on a simple idea: avoid some string copies and accelerate comparisons with inlined prefixes. Like most great ideas, it is &amp;ldquo;obvious&amp;rdquo; only after &lt;a href="https://db.in.tum.de/~freitag/papers/p29-neumann-cidr20.pdf"&gt;someone describes it clearly&lt;/a&gt;. Although simple, straightforward implementation actually &lt;em&gt;slows down performance for almost every query&lt;/em&gt;. We must, therefore, apply astute observations and diligent engineering to realize the actual benefits from StringView.&lt;/p&gt;
&lt;p&gt;Although this journey was successful, not all research ideas are as lucky. To accelerate the adoption of research into industry, it is valuable to integrate research prototypes with practical systems. Understanding the nuances of real-world systems makes it more likely that research designs[^2] will lead to practical system improvements.&lt;/p&gt;
&lt;p&gt;StringView support was released as part of &lt;a href="https://crates.io/crates/arrow/52.2.0"&gt;arrow-rs v52.2.0&lt;/a&gt; and &lt;a href="https://crates.io/crates/datafusion/41.0.0"&gt;DataFusion v41.0.0&lt;/a&gt;. You can try it by setting the &lt;code&gt;schema_force_view_types&lt;/code&gt; &lt;a href="https://datafusion.apache.org/user-guide/configs.html"&gt;DataFusion configuration option&lt;/a&gt;, and we are&lt;a href="https://github.com/apache/datafusion/issues/11682"&gt; hard at work with the community to &lt;/a&gt;make it the default. We invite everyone to try it out, take advantage of the effort invested so far, and contribute to making it better.&lt;/p&gt;
&lt;p&gt;&lt;img alt="End to end performance improvements for ClickBench queries" class="img-responsive" src="/blog/images/string-view-1/figure1-performance.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: StringView improves string-intensive ClickBench query performance by 20% - 200%&lt;/p&gt;
&lt;h2&gt;What is StringView?&lt;/h2&gt;
&lt;p&gt;&lt;img alt="Diagram of using StringArray and StringViewArray to represent the same string content" class="img-responsive" src="/blog/images/string-view-1/figure2-string-view.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: Use StringArray and StringViewArray to represent the same string content.&lt;/p&gt;
&lt;p&gt;The concept of inlined strings with prefixes (called &amp;ldquo;German Strings&amp;rdquo; &lt;a href="https://x.com/andy_pavlo/status/1813258735965643203"&gt;by Andy Pavlo&lt;/a&gt;, in homage to &lt;a href="https://www.tum.de/"&gt;TUM&lt;/a&gt;, where the &lt;a href="https://db.in.tum.de/~freitag/papers/p29-neumann-cidr20.pdf"&gt;Umbra paper that describes&lt;/a&gt; them originated) 
has been used in many recent database systems (&lt;a href="https://engineering.fb.com/2024/02/20/developer-tools/velox-apache-arrow-15-composable-data-management/"&gt;Velox&lt;/a&gt;, &lt;a href="https://pola.rs/posts/polars-string-type/"&gt;Polars&lt;/a&gt;, &lt;a href="https://duckdb.org/2021/12/03/duck-arrow.html"&gt;DuckDB&lt;/a&gt;, &lt;a href="https://cedardb.com/blog/german_strings/"&gt;CedarDB&lt;/a&gt;, etc.) 
and was introduced to Arrow as a new &lt;a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-view-layout"&gt;StringViewArray&lt;/a&gt;[^3] type. Arrow&amp;rsquo;s original &lt;a href="https://arrow.apache.org/docs/format/Columnar.html#variable-size-binary-layout"&gt;StringArray&lt;/a&gt; is very memory efficient but less effective for certain operations. 
StringViewArray accelerates string-intensive operations via prefix inlining and a more flexible and compact string representation.&lt;/p&gt;
&lt;p&gt;A StringViewArray consists of three components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;&lt;em&gt;view&lt;/em&gt;&lt;/code&gt; array&lt;/li&gt;
&lt;li&gt;The buffers&lt;/li&gt;
&lt;li&gt;The buffer pointers (IDs) that map buffer offsets to their physical locations&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each &lt;code&gt;view&lt;/code&gt; is 16 bytes long, and its contents differ based on the string&amp;rsquo;s length:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;string length &amp;lt; 12 bytes: the first four bytes store the string length, and the remaining 12 bytes store the inlined string.&lt;/li&gt;
&lt;li&gt;string length &amp;gt; 12 bytes: the string is stored in a separate buffer. The length is again stored in the first 4 bytes, followed by the buffer id (4 bytes), the buffer offset (4 bytes), and the prefix (first 4 bytes) of the string.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Figure 2 shows an example of the same logical content (left) using StringArray (middle) and StringViewArray (right):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The first string &amp;ndash; &lt;code&gt;"Apache DataFusion"&lt;/code&gt; &amp;ndash; is 17 bytes long, and both StringArray and StringViewArray store the string&amp;rsquo;s bytes at the beginning of the buffer. The StringViewArray also inlines the first 4 bytes &amp;ndash; &lt;code&gt;"Apac"&lt;/code&gt; &amp;ndash; in the view.&lt;/li&gt;
&lt;li&gt;The second string, &lt;code&gt;"InfluxDB"&lt;/code&gt; is only 8 bytes long, so StringViewArray completely inlines the string content in the &lt;code&gt;view&lt;/code&gt; struct while StringArray stores the string in the buffer as well.&lt;/li&gt;
&lt;li&gt;The third string &lt;code&gt;"Arrow Rust Impl"&lt;/code&gt; is 15 bytes long and cannot be fully inlined. StringViewArray stores this in the same form as the first string.&lt;/li&gt;
&lt;li&gt;The last string &lt;code&gt;"Apache DataFusion"&lt;/code&gt; has the same content as the first string. It&amp;rsquo;s possible to use StringViewArray to avoid this duplication and reuse the bytes by pointing the view to the previous location.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;StringViewArray provides three opportunities for outperforming StringArray:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Less copying via the offset + buffer format&lt;/li&gt;
&lt;li&gt;Faster comparisons using the inlined string prefix&lt;/li&gt;
&lt;li&gt;Reusing repeated string values with the flexible &lt;code&gt;view&lt;/code&gt; layout&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The rest of this blog post discusses how to apply these opportunities in real query scenarios to improve performance, what challenges we encountered along the way, and how we solved them.&lt;/p&gt;
&lt;h2&gt;Faster Parquet Loading&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://parquet.apache.org/"&gt;Apache Parquet&lt;/a&gt; is the de facto format for storing large-scale analytical data commonly stored LakeHouse-style, such as &lt;a href="https://iceberg.apache.org"&gt;Apache Iceberg&lt;/a&gt; and &lt;a href="https://delta.io"&gt;Delta Lake&lt;/a&gt;. Efficiently loading data from Parquet is thus critical to query performance in many important real-world workloads.&lt;/p&gt;
&lt;p&gt;Parquet encodes strings (i.e., &lt;a href="https://docs.rs/parquet/latest/parquet/data_type/struct.ByteArray.html"&gt;byte array&lt;/a&gt;) in a slightly different format than required for the original Arrow StringArray. The string length is encoded inline with the actual string data (as shown in Figure 4 left). As mentioned previously, StringArray requires the data buffer to be continuous and compact&amp;mdash;the strings have to follow one after another. This requirement means that reading Parquet string data into an Arrow StringArray requires copying and consolidating the string bytes to a new buffer and tracking offsets in a separate array. Copying these strings is often wasteful. Typical queries filter out most data immediately after loading, so most of the copied data is quickly discarded.&lt;/p&gt;
&lt;p&gt;On the other hand, reading Parquet data as a StringViewArray can re-use the same data buffer as storing the Parquet pages because StringViewArray does not require strings to be contiguous. For example, in Figure 4, the StringViewArray directly references the buffer with the decoded Parquet page. The string &lt;code&gt;"Arrow Rust Impl"&lt;/code&gt; is represented by a &lt;code&gt;view&lt;/code&gt; with offset 37 and length 15 into that buffer.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram showing how StringViewArray can avoid copying by reusing decoded Parquet pages." class="img-responsive" src="/blog/images/string-view-1/figure4-copying.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 4: StringViewArray avoids copying by reusing decoded Parquet pages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mini benchmark&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reusing Parquet buffers is great in theory, but how much does saving a copy actually matter? We can run the following benchmark in arrow-rs to find out:&lt;/p&gt;
&lt;p&gt;Our benchmarking machine shows that loading &lt;em&gt;BinaryViewArray&lt;/em&gt; is almost 2x faster than loading BinaryArray (see next section about why this isn&amp;rsquo;t &lt;em&gt;String&lt;/em&gt; ViewArray).&lt;/p&gt;
&lt;p&gt;You can read more on this arrow-rs issue: &lt;a href="https://github.com/apache/arrow-rs/issues/5904"&gt;https://github.com/apache/arrow-rs/issues/5904&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;From Binary to Strings&lt;/h1&gt;
&lt;p&gt;You may wonder why we reported performance for BinaryViewArray when this post is about StringViewArray. Surprisingly, initially, our implementation to read StringViewArray from Parquet was much &lt;em&gt;slower&lt;/em&gt; than StringArray. Why? TLDR: Although reading StringViewArray copied less data, the initial implementation also spent much more time validating &lt;a href="https://en.wikipedia.org/wiki/UTF-8#:~:text=UTF%2D8%20is%20a%20variable,Unicode%20Standard"&gt;UTF-8&lt;/a&gt; (as shown in Figure 5).&lt;/p&gt;
&lt;p&gt;Strings are stored as byte sequences. When reading data from (potentially untrusted) Parquet files, a Parquet decoder must ensure those byte sequences are valid UTF-8 strings, and most programming languages, including Rust, include highly&lt;a href="https://doc.rust-lang.org/std/str/fn.from_utf8.html"&gt; optimized routines&lt;/a&gt; for doing so.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure showing time to load strings from Parquet and the effect of optimized UTF-8 validation." class="img-responsive" src="/blog/images/string-view-1/figure5-loading-strings.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 5: Time to load strings from Parquet. The UTF-8 validation advantage initially eliminates the advantage of reduced copying for StringViewArray.&lt;/p&gt;
&lt;p&gt;A StringArray can be validated in a single call to the UTF-8 validation function as it has a continuous string buffer. As long as the underlying buffer is UTF-8[^4], all strings in the array must be UTF-8. The Rust parquet reader makes a single function call to validate the entire buffer.&lt;/p&gt;
&lt;p&gt;However, validating an arbitrary StringViewArray requires validating each string with a separate call to the validation function, as the underlying buffer may also contain non-string data (for example, the lengths in Parquet pages).&lt;/p&gt;
&lt;p&gt;UTF-8 validation in Rust is highly optimized and favors longer strings (as shown in Figure 6), likely because it leverages SIMD instructions to perform parallel validation. The benefit of a single function call to validate UTF-8 over a function call for each string more than eliminates the advantage of avoiding the copy for StringViewArray.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure showing UTF-8 validation throughput vs string length." class="img-responsive" src="/blog/images/string-view-1/figure6-utf8-validation.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 6: UTF-8 validation throughput vs string length&amp;mdash;StringArray&amp;rsquo;s contiguous buffer can be validated much faster than StringViewArray&amp;rsquo;s buffer.&lt;/p&gt;
&lt;p&gt;Does this mean we should only use StringArray? No! Thankfully, there&amp;rsquo;s a clever way out. The key observation is that in many real-world datasets,&lt;a href="https://www.vldb.org/pvldb/vol17/p148-zeng.pdf"&gt; 99% of strings are shorter than 128 bytes&lt;/a&gt;, meaning the encoded length values are smaller than 128, &lt;strong&gt;in which case the length itself is also valid UTF-8&lt;/strong&gt; (in fact, it is &lt;a href="https://en.wikipedia.org/wiki/ASCII"&gt;ASCII&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This observation means we can optimize validating UTF-8 strings in Parquet pages by treating the length bytes as part of a single large string as long as the length &lt;em&gt;value&lt;/em&gt; is less than 128. Put another way, prior to this optimization, the length bytes act as string boundaries, which require a UTF-8 validation on each string. After this optimization, only those strings with lengths larger than 128 bytes (less than 1% of the strings in the ClickBench dataset) are string boundaries, significantly increasing the UTF-8 validation chunk size and thus improving performance.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/apache/arrow-rs/pull/6009/files"&gt;actual implementation&lt;/a&gt; is only nine lines of Rust (with 30 lines of comments). You can find more details in the related arrow-rs issue:&lt;a href="https://github.com/apache/arrow-rs/issues/5995"&gt; https://github.com/apache/arrow-rs/issues/5995&lt;/a&gt;. As expected, with this optimization, loading StringViewArray is almost 2x faster than loading StringArray.&lt;/p&gt;
&lt;h1&gt;Be Careful About Implicit Copies&lt;/h1&gt;
&lt;p&gt;After all the work to avoid copying strings when loading from Parquet, performance was still not as good as expected. We tracked the problem to a few implicit data copies that we weren't aware of, as described in&lt;a href="https://github.com/apache/arrow-rs/issues/6033"&gt; this issue&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The copies we eventually identified come from the following innocent-looking line of Rust code, where &lt;code&gt;self.buf&lt;/code&gt; is a &lt;a href="https://en.wikipedia.org/wiki/Reference_counting"&gt;reference counted&lt;/a&gt; pointer that should transform without copying into a buffer for use in StringViewArray.&lt;/p&gt;
&lt;p&gt;However, Rust-type coercion rules favored a blanket implementation that &lt;em&gt;did&lt;/em&gt; copy data. This implementation is shown in the following code block where the &lt;code&gt;impl&amp;lt;T: AsRef&amp;lt;[u8]&amp;gt;&amp;gt;&lt;/code&gt; will accept any type that implements &lt;code&gt;AsRef&amp;lt;[u8]&amp;gt;&lt;/code&gt; and copies the data to create a new buffer. To avoid copying, users need to explicitly call &lt;code&gt;from_vec&lt;/code&gt;, which consumes the &lt;code&gt;Vec&lt;/code&gt; and transforms it into a buffer.&lt;/p&gt;
&lt;p&gt;Diagnosing this implicit copy was time-consuming as it relied on subtle Rust language semantics. We needed to track every step of the data flow to ensure every copy was necessary. To help other users and prevent future mistakes, we also &lt;a href="https://github.com/apache/arrow-rs/pull/6043"&gt;removed&lt;/a&gt; the implicit API from arrow-rs in favor of an explicit API. Using this approach, we found and fixed several &lt;a href="https://github.com/apache/arrow-rs/pull/6039"&gt;other unintentional copies&lt;/a&gt; in the code base&amp;mdash;hopefully, the change will help other &lt;a href="https://github.com/spiraldb/vortex/pull/504"&gt;downstream users&lt;/a&gt; avoid unnecessary copies.&lt;/p&gt;
&lt;h1&gt;Help the Compiler by Giving it More Information&lt;/h1&gt;
&lt;p&gt;The Rust compiler&amp;rsquo;s automatic optimizations mostly work very well for a wide variety of use cases, but sometimes, it needs additional hints to generate the most efficient code. When profiling the performance of &lt;code&gt;view&lt;/code&gt; construction, we found, counterintuitively, that constructing &lt;strong&gt;long&lt;/strong&gt; strings was 10x faster than constructing &lt;strong&gt;short&lt;/strong&gt; strings, which made short strings slower on StringViewArray than on StringArray!&lt;/p&gt;
&lt;p&gt;As described in the first section, StringViewArray treats long and short strings differently. Short strings (&amp;lt;12 bytes) directly inline to the &lt;code&gt;view&lt;/code&gt; struct, while long strings only inline the first 4 bytes. The code to construct a &lt;code&gt;view&lt;/code&gt; looks something like this:&lt;/p&gt;
&lt;p&gt;It appears that both branches of the code should be fast: they both involve copying at most 16 bytes of data and some memory shift/store operations. How could the branch for short strings be 10x slower?&lt;/p&gt;
&lt;p&gt;Looking at the assembly code using &lt;a href="https://godbolt.org/"&gt;Compiler Explorer&lt;/a&gt;, we (with help from &lt;a href="https://github.com/aoli-al"&gt;Ao Li&lt;/a&gt;) found the compiler used CPU &lt;strong&gt;load instructions&lt;/strong&gt; to copy the fixed-sized 4 bytes to the &lt;code&gt;view&lt;/code&gt; for long strings, but it calls a function, &lt;a href="https://doc.rust-lang.org/std/ptr/fn.copy_nonoverlapping.html"&gt;&lt;code&gt;ptr::copy_non_overlapping&lt;/code&gt;&lt;/a&gt;, to copy the inlined bytes to the &lt;code&gt;view&lt;/code&gt; for short strings. The difference is that long strings have a prefix size (4 bytes) known at compile time, so the compiler directly uses efficient CPU instructions. But, since the size of the short string is unknown to the compiler, it has to call the general-purpose function &lt;code&gt;ptr::copy_non_coverlapping&lt;/code&gt;. Making a function call is significant unnecessary overhead compared to a CPU copy instruction.&lt;/p&gt;
&lt;p&gt;However, we know something the compiler doesn&amp;rsquo;t know: the short string size is not arbitrary&amp;mdash;it must be between 0 and 12 bytes, and we can leverage this information to avoid the function call. Our solution generates 13 copies of the function using generics, one for each of the possible prefix lengths. The code looks as follows, and &lt;a href="https://godbolt.org/z/685YPsd5G"&gt;checking the assembly code&lt;/a&gt;, we confirmed there are no calls to &lt;code&gt;ptr::copy_non_overlapping&lt;/code&gt;, and only native CPU instructions are used. For more details, see &lt;a href="https://github.com/apache/arrow-rs/issues/6034"&gt;the ticket&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;End-to-End Query Performance&lt;/h1&gt;
&lt;p&gt;In the previous sections, we went out of our way to make sure loading StringViewArray is faster than StringArray. Before going further, we wanted to verify if obsessing about reducing copies and function calls has actually improved end-to-end performance in real-life queries. To do this, we evaluated a ClickBench query (Q20) in DataFusion that counts how many URLs contain the word &lt;code&gt;"google"&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;This is a relatively simple query; most of the time is spent on loading the &amp;ldquo;URL&amp;rdquo; column to find matching rows. The query plan looks like this:&lt;/p&gt;
&lt;p&gt;We ran the benchmark in the DataFusion repo like this:&lt;/p&gt;
&lt;p&gt;With StringViewArray we saw a 24% end-to-end performance improvement, as shown in Figure 7. With the &lt;code&gt;--string-view&lt;/code&gt; argument, the end-to-end query time is &lt;code&gt;944.3 ms, 869.6 ms, 861.9 ms&lt;/code&gt; (three iterations). Without &lt;code&gt;--string-view&lt;/code&gt;, the end-to-end query time is &lt;code&gt;1186.1 ms, 1126.1 ms, 1138.3 ms&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure showing StringView improves end to end performance by 24 percent." class="img-responsive" src="/blog/images/string-view-1/figure7-end-to-end.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 7: StringView reduces end-to-end query time by 24% on ClickBench Q20.&lt;/p&gt;
&lt;p&gt;We also double-checked with detailed profiling and verified that the time reduction is indeed due to faster Parquet loading.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this first blog post, we have described what it took to improve the
performance of simply reading strings from Parquet files using StringView. While
this resulted in real end-to-end query performance improvements, in our &lt;a href="https://datafusion.apache.org/blog/2024/09/13/using-stringview-to-make-queries-faster-part-2.html"&gt;next
post&lt;/a&gt;, we explore additional optimizations enabled by StringView in DataFusion,
along with some of the pitfalls we encountered while implementing them.&lt;/p&gt;
&lt;h1&gt;Footnotes&lt;/h1&gt;
&lt;p&gt;[^1]: Benchmarked with AMD Ryzen 7600x (12 core, 24 threads, 32 MiB L3), WD Black SN770 NVMe SSD (5150MB/4950MB seq RW bandwidth)&lt;/p&gt;
&lt;p&gt;[^2]: Xiangpeng is a PhD student at the University of Wisconsin-Madison&lt;/p&gt;
&lt;p&gt;[^3]: There is also a corresponding &lt;em&gt;BinaryViewArray&lt;/em&gt; which is similar except that the data is not constrained to be UTF-8 encoded strings.&lt;/p&gt;
&lt;p&gt;[^4]: We also make sure that offsets do not break a UTF-8 code point, which is &lt;a href="https://github.com/apache/arrow-rs/blob/master/parquet/src/arrow/buffer/offset_buffer.rs#L62-L71"&gt;cheaply validated&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Using StringView / German Style Strings to make Queries Faster: Part 2 - String Operations</title><link href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-2" rel="alternate"></link><published>2024-09-13T00:00:00+00:00</published><updated>2024-09-13T00:00:00+00:00</updated><author><name>Xiangpeng Hao, Andrew Lamb</name></author><id>tag:datafusion.apache.org,2024-09-13:/blog/2024/09/13/string-view-german-style-strings-part-2</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog series was first published on the &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-two-influxdb/"&gt;InfluxData blog&lt;/a&gt;. Thanks to InfluxData for sponsoring this work as &lt;a href="https://haoxp.xyz/"&gt;Xiangpeng Hao&lt;/a&gt;'s summer intern project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1"&gt;first post&lt;/a&gt;, we discussed the nuances required to accelerate Parquet loading using StringViewArray by reusing buffers and reducing copies. 
In this second …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;&lt;em&gt;Editor's Note: This blog series was first published on the &lt;a href="https://www.influxdata.com/blog/faster-queries-with-stringview-part-two-influxdb/"&gt;InfluxData blog&lt;/a&gt;. Thanks to InfluxData for sponsoring this work as &lt;a href="https://haoxp.xyz/"&gt;Xiangpeng Hao&lt;/a&gt;'s summer intern project&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the &lt;a href="https://datafusion.apache.org/blog/2024/09/13/string-view-german-style-strings-part-1"&gt;first post&lt;/a&gt;, we discussed the nuances required to accelerate Parquet loading using StringViewArray by reusing buffers and reducing copies. 
In this second part of the post, we describe the rest of the journey: implementing additional efficient operations for real query processing.&lt;/p&gt;
&lt;h2&gt;Faster String Operations&lt;/h2&gt;
&lt;h1&gt;Faster comparison&lt;/h1&gt;
&lt;p&gt;String comparison is ubiquitous; it is the core of 
&lt;a href="https://docs.rs/arrow/latest/arrow/compute/kernels/cmp/index.html"&gt;&lt;code&gt;cmp&lt;/code&gt;&lt;/a&gt;, 
&lt;a href="https://docs.rs/arrow/latest/arrow/compute/fn.min.html"&gt;&lt;code&gt;min&lt;/code&gt;&lt;/a&gt;/&lt;a href="https://docs.rs/arrow/latest/arrow/compute/fn.max.html"&gt;&lt;code&gt;max&lt;/code&gt;&lt;/a&gt;, 
and &lt;a href="https://docs.rs/arrow/latest/arrow/compute/kernels/comparison/fn.like.html"&gt;&lt;code&gt;like&lt;/code&gt;&lt;/a&gt;/&lt;a href="https://docs.rs/arrow/latest/arrow/compute/kernels/comparison/fn.ilike.html"&gt;&lt;code&gt;ilike&lt;/code&gt;&lt;/a&gt; kernels. StringViewArray is designed to accelerate such comparisons using the inlined prefix&amp;mdash;the key observation is that, in many cases, only the first few bytes of the string determine the string comparison results.&lt;/p&gt;
&lt;p&gt;For example, to compare the strings &lt;code&gt;InfluxDB&lt;/code&gt; with &lt;code&gt;Apache DataFusion&lt;/code&gt;, we only need to look at the first byte to determine the string ordering or equality. In this case, since &lt;code&gt;A&lt;/code&gt; is earlier in the alphabet than &lt;code&gt;I,&lt;/code&gt; &lt;code&gt;Apache DataFusion&lt;/code&gt; sorts first, and we know the strings are not equal. Despite only needing the first byte, comparing these strings when stored as a StringArray requires two memory accesses: 1) load the string offset and 2) use the offset to locate the string bytes. For low-level operations such as &lt;code&gt;cmp&lt;/code&gt; that are invoked millions of times in the very hot paths of queries, avoiding this extra memory access can make a measurable difference in query performance.&lt;/p&gt;
&lt;p&gt;For StringViewArray, typically, only one memory access is needed to load the view struct. Only if the result can not be determined from the prefix is the second memory access required. For the example above, there is no need for the second access. This technique is very effective in practice: the second access is never necessary for the more than &lt;a href="https://www.vldb.org/pvldb/vol17/p148-zeng.pdf"&gt;60% of real-world strings which are shorter than 12 bytes&lt;/a&gt;, as they are stored completely in the prefix.&lt;/p&gt;
&lt;p&gt;However, functions that operate on strings must be specialized to take advantage of the inlined prefix. In addition to low-level comparison kernels, we implemented &lt;a href="https://github.com/apache/arrow-rs/issues/5374"&gt;a wide range&lt;/a&gt; of other StringViewArray operations that cover the functions and operations seen in ClickBench queries. Supporting StringViewArray in all string operations takes quite a bit of effort, and thankfully the Arrow and DataFusion communities are already hard at work doing so (see &lt;a href="https://github.com/apache/datafusion/issues/11752"&gt;https://github.com/apache/datafusion/issues/11752&lt;/a&gt; if you want to help out).&lt;/p&gt;
&lt;h1&gt;Faster &lt;code&gt;take&lt;/code&gt;and&lt;code&gt;filter&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;After a filter operation such as &lt;code&gt;WHERE url &amp;lt;&amp;gt; ''&lt;/code&gt; to avoid processing empty urls, DataFusion will often &lt;em&gt;coalesce&lt;/em&gt; results to form a new array with only the passing elements. 
This coalescing ensures the batches are sufficiently sized to benefit from &lt;a href="https://www.vldb.org/pvldb/vol11/p2209-kersten.pdf"&gt;vectorized processing&lt;/a&gt; in subsequent steps.&lt;/p&gt;
&lt;p&gt;The coalescing operation is implemented using the &lt;a href="https://docs.rs/arrow/latest/arrow/compute/fn.take.html"&gt;take&lt;/a&gt; and &lt;a href="https://arrow.apache.org/rust/arrow/compute/kernels/filter/fn.filter.html"&gt;filter&lt;/a&gt; kernels in arrow-rs. For StringArray, these kernels require copying the string contents to a new buffer without &amp;ldquo;holes&amp;rdquo; in between. This copy can be expensive especially when the new array is large.&lt;/p&gt;
&lt;p&gt;However, &lt;code&gt;take&lt;/code&gt; and &lt;code&gt;filter&lt;/code&gt; for StringViewArray can avoid the copy by reusing buffers from the old array. The kernels only need to create a new list of  &lt;code&gt;view&lt;/code&gt;s that point at the same strings within the old buffers. 
Figure 1 illustrates the difference between the output of both string representations. StringArray creates two new strings at offsets 0-17 and 17-32, while StringViewArray simply points to the original buffer at offsets 0 and 25.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Diagram showing Zero-copy &amp;lt;code&amp;gt;take&amp;lt;/code&amp;gt;/&amp;lt;code&amp;gt;filter&amp;lt;/code&amp;gt; for StringViewArray" class="img-responsive" src="/blog/images/string-view-2/figure1-zero-copy-take.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 1: Zero-copy &lt;code&gt;take&lt;/code&gt;/&lt;code&gt;filter&lt;/code&gt; for StringViewArray&lt;/p&gt;
&lt;h1&gt;When to GC?&lt;/h1&gt;
&lt;p&gt;Zero-copy &lt;code&gt;take/filter&lt;/code&gt; is great for generating large arrays quickly, but it is suboptimal for highly selective filters, where most of the strings are filtered out. When the cardinality drops, StringViewArray buffers become sparse&amp;mdash;only a small subset of the bytes in the buffer&amp;rsquo;s memory are referred to by any &lt;code&gt;view&lt;/code&gt;. This leads to excessive memory usage, especially in a &lt;a href="https://github.com/apache/datafusion/issues/11628"&gt;filter-then-coalesce scenario&lt;/a&gt;. For example, a StringViewArray with 10M strings may only refer to 1M strings after some filter operations; however, due to zero-copy take/filter, the (reused) 10M buffers can not be released/reused.&lt;/p&gt;
&lt;p&gt;To release unused memory, we implemented a &lt;a href="https://docs.rs/arrow/latest/arrow/array/struct.GenericByteViewArray.html#method.gc"&gt;garbage collection (GC)&lt;/a&gt; routine to consolidate the data into a new buffer to release the old sparse buffer(s). As the GC operation copies strings, similarly to StringArray, we must be careful about when to call it. If we call GC too early, we cause unnecessary copying, losing much of the benefit of StringViewArray. If we call GC too late, we hold large buffers for too long, increasing memory use and decreasing cache efficiency. The &lt;a href="https://pola.rs/posts/polars-string-type/"&gt;Polars blog&lt;/a&gt; on StringView also refers to the challenge presented by garbage collection timing.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;arrow-rs&lt;/code&gt; implements the GC process, but it is up to users to decide when to call it. We leverage the semantics of the query engine and observed that the &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/coalesce_batches/struct.CoalesceBatchesExec.html"&gt;&lt;code&gt;CoalseceBatchesExec&lt;/code&gt;&lt;/a&gt; operator, which merge smaller batches to a larger batch, is often used after the record cardinality is expected to shrink, which aligns perfectly with the scenario of GC in StringViewArray. 
We, therefore,&lt;a href="https://github.com/apache/datafusion/pull/11587"&gt; implemented the GC procedure&lt;/a&gt; inside &lt;code&gt;CoalseceBatchesExec&lt;/code&gt;[^5] with a heuristic that estimates when the buffers are too sparse.&lt;/p&gt;
&lt;h2&gt;The art of function inlining: not too much, not too little&lt;/h2&gt;
&lt;p&gt;Like string inlining, &lt;em&gt;function&lt;/em&gt; inlining is the process of embedding a short function into the caller to avoid the overhead of function calls (caller/callee save). 
Usually, the Rust compiler does a good job of deciding when to inline. However, it is possible to override its default using the &lt;a href="https://doc.rust-lang.org/reference/attributes/codegen.html#the-inline-attribute"&gt;&lt;code&gt;#[inline(always)]&lt;/code&gt; directive&lt;/a&gt;. 
In performance-critical code, inlined code allows us to organize large functions into smaller ones without paying the runtime cost of function invocation.&lt;/p&gt;
&lt;p&gt;However, function inlining is &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; always better, as it leads to larger function bodies that are harder for LLVM to optimize (for example, suboptimal &lt;a href="https://en.wikipedia.org/wiki/Register_allocation"&gt;register spilling&lt;/a&gt;) and risk overflowing the CPU&amp;rsquo;s instruction cache. We observed several performance regressions where function inlining caused &lt;em&gt;slower&lt;/em&gt; performance when implementing the StringViewArray comparison kernels. Careful inspection and tuning of the code was required to aid the compiler in generating efficient code. More details can be found in this PR: &lt;a href="https://github.com/apache/arrow-rs/pull/5900"&gt;https://github.com/apache/arrow-rs/pull/5900&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Buffer size tuning&lt;/h2&gt;
&lt;p&gt;StringViewArray permits multiple buffers, which enables a flexible buffer layout and potentially reduces the need to copy data. However, a large number of buffers slows down the performance of other operations. 
For example, &lt;a href="https://docs.rs/arrow/latest/arrow/array/trait.Array.html#tymethod.get_array_memory_size"&gt;&lt;code&gt;get_array_memory_size&lt;/code&gt;&lt;/a&gt; needs to sum the memory size of each buffer, which takes a long time with thousands of small buffers. 
In certain cases, we found that multiple calls to &lt;a href="https://docs.rs/arrow/latest/arrow/compute/fn.concat_batches.html"&gt;&lt;code&gt;concat_batches&lt;/code&gt;&lt;/a&gt; lead to arrays with millions of buffers, which was prohibitively expensive.&lt;/p&gt;
&lt;p&gt;For example, consider a StringViewArray with the previous default buffer size of 8 KB. With this configuration, holding 4GB of string data requires almost half a million buffers! Larger buffer sizes are needed for larger arrays, but we cannot arbitrarily increase the default buffer size, as small arrays would consume too much memory (most arrays require at least one buffer). Buffer sizing is especially problematic in query processing, as we often need to construct small batches of string arrays, and the sizes are unknown at planning time.&lt;/p&gt;
&lt;p&gt;To balance the buffer size trade-off, we again leverage the query processing (DataFusion) semantics to decide when to use larger buffers. While coalescing batches, we combine multiple small string arrays and set a smaller buffer size to keep the total memory consumption low. In string aggregation, we aggregate over an entire Datafusion partition, which can generate a large number of strings, so we set a larger buffer size (2MB).&lt;/p&gt;
&lt;p&gt;To assist situations where the semantics are unknown, we also &lt;a href="https://github.com/apache/arrow-rs/pull/6136"&gt;implemented&lt;/a&gt; a classic dynamic exponential buffer size growth strategy, which starts with a small buffer size (8KB) and doubles the size of each new buffer up to 2MB. We implemented this strategy in arrow-rs and enabled it by default so that other users of StringViewArray can also benefit from this optimization. See this issue for more details: &lt;a href="https://github.com/apache/arrow-rs/issues/6094"&gt;https://github.com/apache/arrow-rs/issues/6094&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;End-to-end query performance&lt;/h2&gt;
&lt;p&gt;We have made significant progress in optimizing StringViewArray filtering operations. Now, let&amp;rsquo;s test it in the real world to see how it works!&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s consider ClickBench query 22, which selects multiple string fields (&lt;code&gt;URL&lt;/code&gt;, &lt;code&gt;Title&lt;/code&gt;, and &lt;code&gt;SearchPhase&lt;/code&gt;) and applies several filters.&lt;/p&gt;
&lt;p&gt;We ran the benchmark using the following command in the DataFusion repo. Again, the &lt;code&gt;--string-view&lt;/code&gt; option means we use StringViewArray instead of StringArray.&lt;/p&gt;
&lt;p&gt;To eliminate the impact of the faster Parquet reading using StringViewArray (see the first part of this blog), Figure 2 plots only the time spent in &lt;code&gt;FilterExec&lt;/code&gt;. Without StringViewArray, the filter takes 7.17s; with StringViewArray, the filter only takes 4.86s, a 32% reduction in time. Moreover, we see a 17% improvement in end-to-end query performance.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure showing StringViewArray reduces the filter time by 32% on ClickBench query 22." class="img-responsive" src="/blog/images/string-view-2/figure2-filter-time.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Figure 2: StringViewArray reduces the filter time by 32% on ClickBench query 22.&lt;/p&gt;
&lt;h1&gt;Faster String Aggregation&lt;/h1&gt;
&lt;p&gt;So far, we have discussed how to exploit two StringViewArray features: reduced copy and faster filtering. This section focuses on reusing string bytes to repeat string values.&lt;/p&gt;
&lt;p&gt;As described in part one of this blog, if two strings have identical values, StringViewArray can use two different &lt;code&gt;view&lt;/code&gt;s pointing at the same buffer range, thus avoiding repeating the string bytes in the buffer. This makes StringViewArray similar to an Arrow &lt;a href="https://docs.rs/arrow/latest/arrow/array/struct.DictionaryArray.html"&gt;DictionaryArray&lt;/a&gt; that stores Strings&amp;mdash;both array types work well for strings with only a few distinct values.&lt;/p&gt;
&lt;p&gt;Deduplicating string values can significantly reduce memory consumption in StringViewArray. However, this process is expensive and involves hashing every string and maintaining a hash table, and so it cannot be done by default when creating a StringViewArray. We introduced an&lt;a href="https://docs.rs/arrow/latest/arrow/array/builder/struct.GenericByteViewBuilder.html#method.with_deduplicate_strings"&gt; opt-in string deduplication mode&lt;/a&gt; in arrow-rs for advanced users who know their data has a small number of distinct values, and where the benefits of reduced memory consumption outweigh the additional overhead of array construction.&lt;/p&gt;
&lt;p&gt;Once again, we leverage DataFusion query semantics to identify StringViewArray with duplicate values, such as aggregation queries with multiple group keys. For example, some &lt;a href="https://github.com/apache/datafusion/blob/main/benchmarks/queries/clickbench/queries.sql"&gt;ClickBench queries&lt;/a&gt; group by two columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;UserID&lt;/code&gt; (an integer with close to 1 M distinct values)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MobilePhoneModel&lt;/code&gt; (a string with less than a hundred distinct values)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this case, the output row count is&lt;code&gt;count(distinct UserID) * count(distinct MobilePhoneModel)&lt;/code&gt;,  which is 100M. Each string value of  &lt;code&gt;MobilePhoneModel&lt;/code&gt; is repeated 1M times. With StringViewArray, we can save space by pointing the repeating values to the same underlying buffer.&lt;/p&gt;
&lt;p&gt;Faster string aggregation with StringView is part of a larger project to &lt;a href="https://github.com/apache/datafusion/issues/7000"&gt;improve DataFusion aggregation performance&lt;/a&gt;. We have a &lt;a href="https://github.com/apache/datafusion/pull/11794"&gt;proof of concept implementation&lt;/a&gt; with StringView that can improve the multi-column string aggregation by 20%. We would love your help to get it production ready!&lt;/p&gt;
&lt;h1&gt;StringView Pitfalls&lt;/h1&gt;
&lt;p&gt;Most existing blog posts (including this one) focus on the benefits of using StringViewArray over other string representations such as StringArray. As we have discussed, even though it requires a significant engineering investment to realize, StringViewArray is a major improvement over StringArray in many cases.&lt;/p&gt;
&lt;p&gt;However, there are several cases where StringViewArray is slower than StringArray. For completeness, we have listed those instances here:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Tiny strings (when strings are shorter than 8 bytes)&lt;/strong&gt;: every element of the StringViewArray consumes at least 16 bytes of memory&amp;mdash;the size of the &lt;code&gt;view&lt;/code&gt; struct. For an array of tiny strings, StringViewArray consumes more memory than StringArray and thus can cause slower performance due to additional memory pressure on the CPU cache.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Many repeated short strings&lt;/strong&gt;: Similar to the first point, StringViewArray can be slower and require more memory than a DictionaryArray because 1) it can only reuse the bytes in the buffer when the strings are longer than 12 bytes and 2) 32-bit offsets are always used, even when a smaller size (8 bit or 16 bit) could represent all the distinct values.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Filtering:&lt;/strong&gt; As we mentioned above, StringViewArrays often consume more memory than the corresponding StringArray, and memory bloat quickly dominates the performance without GC. However, invoking GC also reduces the benefits of less copying so must be carefully tuned.&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;Conclusion and Takeaways&lt;/h1&gt;
&lt;p&gt;In these two blog posts, we discussed what it takes to implement StringViewArray in arrow-rs and then integrate it into DataFusion. Our evaluations on ClickBench queries show that StringView can improve the performance of string-intensive workloads by up to 2x.&lt;/p&gt;
&lt;p&gt;Given that DataFusion already &lt;a href="https://benchmark.clickhouse.com/#eyJzeXN0ZW0iOnsiQWxsb3lEQiI6ZmFsc2UsIkF0aGVuYSAocGFydGl0aW9uZWQpIjpmYWxzZSwiQXRoZW5hIChzaW5nbGUpIjpmYWxzZSwiQXVyb3JhIGZvciBNeVNRTCI6ZmFsc2UsIkF1cm9yYSBmb3IgUG9zdGdyZVNRTCI6ZmFsc2UsIkJ5Q29uaXR5IjpmYWxzZSwiQnl0ZUhvdXNlIjpmYWxzZSwiY2hEQiAoUGFycXVldCwgcGFydGl0aW9uZWQpIjpmYWxzZSwiY2hEQiI6ZmFsc2UsIkNpdHVzIjpmYWxzZSwiQ2xpY2tIb3VzZSBDbG91ZCAoYXdzKSI6ZmFsc2UsIkNsaWNrSG91c2UgQ2xvdWQgKGF3cykgUGFyYWxsZWwgUmVwbGljYXMgT04iOmZhbHNlLCJDbGlja0hvdXNlIENsb3VkIChBenVyZSkiOmZhbHNlLCJDbGlja0hvdXNlIENsb3VkIChBenVyZSkgUGFyYWxsZWwgUmVwbGljYSBPTiI6ZmFsc2UsIkNsaWNrSG91c2UgQ2xvdWQgKEF6dXJlKSBQYXJhbGxlbCBSZXBsaWNhcyBPTiI6ZmFsc2UsIkNsaWNrSG91c2UgQ2xvdWQgKGdjcCkiOmZhbHNlLCJDbGlja0hvdXNlIENsb3VkIChnY3ApIFBhcmFsbGVsIFJlcGxpY2FzIE9OIjpmYWxzZSwiQ2xpY2tIb3VzZSAoZGF0YSBsYWtlLCBwYXJ0aXRpb25lZCkiOmZhbHNlLCJDbGlja0hvdXNlIChkYXRhIGxha2UsIHNpbmdsZSkiOmZhbHNlLCJDbGlja0hvdXNlIChQYXJxdWV0LCBwYXJ0aXRpb25lZCkiOmZhbHNlLCJDbGlja0hvdXNlIChQYXJxdWV0LCBzaW5nbGUpIjpmYWxzZSwiQ2xpY2tIb3VzZSAod2ViKSI6ZmFsc2UsIkNsaWNrSG91c2UiOmZhbHNlLCJDbGlja0hvdXNlICh0dW5lZCkiOmZhbHNlLCJDbGlja0hvdXNlICh0dW5lZCwgbWVtb3J5KSI6ZmFsc2UsIkNsb3VkYmVycnkiOmZhbHNlLCJDcmF0ZURCIjpmYWxzZSwiQ3J1bmNoeSBCcmlkZ2UgZm9yIEFuYWx5dGljcyAoUGFycXVldCkiOmZhbHNlLCJEYXRhYmVuZCI6ZmFsc2UsIkRhdGFGdXNpb24gKFBhcnF1ZXQsIHBhcnRpdGlvbmVkKSI6dHJ1ZSwiRGF0YUZ1c2lvbiAoUGFycXVldCwgc2luZ2xlKSI6ZmFsc2UsIkFwYWNoZSBEb3JpcyI6ZmFsc2UsIkRydWlkIjpmYWxzZSwiRHVja0RCIChQYXJxdWV0LCBwYXJ0aXRpb25lZCkiOnRydWUsIkR1Y2tEQiI6ZmFsc2UsIkVsYXN0aWNzZWFyY2giOmZhbHNlLCJFbGFzdGljc2VhcmNoICh0dW5lZCkiOmZhbHNlLCJHbGFyZURCIjpmYWxzZSwiR3JlZW5wbHVtIjpmYWxzZSwiSGVhdnlBSSI6ZmFsc2UsIkh5ZHJhIjpmYWxzZSwiSW5mb2JyaWdodCI6ZmFsc2UsIktpbmV0aWNhIjpmYWxzZSwiTWFyaWFEQiBDb2x1bW5TdG9yZSI6ZmFsc2UsIk1hcmlhREIiOmZhbHNlLCJNb25ldERCIjpmYWxzZSwiTW9uZ29EQiI6ZmFsc2UsIk1vdGhlcmR1Y2siOmZhbHNlLCJNeVNRTCAoTXlJU0FNKSI6ZmFsc2UsIk15U1FMIjpmYWxzZSwiT3hsYSI6ZmFsc2UsIlBhcmFkZURCIChQYXJxdWV0LCBwYXJ0aXRpb25lZCkiOmZhbHNlLCJQYXJhZGVEQiAoUGFycXVldCwgc2luZ2xlKSI6ZmFsc2UsIlBpbm90IjpmYWxzZSwiUG9zdGdyZVNRTCAodHVuZWQpIjpmYWxzZSwiUG9zdGdyZVNRTCI6ZmFsc2UsIlF1ZXN0REIgKHBhcnRpdGlvbmVkKSI6ZmFsc2UsIlF1ZXN0REIiOmZhbHNlLCJSZWRzaGlmdCI6ZmFsc2UsIlNlbGVjdERCIjpmYWxzZSwiU2luZ2xlU3RvcmUiOmZhbHNlLCJTbm93Zmxha2UiOmZhbHNlLCJTUUxpdGUiOmZhbHNlLCJTdGFyUm9ja3MiOmZhbHNlLCJUYWJsZXNwYWNlIjpmYWxzZSwiVGVtYm8gT0xBUCAoY29sdW1uYXIpIjpmYWxzZSwiVGltZXNjYWxlREIgKGNvbXByZXNzaW9uKSI6ZmFsc2UsIlRpbWVzY2FsZURCIjpmYWxzZSwiVW1icmEiOmZhbHNlfSwidHlwZSI6eyJDIjp0cnVlLCJjb2x1bW4tb3JpZW50ZWQiOnRydWUsIlBvc3RncmVTUUwgY29tcGF0aWJsZSI6dHJ1ZSwibWFuYWdlZCI6dHJ1ZSwiZ2NwIjp0cnVlLCJzdGF0ZWxlc3MiOnRydWUsIkphdmEiOnRydWUsIkMrKyI6dHJ1ZSwiTXlTUUwgY29tcGF0aWJsZSI6dHJ1ZSwicm93LW9yaWVudGVkIjp0cnVlLCJDbGlja0hvdXNlIGRlcml2YXRpdmUiOnRydWUsImVtYmVkZGVkIjp0cnVlLCJzZXJ2ZXJsZXNzIjp0cnVlLCJhd3MiOnRydWUsInBhcmFsbGVsIHJlcGxpY2FzIjp0cnVlLCJBenVyZSI6dHJ1ZSwiYW5hbHl0aWNhbCI6dHJ1ZSwiUnVzdCI6dHJ1ZSwic2VhcmNoIjp0cnVlLCJkb2N1bWVudCI6dHJ1ZSwic29tZXdoYXQgUG9zdGdyZVNRTCBjb21wYXRpYmxlIjp0cnVlLCJ0aW1lLXNlcmllcyI6dHJ1ZX0sIm1hY2hpbmUiOnsiMTYgdkNQVSAxMjhHQiI6dHJ1ZSwiOCB2Q1BVIDY0R0IiOnRydWUsInNlcnZlcmxlc3MiOnRydWUsIjE2YWN1Ijp0cnVlLCJjNmEuNHhsYXJnZSwgNTAwZ2IgZ3AyIjp0cnVlLCJMIjp0cnVlLCJNIjp0cnVlLCJTIjp0cnVlLCJYUyI6dHJ1ZSwiYzZhLm1ldGFsLCA1MDBnYiBncDIiOnRydWUsIjE5MkdCIjp0cnVlLCIyNEdCIjp0cnVlLCIzNjBHQiI6dHJ1ZSwiNDhHQiI6dHJ1ZSwiNzIwR0IiOnRydWUsIjk2R0IiOnRydWUsIjE0MzBHQiI6dHJ1ZSwiZGV2Ijp0cnVlLCI3MDhHQiI6dHJ1ZSwiYzVuLjR4bGFyZ2UsIDUwMGdiIGdwMiI6dHJ1ZSwiQW5hbHl0aWNzLTI1NkdCICg2NCB2Q29yZXMsIDI1NiBHQikiOnRydWUsImM1LjR4bGFyZ2UsIDUwMGdiIGdwMiI6dHJ1ZSwiYzZhLjR4bGFyZ2UsIDE1MDBnYiBncDIiOnRydWUsImNsb3VkIjp0cnVlLCJkYzIuOHhsYXJnZSI6dHJ1ZSwicmEzLjE2eGxhcmdlIjp0cnVlLCJyYTMuNHhsYXJnZSI6dHJ1ZSwicmEzLnhscGx1cyI6dHJ1ZSwiUzIiOnRydWUsIlMyNCI6dHJ1ZSwiMlhMIjp0cnVlLCIzWEwiOnRydWUsIjRYTCI6dHJ1ZSwiWEwiOnRydWUsIkwxIC0gMTZDUFUgMzJHQiI6dHJ1ZSwiYzZhLjR4bGFyZ2UsIDUwMGdiIGdwMyI6dHJ1ZX0sImNsdXN0ZXJfc2l6ZSI6eyIxIjp0cnVlLCIyIjp0cnVlLCI0Ijp0cnVlLCI4Ijp0cnVlLCIxNiI6dHJ1ZSwiMzIiOnRydWUsIjY0Ijp0cnVlLCIxMjgiOnRydWUsInNlcnZlcmxlc3MiOnRydWUsImRlZGljYXRlZCI6dHJ1ZX0sIm1ldHJpYyI6ImhvdCIsInF1ZXJpZXMiOlt0cnVlLHRydWUsdHJ1ZSx0cnVlLHRydWUsdHJ1ZSx0cnVlLHRydWUsdHJ1ZSx0cnVlLHRydWUsdHJ1ZSx0cnVlLHRydWUsdHJ1ZSx0cnVlLH"&gt;performs very well on ClickBench&lt;/a&gt;, the level of end-to-end performance improvement using StringViewArray shows the power of this technique and, of course, is a win for DataFusion and the systems that build upon it.&lt;/p&gt;
&lt;p&gt;StringView is a big project that has received tremendous community support. Specifically, we would like to thank &lt;a href="https://github.com/tustvold"&gt;@tustvold&lt;/a&gt;, &lt;a href="https://github.com/ariesdevil"&gt;@ariesdevil&lt;/a&gt;, &lt;a href="https://github.com/RinChanNOWWW"&gt;@RinChanNOWWW&lt;/a&gt;, &lt;a href="https://github.com/ClSlaid"&gt;@ClSlaid&lt;/a&gt;, &lt;a href="https://github.com/2010YOUY01"&gt;@2010YOUY01&lt;/a&gt;, &lt;a href="https://github.com/chloro-pn"&gt;@chloro-pn&lt;/a&gt;, &lt;a href="https://github.com/a10y"&gt;@a10y&lt;/a&gt;, &lt;a href="https://github.com/Kev1n8"&gt;@Kev1n8&lt;/a&gt;, &lt;a href="https://github.com/Weijun-H"&gt;@Weijun-H&lt;/a&gt;, &lt;a href="https://github.com/PsiACE"&gt;@PsiACE&lt;/a&gt;, &lt;a href="https://github.com/tshauck"&gt;@tshauck&lt;/a&gt;, and &lt;a href="https://github.com/xinlifoobar"&gt;@xinlifoobar&lt;/a&gt; for their valuable contributions!&lt;/p&gt;
&lt;p&gt;As the introduction states, &amp;ldquo;German Style Strings&amp;rdquo; is a relatively straightforward research idea that avoid some string copies and accelerates comparisons. However, applying this (great) idea in practice requires a significant investment in careful software engineering. Again, we encourage the research community to continue to help apply research ideas to industrial systems, such as DataFusion, as doing so provides valuable perspectives when evaluating future research questions for the greatest potential impact.&lt;/p&gt;
&lt;h3&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;[^5]: There are additional optimizations possible in this operation that the community is working on, such as  &lt;a href="https://github.com/apache/datafusion/issues/7957"&gt;https://github.com/apache/datafusion/issues/7957&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.2.0 Release</title><link href="https://datafusion.apache.org/blog/2024/08/28/datafusion-comet-0.2.0" rel="alternate"></link><published>2024-08-28T00:00:00+00:00</published><updated>2024-08-28T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-08-28:/blog/2024/08/28/datafusion-comet-0.2.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.2.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce version 0.2.0 of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers approximately four weeks of development work and is the result of merging 87 PRs from 14 
contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.2.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;h3&gt;Docker Images&lt;/h3&gt;
&lt;p&gt;Docker images are now available from the &lt;a href="https://github.com/apache/datafusion-comet/pkgs/container/datafusion-comet/265110454?tag=spark-3.4-scala-2.12-0.2.0"&gt;GitHub Container Registry&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Performance improvements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Native shuffle is now enabled by default&lt;/li&gt;
&lt;li&gt;Improved handling of decimal types&lt;/li&gt;
&lt;li&gt;Reduced some redundant copying of batches in Filter/Scan operations&lt;/li&gt;
&lt;li&gt;Optimized performance of count aggregates&lt;/li&gt;
&lt;li&gt;Optimized performance of  CASE expressions for specific uses:&lt;/li&gt;
&lt;li&gt;CASE WHEN expr THEN column ELSE null END&lt;/li&gt;
&lt;li&gt;CASE WHEN expr THEN literal ELSE literal END&lt;/li&gt;
&lt;li&gt;Optimized performance of IS NOT NULL&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;New Features&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Window operations now support count and sum aggregates&lt;/li&gt;
&lt;li&gt;CreateArray&lt;/li&gt;
&lt;li&gt;GetStructField&lt;/li&gt;
&lt;li&gt;Support nested types in hash join&lt;/li&gt;
&lt;li&gt;Basic implementation of RLIKE expression&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Current Performance&lt;/h2&gt;
&lt;p&gt;We use benchmarks derived from the industry standard TPC-H and TPC-DS benchmarks for tracking progress with
performance. The following charts shows the time it takes to run the queries against 100 GB of data in
Parquet format using a single executor with eight cores. See the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;
for details of the environment used for these benchmarks.&lt;/p&gt;
&lt;h3&gt;Benchmark derived from TPC-H&lt;/h3&gt;
&lt;p&gt;Comet 0.2.0 provides a 62% speedup compared to Spark. This is slightly better than the Comet 0.1.0 release.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.2.0" class="img-responsive" src="/blog/images/comet-0.2.0/tpch_allqueries.png" width="100%"/&gt;&lt;/p&gt;
&lt;h3&gt;Benchmark derived from TPC-DS&lt;/h3&gt;
&lt;p&gt;Comet 0.2.0 provides a 21% speedup compared to Spark, which is a significant improvement compared to 
Comet 0.1.0, which did not provide any speedup for this benchmark.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-DS benchmark results for Comet 0.2.0" class="img-responsive" src="/blog/images/comet-0.2.0/tpcds_allqueries.png" width="100%"/&gt;&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Python 40.1.0 Released, Significant usability updates</title><link href="https://datafusion.apache.org/blog/2024/08/20/python-datafusion-40.0.0" rel="alternate"></link><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><author><name>timsaucer</name></author><id>tag:datafusion.apache.org,2024-08-20:/blog/2024/08/20/python-datafusion-40.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/40.1.0/"&gt;DataFusion in Python 40.1.0&lt;/a&gt; has been released. In addition to
bringing in all of the new features of the core &lt;a href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/"&gt;DataFusion 40.0.0&lt;/a&gt; package, this release
contains &lt;em&gt;significant&lt;/em&gt; updates to the user interface and documentation. We listened to the python …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are happy to announce that &lt;a href="https://pypi.org/project/datafusion/40.1.0/"&gt;DataFusion in Python 40.1.0&lt;/a&gt; has been released. In addition to
bringing in all of the new features of the core &lt;a href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0/"&gt;DataFusion 40.0.0&lt;/a&gt; package, this release
contains &lt;em&gt;significant&lt;/em&gt; updates to the user interface and documentation. We listened to the python
user community to create a more &lt;em&gt;pythonic&lt;/em&gt; experience. If you have not used the python interface to
DataFusion before, this is an excellent time to give it a try!&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Until now, the python bindings for DataFusion have primarily been a thin layer to expose the
underlying Rust functionality. This has been worked well for early adopters to use DataFusion
within their Python projects, but some users have found it difficult to work with. As compared to
other DataFrame libraries, these issues were raised:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Most of the functions had little or no documentation. Users often had to refer to the Rust
documentation or code to learn how to use DataFusion. This alienated some python users.&lt;/li&gt;
&lt;li&gt;Users could not take advantage of modern IDE features such as type hinting. These are valuable
tools for rapid testing and development.&lt;/li&gt;
&lt;li&gt;Some of the interfaces felt &amp;ldquo;clunky&amp;rdquo; to users since some Python concepts do not always map well
to their Rust counterparts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This release aims to bring a better user experience to the DataFusion Python community.&lt;/p&gt;
&lt;h2&gt;What's Changed&lt;/h2&gt;
&lt;p&gt;The most significant difference is that we have added wrapper functions and classes for most of the
user facing interface. These wrappers, written in Python, contain both documentation and type
annotations.&lt;/p&gt;
&lt;p&gt;This documenation is now available on the &lt;a href="https://datafusion.apache.org/python/autoapi/datafusion/index.html"&gt;DataFusion in Python API&lt;/a&gt; website. There you can browse
the available functions and classes to see the breadth of available functionality.&lt;/p&gt;
&lt;p&gt;Modern IDEs use language servers such as
&lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance"&gt;Pylance&lt;/a&gt; or
&lt;a href="https://jedi.readthedocs.io/en/latest/"&gt;Jedi&lt;/a&gt; to perform analysis of python code, provide useful
hints, and identify usage errors. These are major tools in the python user community. With this
release, users can fully use these tools in their workflow.&lt;/p&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Enhanced tooltips in an IDE." class="img-responsive" src="/blog/images/python-datafusion-40.0.0/vscode_hover_tooltip.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 1&lt;/b&gt;: With the enhanced python wrappers, users can see helpful tool tips with
   type annotations directly in modern IDEs.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;By having the type annotations, these IDEs can also identify quickly when a user has incorrectly
used a function's arguments as shown in Figure 2.&lt;/p&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 2: Error checking in static analysis" class="img-responsive" src="/blog/images/python-datafusion-40.0.0/pylance_error_checking.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 2&lt;/b&gt;: Modern Python language servers can perform static analysis and quickly find
   errors in the arguments to functions.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In addition to these wrapper libraries, we have enhancements to some of the functions to feel more
easy to use.&lt;/p&gt;
&lt;h3&gt;Improved DataFrame filter arguments&lt;/h3&gt;
&lt;p&gt;You can now apply multiple &lt;code&gt;filter&lt;/code&gt; statements in a single step. When using &lt;code&gt;DataFrame.filter&lt;/code&gt; you
can pass in multiple arguments, separated by a comma. These will act as a logical &lt;code&gt;AND&lt;/code&gt; of all of
the filter arguments. The following two statements are equivalent:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.filter(col("size") &amp;lt; col("max_size")).filter(col("color") == lit("green"))
df.filter(col("size") &amp;lt; col("max_size"), col("color") == lit("green"))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Comparison against literal values&lt;/h3&gt;
&lt;p&gt;It is very common to write DataFrame operations that compare an expression to some fixed value.
For example, filtering a DataFrame might have an operation such as &lt;code&gt;df.filter(col("size") &amp;lt; lit(16))&lt;/code&gt;.
To make these common operations more ergonomic, you can now simply use &lt;code&gt;df.filter(col("size") &amp;lt; 16)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For the right hand side of the comparison operator, you can now use any Python value that can be
coerced into a &lt;code&gt;Literal&lt;/code&gt;. This gives an easy to ready expression. For example, consider these few
lines from one of the
&lt;a href="https://github.com/apache/datafusion-python/tree/main/examples/tpch"&gt;TPC-H examples&lt;/a&gt; provided in
the DataFusion Python repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = (
    df_lineitem.filter(col("l_shipdate") &amp;gt;= lit(date))
    .filter(col("l_discount") &amp;gt;= lit(DISCOUNT) - lit(DELTA))
    .filter(col("l_discount") &amp;lt;= lit(DISCOUNT) + lit(DELTA))
    .filter(col("l_quantity") &amp;lt; lit(QUANTITY))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code mirrors closely how these filters would need to be applied in rust. With this new
release, the user can simplify these lines. Also shown in the example below is that &lt;code&gt;filter()&lt;/code&gt;
now accepts a variable number of arguments and filters on all such arguments (boolean AND).&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df = df_lineitem.filter(
    col("l_shipdate") &amp;gt;= date,
    col("l_discount") &amp;gt;= DISCOUNT - DELTA,
    col("l_discount") &amp;lt;= DISCOUNT + DELTA,
    col("l_quantity") &amp;lt; QUANTITY,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Select columns by name&lt;/h3&gt;
&lt;p&gt;It is very common for users to perform &lt;code&gt;DataFrame&lt;/code&gt; selection where they simply want a column. For
this we have had the function &lt;code&gt;select_columns("a", "b")&lt;/code&gt; or the user could perform
&lt;code&gt;select(col("a"), col("b"))&lt;/code&gt;. In the new release, we accept either full expressions in &lt;code&gt;select()&lt;/code&gt;
or strings of the column names. You can mix these as well.&lt;/p&gt;
&lt;p&gt;Where before you may have to do an operation like&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df_subset = df.select(col("a"), col("b"), f.abs(col("c")))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now simplify this to&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df_subset = df.select("a", "b", f.abs(col("c")))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Creating named structs&lt;/h3&gt;
&lt;p&gt;Creating a &lt;code&gt;struct&lt;/code&gt; with named fields was previously difficult to use and allowed for potential
user errors when specifying the name of each field. Now we have a cleaner interface where the
user passes a list of tuples containing the name of the field and the expression to create.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;df.select(f.named_struct([
  ("a", col("a")),
  ("b", col("b"))
]))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;While most of the user facing classes and functions have been exposed, there are a few that require
exposure. Namely the classes in &lt;code&gt;datafusion.object_store&lt;/code&gt; and the logical plans used by
&lt;code&gt;datafusion.substrait&lt;/code&gt;. The team is working on
&lt;a href="https://github.com/apache/datafusion-python/issues/767"&gt;these issues&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Additionally, in the next release of DataFusion there have been improvements made to the user-defined
aggregate and window functions to make them easier to use. We plan on
&lt;a href="https://github.com/apache/datafusion-python/issues/780"&gt;bringing these enhancements&lt;/a&gt; to this project.&lt;/p&gt;
&lt;h2&gt;Thank You&lt;/h2&gt;
&lt;p&gt;We would like to thank the following members for their very helpful discussions regarding these
updates: &lt;a href="https://github.com/andygrove"&gt;@andygrove&lt;/a&gt;, &lt;a href="https://github.com/max-muoto"&gt;@max-muoto&lt;/a&gt;, &lt;a href="https://github.com/slyons"&gt;@slyons&lt;/a&gt;, &lt;a href="https://github.com/Throne3d"&gt;@Throne3d&lt;/a&gt;, &lt;a href="https://github.com/Michael-J-Ward"&gt;@Michael-J-Ward&lt;/a&gt;, &lt;a href="https://github.com/datapythonista"&gt;@datapythonista&lt;/a&gt;,
&lt;a href="https://github.com/austin362667"&gt;@austin362667&lt;/a&gt;, &lt;a href="https://github.com/kylebarron"&gt;@kylebarron&lt;/a&gt;, &lt;a href="https://github.com/simicd"&gt;@simicd&lt;/a&gt;. The &lt;a href="https://github.com/apache/datafusion-python/pull/750"&gt;primary PR (#750)&lt;/a&gt; that includes these updates
had an extensive conversation, leading to a significantly improved end product. Again, thank you
to all who provided input!&lt;/p&gt;
&lt;p&gt;We would like to give an special thank you to &lt;a href="https://github.com/3ok"&gt;@3ok&lt;/a&gt; who created the initial version of the wrapper
definitions. The work they did was time consuming and required exceptional attention to detail. It
provided enormous value to starting this project. Thank you!&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;The DataFusion Python team is an active and engaging community and we would love
to have you join us and help the project.&lt;/p&gt;
&lt;p&gt;Here are some ways to get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learn more by visiting the &lt;a href="https://datafusion.apache.org/python/index.html"&gt;DataFusion Python project&lt;/a&gt;
page.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try out the project and provide feedback, file issues, and contribute code.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion 40.0.0 Released</title><link href="https://datafusion.apache.org/blog/2024/07/24/datafusion-40.0.0" rel="alternate"></link><published>2024-07-24T00:00:00+00:00</published><updated>2024-07-24T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-07-24:/blog/2024/07/24/datafusion-40.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/9602 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are proud to announce &lt;a href="https://crates.io/crates/datafusion/40.0.0"&gt;DataFusion 40.0.0&lt;/a&gt;. This blog highlights some of the
many major improvements since we released &lt;a href="https://datafusion.apache.org/blog/2024/01/19/datafusion-34.0.0/"&gt;DataFusion 34.0.0&lt;/a&gt; and a preview of
what the community is thinking about in the next 6 months. We are hoping to make
more regular blog posts …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- see https://github.com/apache/datafusion/issues/9602 for details --&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We are proud to announce &lt;a href="https://crates.io/crates/datafusion/40.0.0"&gt;DataFusion 40.0.0&lt;/a&gt;. This blog highlights some of the
many major improvements since we released &lt;a href="https://datafusion.apache.org/blog/2024/01/19/datafusion-34.0.0/"&gt;DataFusion 34.0.0&lt;/a&gt; and a preview of
what the community is thinking about in the next 6 months. We are hoping to make
more regular blog posts -- if you are interested in helping write them, please
reach out!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt; is an extensible query engine, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that
uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion is used by developers to
create new, fast data centric systems such as databases, dataframe libraries,
machine learning and streaming applications. While &lt;a href="https://datafusion.apache.org/user-guide/introduction.html#project-goals"&gt;DataFusion&amp;rsquo;s primary design
goal&lt;/a&gt; is to accelerate the creation of other data centric systems, it has a
reasonable experience directly out of the box as a &lt;a href="https://datafusion.apache.org/python/"&gt;dataframe library&lt;/a&gt; and
&lt;a href="https://datafusion.apache.org/user-guide/cli/"&gt;command line SQL tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion's core thesis is that as a community, together we can build much more
advanced technology than any of us as individuals or companies could do alone. 
Without DataFusion, highly performant vectorized query engines would remain
the domain of a few large companies and world-class research institutions. 
With DataFusion, we can all build on top of a shared foundation, and focus on
what makes our projects unique.&lt;/p&gt;
&lt;h2&gt;Community Growth  📈&lt;/h2&gt;
&lt;p&gt;In the last 6 months, between &lt;code&gt;34.0.0&lt;/code&gt; and &lt;code&gt;40.0.0&lt;/code&gt;, our community continues to
grow in new and exciting ways.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DataFusion became a top level Apache Software Foundation project (read the
   &lt;a href="https://news.apache.org/foundation/entry/apache-software-foundation-announces-new-top-level-project-apache-datafusion"&gt;press release&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/blog/2024/05/07/datafusion-tlp/"&gt;blog post&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;We added several PMC members and new
   committers: &lt;a href="https://github.com/comphead"&gt;@comphead&lt;/a&gt;, &lt;a href="https://github.com/mustafasrepo"&gt;@mustafasrepo&lt;/a&gt;, &lt;a href="https://github.com/ozankabak"&gt;@ozankabak&lt;/a&gt;, and &lt;a href="https://github.com/waynexia"&gt;@waynexia&lt;/a&gt; joined the PMC,
   &lt;a href="https://github.com/jonahgao"&gt;@jonahgao&lt;/a&gt; and &lt;a href="https://github.com/lewiszlw"&gt;@lewiszlw&lt;/a&gt; joined as committers. See the &lt;a href="https://lists.apache.org/list.html?dev@datafusion.apache.org"&gt;mailing list&lt;/a&gt; for
   more details.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://datafusion.apache.org/comet/"&gt;DataFusion Comet&lt;/a&gt; was &lt;a href="https://arrow.apache.org/blog/2024/03/06/comet-donation/"&gt;donated&lt;/a&gt; and is nearing its first release.&lt;/li&gt;
&lt;li&gt;In the &lt;a href="https://github.com/apache/arrow-datafusion"&gt;core DataFusion repo&lt;/a&gt; alone we reviewed and accepted almost 1500 PRs from 182 different
   committers, created over 1000 issues and closed 781 of them 🚀. This is up
   almost 50% from our last post (1000 PRs from 124 committers with 650 issues
   created in our last post) 🤯. All changes are listed in the detailed
   &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;DataFusion focused meetups happened or are happening in multiple cities 
   around the world: &lt;a href="https://github.com/apache/datafusion/discussions/8522"&gt;Austin&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/10800"&gt;San Francisco&lt;/a&gt;, &lt;a href="https://www.huodongxing.com/event/5761971909400?td=1965290734055"&gt;Hangzhou&lt;/a&gt;, &lt;a href="https://github.com/apache/datafusion/discussions/11213"&gt;New York&lt;/a&gt;, and
   &lt;a href="https://github.com/apache/datafusion/discussions/11431"&gt;Belgrade&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Many new projects started in the &lt;a href="https://github.com/datafusion-contrib"&gt;datafusion-contrib&lt;/a&gt; organization, including
   &lt;a href="https://github.com/datafusion-contrib/datafusion-table-providers"&gt;Table Providers&lt;/a&gt;, &lt;a href="https://github.com/datafusion-contrib/datafusion-sqlancer"&gt;SQLancer&lt;/a&gt;, &lt;a href="https://github.com/datafusion-contrib/datafusion-functions-variant"&gt;Open Variant&lt;/a&gt;, &lt;a href="https://github.com/datafusion-contrib/datafusion-functions-json"&gt;JSON&lt;/a&gt;, and &lt;a href="https://github.com/datafusion-contrib/datafusion-orc"&gt;ORC&lt;/a&gt;.  &lt;/li&gt;
&lt;/ol&gt;
&lt;!--
$ git log --pretty=oneline 34.0.0..40.0.0 . | wc -l
     1453 (up from 1009)

$ git shortlog -sn 34.0.0..40.0.0 . | wc -l
      182 (up from 124)

https://crates.io/crates/datafusion/34.0.0
DataFusion 34 released Dec 17, 2023

https://crates.io/crates/datafusion/40.0.0
DataFusion 34 released July 12, 2024

Issues created in this time: 321 open, 781 closed (up from 214 open, 437 closed)
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+created%3A2023-12-17..2024-07-12

Issues closed: 911 (up from 517)
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+closed%3A2023-12-17..2024-07-12

PRs merged in this time 1490 (up from 908)
https://github.com/apache/arrow-datafusion/pulls?q=is%3Apr+merged%3A2023-12-17..2024-07-12

--&gt;
&lt;p&gt;In addition, DataFusion has been appearing publicly more and more, both online and offline. Here are some highlights:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://dl.acm.org/doi/10.1145/3626246.3653368"&gt;Apache Arrow DataFusion: A Fast, Embeddable, Modular Analytic Query Engine&lt;/a&gt;, was presented in &lt;a href="https://2024.sigmod.org/"&gt;SIGMOD '24&lt;/a&gt;, one of the major database conferences&lt;/li&gt;
&lt;li&gt;As part of the trend to define "the POSIX of databases" in &lt;a href="https://db.cs.cmu.edu/papers/2024/whatgoesaround-sigmodrec2024.pdf"&gt;"What Goes Around Comes Around... And Around..."&lt;/a&gt; from Andy Pavlo and Mike Stonebraker&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cpard.xyz/posts/datafusion/"&gt;"Why you should keep an eye on Apache DataFusion and its community"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tisonkun.org/2024/07/15/datafusion-meetup-san-francisco/"&gt;Apache DataFusion offline meetup in the Bay Area&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Improved Performance 🚀&lt;/h2&gt;
&lt;p&gt;Performance is a key feature of DataFusion, and the community continues to work
to keep DataFusion state of the art in this area. One major area DataFusion
improved is the time it takes to convert a SQL query into a plan that can be
executed. Planning is now almost 2x faster for TPC-DS and TPC-H queries, and
over 10x faster for some queries with many columns.&lt;/p&gt;
&lt;p&gt;Here is a chart showing the improvement due to the concerted effort of many
contributors including &lt;a href="https://github.com/jackwener"&gt;@jackwener&lt;/a&gt;, &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;, &lt;a href="https://github.com/Lordworms"&gt;@Lordworms&lt;/a&gt;, &lt;a href="https://github.com/dmitrybugakov"&gt;@dmitrybugakov&lt;/a&gt;,
&lt;a href="https://github.com/appletreeisyellow"&gt;@appletreeisyellow&lt;/a&gt;, &lt;a href="https://github.com/ClSlaid"&gt;@ClSlaid&lt;/a&gt;, &lt;a href="https://github.com/rohitrastogi"&gt;@rohitrastogi&lt;/a&gt;, &lt;a href="https://github.com/emgeee"&gt;@emgeee&lt;/a&gt;, &lt;a href="https://github.com/kevinmingtarja"&gt;@kevinmingtarja&lt;/a&gt;,
and &lt;a href="https://github.com/peter-toth"&gt;@peter-toth&lt;/a&gt; over several months (see &lt;a href="https://github.com/apache/arrow-datafusion/issues/8045"&gt;ticket&lt;/a&gt; for more details)&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/datafusion-40.0.0/improved-planning-time.png" width="700"/&gt;&lt;/p&gt;
&lt;p&gt;DataFusion is now up to 40% faster for queries that &lt;code&gt;GROUP BY&lt;/code&gt; a single string
or binary column due to a &lt;a href="https://github.com/apache/datafusion/pull/8827"&gt;specialization for single
Uft8/LargeUtf8/Binary/LargeBinary&lt;/a&gt;. We are working on improving performance when
there are [multiple variable length columns in the &lt;code&gt;GROUP BY&lt;/code&gt; clause].&lt;/p&gt;
&lt;p&gt;We are also in the final phases of &lt;a href="https://github.com/apache/datafusion/issues/10918"&gt;integrating&lt;/a&gt; the new &lt;a href="https://docs.rs/arrow/latest/arrow/array/struct.GenericByteViewArray.html"&gt;Arrow StringView&lt;/a&gt;
which significantly improves performance for workloads that scan, filter and
group by variable length string and binary data. We expect the improvement to be
especially pronounced for Parquet files due to &lt;a href="https://github.com/apache/arrow-rs/issues/5530"&gt;upstream work in the parquet
reader&lt;/a&gt;. Kudos to &lt;a href="https://github.com/XiangpengHong"&gt;@XiangpengHong&lt;/a&gt;, &lt;a href="https://github.com/AriesDevil"&gt;@AriesDevil&lt;/a&gt;, &lt;a href="https://github.com/PsiACE"&gt;@PsiACE&lt;/a&gt;, &lt;a href="https://github.com/Weijun-H"&gt;@Weijun-H&lt;/a&gt;,
&lt;a href="https://github.com/a10y"&gt;@a10y&lt;/a&gt;, and &lt;a href="https://github.com/RinChanNOWWW"&gt;@RinChanNOWWW&lt;/a&gt; for driving this project.&lt;/p&gt;
&lt;h2&gt;Improved Quality 📋&lt;/h2&gt;
&lt;p&gt;DataFusion continues to improve overall in quality. In addition to ongoing bug
fixes, one of the most exciting improvements is the addition of a new &lt;a href="https://github.com/datafusion-contrib/datafusion-sqlancer"&gt;SQLancer&lt;/a&gt;
based &lt;a href="https://github.com/apache/datafusion/issues/11030"&gt;DataFusion Fuzzing&lt;/a&gt; suite thanks to &lt;a href="https://github.com/2010YOUY01"&gt;@2010YOUY01&lt;/a&gt; that has already found
several bugs and thanks to &lt;a href="https://github.com/jonahgao"&gt;@jonahgao&lt;/a&gt;, &lt;a href="https://github.com/tshauck"&gt;@tshauck&lt;/a&gt;, &lt;a href="https://github.com/xinlifoobar"&gt;@xinlifoobar&lt;/a&gt;,
&lt;a href="https://github.com/LorrensP-2158466"&gt;@LorrensP-2158466&lt;/a&gt; for fixing them so fast.&lt;/p&gt;
&lt;h2&gt;Improved Documentation 📚&lt;/h2&gt;
&lt;p&gt;We continue to improve the documentation to make it easier to get started using DataFusion with
the &lt;a href="https://datafusion.apache.org/library-user-guide/index.html"&gt;Library Users Guide&lt;/a&gt;, &lt;a href="https://docs.rs/datafusion/latest/datafusion/index.html"&gt;API documentation&lt;/a&gt;, and &lt;a href="https://github.com/apache/datafusion/tree/main/datafusion-examples"&gt;Examples&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some notable new examples include:
* &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/sql_analysis.rs"&gt;sql_analysis.rs&lt;/a&gt; to analyse SQL queries with DataFusion structures (thanks &lt;a href="https://github.com/LorrensP-2158466"&gt;@LorrensP-2158466&lt;/a&gt;)
* &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/function_factory.rs"&gt;function_factory.rs&lt;/a&gt; to create custom functions via SQL (thanks &lt;a href="https://github.com/milenkovicm"&gt;@milenkovicm&lt;/a&gt;)
* &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/plan_to_sql.rs"&gt;plan_to_sql.rs&lt;/a&gt; to generate SQL from DataFusion Expr and LogicalPlan (thanks &lt;a href="https://github.com/edmondop"&gt;@edmondop&lt;/a&gt;)
* &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs"&gt;parquet_index.rs&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs"&gt;advanced_parquet_index.rs&lt;/a&gt; for parquet indexing, described more below (thanks &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt;)&lt;/p&gt;
&lt;h2&gt;New Features ✨&lt;/h2&gt;
&lt;p&gt;There are too many new features in the last 6 months to list them all, but here
are some highlights:&lt;/p&gt;
&lt;h1&gt;SQL&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Support for &lt;code&gt;UNNEST&lt;/code&gt; (thanks &lt;a href="https://github.com/duongcongtoai"&gt;@duongcongtoai&lt;/a&gt;, &lt;a href="https://github.com/JasonLi-cn"&gt;@JasonLi-cn&lt;/a&gt; and &lt;a href="https://github.com/jayzhan211"&gt;@jayzhan211&lt;/a&gt;) &lt;/li&gt;
&lt;li&gt;Support for &lt;a href="https://github.com/apache/datafusion/issues/462"&gt;Recursive CTEs&lt;/a&gt; (thanks &lt;a href="https://github.com/jonahgao"&gt;@jonahgao&lt;/a&gt; and &lt;a href="https://github.com/matthewgapp"&gt;@matthewgapp&lt;/a&gt;) &lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;CREATE FUNCTION&lt;/code&gt; (see below) &lt;/li&gt;
&lt;li&gt;Many new SQL functions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DataFusion now has much improved support for structured types such &lt;code&gt;STRUCT&lt;/code&gt;,
&lt;code&gt;LIST&lt;/code&gt;/&lt;code&gt;ARRAY&lt;/code&gt; and &lt;code&gt;MAP&lt;/code&gt;. For example, you can now create &lt;code&gt;STRUCT&lt;/code&gt; literals 
in SQL like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;&amp;gt; select {'foo': {'bar': 2}};
+--------------------------------------------------------------+
| named_struct(Utf8("foo"),named_struct(Utf8("bar"),Int64(2))) |
+--------------------------------------------------------------+
| {foo: {bar: 2}}                                              |
+--------------------------------------------------------------+
1 row(s) fetched.
Elapsed 0.002 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;SQL Unparser (SQL Formatter)&lt;/h1&gt;
&lt;p&gt;DataFusion now supports converting &lt;code&gt;Expr&lt;/code&gt;s and &lt;code&gt;LogicalPlan&lt;/code&gt;s BACK to SQL text.
This can be useful in query federation to push predicates down into other
systems that only accept SQL, and for building systems that generate SQL.&lt;/p&gt;
&lt;p&gt;For example, you can now convert a logical expression back to SQL text:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// Form a logical expression that represents the SQL "a &amp;lt; 5 OR a = 8"
let expr = col("a").lt(lit(5)).or(col("a").eq(lit(8)));
// convert the expression back to SQL text
let sql = expr_to_sql(&amp;amp;expr)?.to_string();
assert_eq!(sql, "a &amp;lt; 5 OR a = 8");
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also do complex things like parsing SQL, modifying the plan, and convert
it back to SQL:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;let df = ctx
  // Use SQL to read some data from the parquet file
  .sql("SELECT int_col, double_col, CAST(date_string_col as VARCHAR) FROM alltypes_plain")
  .await?;
// Programmatically add new filters `id &amp;gt; 1 and tinyint_col &amp;lt; double_col`
let df = df.filter(col("id").gt(lit(1)).and(col("tinyint_col").lt(col("double_col"))))?
// Convert the new logical plan back to SQL
let sql = plan_to_sql(df.logical_plan())?.to_string();
assert_eq!(sql, 
           "SELECT alltypes_plain.int_col, alltypes_plain.double_col, CAST(alltypes_plain.date_string_col AS VARCHAR) \
           FROM alltypes_plain WHERE ((alltypes_plain.id &amp;gt; 1) AND (alltypes_plain.tinyint_col &amp;lt; alltypes_plain.double_col))")
);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/plan_to_sql.rs"&gt;Plan to SQL example&lt;/a&gt; or the APIs &lt;a href="https://docs.rs/datafusion/latest/datafusion/sql/unparser/fn.expr_to_sql.html"&gt;expr_to_sql&lt;/a&gt; and &lt;a href="https://docs.rs/datafusion/latest/datafusion/sql/unparser/fn.plan_to_sql.html"&gt;plan_to_sql&lt;/a&gt; for more details.&lt;/p&gt;
&lt;h1&gt;Low Level APIs for Fast Parquet Access (indexing)&lt;/h1&gt;
&lt;p&gt;With their rising prevalence, supporting efficient access to Parquet files
stored remotely on object storage is important. Part of doing this efficiently
is minimizing the number of object store requests made by caching metadata and
skipping over parts of the file that are not needed (e.g. via an index).&lt;/p&gt;
&lt;p&gt;DataFusion's Parquet reader has long internally supported &lt;a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"&gt;advanced predicate
pushdown&lt;/a&gt; by reading the parquet metadata from the file footer and pruning based
on row group and data page statistics. DataFusion now also supports users
supplying their own low level pruning information via the [&lt;code&gt;ParquetAccessPlan&lt;/code&gt;]
API.&lt;/p&gt;
&lt;p&gt;This API can be used along with index information to selectively skip decoding
parts of the file. For example, Spice AI used this feature to add &lt;a href="https://github.com/spiceai/spiceai/pull/1891"&gt;efficient
support&lt;/a&gt; for reading from DeltaLake tables and handling &lt;a href="https://docs.delta.io/latest/delta-deletion-vectors.html"&gt;deletion vectors&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;        &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;   If the RowSelection does not include any
        &amp;boxv;          ...          &amp;boxv;   rows from a particular Data Page, that
        &amp;boxv;                       &amp;boxv;   Data Page is not fetched or decoded.
        &amp;boxv; &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv;   Note this requires a PageIndex
        &amp;boxv; &amp;boxv;     &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv; &amp;boxv;
Row     &amp;boxv; &amp;boxv;     &amp;boxv;DataPage 0&amp;boxv;  &amp;boxv; &amp;boxv;                 &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
Groups  &amp;boxv; &amp;boxv;     &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxv; &amp;boxv;                 &amp;boxv;                    &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv; &amp;boxv;                 &amp;boxv;    ParquetExec     &amp;boxv;
        &amp;boxv; &amp;boxv; ... &amp;boxv;DataPage 1&amp;boxv; ◀&amp;boxvh; &amp;boxvh; &amp;boxh; &amp;boxh; &amp;boxh;           &amp;boxv;  (Parquet Reader)  &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxv; &amp;boxv;      &amp;boxur; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh;&amp;boxv;                    &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv; &amp;boxv;                 &amp;boxv; &amp;boxDR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxDL;  &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxv;DataPage 2&amp;boxv;  &amp;boxv; &amp;boxv; If only rows    &amp;boxv; &amp;boxV;ParquetMetadata&amp;boxV;  &amp;boxv;
        &amp;boxv; &amp;boxv;     &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxv; &amp;boxv; from DataPage 1 &amp;boxv; &amp;boxUR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxUL;  &amp;boxv;
        &amp;boxv; &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv; are selected,   &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
        &amp;boxv;                       &amp;boxv; only DataPage 1
        &amp;boxv;          ...          &amp;boxv; is fetched and
        &amp;boxv;                       &amp;boxv; decoded
        &amp;boxv; &amp;boxDR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxDL; &amp;boxv;
        &amp;boxv; &amp;boxV;  Thrift metadata  &amp;boxV; &amp;boxv;
        &amp;boxv; &amp;boxUR;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxH;&amp;boxUL; &amp;boxv;
        &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
         Parquet File
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;See the &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/parquet_index.rs"&gt;parquet_index.rs&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/advanced_parquet_index.rs"&gt;advanced_parquet_index.rs&lt;/a&gt; examples for more details. &lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/alamb"&gt;@alamb&lt;/a&gt; and &lt;a href="https://github.com/Ted-Jiang"&gt;@Ted-Jiang&lt;/a&gt; for this feature.  &lt;/p&gt;
&lt;h2&gt;Building Systems is Easier with DataFusion 🛠️&lt;/h2&gt;
&lt;p&gt;In addition to many incremental API improvements, there are several new APIs that make
it easier to build systems on top of DataFusion:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Faster and easier to use &lt;a href="https://docs.rs/datafusion/latest/datafusion/common/tree_node/trait.TreeNode.html#overview"&gt;TreeNode API&lt;/a&gt; for traversing and manipulating plans and expressions.&lt;/li&gt;
&lt;li&gt;All functions now use the same &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/trait.ScalarUDFImpl.html"&gt;Scalar User Defined Function API&lt;/a&gt;, making it easier to customize
  DataFusion's behavior without sacrificing performance. See &lt;a href="https://github.com/apache/arrow-datafusion/issues/8045"&gt;ticket&lt;/a&gt; for more details.&lt;/li&gt;
&lt;li&gt;DataFusion can now be compiled to &lt;a href="https://github.com/apache/datafusion/discussions/9834"&gt;WASM&lt;/a&gt;. &lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;User Defined SQL Parsing Extensions&lt;/h1&gt;
&lt;p&gt;As of DataFusion 40.0.0, you can use the [&lt;code&gt;ExprPlanner&lt;/code&gt;] trait to extend
DataFusion's SQL planner to support custom operators or syntax.&lt;/p&gt;
&lt;p&gt;For example the &lt;a href="https://github.com/datafusion-contrib/datafusion-functions-json"&gt;datafusion-functions-json&lt;/a&gt; project uses this API to support
JSON operators in SQL queries. It provides a custom implementation for
planning JSON operators such as &lt;code&gt;-&amp;gt;&lt;/code&gt; and &lt;code&gt;-&amp;gt;&amp;gt;&lt;/code&gt; with code like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;struct MyCustomPlanner;

impl ExprPlanner for MyCustomPlanner {
    // Provide custom implementation for planning a binary operators
    // such as `-&amp;gt;` and `-&amp;gt;&amp;gt;`
    fn plan_binary_op(
        &amp;amp;self,
        expr: RawBinaryExpr,
        _schema: &amp;amp;DFSchema,
    ) -&amp;gt; Result&amp;lt;PlannerResult&amp;lt;RawBinaryExpr&amp;gt;&amp;gt; {
        match &amp;amp;expr.op {
           BinaryOperator::Arrow =&amp;gt; { /* plan -&amp;gt; operator */ }
           BinaryOperator::LongArrow =&amp;gt; { /* plan -&amp;gt;&amp;gt; operator */ }
           ...
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thanks to &lt;a href="https://github.com/samuelcolvin"&gt;@samuelcolvin&lt;/a&gt;, &lt;a href="https://github.com/jayzhan211"&gt;@jayzhan211&lt;/a&gt; and &lt;a href="https://github.com/dharanad"&gt;@dharanad&lt;/a&gt; for helping make this
feature happen.&lt;/p&gt;
&lt;h1&gt;Pluggable Support for &lt;code&gt;CREATE FUNCTION&lt;/code&gt;&lt;/h1&gt;
&lt;p&gt;DataFusion's new [&lt;code&gt;FunctionFactory&lt;/code&gt;] API let's users provide a handler for
&lt;code&gt;CREATE FUNCTION&lt;/code&gt; SQL statements. This feature lets you build systems that
support defining functions in SQL such as&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;-- SQL based functions
CREATE FUNCTION my_func(DOUBLE, DOUBLE) RETURNS DOUBLE
    RETURN $1 + $3
;

-- ML Models
CREATE FUNCTION iris(FLOAT[]) RETURNS FLOAT[] 
LANGUAGE TORCH AS 'models:/iris@champion';

-- WebAssembly
CREATE FUNCTION func(FLOAT[]) RETURNS FLOAT[] 
LANGUAGE WASM AS 'func.wasm'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Huge thanks to &lt;a href="https://github.com/milenkovicm"&gt;@milenkovicm&lt;/a&gt; for this feature. There is an example of how to
make macro like functions in &lt;a href="https://github.com/apache/datafusion/blob/main/datafusion-examples/examples/function_factory.rs"&gt;function_factory.rs&lt;/a&gt;. It would be
great if &lt;a href="https://github.com/apache/datafusion/issues/9326"&gt;someone made a demo&lt;/a&gt; showing how to create WASMs 🎣.&lt;/p&gt;
&lt;h2&gt;Looking Ahead: The Next Six Months 🔭&lt;/h2&gt;
&lt;p&gt;The community has been &lt;a href="https://github.com/apache/datafusion/issues/11442"&gt;discussing what we will work on in the next six months&lt;/a&gt;.
Some major initiatives from that discussion are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Performance&lt;/em&gt;: Improve the speed of &lt;a href="https://github.com/apache/arrow-datafusion/issues/7000"&gt;aggregating "high cardinality"&lt;/a&gt;
  data when there are many (e.g. millions) of distinct groups as well as additional
  ideas to improve parquet performance. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Modularity&lt;/em&gt;: Make DataFusion even more modular, by completely unifying
   built in and user &lt;a href="https://github.com/apache/datafusion/issues/8708"&gt;aggregate functions&lt;/a&gt; and &lt;a href="https://github.com/apache/datafusion/issues/8709"&gt;window functions&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;LogicalTypes&lt;/em&gt;: &lt;a href="https://github.com/apache/datafusion/issues/11513"&gt;Introduce Logical Types&lt;/a&gt; to make it easier to use
   different encodings like &lt;code&gt;StringView&lt;/code&gt;, &lt;code&gt;RunEnd&lt;/code&gt; and &lt;code&gt;Dictionary&lt;/code&gt; arrays as well
   as user defined types. Thanks &lt;a href="https://github.com/notfilippo"&gt;@notfilippo&lt;/a&gt; for driving this. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Improved Documentation&lt;/em&gt;: Write blog posts and videos explaining
   how to use DataFusion for real-world use cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Testing&lt;/em&gt;: Improve CI infrastructure and test coverage, more fuzz
   testing, and better functional and performance regression testing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;DataFusion is not a project built or driven by a single person, company, or
foundation. Rather, our community of users and contributors work together to
build a shared technology that none of us could have built alone.&lt;/p&gt;
&lt;p&gt;If you are interested in joining us we would love to have you. You can try out
DataFusion on some of your own data and projects and let us know how it goes,
contribute suggestions, documentation, bug reports, or a PR with documentation,
tests or code. A list of open issues suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt; and you
can find how to reach us on the &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;communication doc&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache DataFusion Comet 0.1.0 Release</title><link href="https://datafusion.apache.org/blog/2024/07/20/datafusion-comet-0.1.0" rel="alternate"></link><published>2024-07-20T00:00:00+00:00</published><updated>2024-07-20T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-07-20:/blog/2024/07/20/datafusion-comet-0.1.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce the first official source release of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache DataFusion PMC is pleased to announce the first official source release of the &lt;a href="https://datafusion.apache.org/comet/"&gt;Comet&lt;/a&gt; subproject.&lt;/p&gt;
&lt;p&gt;Comet is an accelerator for Apache Spark that translates Spark physical plans to DataFusion physical plans for
improved performance and efficiency without requiring any code changes.&lt;/p&gt;
&lt;p&gt;Comet runs on commodity hardware and aims to provide 100% compatibility with Apache Spark. Any operators or
expressions that are not fully compatible will fall back to Spark unless explicitly enabled by the user. Refer
to the &lt;a href="https://datafusion.apache.org/comet/user-guide/compatibility.html"&gt;compatibility guide&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This release covers five months of development work since the project was &lt;a href="https://datafusion.apache.org/blog/2024/03/06/comet-donation/"&gt;donated&lt;/a&gt; to the Apache DataFusion
project and is the result of merging 343 PRs from 41 contributors. See the &lt;a href="https://github.com/apache/datafusion-comet/blob/main/dev/changelog/0.1.0.md"&gt;change log&lt;/a&gt; for more information.&lt;/p&gt;
&lt;p&gt;This first release supports 15 &lt;a href="https://datafusion.apache.org/comet/user-guide/datatypes.html#"&gt;data types&lt;/a&gt;, 13 &lt;a href="https://datafusion.apache.org/comet/user-guide/operators.html#"&gt;operators&lt;/a&gt;, and 106 &lt;a href="https://datafusion.apache.org/comet/user-guide/expressions.html#"&gt;expressions&lt;/a&gt;. Comet is compatible with Apache
Spark versions 3.3, 3.4, and 3.5. There is also experimental support for preview versions of Spark 4.0.&lt;/p&gt;
&lt;h2&gt;Project Status&lt;/h2&gt;
&lt;p&gt;The project's recent focus has been on fixing correctness and stability issues and implementing additional
native operators and expressions so that a broader range of queries can be executed natively.&lt;/p&gt;
&lt;p&gt;Here are some of the highlights since the project was donated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implemented native support for:&lt;/li&gt;
&lt;li&gt;SortMergeJoin&lt;/li&gt;
&lt;li&gt;HashJoin&lt;/li&gt;
&lt;li&gt;BroadcastHashJoin&lt;/li&gt;
&lt;li&gt;Columnar Shuffle&lt;/li&gt;
&lt;li&gt;More aggregate expressions&lt;/li&gt;
&lt;li&gt;Window aggregates&lt;/li&gt;
&lt;li&gt;Many Spark-compatible CAST expressions&lt;/li&gt;
&lt;li&gt;Implemented a simple Spark Fuzz Testing utility to find correctness issues&lt;/li&gt;
&lt;li&gt;Published a &lt;a href="https://datafusion.apache.org/comet/user-guide/overview.html"&gt;User Guide&lt;/a&gt; and &lt;a href="https://datafusion.apache.org/comet/contributor-guide/contributing.html"&gt;Contributors Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Created a &lt;a href="https://github.com/apache/datafusion-benchmarks"&gt;DataFusion Benchmarks&lt;/a&gt; repository with scripts and documentation for running benchmarks derived&lt;br/&gt;
  from TPC-H and TPC-DS with DataFusion and Comet&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Current Performance&lt;/h2&gt;
&lt;p&gt;Comet already delivers a modest performance speedup for many queries, enabling faster data processing and
shorter time-to-insights.&lt;/p&gt;
&lt;p&gt;We use benchmarks derived from the industry standard TPC-H and TPC-DS benchmarks for tracking progress with
performance. The following chart shows the time it takes to run the 22 TPC-H queries against 100 GB of data in
Parquet format using a single executor with eight cores. See the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;
for details of the environment used for these benchmarks.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.1.0" class="img-responsive" src="/blog/images/comet-0.1.0/tpch_allqueries.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;Comet reduces the overall execution time from 626 seconds to 407 seconds, a 54% speedup (1.54x faster).&lt;/p&gt;
&lt;p&gt;Running the same queries with DataFusion standalone using the same number of cores results in a 3.9x speedup
compared to Spark. Although this isn&amp;rsquo;t a fair comparison (DataFusion does not have shuffle or match Spark
semantics in some cases, for example), it does give some idea about the potential future performance of
Comet. Comet aims to provide a 2x-4x speedup for a wide range of queries once more operators and expressions
can run natively.&lt;/p&gt;
&lt;p&gt;The following chart shows how much Comet currently accelerates each query from the benchmark.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Chart showing TPC-H benchmark results for Comet 0.1.0" class="img-responsive" src="/blog/images/comet-0.1.0/tpch_queries_speedup.png" width="100%"/&gt;&lt;/p&gt;
&lt;p&gt;These benchmarks can be reproduced in any environment using the documentation in the &lt;a href="https://datafusion.apache.org/comet/contributor-guide/benchmarking.html"&gt;Comet Benchmarking Guide&lt;/a&gt;. We
encourage you to run these benchmarks in your environment or, even better, try Comet out with your existing Spark jobs.&lt;/p&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;Comet is an open-source project, and contributors are welcome to work on any features they are interested in, but
here are some current focus areas.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Improve Performance &amp;amp; Reliability:&lt;/li&gt;
&lt;li&gt;Implement the remaining features needed so that all TPC-H queries can run entirely natively&lt;/li&gt;
&lt;li&gt;Implement spill support in SortMergeJoin&lt;/li&gt;
&lt;li&gt;Enable columnar shuffle by default&lt;/li&gt;
&lt;li&gt;Fully support Spark version 4.0.0&lt;/li&gt;
&lt;li&gt;Support more Spark operators and expressions&lt;/li&gt;
&lt;li&gt;We would like to support many more expressions natively in Comet, and this is a great place to start
    contributing. The contributors' guide has a section covering &lt;a href="https://datafusion.apache.org/comet/contributor-guide/adding_a_new_expression.html"&gt;adding support for new expressions&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Move more Spark expressions into the &lt;a href="https://crates.io/crates/datafusion-comet-spark-expr"&gt;datafusion-comet-spark-expr&lt;/a&gt; crate. Although the main focus of the Comet
  project is to provide an accelerator for Apache Spark, we also publish a standalone crate containing
  Spark-compatible expressions that can be used by any project using DataFusion, without adding any dependencies
  on JVM or Apache Spark.&lt;/li&gt;
&lt;li&gt;Release Process &amp;amp; Documentation&lt;/li&gt;
&lt;li&gt;Implement a binary release process so that we can publish JAR files to Maven for all supported platforms&lt;/li&gt;
&lt;li&gt;Add documentation for running Spark and Comet in Kubernetes, and add example Dockerfiles.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;The Comet project welcomes new contributors. We use the same &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html#slack-and-discord"&gt;Slack and Discord&lt;/a&gt; channels as the main DataFusion
project, and there is a Comet community video call held every four weeks on Wednesdays at 11:30 a.m. Eastern Time,
which is 16:30 UTC during Eastern Standard Time and 15:30 UTC during Eastern Daylight Time. See the
&lt;a href="https://docs.google.com/document/d/1NBpkIAuU7O9h8Br5CbFksDhX-L9TyO9wmGLPMe0Plc8/edit?usp=sharing"&gt;Comet Community Meeting&lt;/a&gt; Google Document for the next scheduled meeting date, the video call link, and
recordings of previous calls.&lt;/p&gt;
&lt;p&gt;The easiest way to get involved is to test Comet with your current Spark jobs and file issues for any bugs or
performance regressions that you find. See the &lt;a href="https://datafusion.apache.org/comet/user-guide/installation.html"&gt;Getting Started&lt;/a&gt; guide for instructions on downloading and installing
Comet.&lt;/p&gt;
&lt;p&gt;There are also many &lt;a href="https://github.com/apache/datafusion-comet/contribute"&gt;good first issues&lt;/a&gt; waiting for contributions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Announcing Apache Arrow DataFusion is now Apache DataFusion</title><link href="https://datafusion.apache.org/blog/2024/05/07/datafusion-tlp" rel="alternate"></link><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-05-07:/blog/2024/05/07/datafusion-tlp</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;TLDR; &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; DataFusion --&amp;gt; &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Arrow PMC and newly created DataFusion PMC are happy to announce that as of
April 16, 2024 the Apache Arrow DataFusion subproject is now a top level
&lt;a href="https://www.apache.org/"&gt;Apache Software Foundation&lt;/a&gt; project.&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Apache DataFusion is a fast, extensible query engine for building …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;TLDR; &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; DataFusion --&amp;gt; &lt;a href="https://datafusion.apache.org/"&gt;Apache DataFusion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The Arrow PMC and newly created DataFusion PMC are happy to announce that as of
April 16, 2024 the Apache Arrow DataFusion subproject is now a top level
&lt;a href="https://www.apache.org/"&gt;Apache Software Foundation&lt;/a&gt; project.&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Apache DataFusion is a fast, extensible query engine for building high-quality
data-centric systems in Rust, using the Apache Arrow in-memory format.&lt;/p&gt;
&lt;p&gt;When DataFusion was &lt;a href="https://arrow.apache.org/blog/2019/02/04/datafusion-donation/"&gt;donated to the Apache Software Foundation&lt;/a&gt; in 2019, the
DataFusion community was not large enough to stand on its own and the Arrow
project agreed to help support it. The community has grown significantly since
2019, benefiting immensely from being part of Arrow and following &lt;a href="https://www.apache.org/theapacheway/"&gt;The Apache
Way&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Why now?&lt;/h2&gt;
&lt;p&gt;The community &lt;a href="https://github.com/apache/datafusion/discussions/6475"&gt;discussed graduating to a top level project publicly&lt;/a&gt; for almost
a year, as the project seemed ready to stand on its own and would benefit from
more focused governance. For example, earlier in DataFusion's life many
contributed to both &lt;a href="https://github.com/apache/arrow-rs"&gt;arrow-rs&lt;/a&gt; and DataFusion, but as DataFusion has matured many
contributors, committers and PMC members focused more and more exclusively on
DataFusion.&lt;/p&gt;
&lt;h2&gt;Looking forward&lt;/h2&gt;
&lt;p&gt;The future looks bright. There are now &lt;a href="https://datafusion.apache.org/user-guide/introduction.html#known-users"&gt;10s of known projects built with
DataFusion&lt;/a&gt;, and that number continues to grow. We recently held our &lt;a href="https://github.com/apache/datafusion/discussions/8522"&gt;first in
person meetup&lt;/a&gt; passed &lt;a href="https://github.com/apache/datafusion/stargazers"&gt;5000 stars&lt;/a&gt; on GitHub, &lt;a href="https://github.com/apache/datafusion/issues/8373#issuecomment-2025133714"&gt;wrote a paper that was accepted
at SIGMOD 2024&lt;/a&gt;, and began work on &lt;a href="https://github.com/apache/datafusion-comet"&gt;Comet&lt;/a&gt;, an &lt;a href="https://spark.apache.org/"&gt;Apache Spark&lt;/a&gt; accelerator
&lt;a href="https://arrow.apache.org/blog/2024/03/06/comet-donation/"&gt;initially donated by Apple&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Thank you to everyone in the Arrow community who helped DataFusion grow and
mature over the years, and we look forward to continuing our collaboration as
projects. All future blogs and announcements will be posted on the &lt;a href="https://datafusion.apache.org/"&gt;Apache
DataFusion&lt;/a&gt; website.&lt;/p&gt;
&lt;h2&gt;Get Involved&lt;/h2&gt;
&lt;p&gt;If you are interested in joining the community, we would love to have you join
us. Get in touch using &lt;a href="https://datafusion.apache.org/contributor-guide/communication.html"&gt;Communication Doc&lt;/a&gt; and learn how to get involved in the
&lt;a href="https://datafusion.apache.org/contributor-guide/index.html"&gt;Contributor Guide&lt;/a&gt;. We welcome everyone to try DataFusion on their
own data and projects and let us know how it goes, contribute suggestions,
documentation, bug reports, or a PR with documentation, tests or code.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Announcing Apache Arrow DataFusion Comet</title><link href="https://datafusion.apache.org/blog/2024/03/06/comet-donation" rel="alternate"></link><published>2024-03-06T00:00:00+00:00</published><updated>2024-03-06T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-03-06:/blog/2024/03/06/comet-donation</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The Apache Arrow PMC is pleased to announce the donation of the &lt;a href="https://github.com/apache/arrow-datafusion-comet"&gt;Comet project&lt;/a&gt;,
a native Spark SQL Accelerator built on &lt;a href="https://arrow.apache.org/datafusion"&gt;Apache Arrow DataFusion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Comet is an Apache Spark plugin that uses Apache Arrow DataFusion to
accelerate Spark workloads. It is designed as a drop-in
replacement for Spark's JVM …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;The Apache Arrow PMC is pleased to announce the donation of the &lt;a href="https://github.com/apache/arrow-datafusion-comet"&gt;Comet project&lt;/a&gt;,
a native Spark SQL Accelerator built on &lt;a href="https://arrow.apache.org/datafusion"&gt;Apache Arrow DataFusion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Comet is an Apache Spark plugin that uses Apache Arrow DataFusion to
accelerate Spark workloads. It is designed as a drop-in
replacement for Spark's JVM based SQL execution engine and offers significant
performance improvements for some workloads as shown below.&lt;/p&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Adaptive Arrow schema architecture overview." class="img-responsive" src="/blog/images/datafusion-comet/comet-architecture.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 1&lt;/b&gt;: With Comet, users interact with the same Spark ecosystem, tools
    and APIs such as Spark SQL. Queries still run through Spark's query optimizer and planner. 
    However, the execution is delegated to Comet,
    which is significantly faster and more resource efficient than a JVM based
    implementation.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Comet is one of a growing class of projects that aim to accelerate Spark using
native columnar engines such as the proprietary &lt;a href="https://www.databricks.com/product/photon"&gt;Databricks Photon Engine&lt;/a&gt; and
open source projects &lt;a href="https://incubator.apache.org/projects/gluten.html"&gt;Gluten&lt;/a&gt;, &lt;a href="https://github.com/NVIDIA/spark-rapids"&gt;Spark RAPIDS&lt;/a&gt;, and &lt;a href="https://github.com/kwai/blaze"&gt;Blaze&lt;/a&gt; (also built using
DataFusion).&lt;/p&gt;
&lt;p&gt;Comet was originally implemented at Apple and the engineers who worked on the
project are also significant contributors to Arrow and DataFusion. Bringing 
Comet into the Apache Software Foundation will accelerate its development and 
grow its community of contributors and users.&lt;/p&gt;
&lt;h1&gt;Get Involved&lt;/h1&gt;
&lt;p&gt;Comet is still in the early stages of development and we would love to have you
join us and help shape the project. We are working on an initial release, and 
expect to post another update with more details at that time.&lt;/p&gt;
&lt;p&gt;Before then, here are some ways to get involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Learn more by visiting the &lt;a href="https://github.com/apache/arrow-datafusion-comet"&gt;Comet project&lt;/a&gt; page and reading the &lt;a href="https://lists.apache.org/thread/0q1rb11jtpopc7vt1ffdzro0omblsh0s"&gt;mailing list
  discussion&lt;/a&gt; about the initial donation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Help us plan out the &lt;a href="https://github.com/apache/arrow-datafusion-comet/issues/19"&gt;roadmap&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Try out the project and provide feedback, file issues, and contribute code.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 34.0.0 Released, Looking Forward to 2024</title><link href="https://datafusion.apache.org/blog/2024/01/19/datafusion-34.0.0" rel="alternate"></link><published>2024-01-19T00:00:00+00:00</published><updated>2024-01-19T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2024-01-19:/blog/2024/01/19/datafusion-34.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We recently &lt;a href="https://crates.io/crates/datafusion/34.0.0"&gt;released DataFusion 34.0.0&lt;/a&gt;. This blog highlights some of the major
improvements since we &lt;a href="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/."&gt;released DataFusion 26.0.0&lt;/a&gt; (spoiler alert there are many)
and a preview of where the community plans to focus in the next 6 months.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;We recently &lt;a href="https://crates.io/crates/datafusion/34.0.0"&gt;released DataFusion 34.0.0&lt;/a&gt;. This blog highlights some of the major
improvements since we &lt;a href="https://arrow.apache.org/blog/2023/06/24/datafusion-25.0.0/."&gt;released DataFusion 26.0.0&lt;/a&gt; (spoiler alert there are many)
and a preview of where the community plans to focus in the next 6 months.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query engine, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that
uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion is used by developers to
create new, fast data centric systems such as databases, dataframe libraries,
machine learning and streaming applications. While &lt;a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#project-goals"&gt;DataFusion&amp;rsquo;s primary design
goal&lt;/a&gt; is to accelerate creating other data centric systems, it has a
reasonable experience directly out of the box as a &lt;a href="https://arrow.apache.org/datafusion-python/"&gt;dataframe library&lt;/a&gt; and
&lt;a href="https://arrow.apache.org/datafusion/user-guide/cli.html"&gt;command line SQL tool&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This may also be our last update on the Apache Arrow Site. Future
updates will likely be on the DataFusion website as we are working to &lt;a href="https://github.com/apache/arrow-datafusion/discussions/6475"&gt;graduate
to a top level project&lt;/a&gt; (Apache Arrow DataFusion &amp;rarr; Apache DataFusion!) which
will help focus governance and project growth. Also exciting, our &lt;a href="https://github.com/apache/arrow-datafusion/discussions/8522"&gt;first
DataFusion in person meetup&lt;/a&gt; is planned for March 2024.&lt;/p&gt;
&lt;p&gt;DataFusion is very much a community endeavor. Our core thesis is that as a
community we can build much more advanced technology than any of us as
individuals or companies could alone. In the last 6 months between &lt;code&gt;26.0.0&lt;/code&gt; and
&lt;code&gt;34.0.0&lt;/code&gt;, community growth has been strong. We accepted and reviewed over a
thousand PRs from 124 different committers, created over 650 issues and closed 517
of them.
You can find a list of all changes in the detailed &lt;a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt;.&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 26.0.0..34.0.0 . | wc -l
     1009

$ git shortlog -sn 26.0.0..34.0.0 . | wc -l
      124

https://crates.io/crates/datafusion/26.0.0
DataFusion 26 released June 7, 2023

https://crates.io/crates/datafusion/34.0.0
DataFusion 34 released Dec 17, 2023

Issues created in this time: 214 open, 437 closed
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+created%3A2023-06-23..2023-12-17

Issues closes: 517
https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+closed%3A2023-06-23..2023-12-17+

PRs merged in this time 908
https://github.com/apache/arrow-datafusion/pulls?q=is%3Apr+merged%3A2023-06-23..2023-12-17
--&gt;
&lt;h1&gt;Improved Performance 🚀&lt;/h1&gt;
&lt;p&gt;Performance is a key feature of DataFusion, DataFusion is 
more than 2x faster on &lt;a href="https://benchmark.clickhouse.com/"&gt;ClickBench&lt;/a&gt; compared to version &lt;code&gt;25.0.0&lt;/code&gt;, as shown below:&lt;/p&gt;
&lt;!--
  Scripts: https://github.com/alamb/datafusion-duckdb-benchmark/tree/datafusion-25-34
  Spreadsheet: https://docs.google.com/spreadsheets/d/1FtI3652WIJMC5LmJbLfT3G06w0JQIxEPG4yfMafexh8/edit#gid=1879366976
  Average runtime on 25.0.0: 7.2s (for the queries that actually ran)
  Average runtime on 34.0.0: 3.6s (for the same queries that ran in 25.0.0)
--&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Adaptive Arrow schema architecture overview." class="img-responsive" src="/blog/images/datafusion-34.0.0/compare-new.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 1&lt;/b&gt;: Performance improvement between &lt;code&gt;25.0.0&lt;/code&gt; and &lt;code&gt;34.0.0&lt;/code&gt; on ClickBench. 
    Note that DataFusion &lt;code&gt;25.0.0&lt;/code&gt;, could not run several queries due to 
    unsupported SQL (Q9, Q11, Q12, Q14) or memory requirements (Q33).
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure style="text-align: center;"&gt;
&lt;img alt="Fig 1: Adaptive Arrow schema architecture overview." class="img-responsive" src="/blog/images/datafusion-34.0.0/compare.png" width="100%"/&gt;
&lt;figcaption&gt;
&lt;b&gt;Figure 2&lt;/b&gt;: Total query runtime for DataFusion &lt;code&gt;34.0.0&lt;/code&gt; and DataFusion &lt;code&gt;25.0.0&lt;/code&gt;.
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Here are some specific enhancements we have made to improve performance:
* &lt;a href="https://arrow.apache.org/blog/2023/08/05/datafusion_fast_grouping/"&gt;2-3x better aggregation performance with many distinct groups&lt;/a&gt;
* Partially ordered grouping / streaming grouping
* [Specialized operator for "TopK" &lt;code&gt;ORDER BY LIMIT XXX&lt;/code&gt;] 
* [Specialized operator for &lt;code&gt;min(col) GROUP BY .. ORDER by min(col) LIMIT XXX&lt;/code&gt;]
* &lt;a href="https://github.com/apache/arrow-datafusion/pull/8126"&gt;Improved join performance&lt;/a&gt;
* Eliminate redundant sorting with sort order aware optimizers&lt;/p&gt;
&lt;h1&gt;New Features ✨&lt;/h1&gt;
&lt;h2&gt;DML / Insert / Creating Files&lt;/h2&gt;
&lt;p&gt;DataFusion now supports writing data in parallel, to individual or multiple
files, using &lt;code&gt;Parquet&lt;/code&gt;, &lt;code&gt;CSV&lt;/code&gt;, &lt;code&gt;JSON&lt;/code&gt;, &lt;code&gt;ARROW&lt;/code&gt; and user defined formats.
&lt;a href="https://github.com/apache/arrow-datafusion/pull/7655"&gt;Benchmark results&lt;/a&gt; show improvements up to 5x in some cases.&lt;/p&gt;
&lt;p&gt;Similarly to reading, data can now be written to any [&lt;code&gt;ObjectStore&lt;/code&gt;]
implementation, including AWS S3, Azure Blob Storage, GCP Cloud Storage, local
files, and user defined implementations. While reading from &lt;a href="https://docs.rs/datafusion/latest/datafusion/datasource/listing/struct.ListingTable.html#features"&gt;hive style
partitioned tables&lt;/a&gt; has long been supported, it is now possible to write to such
tables as well.&lt;/p&gt;
&lt;p&gt;For example, to write to a local file:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ CREATE EXTERNAL TABLE awesome_table(x INT) STORED AS PARQUET LOCATION '/tmp/my_awesome_table';
0 rows in set. Query took 0.003 seconds.

❯ INSERT INTO awesome_table SELECT x * 10 FROM my_source_table;
+-------+
| count |
+-------+
| 3     |
+-------+
1 row in set. Query took 0.024 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also write to files with the [&lt;code&gt;COPY&lt;/code&gt;], similarly to [DuckDB&amp;rsquo;s &lt;code&gt;COPY&lt;/code&gt;]:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ COPY (SELECT x + 1 FROM my_source_table) TO '/tmp/output.json';
+-------+
| count |
+-------+
| 3     |
+-------+
1 row in set. Query took 0.014 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-shell"&gt;$ cat /tmp/output.json
{"x":1}
{"x":2}
{"x":3}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Improved &lt;code&gt;STRUCT&lt;/code&gt; and &lt;code&gt;ARRAY&lt;/code&gt; support&lt;/h2&gt;
&lt;p&gt;DataFusion &lt;code&gt;34.0.0&lt;/code&gt; has much improved &lt;code&gt;STRUCT&lt;/code&gt; and &lt;code&gt;ARRAY&lt;/code&gt;
support, including a full range of &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/scalar_functions.html#struct-functions"&gt;struct functions&lt;/a&gt; and &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/scalar_functions.html#array-functions"&gt;array functions&lt;/a&gt;.&lt;/p&gt;
&lt;!--
❯ create table my_table as values ([1,2,3]), ([2]), ([4,5]);
--&gt;
&lt;p&gt;For example, you can now use &lt;code&gt;[]&lt;/code&gt; syntax and &lt;code&gt;array_length&lt;/code&gt; to access and inspect arrays:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ SELECT column1, 
         column1[1] AS first_element, 
         array_length(column1) AS len 
  FROM my_table;
+-----------+---------------+-----+
| column1   | first_element | len |
+-----------+---------------+-----+
| [1, 2, 3] | 1             | 3   |
| [2]       | 2             | 1   |
| [4, 5]    | 4             | 2   |
+-----------+---------------+-----+
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ SELECT column1, column1['c0'] FROM  my_table;
+------------------+----------------------+
| column1          | my_table.column1[c0] |
+------------------+----------------------+
| {c0: foo, c1: 1} | foo                  |
| {c0: bar, c1: 2} | bar                  |
+------------------+----------------------+
2 rows in set. Query took 0.002 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Other Features&lt;/h2&gt;
&lt;p&gt;Other notable features include:
* Support aggregating datasets that exceed memory size, with &lt;a href="https://github.com/apache/arrow-datafusion/pull/7400"&gt;group by spill to disk&lt;/a&gt;
* All operators now track and limit their memory consumption, including Joins&lt;/p&gt;
&lt;h1&gt;Building Systems is Easier with DataFusion 🛠️&lt;/h1&gt;
&lt;h2&gt;Documentation&lt;/h2&gt;
&lt;p&gt;It is easier than ever to get started using DataFusion with the
new &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/index.html"&gt;Library Users Guide&lt;/a&gt; as well as significantly improved the &lt;a href="https://docs.rs/datafusion/latest/datafusion/index.html"&gt;API documentation&lt;/a&gt;. &lt;/p&gt;
&lt;h2&gt;User Defined Window and Table Functions&lt;/h2&gt;
&lt;p&gt;In addition to DataFusion's &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-a-scalar-udf"&gt;User Defined Scalar Functions&lt;/a&gt;, and &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-an-aggregate-udf"&gt;User Defined Aggregate Functions&lt;/a&gt;, DataFusion now supports &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-a-window-udf"&gt;User Defined Window Functions&lt;/a&gt; 
 and &lt;a href="https://arrow.apache.org/datafusion/library-user-guide/adding-udfs.html#adding-a-user-defined-table-function"&gt;User Defined Table Functions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For example, [the &lt;code&gt;datafusion-cli&lt;/code&gt;] implements a DuckDB style [&lt;code&gt;parquet_metadata&lt;/code&gt;]
function as a user defined table function (&lt;a href="https://github.com/apache/arrow-datafusion/blob/3f219bc929cfd418b0e3d3501f8eba1d5a2c87ae/datafusion-cli/src/functions.rs#L222-L248"&gt;source code here&lt;/a&gt;): &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;❯ SELECT 
      path_in_schema, row_group_id, row_group_num_rows, stats_min, stats_max, total_compressed_size 
FROM 
      parquet_metadata('hits.parquet')
WHERE path_in_schema = '"WatchID"' 
LIMIT 3;

+----------------+--------------+--------------------+---------------------+---------------------+-----------------------+
| path_in_schema | row_group_id | row_group_num_rows | stats_min           | stats_max           | total_compressed_size |
+----------------+--------------+--------------------+---------------------+---------------------+-----------------------+
| "WatchID"      | 0            | 450560             | 4611687214012840539 | 9223369186199968220 | 3883759               |
| "WatchID"      | 1            | 612174             | 4611689135232456464 | 9223371478009085789 | 5176803               |
| "WatchID"      | 2            | 344064             | 4611692774829951781 | 9223363791697310021 | 3031680               |
+----------------+--------------+--------------------+---------------------+---------------------+-----------------------+
3 rows in set. Query took 0.053 seconds.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Growth of DataFusion 📈&lt;/h3&gt;
&lt;p&gt;DataFusion has been appearing more publically in the wild. For example
* New projects built using DataFusion such as &lt;a href="https://lancedb.com/"&gt;lancedb&lt;/a&gt;, &lt;a href="https://glaredb.com/"&gt;GlareDB&lt;/a&gt;, &lt;a href="https://www.arroyo.dev/"&gt;Arroyo&lt;/a&gt;, and &lt;a href="https://github.com/cmu-db/optd"&gt;optd&lt;/a&gt;.
* Public talks such as &lt;a href="https://www.youtube.com/watch?v=AJU9rdRNk9I"&gt;Apache Arrow Datafusion: Vectorized
  Execution Framework For Maximum Performance&lt;/a&gt; in &lt;a href="https://www.bagevent.com/event/8432178"&gt;CommunityOverCode Asia 2023&lt;/a&gt; 
* Blogs posts such as &lt;a href="https://www.synnada.ai/blog/apache-arrow-arrow-datafusion-ai-native-data-infra-an-interview-with-our-ceo-ozan"&gt;Apache Arrow, Arrow/DataFusion, AI-native Data Infra&lt;/a&gt;,
  &lt;a href="https://www.influxdata.com/blog/flight-datafusion-arrow-parquet-fdap-architecture-influxdb/"&gt;Flight, DataFusion, Arrow, and Parquet: Using the FDAP Architecture to build InfluxDB 3.0&lt;/a&gt;, and 
  &lt;a href="https://www.linkedin.com/pulse/guide-user-defined-functions-apache-arrow-datafusion-dade-aderemi/"&gt;A Guide to User-Defined Functions in Apache Arrow DataFusion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We have also &lt;a href="https://github.com/apache/arrow-datafusion/issues/6782"&gt;submitted a paper&lt;/a&gt; to &lt;a href="https://2024.sigmod.org/"&gt;SIGMOD 2024&lt;/a&gt;, one of the
premiere database conferences, describing DataFusion in a technically formal
style and making the case that it is possible to create a modular and extensive query engine 
without sacrificing performance. We hope this paper helps people 
evaluating DataFusion for their needs understand it better.&lt;/p&gt;
&lt;h1&gt;DataFusion in 2024 🥳&lt;/h1&gt;
&lt;p&gt;Some major initiatives from contributors we know of this year are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Modularity&lt;/em&gt;: Make DataFusion even more modular, such as &lt;a href="https://github.com/apache/arrow-datafusion/issues/8045"&gt;unifying
   built in and user functions&lt;/a&gt;, making it easier to customize 
   DataFusion's behavior.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Community Growth&lt;/em&gt;: Graduate to our own top level Apache project, and
   subsequently add more committers and PMC members to keep pace with project
   growth.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Use case white papers&lt;/em&gt;: Write blog posts and videos explaining
   how to use DataFusion for real-world use cases.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Testing&lt;/em&gt;: Improve CI infrastructure and test coverage, more fuzz
   testing, and better functional and performance regression testing.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Planning Time&lt;/em&gt;: Reduce the time taken to plan queries, both &lt;a href="https://github.com/apache/arrow-datafusion/issues/7698"&gt;wide
   tables of 1000s of columns&lt;/a&gt;, and in &lt;a href="https://github.com/apache/arrow-datafusion/issues/5637"&gt;general&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Aggregate Performance&lt;/em&gt;: Improve the speed of &lt;a href="https://github.com/apache/arrow-datafusion/issues/7000"&gt;aggregating "high cardinality"&lt;/a&gt; data
   when there are many (e.g. millions) of distinct groups.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Statistics&lt;/em&gt;: &lt;a href="https://github.com/apache/arrow-datafusion/issues/8227"&gt;Improved statistics handling&lt;/a&gt; with an eye towards more
   sophisticated expression analysis and cost models.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion we would love to have you
join us. You can try out DataFusion on some of your own data and projects and
let us know how it goes, contribute suggestions, documentation, bug reports, or
a PR with documentation, tests or code. A list of open issues
suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As the community grows, we are also looking to restart biweekly calls /
meetings. Timezones are always a challenge for such meetings, but we hope to
have two calls that can work for most attendees. If you are interested
in helping, or just want to say hi, please drop us a note via one of 
the methods listed in our &lt;a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html"&gt;Communication Doc&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Aggregating Millions of Groups Fast in Apache Arrow DataFusion 28.0.0</title><link href="https://datafusion.apache.org/blog/2023/08/05/datafusion_fast_grouping" rel="alternate"></link><published>2023-08-05T00:00:00+00:00</published><updated>2023-08-05T00:00:00+00:00</updated><author><name>alamb, Dandandan, tustvold</name></author><id>tag:datafusion.apache.org,2023-08-05:/blog/2023/08/05/datafusion_fast_grouping</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- Converted from Google Docs using https://www.buymeacoffee.com/docstomarkdown --&gt;
&lt;h2&gt;Aggregating Millions of Groups Fast in Apache Arrow DataFusion&lt;/h2&gt;
&lt;p&gt;Andrew Lamb, Dani&amp;euml;l Heres, Raphael Taylor-Davies,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: this article was originally published on the &lt;a href="https://www.influxdata.com/blog/aggregating-millions-groups-fast-apache-arrow-datafusion"&gt;InfluxData Blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;TLDR&lt;/h2&gt;
&lt;p&gt;Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. &lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt;&amp;rsquo;s parallel aggregation capability …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;!-- Converted from Google Docs using https://www.buymeacoffee.com/docstomarkdown --&gt;
&lt;h2&gt;Aggregating Millions of Groups Fast in Apache Arrow DataFusion&lt;/h2&gt;
&lt;p&gt;Andrew Lamb, Dani&amp;euml;l Heres, Raphael Taylor-Davies,&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: this article was originally published on the &lt;a href="https://www.influxdata.com/blog/aggregating-millions-groups-fast-apache-arrow-datafusion"&gt;InfluxData Blog&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;TLDR&lt;/h2&gt;
&lt;p&gt;Grouped aggregations are a core part of any analytic tool, creating understandable summaries of huge data volumes. &lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt;&amp;rsquo;s parallel aggregation capability is 2-3x faster in the &lt;a href="https://crates.io/crates/datafusion/28.0.0"&gt;newly released version &lt;code&gt;28.0.0&lt;/code&gt;&lt;/a&gt; for queries with a large number (10,000 or more) of groups.&lt;/p&gt;
&lt;p&gt;Improving aggregation performance matters to all users of DataFusion. For example, both InfluxDB, a &lt;a href="https://github.com/influxdata/influxdb"&gt;time series data platform&lt;/a&gt; and Coralogix, a &lt;a href="https://coralogix.com/?utm_source=InfluxDB&amp;amp;utm_medium=Blog&amp;amp;utm_campaign=organic"&gt;full-stack observability&lt;/a&gt; platform, aggregate vast amounts of raw data to monitor and create insights for our customers. Improving DataFusion&amp;rsquo;s performance lets us provide better user experiences by generating insights faster with fewer resources. Because DataFusion is open source and released under the permissive &lt;a href="https://github.com/apache/arrow-datafusion/blob/main/LICENSE.txt"&gt;Apache 2.0&lt;/a&gt; license, the whole DataFusion community benefits as well.&lt;/p&gt;
&lt;p&gt;With the new optimizations, DataFusion&amp;rsquo;s grouping speed is now close to DuckDB, a system that regularly reports &lt;a href="https://duckdblabs.github.io/db-benchmark/"&gt;great&lt;/a&gt; &lt;a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html#experiments"&gt;grouping&lt;/a&gt; benchmark performance numbers. Figure 1 contains a representative sample of &lt;a href="https://github.com/ClickHouse/ClickBench/tree/main"&gt;ClickBench&lt;/a&gt; on a single Parquet file, and the full results are at the end of this article.&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/datafusion_fast_grouping/summary.png" width="700"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;: Query performance for ClickBench queries on queries 16, 17, 18 and 19 on a single Parquet file for DataFusion &lt;code&gt;27.0.0&lt;/code&gt;, DataFusion &lt;code&gt;28.0.0&lt;/code&gt; and DuckDB &lt;code&gt;0.8.1&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Introduction to high cardinality grouping&lt;/h2&gt;
&lt;p&gt;Aggregation is a fancy word for computing summary statistics across many rows that have the same value in one or more columns. We call the rows with the same values &lt;em&gt;groups&lt;/em&gt; and &amp;ldquo;high cardinality&amp;rdquo; means there are a large number of distinct groups in the dataset. At the time of writing, a &amp;ldquo;large&amp;rdquo; number of groups in analytic engines is around 10,000.&lt;/p&gt;
&lt;p&gt;For example the &lt;a href="https://github.com/ClickHouse/ClickBench"&gt;ClickBench&lt;/a&gt; &lt;em&gt;hits&lt;/em&gt; dataset contains 100 million anonymized user clicks across a set of websites. ClickBench Query 17 is:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT "UserID", "SearchPhrase", COUNT(*)
FROM hits
GROUP BY "UserID", "SearchPhrase"
ORDER BY COUNT(*)
DESC LIMIT 10;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In English, this query finds &amp;ldquo;the top ten (user, search phrase) combinations, across all clicks&amp;rdquo; and produces the following results (there are no search phrases for the top ten users):&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;+---------------------+--------------+-----------------+
| UserID              | SearchPhrase | COUNT(UInt8(1)) |
+---------------------+--------------+-----------------+
| 1313338681122956954 |              | 29097           |
| 1907779576417363396 |              | 25333           |
| 2305303682471783379 |              | 10597           |
| 7982623143712728547 |              | 6669            |
| 7280399273658728997 |              | 6408            |
| 1090981537032625727 |              | 6196            |
| 5730251990344211405 |              | 6019            |
| 6018350421959114808 |              | 5990            |
| 835157184735512989  |              | 5209            |
| 770542365400669095  |              | 4906            |
+---------------------+--------------+-----------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The ClickBench dataset contains&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;99,997,497 total rows[^1]&lt;/li&gt;
&lt;li&gt;17,630,976 different users (distinct UserIDs)[^2]&lt;/li&gt;
&lt;li&gt;6,019,103 different search phrases[^3]&lt;/li&gt;
&lt;li&gt;24,070,560 distinct combinations[^4] of (UserID, SearchPhrase)
Thus, to answer the query, DataFusion must map each of the 100M different input rows into one of the &lt;strong&gt;24 million different groups&lt;/strong&gt;, and keep count of how many such rows there are in each group.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The solution&lt;/h2&gt;
&lt;p&gt;Like most concepts in databases and other analytic systems, the basic ideas of this algorithm are straightforward and taught in introductory computer science courses. You could compute the query with a program such as this[^5]:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
from collections import defaultdict
from operator import itemgetter

# read file
hits = pd.read_parquet('hits.parquet', engine='pyarrow')

# build groups
counts = defaultdict(int)
for index, row in hits.iterrows():
    group = (row['UserID'], row['SearchPhrase']);
    # update the dict entry for the corresponding key
    counts[group] += 1

# Print the top 10 values
print (dict(sorted(counts.items(), key=itemgetter(1), reverse=True)[:10]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach, while simple, is both slow and very memory inefficient. It requires over 40 seconds to compute the results for less than 1% of the dataset[^6]. Both DataFusion &lt;code&gt;28.0.0&lt;/code&gt; and DuckDB &lt;code&gt;0.8.1&lt;/code&gt; compute results in under 10 seconds for the &lt;em&gt;entire&lt;/em&gt; dataset.&lt;/p&gt;
&lt;p&gt;To answer this query quickly and efficiently, you have to write your code such that it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Keeps all cores busy aggregating via parallelized computation&lt;/li&gt;
&lt;li&gt;Updates aggregate values quickly, using vectorizable loops that are easy for compilers to translate into the high performance &lt;a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data"&gt;SIMD&lt;/a&gt; instructions available in modern CPUs.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The rest of this article explains how grouping works in DataFusion and the improvements we made in &lt;code&gt;28.0.0&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Two phase parallel partitioned grouping&lt;/h3&gt;
&lt;p&gt;Both DataFusion &lt;code&gt;27.0.&lt;/code&gt; and &lt;code&gt;28.0.0&lt;/code&gt; use state-of-the-art, two phase parallel hash partitioned grouping, similar to other high-performance vectorized engines like &lt;a href="https://duckdb.org/2022/03/07/aggregate-hashtable.html"&gt;DuckDB&amp;rsquo;s Parallel Grouped Aggregates&lt;/a&gt;. In pictures this looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-text"&gt;            ▲                        ▲
            &amp;boxv;                        &amp;boxv;
            &amp;boxv;                        &amp;boxv;
            &amp;boxv;                        &amp;boxv;
&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv;        GroupBy        &amp;boxv;  &amp;boxv;      GroupBy      &amp;boxv;      Step 4
&amp;boxv;        (Final)        &amp;boxv;  &amp;boxv;      (Final)      &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
            ▲                        ▲
            &amp;boxv;                        &amp;boxv;
            &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhd;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
                         &amp;boxv;
                         &amp;boxv;
            &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
            &amp;boxv;       Repartition       &amp;boxv;               Step 3
            &amp;boxv;         HASH(x)         &amp;boxv;
            &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
                         ▲
                         &amp;boxv;
            &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxhu;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
            &amp;boxv;                       &amp;boxv;
            &amp;boxv;                       &amp;boxv;
 &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
 &amp;boxv;      GroupyBy      &amp;boxv;  &amp;boxv;       GroupBy       &amp;boxv;      Step 2
 &amp;boxv;     (Partial)      &amp;boxv;  &amp;boxv;      (Partial)      &amp;boxv;
 &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
            ▲                       ▲
         &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxul;                       &amp;boxur;&amp;boxh;&amp;boxdl;
         &amp;boxv;                            &amp;boxv;
    .&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;.                  .&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;.
 ,&amp;boxh;'           '&amp;boxh;.            ,&amp;boxh;'           '&amp;boxh;.
;      Input      :          ;      Input      :      Step 1
:    Stream 1     ;          :    Stream 2     ;
 ╲               ╱            ╲               ╱
  '&amp;boxh;.         ,&amp;boxh;'              '&amp;boxh;.         ,&amp;boxh;'
     `&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;'                    `&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 2&lt;/strong&gt;: Two phase repartitioned grouping: data flows from bottom (source) to top (results) in two phases. First (Steps 1 and 2), each core reads the data into a core-specific hash table, computing intermediate aggregates without any cross-core coordination. Then (Steps 3 and 4) DataFusion divides the data (&amp;ldquo;repartitions&amp;rdquo;) into distinct subsets by group value, and each subset is sent to a specific core which computes the final aggregate.&lt;/p&gt;
&lt;p&gt;The two phases are critical for keeping cores busy in a multi-core system. Both phases use the same hash table approach (explained in the next section), but differ in how the groups are distributed and the partial results emitted from the accumulators. The first phase aggregates data as soon as possible after it is produced. However, as shown in Figure 2, the groups can be anywhere in any input, so the same group is often found on many different cores. The second phase uses a hash function to redistribute data evenly across the cores, so each group value is processed by exactly one core which emits the final results for that group.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  2  &amp;boxv;    &amp;boxv;  4  &amp;boxv;   2. After Repartitioning: each
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;   group key  appears in exactly
    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;   one partition
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  2  &amp;boxv;    &amp;boxv;  4  &amp;boxv;
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;

&amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh;

    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
    &amp;boxv;  2  &amp;boxv;    &amp;boxv;  2  &amp;boxv;
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  2  &amp;boxv;
    &amp;boxv;  3  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  4  &amp;boxv;    &amp;boxv;  1  &amp;boxv;
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    1. Input Stream: groups
      ...        ...      values are spread
    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;    arbitrarily over each input
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  4  &amp;boxv;
    &amp;boxv;  4  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  1  &amp;boxv;    &amp;boxv;  1  &amp;boxv;
    &amp;boxv;  4  &amp;boxv;    &amp;boxv;  3  &amp;boxv;
    &amp;boxv;  3  &amp;boxv;    &amp;boxv;  2  &amp;boxv;
    &amp;boxv;  2  &amp;boxv;    &amp;boxv;  2  &amp;boxv;
    &amp;boxv;  2  &amp;boxv;    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;

    Core A      Core B

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 3&lt;/strong&gt;: Group value distribution across 2 cores during aggregation phases. In the first phase, every group value &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt;, &lt;code&gt;3&lt;/code&gt;, &lt;code&gt;4&lt;/code&gt;, is present in the input stream processed by each core. In the second phase, after repartitioning, the group values &lt;code&gt;1&lt;/code&gt; and &lt;code&gt;2&lt;/code&gt; are processed by core A, and values &lt;code&gt;3&lt;/code&gt; and &lt;code&gt;4&lt;/code&gt; are processed only by core B.&lt;/p&gt;
&lt;p&gt;There are some additional subtleties in the &lt;a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/core/src/physical_plan/aggregates/row_hash.rs"&gt;DataFusion implementation&lt;/a&gt; not mentioned above due to space constraints, such as:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The policy of when to emit data from the first phase&amp;rsquo;s hash table (e.g. because the data is partially sorted)&lt;/li&gt;
&lt;li&gt;Handling specific filters per aggregate (due to the &lt;code&gt;FILTER&lt;/code&gt; SQL clause)&lt;/li&gt;
&lt;li&gt;Data types of intermediate values (which may not be the same as the final output for some aggregates such as &lt;code&gt;AVG&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Action taken when memory use exceeds its budget.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Hash grouping&lt;/h3&gt;
&lt;p&gt;DataFusion queries can compute many different aggregate functions for each group, both &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/aggregate_functions.html"&gt;built in&lt;/a&gt; and/or user defined &lt;a href="https://docs.rs/datafusion/latest/datafusion/logical_expr/struct.AggregateUDF.html"&gt;&lt;code&gt;AggregateUDFs&lt;/code&gt;&lt;/a&gt;. The state for each aggregate function, called an &lt;em&gt;accumulator&lt;/em&gt;, is tracked with a hash table (DataFusion uses the excellent &lt;a href="https://docs.rs/hashbrown/latest/hashbrown/index.html"&gt;HashBrown&lt;/a&gt; &lt;a href="https://docs.rs/hashbrown/latest/hashbrown/raw/struct.RawTable.html"&gt;RawTable API&lt;/a&gt;), which logically stores the &amp;ldquo;index&amp;rdquo;  identifying the specific group value.&lt;/p&gt;
&lt;h3&gt;Hash grouping in &lt;code&gt;27.0.0&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;As shown in Figure 3, DataFusion &lt;code&gt;27.0.0&lt;/code&gt; stores the data in a &lt;a href="https://github.com/apache/arrow-datafusion/blob/4d93b6a3802151865b68967bdc4c7d7ef425b49a/datafusion/core/src/physical_plan/aggregates/utils.rs#L38-L50"&gt;&lt;code&gt;GroupState&lt;/code&gt;&lt;/a&gt; structure which, unsurprisingly, tracks the state for each group. The state for each group consists of:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The actual value of the group columns, in &lt;a href="https://docs.rs/arrow-row/latest/arrow_row/index.html"&gt;Arrow Row&lt;/a&gt; format.&lt;/li&gt;
&lt;li&gt;In-progress accumulations (e.g. the running counts for the &lt;code&gt;COUNT&lt;/code&gt; aggregate) for each group, in one of two possible formats (&lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/expr/src/accumulator.rs#L24-L49"&gt;&lt;code&gt;Accumulator&lt;/code&gt;&lt;/a&gt;  or &lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/row_accumulator.rs#L26-L46"&gt;&lt;code&gt;RowAccumulator&lt;/code&gt;&lt;/a&gt;).&lt;/li&gt;
&lt;li&gt;Scratch space for tracking which rows match each aggregate in each batch.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;                           &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
                           &amp;boxv;                                      &amp;boxv;
                           &amp;boxv;                  ...                 &amp;boxv;
                           &amp;boxv; ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ &amp;boxv;
                           &amp;boxv; ┃                                  ┃ &amp;boxv;
    &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;            &amp;boxv; ┃ &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; ┃ &amp;boxv;
    &amp;boxv;         &amp;boxv;            &amp;boxv; ┃ &amp;boxv;group values: OwnedRow        &amp;boxv; ┃ &amp;boxv;
    &amp;boxv; &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv;            &amp;boxv; ┃ &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; ┃ &amp;boxv;
    &amp;boxv; &amp;boxv;  5  &amp;boxv; &amp;boxv;            &amp;boxv; ┃ &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; ┃ &amp;boxv;
    &amp;boxv; &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl; &amp;boxv;            &amp;boxv; ┃ &amp;boxv;Row accumulator:              &amp;boxv; ┃ &amp;boxv;
    &amp;boxv; &amp;boxv;  9  &amp;boxv;&amp;boxh;&amp;boxvh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;       &amp;boxv; ┃ &amp;boxv;Vec&amp;lt;u8&amp;gt;                       &amp;boxv; ┃ &amp;boxv;
    &amp;boxv; &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; ┃ &amp;boxv;
    &amp;boxv; &amp;boxv; ... &amp;boxv; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxv;  1  &amp;boxv; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxv;Accumulator 1 &amp;boxv;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxv; ... &amp;boxv; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv; &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxv;Accumulator 2 &amp;boxv;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxv;         &amp;boxv;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;      &amp;boxv;         ┃ &amp;boxv;
    &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;    &amp;boxv;       &amp;boxv; ┃ &amp;boxv; Box&amp;lt;dyn Accumulator&amp;gt; &amp;boxv;         ┃ &amp;boxv;
    Hash Table     &amp;boxv;       &amp;boxv; ┃ &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;         ┃ &amp;boxv;
                   &amp;boxv;       &amp;boxv; ┃ &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;      ┃ &amp;boxv;
                   &amp;boxv;       &amp;boxv; ┃ &amp;boxv;scratch indices: Vec&amp;lt;u32&amp;gt;&amp;boxv;      ┃ &amp;boxv;
                   &amp;boxv;       &amp;boxv; ┃ &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;      ┃ &amp;boxv;
                   &amp;boxv;       &amp;boxv; ┃ GroupState                       ┃ &amp;boxv;
                   &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;▶ &amp;boxv; ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ &amp;boxv;
                           &amp;boxv;                                      &amp;boxv;
  Hash table tracks an     &amp;boxv;                 ...                  &amp;boxv;
  index into group_states  &amp;boxv;                                      &amp;boxv;
                           &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
                           group_states: Vec&amp;lt;GroupState&amp;gt;

                           There is one GroupState PER GROUP

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 4&lt;/strong&gt;: Hash group operator structure in DataFusion &lt;code&gt;27.0.0&lt;/code&gt;. A hash table maps each group to a GroupState which contains all the per-group states.&lt;/p&gt;
&lt;p&gt;To compute the aggregate, DataFusion performs the following steps for each input batch:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Calculate hash using &lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/hash_utils.rs#L264-L307"&gt;efficient vectorized code&lt;/a&gt;, specialized for each data type.&lt;/li&gt;
&lt;li&gt;Determine group indexes for each input row using the hash table (creating new entries for newly seen groups).&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/blob/4ab8be57dee3bfa72dd105fbd7b8901b873a4878/datafusion/core/src/physical_plan/aggregates/row_hash.rs#L562-L602"&gt;Update Accumulators for each group that had input rows,&lt;/a&gt; assembling the rows into a contiguous range for vectorized accumulator if there are a sufficient number of them.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;DataFusion also stores the hash values in the table to avoid potentially costly hash recomputation when resizing the hash table.&lt;/p&gt;
&lt;p&gt;This scheme works very well for a relatively small number of distinct groups: all accumulators are efficiently updated with large contiguous batches of rows.&lt;/p&gt;
&lt;p&gt;However, this scheme is not ideal for high cardinality grouping due to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Multiple allocations per group&lt;/strong&gt; for the group value row format, as well as for the &lt;code&gt;RowAccumulator&lt;/code&gt;s and each  &lt;code&gt;Accumulator&lt;/code&gt;. The &lt;code&gt;Accumulator&lt;/code&gt; may have additional allocations within it as well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Non-vectorized updates:&lt;/strong&gt; Accumulator updates often fall back to a slower non-vectorized form because the number of distinct groups is large (and thus number of values per group is small) in each input batch.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Hash grouping in &lt;code&gt;28.0.0&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;For &lt;code&gt;28.0.0&lt;/code&gt;, we rewrote the core group by implementation following traditional system optimization principles: fewer allocations, type specialization, and aggressive vectorization.&lt;/p&gt;
&lt;p&gt;DataFusion &lt;code&gt;28.0.0&lt;/code&gt; uses the same RawTable and still stores group indexes. The major differences, as shown in Figure 4, are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Group values are stored either&lt;ol&gt;
&lt;li&gt;Inline in the &lt;code&gt;RawTable&lt;/code&gt; (for single columns of primitive types), where the conversion to Row format costs more than its benefit&lt;/li&gt;
&lt;li&gt;In a separate &lt;a href="https://docs.rs/arrow-row/latest/arrow_row/struct.Row.html"&gt;Rows&lt;/a&gt; structure with a single contiguous allocation for all groups values, rather than an allocation per group. Accumulators manage the state for all the groups internally, so the code to update intermediate values is a tight type specialized loop. The new &lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/mod.rs#L66-L75"&gt;&lt;code&gt;GroupsAccumulator&lt;/code&gt;&lt;/a&gt; interface results in highly efficient type accumulator update loops.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;     &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;
&amp;boxv; &amp;boxdr; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxdl;  &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;&amp;boxv;     &amp;boxv; ┏━━━━━━━━━━━━━━━━━━━┓ &amp;boxv;
&amp;boxv;                &amp;boxv;                 &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxdr; &amp;boxh; &amp;boxh; &amp;boxdl;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv; ┃ &amp;boxv;
&amp;boxv;                &amp;boxv;    X   &amp;boxv;  5  &amp;boxv;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxv;  value1   &amp;boxv; &amp;boxv; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxvr; &amp;boxh; &amp;boxh; &amp;boxvl;&amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv; ┃ &amp;boxv;
&amp;boxv;                &amp;boxv;    Q   &amp;boxv;  9  &amp;boxv;&amp;boxh;&amp;boxh;&amp;boxvh;&amp;boxvh;&amp;boxh;&amp;boxh;&amp;boxdl;  &amp;boxv; ┃  &amp;boxv;     ...      &amp;boxv; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxvr; &amp;boxh; &amp;boxh; &amp;boxvl;&amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl;  &amp;boxv;&amp;boxv;  &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxvh;&amp;boxh;╋&amp;boxh;▶&amp;boxv;              &amp;boxv; ┃ &amp;boxv;
&amp;boxv;                &amp;boxv;   ...  &amp;boxv; ... &amp;boxv;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxdr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxdl; &amp;boxv; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxvr; &amp;boxh; &amp;boxh; &amp;boxvl;&amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxv;  valueN   &amp;boxv; &amp;boxv; ┃ &amp;boxv;
&amp;boxv;                &amp;boxv;    H   &amp;boxv;  1  &amp;boxv;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; &amp;boxv; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxvr; &amp;boxh; &amp;boxh; &amp;boxvl;&amp;boxvr;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxvl;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxv;values: Vec&amp;lt;T&amp;gt;&amp;boxv; ┃ &amp;boxv;
&amp;boxv;     Rows       &amp;boxv;   ...  &amp;boxv; ... &amp;boxv;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃  &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul; ┃ &amp;boxv;
&amp;boxv; &amp;boxv;           &amp;boxv;  &amp;boxv; &amp;boxur; &amp;boxh; &amp;boxh; &amp;boxul;&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;  &amp;boxv;&amp;boxv;     &amp;boxv; ┃                   ┃ &amp;boxv;
&amp;boxv;  &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh; &amp;boxh;   &amp;boxv;                 &amp;boxv;&amp;boxv;     &amp;boxv; ┃ GroupsAccumulator ┃ &amp;boxv;
&amp;boxv;                &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;&amp;boxv;     &amp;boxv; ┗━━━━━━━━━━━━━━━━━━━┛ &amp;boxv;
&amp;boxv;                  Hash Table       &amp;boxv;     &amp;boxv;                       &amp;boxv;
&amp;boxv;                                   &amp;boxv;     &amp;boxv;          ...          &amp;boxv;
&amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;     &amp;boxur;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxh;&amp;boxul;
  GroupState                               Accumulators


Hash table value stores group_indexes     One  GroupsAccumulator
and group values.                         per aggregate. Each
                                          stores the state for
Group values are stored either inline     *ALL* groups, typically
in the hash table or in a single          using a native Vec&amp;lt;T&amp;gt;
allocation using the arrow Row format
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Figure 5&lt;/strong&gt;: Hash group operator structure in DataFusion &lt;code&gt;28.0.0&lt;/code&gt;. Group values are stored either directly in the hash table, or in a single allocation using the arrow Row format. The hash table contains group indexes. A single &lt;code&gt;GroupsAccumulator&lt;/code&gt; stores the per-aggregate state for &lt;em&gt;all&lt;/em&gt; groups.&lt;/p&gt;
&lt;p&gt;This new structure improves performance significantly for high cardinality groups due to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reduced allocations&lt;/strong&gt;: There are no longer any individual allocations per group.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Contiguous native accumulator states&lt;/strong&gt;: Type-specialized accumulators store the values for all groups in a single contiguous allocation using a &lt;a href="https://doc.rust-lang.org/std/vec/struct.Vec.html"&gt;Rust Vec&amp;lt;T&amp;gt;&lt;/a&gt; of some native type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vectorized state update&lt;/strong&gt;: The inner aggregate update loops, which are type-specialized and in terms of native &lt;code&gt;Vec&lt;/code&gt;s, are well-vectorized by the Rust compiler (thanks &lt;a href="https://llvm.org/"&gt;LLVM&lt;/a&gt;!).&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Notes&lt;/h3&gt;
&lt;p&gt;Some vectorized grouping implementations store the accumulator state row-wise directly in the hash table, which often uses modern CPU caches efficiently. Managing accumulator state in columnar fashion may sacrifice some cache locality, however it ensures the size of the hash table remains small, even when there are large numbers of groups and aggregates, making it easier for the compiler to vectorize the accumulator update.&lt;/p&gt;
&lt;p&gt;Depending on the cost of recomputing hash values, DataFusion &lt;code&gt;28.0.0&lt;/code&gt; may or may not store the hash values in the table. This optimizes the tradeoff between the cost of computing the hash value (which is expensive for strings, for example) vs. the cost of storing it in the hash table.&lt;/p&gt;
&lt;p&gt;One subtlety that arises from pushing state updates into GroupsAccumulators is that each accumulator must handle similar variations with/without filtering and with/without nulls in the input. DataFusion &lt;code&gt;28.0.0&lt;/code&gt; uses a templated &lt;a href="https://github.com/apache/arrow-datafusion/blob/a6dcd943051a083693c352c6b4279156548490a0/datafusion/physical-expr/src/aggregate/groups_accumulator/accumulate.rs#L28-L54"&gt;&lt;code&gt;NullState&lt;/code&gt;&lt;/a&gt; which encapsulates these common patterns across accumulators.&lt;/p&gt;
&lt;p&gt;The code structure is heavily influenced by the fact DataFusion is implemented using &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, a new(ish) systems programming language focused on speed and safety. Rust heavily discourages many of the traditional pointer casting &amp;ldquo;tricks&amp;rdquo; used in C/C++ hash grouping implementations. The DataFusion aggregation code is almost entirely &lt;a href="https://doc.rust-lang.org/nomicon/meet-safe-and-unsafe.html#:~:text=Safe%20Rust%20is%20the%20true,Undefined%20Behavior%20(a.k.a.%20UB)."&gt;&lt;code&gt;safe&lt;/code&gt;&lt;/a&gt;, deviating into &lt;code&gt;unsafe&lt;/code&gt; only when necessary. (Rust is a great choice because it makes DataFusion fast, easy to embed, and prevents many crashes and security issues often associated with multi-threaded C/C++ code).&lt;/p&gt;
&lt;h2&gt;ClickBench results&lt;/h2&gt;
&lt;p&gt;The full results of running the &lt;a href="https://github.com/ClickHouse/ClickBench/tree/main"&gt;ClickBench&lt;/a&gt; queries against the single Parquet file with DataFusion &lt;code&gt;27.0.0&lt;/code&gt;, DataFusion &lt;code&gt;28.0.0&lt;/code&gt;, and DuckDB &lt;code&gt;0.8.1&lt;/code&gt; are below. These numbers were run on a GCP &lt;code&gt;e2-standard-8 machine&lt;/code&gt; with 8 cores and 32 GB of RAM, using the scripts &lt;a href="https://github.com/alamb/datafusion-duckdb-benchmark"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As the industry moves towards data systems assembled from components, it is increasingly important that they exchange data using open standards such as &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; and &lt;a href="https://parquet.apache.org/"&gt;Parquet&lt;/a&gt; rather than custom storage and in-memory formats. Thus, this benchmark uses a single input Parquet file representative of many DataFusion users and aligned with the current trend in analytics of avoiding a costly load/transformation into a custom storage format prior to query.&lt;/p&gt;
&lt;p&gt;DataFusion now reaches near-DuckDB-speeds querying Parquet data. While we don&amp;rsquo;t plan to engage in a benchmarking shootout with a team that literally wrote &lt;a href="https://dl.acm.org/doi/abs/10.1145/3209950.3209955"&gt;Fair Benchmarking Considered Difficult&lt;/a&gt;, hopefully everyone can agree that DataFusion &lt;code&gt;28.0.0&lt;/code&gt; is a significant improvement.&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/datafusion_fast_grouping/full.png" width="700"/&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Figure 6&lt;/strong&gt;: Performance of DataFusion &lt;code&gt;27.0.0&lt;/code&gt;, DataFusion &lt;code&gt;28.0.0&lt;/code&gt;, and DuckDB &lt;code&gt;0.8.1&lt;/code&gt; on all 43 ClickBench queries against a single &lt;code&gt;hits.parquet&lt;/code&gt; file. Lower is better.&lt;/p&gt;
&lt;h3&gt;Notes&lt;/h3&gt;
&lt;p&gt;DataFusion &lt;code&gt;27.0.0&lt;/code&gt; was not able to run several queries due to either planner bugs (Q9, Q11, Q12, 14) or running out of memory (Q33). DataFusion &lt;code&gt;28.0.0&lt;/code&gt; solves those issues.&lt;/p&gt;
&lt;p&gt;DataFusion is faster than DuckDB for query 21 and 22, likely due to optimized implementations of string pattern matching.&lt;/p&gt;
&lt;h2&gt;Conclusion: performance matters&lt;/h2&gt;
&lt;p&gt;Improving aggregation performance by more than a factor of two allows developers building products and projects with DataFusion to spend more time on value-added domain specific features. We believe building systems with DataFusion is much faster than trying to build something similar from scratch. DataFusion increases productivity because it eliminates the need to rebuild well-understood, but costly to implement, analytic database technology. While we&amp;rsquo;re pleased with the improvements in DataFusion &lt;code&gt;28.0.0&lt;/code&gt;, we are by no means done and are pursuing &lt;a href="https://github.com/apache/arrow-datafusion/issues/7000"&gt;(Even More) Aggregation Performance&lt;/a&gt;. The future for performance is bright.&lt;/p&gt;
&lt;h2&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;DataFusion is a &lt;a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html"&gt;community effort&lt;/a&gt; and this work was not possible without contributions from many in the community. A special shout out to &lt;a href="https://github.com/sunchao"&gt;sunchao&lt;/a&gt;, &lt;a href="https://github.com/jyshen"&gt;yjshen&lt;/a&gt;, &lt;a href="https://github.com/yahoNanJing"&gt;yahoNanJing&lt;/a&gt;, &lt;a href="https://github.com/mingmwang"&gt;mingmwang&lt;/a&gt;, &lt;a href="https://github.com/ozankabak"&gt;ozankabak&lt;/a&gt;, &lt;a href="https://github.com/mustafasrepo"&gt;mustafasrepo&lt;/a&gt;, and everyone else who contributed ideas, reviews, and encouragement &lt;a href="https://github.com/apache/arrow-datafusion/pull/6800"&gt;during&lt;/a&gt; this &lt;a href="https://github.com/apache/arrow-datafusion/pull/6904"&gt;work&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;About DataFusion&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query engine and database toolkit, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that uses &lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; as its in-memory format. DataFusion, along with &lt;a href="https://calcite.apache.org/"&gt;Apache Calcite&lt;/a&gt;, Facebook&amp;rsquo;s &lt;a href="https://github.com/facebookincubator/velox"&gt;Velox&lt;/a&gt;, and similar technology are part of the next generation &amp;ldquo;&lt;a href="https://www.usenix.org/publications/login/winter2018/khurana"&gt;Deconstructed Database&lt;/a&gt;&amp;rdquo; architectures, where new systems are built on a foundation of fast, modular components, rather than as a single tightly integrated system.&lt;/p&gt;
&lt;!-- Footnotes themselves at the bottom. --&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;p&gt;[^1]: &lt;code&gt;SELECT COUNT(*) FROM 'hits.parquet';&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[^2]: &lt;code&gt;SELECT COUNT(DISTINCT "UserID") as num_users FROM 'hits.parquet';&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[^3]: &lt;code&gt;SELECT COUNT(DISTINCT "SearchPhrase") as num_phrases FROM 'hits.parquet';&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[^4]: &lt;code&gt;SELECT COUNT(*) FROM (SELECT DISTINCT "UserID", "SearchPhrase" FROM 'hits.parquet')&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;[^5]: Full script at &lt;a href="https://github.com/alamb/datafusion-duckdb-benchmark/blob/main/hash.py"&gt;hash.py&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[^6]: &lt;a href="https://datasets.clickhouse.com/hits_compatible/athena_partitioned/hits_%7B%7D.parquet"&gt;hits_0.parquet&lt;/a&gt;, one of the files from the partitioned ClickBench dataset, which has &lt;code&gt;100,000&lt;/code&gt; rows and is 117 MB in size. The entire dataset has &lt;code&gt;100,000,000&lt;/code&gt; rows in a single 14 GB Parquet file. The script did not complete on the entire dataset after 40 minutes, and used 212 GB RAM at peak.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 26.0.0</title><link href="https://datafusion.apache.org/blog/2023/06/24/datafusion-25.0.0" rel="alternate"></link><published>2023-06-24T00:00:00+00:00</published><updated>2023-06-24T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2023-06-24:/blog/2023/06/24/datafusion-25.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;It has been a whirlwind 6 months of DataFusion development since &lt;a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0"&gt;our
last update&lt;/a&gt;: the community has grown, many features have been added,
performance improved and we are &lt;a href="https://github.com/apache/arrow-datafusion/discussions/6475"&gt;discussing&lt;/a&gt; branching out to our own
top level Apache Project.&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query engine and database
toolkit …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;It has been a whirlwind 6 months of DataFusion development since &lt;a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0"&gt;our
last update&lt;/a&gt;: the community has grown, many features have been added,
performance improved and we are &lt;a href="https://github.com/apache/arrow-datafusion/discussions/6475"&gt;discussing&lt;/a&gt; branching out to our own
top level Apache Project.&lt;/p&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; is an extensible query engine and database
toolkit, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;, that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory
format.&lt;/p&gt;
&lt;p&gt;DataFusion, along with &lt;a href="https://calcite.apache.org"&gt;Apache Calcite&lt;/a&gt;, Facebook's &lt;a href="https://github.com/facebookincubator/velox"&gt;Velox&lt;/a&gt; and
similar technology are part of the next generation "&lt;a href="https://www.usenix.org/publications/login/winter2018/khurana"&gt;Deconstructed
Database&lt;/a&gt;" architectures, where new systems are built on a foundation
of fast, modular components, rather as a single tightly integrated
system.&lt;/p&gt;
&lt;p&gt;While single tightly integrated systems such as &lt;a href="https://spark.apache.org/"&gt;Spark&lt;/a&gt;, &lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt; and
&lt;a href="https://www.pola.rs/"&gt;Pola.rs&lt;/a&gt; are great pieces of technology, our community believes that
anyone developing new data heavy application, such as those common in
machine learning in the next 5 years, will &lt;strong&gt;require&lt;/strong&gt; a high
performance, vectorized, query engine to remain relevant. The only
practical way to gain access to such technology without investing many
millions of dollars to build a new tightly integrated engine, is
though open source projects like DataFusion and similar enabling
technologies such as &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; and &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion is targeted primarily at developers creating other data
intensive analytics, and offers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;High performance, native, parallel streaming execution engine&lt;/li&gt;
&lt;li&gt;Mature &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html"&gt;SQL support&lt;/a&gt;, featuring  subqueries, window functions, grouping sets, and more&lt;/li&gt;
&lt;li&gt;Built in support for Parquet, Avro, CSV, JSON and Arrow formats and easy extension for others&lt;/li&gt;
&lt;li&gt;Native DataFrame API and &lt;a href="https://arrow.apache.org/datafusion-python/"&gt;python bindings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.rs/datafusion/latest/datafusion/index.html"&gt;Well documented&lt;/a&gt; source code and architecture, designed to be customized to suit downstream project needs&lt;/li&gt;
&lt;li&gt;High quality, easy to use code &lt;a href="https://crates.io/crates/datafusion/versions"&gt;released every 2 weeks to crates.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Welcoming, open community, governed by the highly regarded and well understood &lt;a href="https://www.apache.org/"&gt;Apache Software Foundation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The rest of this post highlights some of the improvements we have made
to DataFusion over the last 6 months and a preview of where we are
heading. You can see a list of all changes in the detailed
&lt;a href="https://github.com/apache/arrow-datafusion/blob/main/datafusion/CHANGELOG.md"&gt;CHANGELOG&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;(Even) Better Performance&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://voltrondata.com/resources/speeds-and-feeds-hardware-and-software-matter"&gt;Various&lt;/a&gt; benchmarks show DataFusion to be quite close or &lt;a href="https://github.com/tustvold/access-log-bench"&gt;even
faster&lt;/a&gt; to the state of the art in analytic performance (at the moment
this seems to be DuckDB). We continually work on improving performance
(see &lt;a href="https://github.com/apache/arrow-datafusion/issues/5546"&gt;#5546&lt;/a&gt; for a list) and would love additional help in this area.&lt;/p&gt;
&lt;p&gt;DataFusion now reads single large Parquet files significantly faster by
&lt;a href="https://github.com/apache/arrow-datafusion/pull/5057"&gt;parallelizing across multiple cores&lt;/a&gt;. Native speeds for reading JSON
and CSV files are also up to 2.5x faster thanks to improvements
upstream in arrow-rs &lt;a href="https://github.com/apache/arrow-rs/pull/3479#issuecomment-1384353159"&gt;JSON reader&lt;/a&gt; and &lt;a href="https://github.com/apache/arrow-rs/pull/3365"&gt;CSV reader&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, we have integrated the &lt;a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/"&gt;arrow-rs Row Format&lt;/a&gt; into DataFusion resulting in up to &lt;a href="https://github.com/apache/arrow-datafusion/pull/6163"&gt;2-3x faster sorting and merging&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Improved Documentation and Website&lt;/h2&gt;
&lt;p&gt;Part of growing the DataFusion community is ensuring that DataFusion's
features are understood and that it is easy to contribute and
participate. To that end the &lt;a href="https://arrow.apache.org/datafusion/"&gt;website&lt;/a&gt; has been cleaned up, &lt;a href="https://docs.rs/datafusion/latest/datafusion/index.html#architecture"&gt;the
architecture guide&lt;/a&gt; expanded, the &lt;a href="https://arrow.apache.org/datafusion/contributor-guide/roadmap.html"&gt;roadmap&lt;/a&gt; updated, and several
overview talks created:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apr 2023 &lt;em&gt;Query Engine&lt;/em&gt;: &lt;a href="https://youtu.be/NVKujPxwSBA"&gt;recording&lt;/a&gt; and &lt;a href="https://docs.google.com/presentation/d/1D3GDVas-8y0sA4c8EOgdCvEjVND4s2E7I6zfs67Y4j8/edit#slide=id.p"&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;April 2023 &lt;em&gt;Logical Plan and Expressions&lt;/em&gt;: &lt;a href="https://youtu.be/EzZTLiSJnhY"&gt;recording&lt;/a&gt; and &lt;a href="https://docs.google.com/presentation/d/1ypylM3-w60kVDW7Q6S99AHzvlBgciTdjsAfqNP85K30"&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;April 2023 &lt;em&gt;Physical Plan and Execution&lt;/em&gt;: &lt;a href="https://youtu.be/2jkWU3_w6z0"&gt;recording&lt;/a&gt; and &lt;a href="https://docs.google.com/presentation/d/1cA2WQJ2qg6tx6y4Wf8FH2WVSm9JQ5UgmBWATHdik0hg"&gt;slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;New Features&lt;/h2&gt;
&lt;h3&gt;More Streaming, Less Memory&lt;/h3&gt;
&lt;p&gt;We have made significant progress on the &lt;a href="https://github.com/apache/arrow-datafusion/issues/4285"&gt;streaming execution roadmap&lt;/a&gt;
such as &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/trait.ExecutionPlan.html#method.unbounded_output"&gt;unbounded datasources&lt;/a&gt;, &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/aggregates/enum.GroupByOrderMode.html"&gt;streaming group by&lt;/a&gt;, sophisticated
&lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/global_sort_selection/index.html"&gt;sort&lt;/a&gt; and &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_optimizer/repartition/index.html"&gt;repartitioning&lt;/a&gt; improvements in the optimizer, and support
for &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/joins/struct.SymmetricHashJoinExec.html"&gt;symmetric hash join&lt;/a&gt; (read more about that in the great &lt;a href="https://www.synnada.ai/blog/general-purpose-stream-joins-via-pruning-symmetric-hash-joins"&gt;Synnada
Blog Post&lt;/a&gt; on the topic). Together, these features both 1) make it
easier to build streaming systems using DataFusion that can
incrementally generate output before (or ever) seeing the end of the
input and 2) allow general queries to use less memory and generate their
results faster.&lt;/p&gt;
&lt;p&gt;We have also improved the runtime &lt;a href="https://docs.rs/datafusion/latest/datafusion/execution/memory_pool/index.html"&gt;memory management&lt;/a&gt; system so that
DataFusion now stays within its declared memory budget &lt;a href="https://github.com/apache/arrow-datafusion/issues/3941"&gt;generate
runtime errors&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;DML Support (&lt;code&gt;INSERT&lt;/code&gt;, &lt;code&gt;DELETE&lt;/code&gt;, &lt;code&gt;UPDATE&lt;/code&gt;, etc)&lt;/h3&gt;
&lt;p&gt;Part of building high performance data systems includes writing data,
and DataFusion supports several features for creating new files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;INSERT INTO&lt;/code&gt; and &lt;code&gt;SELECT ... INTO&lt;/code&gt; support for memory backed and CSV tables&lt;/li&gt;
&lt;li&gt;New &lt;a href="https://docs.rs/datafusion/latest/datafusion/physical_plan/insert/trait.DataSink.html"&gt;API for writing data into TableProviders&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are working on easier to use &lt;a href="https://github.com/apache/arrow-datafusion/issues/5654"&gt;COPY INTO&lt;/a&gt; syntax, better support
for writing parquet, JSON, and AVRO, and more -- see our &lt;a href="https://github.com/apache/arrow-datafusion/issues/6569"&gt;tracking epic&lt;/a&gt;
for more details.&lt;/p&gt;
&lt;h3&gt;Timestamp and Intervals&lt;/h3&gt;
&lt;p&gt;One mark of the maturity of a SQL engine is how it handles the tricky
world of timestamp, date, times and interval arithmetic. DataFusion is
feature complete in this area and behaves as you would expect,
supporting queries such as&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT now() + '1 month' FROM my_table;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still have a long tail of &lt;a href="https://github.com/apache/arrow-datafusion/issues/3148"&gt;date and time improvements&lt;/a&gt;, which we are working on as well.&lt;/p&gt;
&lt;h3&gt;Querying Structured Types (&lt;code&gt;List&lt;/code&gt; and &lt;code&gt;Struct&lt;/code&gt;s)&lt;/h3&gt;
&lt;p&gt;Arrow and Parquet &lt;a href="https://arrow.apache.org/blog/2022/10/08/arrow-parquet-encoding-part-2/"&gt;support nested data&lt;/a&gt; well and DataFusion lets you
easily query such &lt;code&gt;Struct&lt;/code&gt; and &lt;code&gt;List&lt;/code&gt;. For example, you can use
DataFusion to read and query the &lt;a href="https://data.mendeley.com/datasets/ct8f9skv97"&gt;JSON Datasets for Exploratory OLAP -
Mendeley Data&lt;/a&gt; like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;----------
-- Explore structured data using SQL
----------
SELECT delete FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL limit 10;
+---------------------------------------------------------------------------------------------------------------------------+
| delete                                                                                                                    |
+---------------------------------------------------------------------------------------------------------------------------+
| {status: {id: {$numberLong: 135037425050320896}, id_str: 135037425050320896, user_id: 334902461, user_id_str: 334902461}} |
| {status: {id: {$numberLong: 134703982051463168}, id_str: 134703982051463168, user_id: 405383453, user_id_str: 405383453}} |
| {status: {id: {$numberLong: 134773741740765184}, id_str: 134773741740765184, user_id: 64823441, user_id_str: 64823441}}   |
| {status: {id: {$numberLong: 132543659655704576}, id_str: 132543659655704576, user_id: 45917834, user_id_str: 45917834}}   |
| {status: {id: {$numberLong: 133786431926697984}, id_str: 133786431926697984, user_id: 67229952, user_id_str: 67229952}}   |
| {status: {id: {$numberLong: 134619093570560002}, id_str: 134619093570560002, user_id: 182430773, user_id_str: 182430773}} |
| {status: {id: {$numberLong: 134019857527214080}, id_str: 134019857527214080, user_id: 257396311, user_id_str: 257396311}} |
| {status: {id: {$numberLong: 133931546469076993}, id_str: 133931546469076993, user_id: 124539548, user_id_str: 124539548}} |
| {status: {id: {$numberLong: 134397743350296576}, id_str: 134397743350296576, user_id: 139836391, user_id_str: 139836391}} |
| {status: {id: {$numberLong: 127833661767823360}, id_str: 127833661767823360, user_id: 244442687, user_id_str: 244442687}} |
+---------------------------------------------------------------------------------------------------------------------------+

----------
-- Select some deeply nested fields
----------
SELECT
  delete['status']['id']['$numberLong'] as delete_id,
  delete['status']['user_id'] as delete_user_id
FROM 'twitter-sample-head-100000.parquet' WHERE delete IS NOT NULL LIMIT 10;

+--------------------+----------------+
| delete_id          | delete_user_id |
+--------------------+----------------+
| 135037425050320896 | 334902461      |
| 134703982051463168 | 405383453      |
| 134773741740765184 | 64823441       |
| 132543659655704576 | 45917834       |
| 133786431926697984 | 67229952       |
| 134619093570560002 | 182430773      |
| 134019857527214080 | 257396311      |
| 133931546469076993 | 124539548      |
| 134397743350296576 | 139836391      |
| 127833661767823360 | 244442687      |
+--------------------+----------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Subqueries All the Way Down&lt;/h3&gt;
&lt;p&gt;DataFusion can run many different subqueries by rewriting them to
joins. It has been able to run the full suite of TPC-H queries for at
least the last year, but recently we have implemented significant
improvements to this logic, sufficient to run almost all queries in
the TPC-DS benchmark as well.&lt;/p&gt;
&lt;h2&gt;Community and Project Growth&lt;/h2&gt;
&lt;p&gt;The six months since &lt;a href="https://arrow.apache.org/blog/2023/01/19/datafusion-16.0.0"&gt;our last update&lt;/a&gt; saw significant growth in
the DataFusion community. Between versions &lt;code&gt;17.0.0&lt;/code&gt; and &lt;code&gt;26.0.0&lt;/code&gt;,
DataFusion merged 711 PRs from 107 distinct contributors, not
including all the work that goes into our core dependencies such as
&lt;a href="https://crates.io/crates/arrow"&gt;arrow&lt;/a&gt;,
&lt;a href="https://crates.io/crates/parquet"&gt;parquet&lt;/a&gt;, and
&lt;a href="https://crates.io/crates/object_store"&gt;object_store&lt;/a&gt;, that much of
the same community helps support.&lt;/p&gt;
&lt;p&gt;In addition, we have added 7 new committers and 1 new PMC member to
the Apache Arrow project, largely focused on DataFusion, and we
learned about some of the cool &lt;a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#known-users"&gt;new systems&lt;/a&gt; which are using
DataFusion. Given the growth of the community and interest in the
project, we also clarified the &lt;a href="https://github.com/apache/arrow-datafusion/discussions/6441"&gt;mission statement&lt;/a&gt; and are
&lt;a href="https://github.com/apache/arrow-datafusion/discussions/6475"&gt;discussing&lt;/a&gt; "graduate"ing DataFusion to a new top level
Apache Software Foundation project.&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 17.0.0..26.0.0 . | wc -l
     711

$ git shortlog -sn 17.0.0..26.0.0 . | wc -l
      107
--&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;Kudos to everyone in the community who has contributed ideas,
discussions, bug reports, documentation and code. It is exciting to be
innovating on the next generation of database architectures together!&lt;/p&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to
have you join us. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check out our &lt;a href="https://arrow.apache.org/datafusion/contributor-guide/communication.html"&gt;Communication Doc&lt;/a&gt; for more ways to engage with the
community.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 16.0.0 Project Update</title><link href="https://datafusion.apache.org/blog/2023/01/19/datafusion-16.0.0" rel="alternate"></link><published>2023-01-19T00:00:00+00:00</published><updated>2023-01-19T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2023-01-19:/blog/2023/01/19/datafusion-16.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible
query execution framework, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;,
that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its
in-memory format. It is targeted primarily at developers creating data
intensive analytics, and offers mature
&lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html"&gt;SQL support&lt;/a&gt;,
a DataFrame API, and many extension points.&lt;/p&gt;
&lt;p&gt;Systems based on DataFusion perform very well in benchmarks …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible
query execution framework, written in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt;,
that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its
in-memory format. It is targeted primarily at developers creating data
intensive analytics, and offers mature
&lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html"&gt;SQL support&lt;/a&gt;,
a DataFrame API, and many extension points.&lt;/p&gt;
&lt;p&gt;Systems based on DataFusion perform very well in benchmarks,
especially considering they operate directly on parquet files rather
than first loading into a specialized format.  Some recent highlights
include &lt;a href="https://benchmark.clickhouse.com/"&gt;clickbench&lt;/a&gt; and the
&lt;a href="https://www.cloudfuse.io/dashboards/standalone-engines"&gt;Cloudfuse.io standalone query
engines&lt;/a&gt; page.&lt;/p&gt;
&lt;p&gt;DataFusion is also part of a longer term trend, articulated clearly by
&lt;a href="http://www.cs.cmu.edu/~pavlo/"&gt;Andy Pavlo&lt;/a&gt; in his &lt;a href="https://ottertune.com/blog/2022-databases-retrospective/"&gt;2022 Databases
Retrospective&lt;/a&gt;.
Database frameworks are proliferating and it is likely that all OLAP
DBMSs and other data heavy applications, such as machine learning,
will &lt;strong&gt;require&lt;/strong&gt; a vectorized, highly performant query engine in the next
5 years to remain relevant.  The only practical way to make such
technology so widely available without many millions of dollars of
investment is though open source engine such as DataFusion or
&lt;a href="https://github.com/facebookincubator/velox"&gt;Velox&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The rest of this post describes the improvements made to DataFusion
over the last three months and some hints of where we are heading.&lt;/p&gt;
&lt;h2&gt;Community Growth&lt;/h2&gt;
&lt;p&gt;We again saw significant growth in the DataFusion community since &lt;a href="https://arrow.apache.org/blog/2022/10/25/datafusion-13.0.0/"&gt;our last update&lt;/a&gt;. There are some interesting metrics on &lt;a href="https://ossrank.com/p/1573-apache-arrow-datafusion"&gt;OSSRank&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The DataFusion 16.0.0 release consists of 543 PRs from 73 distinct contributors, not including all the work that goes into dependencies such as &lt;a href="https://crates.io/crates/arrow"&gt;arrow&lt;/a&gt;, &lt;a href="https://crates.io/crates/parquet"&gt;parquet&lt;/a&gt;, and &lt;a href="https://crates.io/crates/object_store"&gt;object_store&lt;/a&gt;, that much of the same community helps support. Thank you all for your help&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 13.0.0..16.0.0 . | wc -l
     543

$ git shortlog -sn 13.0.0..16.0.0 . | wc -l
      73
--&gt;
&lt;p&gt;Several &lt;a href="https://github.com/apache/arrow-datafusion#known-uses"&gt;new systems based on DataFusion&lt;/a&gt; were recently added:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/GreptimeTeam/greptimedb"&gt;Greptime DB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://synnada.ai/"&gt;Synnada&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PRQL/prql-query"&gt;PRQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/parseablehq/parseable"&gt;Parseable&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/splitgraph/seafowl"&gt;SeaFowl&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Performance 🚀&lt;/h2&gt;
&lt;p&gt;Performance and efficiency are core values for
DataFusion. While there is still a gap between DataFusion and the best of
breed, tightly integrated systems such as &lt;a href="https://duckdb.org"&gt;DuckDB&lt;/a&gt;
and &lt;a href="https://www.pola.rs/"&gt;Polars&lt;/a&gt;, DataFusion is
closing the gap quickly. Performance highlights from the last three
months:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Up to 30% Faster Sorting and Merging using the new &lt;a href="https://arrow.apache.org/blog/2022/11/07/multi-column-sorts-in-arrow-rust-part-1/"&gt;Row Format&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arrow.apache.org/blog/2022/12/26/querying-parquet-with-millisecond-latency/"&gt;Advanced predicate pushdown&lt;/a&gt;, directly on parquet, directly from object storage, enabling sub millisecond filtering. &lt;!-- Andrew nots: we should really get this turned on by default --&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;70%&lt;/code&gt; faster &lt;code&gt;IN&lt;/code&gt; expressions evaluation (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4057"&gt;#4057&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Sort and partition aware optimizations (&lt;a href="https://github.com/apache/arrow-datafusion/issues/3969"&gt;#3969&lt;/a&gt; and  &lt;a href="https://github.com/apache/arrow-datafusion/issues/4691"&gt;#4691&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Filter selectivity analysis (&lt;a href="https://github.com/apache/arrow-datafusion/issues/3868"&gt;#3868&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Runtime Resource Limits&lt;/h2&gt;
&lt;p&gt;Previously, DataFusion could potentially use unbounded amounts of memory for certain queries that included Sorts, Grouping or Joins.&lt;/p&gt;
&lt;p&gt;In version 16.0.0, it is possible to limit DataFusion's memory usage for Sorting and Grouping. We are looking for help adding similar limiting for Joins as well as expanding our algorithms to optionally spill to secondary storage. See &lt;a href="https://github.com/apache/arrow-datafusion/issues/3941"&gt;#3941&lt;/a&gt; for more detail.&lt;/p&gt;
&lt;h2&gt;SQL Window Functions&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Window_function_(SQL)"&gt;SQL Window Functions&lt;/a&gt; are useful for a variety of analysis and DataFusion's implementation support expanded significantly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Custom window frames such as &lt;code&gt;... OVER (ORDER BY ... RANGE BETWEEN 0.2 PRECEDING AND 0.2 FOLLOWING)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Unbounded window frames such as &lt;code&gt;... OVER (ORDER BY ... RANGE UNBOUNDED ROWS PRECEDING)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Support for the &lt;code&gt;NTILE&lt;/code&gt; window function (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4676"&gt;#4676&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;GROUPS&lt;/code&gt; mode (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4155"&gt;#4155&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Improved Joins&lt;/h1&gt;
&lt;p&gt;Joins are often the most complicated operations to handle well in
analytics systems and DataFusion 16.0.0 offers significant improvements
such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cost based optimizer (CBO) automatically reorders join evaluations, selects algorithms (Merge / Hash), and pick build side based on available statistics and join type (&lt;code&gt;INNER&lt;/code&gt;, &lt;code&gt;LEFT&lt;/code&gt;, etc) (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4219"&gt;#4219&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Fast non &lt;code&gt;column=column&lt;/code&gt; equijoins such as &lt;code&gt;JOIN ON a.x + 5 = b.y&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Better performance on non-equijoins (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4562"&gt;#4562&lt;/a&gt;) &lt;!-- TODO is this a good thing to mention as any time this is usd the query is going to go slow or the data size is small --&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Streaming Execution&lt;/h1&gt;
&lt;p&gt;One emerging use case for Datafusion is as a foundation for
streaming-first data platforms. An important prerequisite
is support for incremental execution for queries that can be computed
incrementally.&lt;/p&gt;
&lt;p&gt;With this release, DataFusion now supports the following streaming features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data ingestion from infinite files such as FIFOs (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4694"&gt;#4694&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Detection of pipeline-breaking queries in streaming use cases (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4694"&gt;#4694&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Automatic input swapping for joins so probe side is a data stream (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4694"&gt;#4694&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Intelligent elision of pipeline-breaking sort operations whenever possible (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4691"&gt;#4691&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Incremental execution for more types of queries; e.g. queries involving finite window frames (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4777"&gt;#4777&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These are a major steps forward, and we plan even more improvements over the next few releases.&lt;/p&gt;
&lt;h1&gt;Better Support for Distributed Catalogs&lt;/h1&gt;
&lt;p&gt;16.0.0 has been enhanced support for asynchronous catalogs (&lt;a href="https://github.com/apache/arrow-datafusion/issues/4607"&gt;#4607&lt;/a&gt;)
to better support distributed metadata stores such as
&lt;a href="https://delta.io/"&gt;Delta.io&lt;/a&gt; and &lt;a href="https://iceberg.apache.org/"&gt;Apache
Iceberg&lt;/a&gt; which require asynchronous I/O
during planning to access remote catalogs. Previously, DataFusion
required synchronous access to all relevant catalog information.&lt;/p&gt;
&lt;h1&gt;Additional SQL Support&lt;/h1&gt;
&lt;p&gt;SQL support continues to improve, including some of these highlights:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Add TPC-DS query planning regression tests &lt;a href="https://github.com/apache/arrow-datafusion/issues/4719"&gt;#4719&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;PREPARE&lt;/code&gt; statement &lt;a href="https://github.com/apache/arrow-datafusion/issues/4490"&gt;#4490&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Automatic coercions ast between Date and Timestamp &lt;a href="https://github.com/apache/arrow-datafusion/issues/4726"&gt;#4726&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support type coercion for timestamp and utf8 &lt;a href="https://github.com/apache/arrow-datafusion/issues/4312"&gt;#4312&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Full support for time32 and time64 literal values (&lt;code&gt;ScalarValue&lt;/code&gt;) &lt;a href="https://github.com/apache/arrow-datafusion/issues/4156"&gt;#4156&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;New functions, incuding &lt;code&gt;uuid()&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/4041"&gt;#4041&lt;/a&gt;, &lt;code&gt;current_time&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/4054"&gt;#4054&lt;/a&gt;, &lt;code&gt;current_date&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/4022"&gt;#4022&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Compressed CSV/JSON support &lt;a href="https://github.com/apache/arrow-datafusion/issues/3642"&gt;#3642&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The community has also invested in new &lt;a href="https://github.com/apache/arrow-datafusion/blob/master/datafusion/core/tests/sqllogictests/README.md"&gt;sqllogic based&lt;/a&gt; tests to keep improving DataFusion's quality with less effort.&lt;/p&gt;
&lt;h1&gt;Plan Serialization and Substrait&lt;/h1&gt;
&lt;p&gt;DataFusion now supports serialization of physical plans, with a custom protocol buffers format. In addition, we are adding initial support for &lt;a href="https://substrait.io/"&gt;Substrait&lt;/a&gt;, a Cross-Language Serialization for Relational Algebra&lt;/p&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together!&lt;/p&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to
have you join us. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is
&lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check out our &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;
&lt;h2&gt;Appendix: Contributor Shoutout&lt;/h2&gt;
&lt;p&gt;Here is a list of people who have contributed PRs to this project over the last three releases, derived from &lt;code&gt;git shortlog -sn 13.0.0..16.0.0 .&lt;/code&gt; Thank you all!&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   113  Andrew Lamb
    58  jakevin
    46  Raphael Taylor-Davies
    30  Andy Grove
    19  Batuhan Taskaya
    19  Remzi Yang
    17  ygf11
    16  Burak
    16  Jeffrey
    16  Marco Neumann
    14  Kun Liu
    12  Yang Jiang
    10  mingmwang
     9  Dani&amp;euml;l Heres
     9  Mustafa akur
     9  comphead
     9  mvanschellebeeck
     9  xudong.w
     7  dependabot[bot]
     7  yahoNanJing
     6  Brent Gardner
     5  AssHero
     4  Jiayu Liu
     4  Wei-Ting Kuo
     4  askoa
     3  Andr&amp;eacute; Calado Coroado
     3  Jie Han
     3  Jon Mease
     3  Metehan Y&amp;inodot;ld&amp;inodot;r&amp;inodot;m
     3  Nga Tran
     3  Ruihang Xia
     3  baishen
     2  Berkay &amp;Scedil;ahin
     2  Dan Harris
     2  Dongyan Zhou
     2  Eduard Karacharov
     2  Kikkon
     2  Liang-Chi Hsieh
     2  Marko Milenkovi&amp;cacute;
     2  Martin Grigorov
     2  Roman Nozdrin
     2  Tim Van Wassenhove
     2  r.4ntix
     2  unconsolable
     2  unvalley
     1  Ajaya Agrawal
     1  Alexander Spies
     1  ArkashaJavelin
     1  Artjoms Iskovs
     1  BoredPerson
     1  Christian Salvati
     1  Creampanda
     1  Data Psycho
     1  Francis Du
     1  Francis Le Roy
     1  LFC
     1  Marko Grujic
     1  Matt Willian
     1  Matthijs Brobbel
     1  Max Burke
     1  Mehmet Ozan Kabak
     1  Rito Takeuchi
     1  Roman Zeyde
     1  Vrishabh
     1  Zhang Li
     1  ZuoTiJia
     1  byteink
     1  cfraz89
     1  nbr
     1  xxchan
     1  yujie.zhang
     1  zembunia
     1  哇呜哇呜呀咦耶
&lt;/code&gt;&lt;/pre&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow Ballista 0.9.0 Release</title><link href="https://datafusion.apache.org/blog/2022/10/28/ballista-0.9.0" rel="alternate"></link><published>2022-10-28T00:00:00+00:00</published><updated>2022-10-28T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-10-28:/blog/2022/10/28/ballista-0.9.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/arrow-ballista"&gt;Ballista&lt;/a&gt; is an Arrow-native distributed SQL query engine implemented in Rust.&lt;/p&gt;
&lt;p&gt;Ballista 0.9.0 is now available and is the most significant release since the project was &lt;a href="http://arrow.apache.org/blog/2021/04/12/ballista-donation/"&gt;donated&lt;/a&gt; to Apache
Arrow in 2021.&lt;/p&gt;
&lt;p&gt;This release represents 4 weeks of work, with 66 commits from 14 contributors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    22  Andy …&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://github.com/apache/arrow-ballista"&gt;Ballista&lt;/a&gt; is an Arrow-native distributed SQL query engine implemented in Rust.&lt;/p&gt;
&lt;p&gt;Ballista 0.9.0 is now available and is the most significant release since the project was &lt;a href="http://arrow.apache.org/blog/2021/04/12/ballista-donation/"&gt;donated&lt;/a&gt; to Apache
Arrow in 2021.&lt;/p&gt;
&lt;p&gt;This release represents 4 weeks of work, with 66 commits from 14 contributors:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    22  Andy Grove
    12  yahoNanJing
     6  Dani&amp;euml;l Heres
     4  Brent Gardner
     4  dependabot[bot]
     4  r.4ntix
     3  Stefan Stanciulescu
     3  mingmwang
     2  Ken Suenobu
     2  Yang Jiang
     1  Metehan Y&amp;inodot;ld&amp;inodot;r&amp;inodot;m
     1  Trent Feda
     1  askoa
     1  yangzhong
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Release Highlights&lt;/h2&gt;
&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes
and improvements have been made: we refer you to the &lt;a href="https://github.com/apache/arrow-ballista/blob/0.9.0-rc2/ballista/CHANGELOG.md"&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Support for Cloud Object Stores and Distributed File Systems&lt;/h3&gt;
&lt;p&gt;This is the first release of Ballista to have documented support for querying data from distributed file systems and
object stores. Currently, S3 and HDFS are supported. Support for Google Cloud Storage and Azure Blob Storage is planned
for the next release.&lt;/p&gt;
&lt;h3&gt;Flight SQL &amp;amp; JDBC support&lt;/h3&gt;
&lt;p&gt;The Ballista scheduler now implements the &lt;a href="https://arrow.apache.org/blog/2022/02/16/introducing-arrow-flight-sql/"&gt;Flight SQL protocol&lt;/a&gt;, enabling any compliant Flight SQL client
to connect to and run queries against a Ballista cluster.&lt;/p&gt;
&lt;p&gt;The Apache Arrow Flight SQL JDBC driver can be used to connect Business Intelligence tools to a Ballista cluster.&lt;/p&gt;
&lt;h3&gt;Python Bindings&lt;/h3&gt;
&lt;p&gt;It is now possible to connect to a Ballista cluster from Python and execute queries using both the DataFrame and SQL
interfaces.&lt;/p&gt;
&lt;h3&gt;Scheduler Web User Interface and REST API&lt;/h3&gt;
&lt;p&gt;The scheduler now has a web user interface for monitoring queries. It is also possible to view graphical query plans
that show how the query was executed, along with metrics.&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/2022-10-28-ballista-web-ui.png" width="800"/&gt;&lt;/p&gt;
&lt;p&gt;The REST API that powers the user interface can also be accessed directly.&lt;/p&gt;
&lt;h3&gt;Simplified Kubernetes Deployment&lt;/h3&gt;
&lt;p&gt;Ballista now provides a &lt;a href="https://github.com/apache/arrow-ballista/tree/master/helm"&gt;Helm chart&lt;/a&gt; for simplified Kubernetes deployment.&lt;/p&gt;
&lt;h3&gt;User Guide&lt;/h3&gt;
&lt;p&gt;The user guide is published at &lt;a href="https://arrow.apache.org/ballista/"&gt;https://arrow.apache.org/ballista/&lt;/a&gt; and provides
deployment instructions for Docker, Docker Compose, and Kubernetes, as well as references for configuring and
tuning Ballista.&lt;/p&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;The Ballista community is currently focused on the following tasks for the next release:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support for Azure Blob Storage and Google Cloud Storage&lt;/li&gt;
&lt;li&gt;Improve benchmark performance by implementing more query optimizations&lt;/li&gt;
&lt;li&gt;Improve scheduler web user interface&lt;/li&gt;
&lt;li&gt;Publish Docker images to GitHub Container Registry&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The detailed list of issues planned for the 0.10.0 release can be found in the &lt;a href="https://github.com/apache/arrow-ballista/issues/361"&gt;tracking issue&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Getting Involved&lt;/h2&gt;
&lt;p&gt;Ballista has a friendly community and we welcome contributions. A good place to start is to following the instructions
in the &lt;a href="https://arrow.apache.org/ballista/"&gt;user guide&lt;/a&gt; and try using Ballista with your own SQL queries and ETL pipelines, and file issues
for any bugs or feature suggestions.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 13.0.0 Project Update</title><link href="https://datafusion.apache.org/blog/2022/10/25/datafusion-13.0.0" rel="alternate"></link><published>2022-10-25T00:00:00+00:00</published><updated>2022-10-25T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-10-25:/blog/2022/10/25/datafusion-13.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; &lt;a href="https://crates.io/crates/datafusion"&gt;&lt;code&gt;13.0.0&lt;/code&gt;&lt;/a&gt; is released, and this blog contains an update on the project for the 5 months since our &lt;a href="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/"&gt;last update in May 2022&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;Apache Arrow DataFusion&lt;/a&gt; &lt;a href="https://crates.io/crates/datafusion"&gt;&lt;code&gt;13.0.0&lt;/code&gt;&lt;/a&gt; is released, and this blog contains an update on the project for the 5 months since our &lt;a href="https://arrow.apache.org/blog/2022/05/16/datafusion-8.0.0/"&gt;last update in May 2022&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;DataFusion is an extensible and embeddable query engine, written in Rust used to create modern, fast and efficient data pipelines, ETL processes, and database systems. You may want to check out DataFusion to extend your Rust project to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Support &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support &lt;a href="https://docs.rs/datafusion/13.0.0/datafusion/dataframe/struct.DataFrame.html"&gt;DataFrame API&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support a Domain Specific Query Language&lt;/li&gt;
&lt;li&gt;Easily and quickly read and process Parquet, JSON, Avro or CSV data.&lt;/li&gt;
&lt;li&gt;Read from remote object stores such as AWS S3, Azure Blob Storage, GCP.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even though DataFusion is 4 years "young," it has seen significant community growth in the last few months and the momentum continues to accelerate.&lt;/p&gt;
&lt;h1&gt;Background&lt;/h1&gt;
&lt;p&gt;DataFusion is used as the engine in &lt;a href="https://github.com/apache/arrow-datafusion#known-uses"&gt;many open source and commercial projects&lt;/a&gt; and was one of the early open source projects to provide this capability. 2022 has validated our belief in the need for such a &lt;a href="https://docs.google.com/presentation/d/1iNX_35sWUakee2q3zMFPyHE4IV2nC3lkCK_H6Y2qK84/edit#slide=id.p"&gt;"LLVM for database and AI systems"&lt;/a&gt;&lt;a href="https://www.slideshare.net/AndrewLamb32/20220623-apache-arrow-and-datafusion-changing-the-game-for-implementing-database-systemspdf"&gt;(alternate link)&lt;/a&gt; with announcements such as the &lt;a href="https://engineering.fb.com/2022/08/31/open-source/velox/"&gt;release of FaceBook's Velox&lt;/a&gt; engine, the major investments in &lt;a href="https://arrow.apache.org/docs/cpp/streaming_execution.html"&gt;Acero&lt;/a&gt; as well as the continued popularity of &lt;a href="https://calcite.apache.org/"&gt;Apache Calcite&lt;/a&gt; and other similar technologies.&lt;/p&gt;
&lt;p&gt;While Velox and Acero focus on execution engines, DataFusion provides the entire suite of components needed to build most analytic systems, including a SQL frontend, a dataframe API, and  extension points for just about everything. Some &lt;a href="https://github.com/apache/arrow-datafusion#known-uses"&gt;DataFusion users&lt;/a&gt; use a subset of the features such as the frontend (e.g. &lt;a href="https://dask-sql.readthedocs.io/en/latest/"&gt;dask-sql&lt;/a&gt;) or the execution engine, (e.g.  &lt;a href="https://github.com/blaze-init/blaze"&gt;Blaze&lt;/a&gt;), and some use many different components to build both SQL based and customized DSL based systems such as &lt;a href="https://github.com/influxdata/influxdb_iox/pulls"&gt;InfluxDB IOx&lt;/a&gt; and &lt;a href="https://github.com/vegafusion/vegafusion"&gt;VegaFusion&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One of DataFusion&amp;rsquo;s advantages is its implementation in &lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; and thus its easy integration with the broader Rust ecosystem. Rust continues to be a major source of benefit, from the &lt;a href="https://www.influxdata.com/blog/using-rustlangs-async-tokio-runtime-for-cpu-bound-tasks/"&gt;ease of parallelization with the high quality and standardized &lt;code&gt;async&lt;/code&gt; ecosystem&lt;/a&gt; , as well as its modern dependency management system and wonderful performance. &lt;!-- I wonder if we should link to clickbench?? --&gt;&lt;/p&gt;
&lt;!--While we haven’t invested in the benchmarking ratings game datafusion continues to be quite speedy (todo quantity this, with some evidence) – maybe clickbench?--&gt;
&lt;!--
Maybe we can do this un a future post
# DataFusion in Action

While DataFusion really shines as an embeddable query engine, if you want to try it out and get a feel for its power, you can use the basic[`datafusion-cli`](https://docs.rs/datafusion-cli/13.0.0/datafusion_cli/) tool to get a sense for what is possible to add in your application

(TODO example here of using datafusion-cli to query from local parquet files on disk)

TODO: also mention you can use the same thing to query data from S3
--&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;We have increased the frequency of DataFusion releases to monthly instead of quarterly. This
makes it easier for the increasing number of projects that now depend on DataFusion.&lt;/p&gt;
&lt;p&gt;We have also completed the "graduation" of &lt;a href="https://github.com/apache/arrow-ballista"&gt;Ballista to its own top-level arrow-ballista repository&lt;/a&gt;
which decouples the two projects and allows each project to move even faster.&lt;/p&gt;
&lt;p&gt;Along with numerous other bug fixes and smaller improvements, here are some of the major advances:&lt;/p&gt;
&lt;h1&gt;Improved Support for Cloud Object Stores&lt;/h1&gt;
&lt;p&gt;DataFusion now supports many major cloud object stores (Amazon S3, Azure Blob Storage, and Google Cloud Storage) "out of the box" via the &lt;a href="https://crates.io/crates/object_store"&gt;object_store&lt;/a&gt; crate. Using this integration, DataFusion optimizes reading parquet files by reading only the parts of the files that are needed.&lt;/p&gt;
&lt;h2&gt;Advanced SQL&lt;/h2&gt;
&lt;p&gt;DataFusion now supports correlated subqueries, by rewriting them as joins. See the &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/subqueries.html"&gt;Subquery&lt;/a&gt; page in the User Guide for more information.&lt;/p&gt;
&lt;p&gt;In addition to numerous other small improvements, the following SQL features are now supported:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ROWS&lt;/code&gt;, &lt;code&gt;RANGE&lt;/code&gt;, &lt;code&gt;PRECEDING&lt;/code&gt; and &lt;code&gt;FOLLOWING&lt;/code&gt; in &lt;code&gt;OVER&lt;/code&gt; clauses &lt;a href="https://github.com/apache/arrow-datafusion/issues/3570"&gt;#3570&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ROLLUP&lt;/code&gt; and &lt;code&gt;CUBE&lt;/code&gt; grouping set expressions  &lt;a href="https://github.com/apache/arrow-datafusion/issues/2446"&gt;#2446&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SUM DISTINCT&lt;/code&gt; aggregate support  &lt;a href="https://github.com/apache/arrow-datafusion/issues/2405"&gt;#2405&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IN&lt;/code&gt; and &lt;code&gt;NOT IN&lt;/code&gt; Subqueries by rewriting them to &lt;code&gt;SEMI&lt;/code&gt; / &lt;code&gt;ANTI&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2885"&gt;#2421&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Non equality predicates in  &lt;code&gt;ON&lt;/code&gt; clause of  &lt;code&gt;LEFT&lt;/code&gt;, &lt;code&gt;RIGHT,&lt;/code&gt;and &lt;code&gt;FULL&lt;/code&gt; joins &lt;a href="https://github.com/apache/arrow-datafusion/issues/2591"&gt;#2591&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Exact &lt;code&gt;MEDIAN&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3009"&gt;#3009&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;GROUPING SETS&lt;/code&gt;/&lt;code&gt;CUBE&lt;/code&gt;/&lt;code&gt;ROLLUP&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2716"&gt;#2716&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;More DDL Support&lt;/h1&gt;
&lt;p&gt;Just as it is important to query, it is also important to give users the ability to define their data sources. We have added:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CREATE VIEW&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2279"&gt;#2279&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DESCRIBE &amp;lt;table&amp;gt;&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2642"&gt;#2642&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Custom / Dynamic table provider factories &lt;a href="https://github.com/apache/arrow-datafusion/issues/3311"&gt;#3311&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SHOW CREATE TABLE&lt;/code&gt; for support for views &lt;a href="https://github.com/apache/arrow-datafusion/issues/2830"&gt;#2830&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Faster Execution&lt;/h1&gt;
&lt;p&gt;Performance is always an important goal for DataFusion, and there are a number of significant new optimizations such as&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Optimizations of TopK (queries with a &lt;code&gt;LIMIT&lt;/code&gt; or &lt;code&gt;OFFSET&lt;/code&gt; clause):  &lt;a href="https://github.com/apache/arrow-datafusion/issues/3527"&gt;#3527&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/2521"&gt;#2521&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Reduce &lt;code&gt;left&lt;/code&gt;/&lt;code&gt;right&lt;/code&gt;/&lt;code&gt;full&lt;/code&gt; joins to &lt;code&gt;inner&lt;/code&gt; join &lt;a href="https://github.com/apache/arrow-datafusion/issues/2750"&gt;#2750&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Convert  cross joins to inner joins when possible &lt;a href="https://github.com/apache/arrow-datafusion/issues/3482"&gt;#3482&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sort preserving &lt;code&gt;SortMergeJoin&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/2699"&gt;#2699&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Improvements in group by and sort performance &lt;a href="https://github.com/apache/arrow-datafusion/issues/2375"&gt;#2375&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adaptive &lt;code&gt;regex_replace&lt;/code&gt; implementation &lt;a href="https://github.com/apache/arrow-datafusion/issues/3518"&gt;#3518&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Optimizer Enhancements&lt;/h1&gt;
&lt;p&gt;Internally the optimizer has been significantly enhanced as well.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Casting / coercion now happens during logical planning &lt;a href="https://github.com/apache/arrow-datafusion/issues/3396"&gt;#3185&lt;/a&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3636"&gt;#3636&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;More sophisticated expression analysis and simplification is available&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Parquet&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The parquet reader can now read directly from parquet files on remote object storage &lt;a href="https://github.com/apache/arrow-datafusion/issues/2677"&gt;#2489&lt;/a&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3051"&gt;#3051&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Experimental support for &amp;ldquo;predicate pushdown&amp;rdquo; with late materialization after filtering during the scan (another blog post on this topic is coming soon).&lt;/li&gt;
&lt;li&gt;Support reading directly from AWS S3 and other object stores via &lt;code&gt;datafusion-cli&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3631"&gt;#3631&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;DataType Support&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Support for &lt;code&gt;TimestampTz&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3660"&gt;#3660&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Expanded support for the &lt;code&gt;Decimal&lt;/code&gt; type, including  &lt;code&gt;IN&lt;/code&gt; list and better built in coercion.&lt;/li&gt;
&lt;li&gt;Expanded support for date/time manipulation such as  &lt;code&gt;date_bin&lt;/code&gt; built-in function , timestamp &lt;code&gt;+/-&lt;/code&gt; interval, &lt;code&gt;TIME&lt;/code&gt; literal values &lt;a href="https://github.com/apache/arrow-datafusion/issues/3010"&gt;#3010&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3110"&gt;#3110&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3034"&gt;#3034&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Binary operations (&lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;XOR&lt;/code&gt;, etc):  &lt;a href="https://github.com/apache/arrow-datafusion/issues/1619"&gt;#3037&lt;/a&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3430"&gt;#3420&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IS TRUE/FALSE&lt;/code&gt; and &lt;code&gt;IS [NOT] UNKNOWN&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/issues/3235"&gt;#3235&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3246"&gt;#3246&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Upcoming Work&lt;/h2&gt;
&lt;p&gt;With the community growing and code accelerating, there is so much great stuff on the horizon. Some features we expect to land in the next few months:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/3462"&gt;Complete Parquet Pushdown&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/3148"&gt;Additional date/time support&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Cost models, Nested Join Optimizations, analysis framework &lt;a href="https://github.com/apache/arrow-datafusion/issues/128"&gt;#128&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3843"&gt;#3843&lt;/a&gt;, &lt;a href="https://github.com/apache/arrow-datafusion/issues/3845"&gt;#3845&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Community Growth&lt;/h1&gt;
&lt;p&gt;The DataFusion 9.0.0 and 13.0.0 releases consists of 433 PRs from 64 distinct contributors. This does not count all the work that goes into our dependencies such as &lt;a href="https://crates.io/crates/arrow"&gt;arrow&lt;/a&gt;,  &lt;a href="https://crates.io/crates/parquet"&gt;parquet&lt;/a&gt;, and &lt;a href="https://crates.io/crates/object_store"&gt;object_store&lt;/a&gt;, that much of the same community helps nurture.&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 9.0.0..13.0.0 . | wc -l
433

$ git shortlog -sn 9.0.0..13.0.0 . | wc -l
65
--&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;Kudos to everyone in the community who contributed ideas, discussions, bug reports, documentation and code. It is exciting to be building something so cool together!&lt;/p&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to
have you join us on our journey to create the most advanced open
source query engine. You can try out DataFusion on some of your own
data and projects and let us know how it goes or contribute a PR with
documentation, tests or code. A list of open issues suitable for
beginners is
&lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check out our &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;
&lt;h2&gt;Appendix: Contributor Shoutout&lt;/h2&gt;
&lt;p&gt;To give a sense of the number of people who contribute to this project regularly, we present for your consideration the following list derived from &lt;code&gt;git shortlog -sn 9.0.0..13.0.0 .&lt;/code&gt; Thank you all again!&lt;/p&gt;
&lt;!-- Note: combined kmitchener and Kirk Mitchener --&gt;
&lt;pre&gt;&lt;code&gt;    87  Andy Grove
    71  Andrew Lamb
    29  Kun Liu
    29  Kirk Mitchener
    17  Wei-Ting Kuo
    14  Yang Jiang
    12  Raphael Taylor-Davies
    11  Batuhan Taskaya
    10  Brent Gardner
    10  Remzi Yang
    10  comphead
    10  xudong.w
     8  AssHero
     7  Ruihang Xia
     6  Dan Harris
     6  Dani&amp;euml;l Heres
     6  Ian Alexander Joiner
     6  Mike Roberts
     6  askoa
     4  BaymaxHWY
     4  gorkem
     4  jakevin
     3  George Andronchik
     3  Sarah Yurick
     3  Stuart Carnie
     2  Dalton Modlin
     2  Dmitry Patsura
     2  JasonLi
     2  Jon Mease
     2  Marco Neumann
     2  yahoNanJing
     1  Adilet Sarsembayev
     1  Ayush Dattagupta
     1  Dezhi Wu
     1  Dhamotharan Sritharan
     1  Eduard Karacharov
     1  Francis Du
     1  Harbour Zheng
     1  Isma&amp;euml;l Mej&amp;iacute;a
     1  Jack Klamer
     1  Jeremy Dyer
     1  Jiayu Liu
     1  Kamil Konior
     1  Liang-Chi Hsieh
     1  Martin Grigorov
     1  Matthijs Brobbel
     1  Mehmet Ozan Kabak
     1  Metehan Y&amp;inodot;ld&amp;inodot;r&amp;inodot;m
     1  Morgan Cassels
     1  Nitish Tiwari
     1  Renjie Liu
     1  Rito Takeuchi
     1  Robert Pack
     1  Thomas Cameron
     1  Vrishabh
     1  Xin Hao
     1  Yijie Shen
     1  byteink
     1  kamille
     1  mateuszkj
     1  nvartolomei
     1  yourenawo
     1  &amp;Ouml;zg&amp;uuml;r Akkurt
&lt;/code&gt;&lt;/pre&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 8.0.0 Release</title><link href="https://datafusion.apache.org/blog/2022/05/16/datafusion-8.0.0" rel="alternate"></link><published>2022-05-16T00:00:00+00:00</published><updated>2022-05-16T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-05-16:/blog/2022/05/16/datafusion-8.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that
uses Apache Arrow as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;,
a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that
uses Apache Arrow as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;,
a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth
checking out.&lt;/p&gt;
&lt;p&gt;DataFusion's SQL, &lt;code&gt;DataFrame&lt;/code&gt;, and manual &lt;code&gt;PlanBuilder&lt;/code&gt; API let users access a sophisticated query optimizer and
execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of
today's multicore hardware. Being written in Rust means DataFusion can offer &lt;em&gt;both&lt;/em&gt; the safety of a dynamic language and
the resource efficiency of a compiled language.&lt;/p&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 8.0.0 release (and also the release of version 0.7.0 of
the Ballista subproject). This covers 3 months of development work and includes 279 commits from the following 49
distinct contributors.&lt;/p&gt;
&lt;!--
$ git log --pretty=oneline 7.0.0..8.0.0 datafusion datafusion-cli datafusion-examples ballista ballista-cli ballista-examples | wc -l
279

$ git shortlog -sn 7.0.0..8.0.0 datafusion datafusion-cli datafusion-examples ballista ballista-cli ballista-examples | wc -l
49

(feynman han, feynman.h, Feynman Han were assumed to be the same person)
--&gt;
&lt;pre&gt;&lt;code&gt;    39  Andy Grove
    33  Andrew Lamb
    21  DuRipeng
    20  Yijie Shen
    19  Yang Jiang
    17  Raphael Taylor-Davies
    11  Dan Harris
    11  Matthew Turner
    11  yahoNanJing
     9  dependabot[bot]
     8  jakevin
     6  Kun Liu
     5  Jiayu Liu
     4  Dani&amp;euml;l Heres
     4  mingmwang
     4  xudong.w
     3  Carol (Nichols || Goulding)
     3  Dmitry Patsura
     3  Eduard Karacharov
     3  Jeremy Dyer
     3  Kaushik
     3  Rich
     3  comphead
     3  gaojun2048
     3  Feynman Han
     2  Jie Han
     2  Jon Mease
     2  Tim Van Wassenhove
     2  Yt
     2  Zhang Li
     2  silence-coding
     1  Alexander Spies
     1  George Andronchik
     1  Guillaume Balaine
     1  Hao Xin
     1  Jiacai Liu
     1  J&amp;ouml;rn Horstmann
     1  Liang-Chi Hsieh
     1  Max Burke
     1  NaincyKumariKnoldus
     1  Nga Tran
     1  Patrick More
     1  Pierre Zemb
     1  Remzi Yang
     1  Sergey Melnychuk
     1  Stephen Carman
     1  doki
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following sections highlight some of the changes in this release. Of course, many other bug fixes and
improvements have been made and we encourage you to check out the
&lt;a href="https://github.com/apache/arrow-datafusion/blob/8.0.0/datafusion/CHANGELOG.md"&gt;changelog&lt;/a&gt; for full details.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;h2&gt;DDL Support&lt;/h2&gt;
&lt;p&gt;DDL support has been expanded to include the following commands for creating databases, schemas, and views. This
allows DataFusion to be used more effectively from the CLI.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CREATE DATABASE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREATE VIEW&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREATE SCHEMA&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREATE EXTERNAL TABLE&lt;/code&gt; now supports JSON files, &lt;code&gt;IF NOT EXISTS&lt;/code&gt;, and partition columns&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;SQL Support&lt;/h2&gt;
&lt;p&gt;The SQL query planner now supports a number of new SQL features, including:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Subqueries&lt;/em&gt;: when used via &lt;code&gt;IN&lt;/code&gt;, &lt;code&gt;EXISTS&lt;/code&gt;, and as scalars&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Grouping Sets&lt;/em&gt;: &lt;code&gt;CUBE&lt;/code&gt; and &lt;code&gt;ROLLUP&lt;/code&gt; grouping sets.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Aggregate functions&lt;/em&gt;: &lt;code&gt;approx_percentile&lt;/code&gt;, &lt;code&gt;approx_percentile_cont&lt;/code&gt;, &lt;code&gt;approx_percentile_cont_with_weight&lt;/code&gt;, &lt;code&gt;approx_distinct&lt;/code&gt;, &lt;code&gt;approx_median&lt;/code&gt; and &lt;code&gt;array&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;&lt;code&gt;null&lt;/code&gt; literals&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;bitwise operations&lt;/em&gt;: for example '&lt;code&gt;|&lt;/code&gt;'&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are also many bug fixes and improvements around normalizing identifiers consistently.&lt;/p&gt;
&lt;p&gt;We continue our tradition of incrementally releasing support for new
features as they are developed. Thus, while the physical plan may not yet
support all new features, it gets more complete each release. These
changes also make DataFusion an increasingly compelling choice for
projects looking for a SQL parser and query planner that can produce
optimized logical plans that can be translated to
their own execution engine.&lt;/p&gt;
&lt;h2&gt;Query Execution &amp;amp; Internals&lt;/h2&gt;
&lt;p&gt;There are several notable improvements and new features in the query execution engine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;code&gt;ExecutionContext&lt;/code&gt; has been renamed to &lt;code&gt;SessionContext&lt;/code&gt; and now supports multi-tenancy&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;ExecutionPlan&lt;/code&gt; trait is no longer &lt;code&gt;async&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A new serialization API for serializing plans to bytes (based on protobuf)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, we have added several foundational features to drive even
more advanced query processing into DataFusion, focusing on running
arbitrary queries larger than available memory, and pushing the
envelope for performance of sorting, grouping, and joining even
further:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Morsel-Driven Scheduler based on &lt;a href="https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf"&gt;"Morsel-Driven Parallelism: A NUMA-Aware Query
  Evaluation Framework for the Many-Core Age"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Consolidated object store implementation and integration with parquet decoding&lt;/li&gt;
&lt;li&gt;Memory Limited Spilling sort operator&lt;/li&gt;
&lt;li&gt;Memory Limited Sort-Merge join operator&lt;/li&gt;
&lt;li&gt;High performance JIT code generation for tuple comparisons&lt;/li&gt;
&lt;li&gt;Memory efficient Row Format&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Improved file support&lt;/h2&gt;
&lt;p&gt;DataFusion now supports JSON, both for reading and writing. There are also new DataFrame methods for writing query
results to files in CSV, Parquet, and JSON format.&lt;/p&gt;
&lt;h2&gt;Ballista&lt;/h2&gt;
&lt;p&gt;Ballista continues to mature and now supports a wider range of operators and expressions. There are also improvements
to the scheduler to support UDFs, and there are some robustness improvements, such as cleaning up work directories
and persisting session configs to allow schedulers to restart and continue processing in-flight jobs.&lt;/p&gt;
&lt;h2&gt;Upcoming Work&lt;/h2&gt;
&lt;p&gt;Here are some of the initiatives that the community plans on working on prior to the next release.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a &lt;a href="https://docs.google.com/document/d/1jNRbadyStSrV5kifwn0khufAwq6OnzGczG4z8oTQJP4/edit?usp=sharing"&gt;proposal to move Ballista to its own top-level arrow-ballista repository&lt;/a&gt;
 to decouple DataFusion and Ballista releases and to allow each project to have documentation better targeted at
  its particular audience.&lt;/li&gt;
&lt;li&gt;We plan on increasing the frequency of DataFusion releases, with monthly releases now instead of quarterly. This
  is driven by requests from the increasing number of projects that now depend on DataFusion.&lt;/li&gt;
&lt;li&gt;There is ongoing work to implement new optimizer rules to rewrite queries containing subquery expressions as
  joins, to support a wider range of queries.&lt;/li&gt;
&lt;li&gt;The new scheduler based on morsel-driven execution will continue to evolve in this next release, with work to
  refine IO abstractions to improve performance and integration with the new scheduler.&lt;/li&gt;
&lt;li&gt;Improved performance for Sort, Grouping and Joins&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, and learning about state-of-the-art query processing, we would
love to have you join us on the journey! You can help by trying out DataFusion on some of your own data and projects
and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable
for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Check out our new &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Introducing Apache Arrow DataFusion Contrib</title><link href="https://datafusion.apache.org/blog/2022/03/21/datafusion-contrib" rel="alternate"></link><published>2022-03-21T00:00:00+00:00</published><updated>2022-03-21T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-03-21:/blog/2022/03/21/datafusion-contrib</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Apache Arrow &lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Apache Arrow &lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that uses &lt;a href="https://arrow.apache.org"&gt;Apache Arrow&lt;/a&gt; as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out. DataFusion's pluggable design makes creating extensions at various points particular easy to build.&lt;/p&gt;
&lt;p&gt;DataFusion's  SQL, &lt;code&gt;DataFrame&lt;/code&gt;, and manual &lt;code&gt;PlanBuilder&lt;/code&gt; API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer &lt;em&gt;both&lt;/em&gt; the safety of dynamic languages as well as the resource efficiency of a compiled language.&lt;/p&gt;
&lt;p&gt;The DataFusion team is pleased to announce the creation of the &lt;a href="https://github.com/datafusion-contrib"&gt;DataFusion-Contrib&lt;/a&gt; GitHub organization to support and accelerate other projects.  While the core DataFusion library remains under Apache governance, the contrib organization provides a more flexible testing ground for new DataFusion features and a home for DataFusion extensions.  With this announcement, we are pleased to introduce the following inaugural DataFusion-Contrib repositories.&lt;/p&gt;
&lt;h2&gt;DataFusion-Python&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-python"&gt;project&lt;/a&gt; provides Python bindings to the core Rust implementation of DataFusion, which allows users to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Work with familiar SQL or DataFrame APIs to run queries in a safe, multi-threaded environment, returning results in Python&lt;/li&gt;
&lt;li&gt;Create User Defined Functions and User Defined Aggregate Functions for complex operations&lt;/li&gt;
&lt;li&gt;Pay no overhead to copy between Python and underlying Rust execution engine (by way of Apache Arrow arrays)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Upcoming enhancements&lt;/h3&gt;
&lt;p&gt;The team is focusing on exposing more features from the underlying Rust implementation of DataFusion and improving documentation.&lt;/p&gt;
&lt;h3&gt;How to install&lt;/h3&gt;
&lt;p&gt;From &lt;code&gt;pip&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install datafusion
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;python -m pip install datafusion
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;DataFusion-ObjectStore-S3&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-objectstore-s3"&gt;crate&lt;/a&gt; provides an &lt;code&gt;ObjectStore&lt;/code&gt; implementation for querying data stored in S3 or S3 compatible storage. This makes it almost as easy to query data that lives on S3 as lives in local files&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ability to create &lt;code&gt;S3FileSystem&lt;/code&gt; to register as part of DataFusion &lt;code&gt;ExecutionContext&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Register files or directories stored on S3 with &lt;code&gt;ctx.register_listing_table&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Upcoming enhancements&lt;/h3&gt;
&lt;p&gt;The current priority is adding python bindings for &lt;code&gt;S3FileSystem&lt;/code&gt;.  After that there will be async improvements as DataFusion adopts more of that functionality and we are looking into S3 Select functionality.&lt;/p&gt;
&lt;h3&gt;How to Install&lt;/h3&gt;
&lt;p&gt;Add the below to your &lt;code&gt;Cargo.toml&lt;/code&gt; in your Rust Project with DataFusion.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-toml"&gt;datafusion-objectstore-s3 = "0.1.0"
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;DataFusion-Substrait&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://substrait.io/"&gt;Substrait&lt;/a&gt; is an emerging standard that provides a cross-language serialization format for relational algebra (e.g. expressions and query plans).&lt;/p&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-substrait"&gt;crate&lt;/a&gt; provides a Substrait producer and consumer for DataFusion.  A producer converts a DataFusion logical plan into a Substrait protobuf and a consumer does the reverse.&lt;/p&gt;
&lt;p&gt;Examples of how to use this crate can be found &lt;a href="https://github.com/datafusion-contrib/datafusion-substrait/blob/main/src/lib.rs"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Potential Use Cases&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Replace custom DataFusion protobuf serialization.&lt;/li&gt;
&lt;li&gt;Make it easier to pass query plans over FFI boundaries, such as from Python to Rust&lt;/li&gt;
&lt;li&gt;Allow Apache Calcite query plans to be executed in DataFusion&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;DataFusion-BigTable&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-bigtable"&gt;crate&lt;/a&gt; implements &lt;a href="https://cloud.google.com/bigtable"&gt;Bigtable&lt;/a&gt; as a data source and physical executor for DataFusion queries.  It currently supports both UTF-8 string and 64-bit big-endian signed integers in Bigtable.  From a SQL perspective it supports both simple and composite row keys with &lt;code&gt;=&lt;/code&gt;, &lt;code&gt;IN&lt;/code&gt;, and &lt;code&gt;BETWEEN&lt;/code&gt; operators as well as projection pushdown.  The physical execution for queries is handled by this crate while any subsequent aggregation, group bys, or joins are handled in DataFusion.&lt;/p&gt;
&lt;h3&gt;Upcoming Enhancements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Predicate pushdown&lt;/li&gt;
&lt;li&gt;Value range&lt;/li&gt;
&lt;li&gt;Value Regex&lt;/li&gt;
&lt;li&gt;Timestamp range&lt;/li&gt;
&lt;li&gt;Multithreaded&lt;/li&gt;
&lt;li&gt;Partition aware execution&lt;/li&gt;
&lt;li&gt;Production ready&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;How to Install&lt;/h3&gt;
&lt;p&gt;Add the below to your &lt;code&gt;Cargo.toml&lt;/code&gt; in your Rust Project with DataFusion.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-toml"&gt;datafusion-bigtable = "0.1.0"
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;DataFusion-HDFS&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-objectstore-hdfs"&gt;crate&lt;/a&gt; introduces &lt;code&gt;HadoopFileSystem&lt;/code&gt; as a remote &lt;code&gt;ObjectStore&lt;/code&gt; which provides the ability to query HDFS files.  For HDFS access the &lt;a href="https://github.com/yahoNanJing/fs-hdfs"&gt;fs-hdfs&lt;/a&gt; library is used.&lt;/p&gt;
&lt;h2&gt;DataFusion-Tokomak&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-tokomak"&gt;crate&lt;/a&gt; provides an e-graph based DataFusion optimization framework based on the Rust &lt;a href="https://egraphs-good.github.io"&gt;egg&lt;/a&gt; library.  An e-graph is a data structure that powers the equality saturation optimization technique.&lt;/p&gt;
&lt;p&gt;As context, the optimizer framework within DataFusion is currently &lt;a href="https://github.com/apache/arrow-datafusion/issues/1972"&gt;under review&lt;/a&gt; with the objective of implementing a more strategic long term solution that is more efficient and simpler to develop.&lt;/p&gt;
&lt;p&gt;Some of the benefits of using &lt;code&gt;egg&lt;/code&gt; within DataFusion are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Implements optimized algorithms that are hard to match with manually written optimization passes&lt;/li&gt;
&lt;li&gt;Makes it easy and less verbose to add optimization rules&lt;/li&gt;
&lt;li&gt;Plugin framework to add more complex optimizations&lt;/li&gt;
&lt;li&gt;Egg does not depend on rule order and can lead to a higher level of optimization by being able to apply multiple rules at the same time until it converges&lt;/li&gt;
&lt;li&gt;Allows for cost-based optimizations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is an exciting new area for DataFusion with lots of opportunity for community involvement!&lt;/p&gt;
&lt;h2&gt;DataFusion-Tui&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-tui"&gt;DataFusion-tui&lt;/a&gt; aka &lt;code&gt;dft&lt;/code&gt; provides a feature rich terminal application for using DataFusion.  It has drawn inspiration and several features from &lt;code&gt;datafusion-cli&lt;/code&gt;.  In contrast to &lt;code&gt;datafusion-cli&lt;/code&gt; the objective of this tool is to provide a light SQL IDE experience for querying data with DataFusion.  This includes features such as the following which are currently implemented:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tab Management to provide clean and structured organization of DataFusion queries, results, &lt;code&gt;ExecutionContext&lt;/code&gt; information, and logs&lt;/li&gt;
&lt;li&gt;SQL Editor&lt;ul&gt;
&lt;li&gt;Text editor for writing SQL queries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Query History&lt;ul&gt;
&lt;li&gt;History of executed queries, their execution time, and the number of returned rows&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ExecutionContext&lt;/code&gt; information&lt;ul&gt;
&lt;li&gt;Expose information on which physical optimizers are used and which &lt;code&gt;ExecutionConfig&lt;/code&gt; settings are set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Logs&lt;ul&gt;
&lt;li&gt;Logs from &lt;code&gt;dft&lt;/code&gt;, DataFusion, and any dependent libraries&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Support for custom &lt;code&gt;ObjectStore&lt;/code&gt;s&lt;/li&gt;
&lt;li&gt;S3&lt;/li&gt;
&lt;li&gt;Preload DDL from &lt;code&gt;~/.datafusionrc&lt;/code&gt; to enable having local "database" available at startup&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Upcoming Enhancements&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;SQL Editor&lt;/li&gt;
&lt;li&gt;Command to write query results to file&lt;/li&gt;
&lt;li&gt;Multiple SQL editor tabs&lt;/li&gt;
&lt;li&gt;Expose more information from &lt;code&gt;ExecutionContext&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;A help tab that provides information on functions&lt;/li&gt;
&lt;li&gt;Query custom &lt;code&gt;TableProvider&lt;/code&gt;s such as &lt;a href="https://github.com/delta-io/delta-rs"&gt;DeltaTable&lt;/a&gt; or &lt;a href="https://github.com/datafusion-contrib/datafusion-bigtable"&gt;BigTable&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;DataFusion-Streams&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-streams"&gt;DataFusion-Stream&lt;/a&gt; is a new testing ground for creating a &lt;code&gt;StreamProvider&lt;/code&gt; in DataFusion that will enable querying streaming data sources such as Apache Kafka.  The implementation for this feature is currently being designed and is under active review.  Once the design is finalized the trait and attendant data structures will be added back to the core DataFusion crate.&lt;/p&gt;
&lt;h2&gt;DataFusion-Java&lt;/h2&gt;
&lt;p&gt;This &lt;a href="https://github.com/datafusion-contrib/datafusion-java"&gt;project&lt;/a&gt; created an initial set of Java bindings to DataFusion.  The project is currently in maintenance mode and is looking for maintainers to drive future development.&lt;/p&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, and learning about state of
the art query processing, we would love to have you join us on the journey! You
can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The best way to find out about creating new extensions within DataFusion-Contrib is reaching out on the &lt;code&gt;#arrow-rust&lt;/code&gt; channel of the Apache Software Foundation &lt;a href="https://join.slack.com/t/the-asf/shared_invite/zt-vlfbf7ch-HkbNHiU_uDlcH_RvaHv9gQ"&gt;Slack&lt;/a&gt; workspace.&lt;/p&gt;
&lt;p&gt;You can also check out our new &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more ways to engage with the community.&lt;/p&gt;
&lt;p&gt;Links for each DataFusion-Contrib repository are provided above if you would like to contribute to those.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 7.0.0 Release</title><link href="https://datafusion.apache.org/blog/2022/02/28/datafusion-7.0.0" rel="alternate"></link><published>2022-02-28T00:00:00+00:00</published><updated>2022-02-28T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2022-02-28:/blog/2022/02/28/datafusion-7.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an extensible query execution framework, written in Rust, that uses Apache Arrow as its in-memory format.&lt;/p&gt;
&lt;p&gt;When you want to extend your Rust project with &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/sql_status.html"&gt;SQL support&lt;/a&gt;, a DataFrame API, or the ability to read and process Parquet, JSON, Avro or CSV data, DataFusion is definitely worth checking out.&lt;/p&gt;
&lt;p&gt;DataFusion's  SQL, &lt;code&gt;DataFrame&lt;/code&gt;, and manual &lt;code&gt;PlanBuilder&lt;/code&gt; API let users access a sophisticated query optimizer and execution engine capable of fast, resource efficient, and parallel execution that takes optimal advantage of todays multicore hardware. Being written in Rust means DataFusion can offer &lt;em&gt;both&lt;/em&gt; the safety of dynamic languages as well as the resource efficiency of a compiled language.&lt;/p&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 7.0.0 release. This covers 4 months of development work
and includes 195 commits from the following 37 distinct contributors.&lt;/p&gt;
&lt;!--
git log --pretty=oneline 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     134

git shortlog -sn 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
      29

      Carlos and xudong963 are same individual
--&gt;
&lt;pre&gt;&lt;code&gt;    44  Andrew Lamb
    24  Kun Liu
    23  Jiayu Liu
    17  xudong.w
    11  Yijie Shen
     9  Matthew Turner
     7  Liang-Chi Hsieh
     5  Lin Ma
     4  Stephen Carman
     4  James Katz
     4  Dmitry Patsura
     4  QP Hou
     3  dependabot[bot]
     3  Remzi Yang
     3  Yang
     3  ic4y
     3  Dani&amp;euml;l Heres
     2  Andy Grove
     2  Raphael Taylor-Davies
     2  Jason Tianyi Wang
     2  Dan Harris
     2  Sergey Melnychuk
     1  Nitish Tiwari
     1  Dom
     1  Eduard Karacharov
     1  Javier Goday
     1  Boaz
     1  Marko Mikulicic
     1  Max Burke
     1  Carol (Nichols || Goulding)
     1  Phillip Cloud
     1  Rich
     1  Toby Hede
     1  Will Jones
     1  r.4ntix
     1  rdettai
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following section highlights some of the improvements in this release. Of course, many other bug fixes and improvements have also been made and we refer you to the complete &lt;a href="https://github.com/apache/arrow-datafusion/blob/7.0.0/datafusion/CHANGELOG.md"&gt;changelog&lt;/a&gt; for the full detail.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;DataFusion Crate&lt;/li&gt;
&lt;li&gt;The DataFusion crate is being split into multiple crates to decrease compilation times and improve the development experience. Initially, &lt;code&gt;datafusion-common&lt;/code&gt; (the core DataFusion components) and &lt;code&gt;datafusion-expr&lt;/code&gt; (DataFusion expressions, functions, and operators) have been split out. There will be additional splits after the 7.0 release.&lt;/li&gt;
&lt;li&gt;Performance Improvements and Optimizations&lt;/li&gt;
&lt;li&gt;Arrow&amp;rsquo;s dyn scalar kernels are now used to enable efficient operations on &lt;code&gt;DictionaryArray&lt;/code&gt;s &lt;a href="https://github.com/apache/arrow-datafusion/pull/1685"&gt;#1685&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Switch from &lt;code&gt;std::sync::Mutex&lt;/code&gt; to &lt;code&gt;parking_lot::Mutex&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1720"&gt;#1720&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;New Features&lt;/li&gt;
&lt;li&gt;Support for memory tracking and spilling to disk&lt;ul&gt;
&lt;li&gt;MemoryMananger and DiskManager &lt;a href="https://github.com/apache/arrow-datafusion/pull/1526"&gt;#1526&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Out of core sort &lt;a href="https://github.com/apache/arrow-datafusion/pull/1526"&gt;#1526&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;New metrics&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Gauge&lt;/code&gt; and &lt;code&gt;CurrentMemoryUsage&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1682"&gt;#1682&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Spill_count&lt;/code&gt; and &lt;code&gt;spilled_bytes&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1641"&gt;#1641&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;New math functions&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Approx_quantile&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1539"&gt;#1529&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;stddev&lt;/code&gt; and &lt;code&gt;variance&lt;/code&gt; (sample and population) &lt;a href="https://github.com/apache/arrow-datafusion/pull/1525"&gt;#1525&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;corr&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1561"&gt;#1561&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Support decimal type &lt;a href="https://github.com/apache/arrow-datafusion/pull/1394"&gt;#1394&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1407"&gt;#1407&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1408"&gt;#1408&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1431"&gt;#1431&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1483"&gt;#1483&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1554"&gt;#1554&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1640"&gt;#1640&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for reading Parquet files with evolved schemas &lt;a href="https://github.com/apache/arrow-datafusion/pull/1622"&gt;#1622&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1709"&gt;#1709&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for registering &lt;code&gt;DataFrame&lt;/code&gt; as table &lt;a href="https://github.com/apache/arrow-datafusion/pull/1699"&gt;#1699&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support for the &lt;code&gt;substring&lt;/code&gt; function &lt;a href="https://github.com/apache/arrow-datafusion/pull/1621"&gt;#1621&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support &lt;code&gt;array_agg(distinct ...)&lt;/code&gt; &lt;a href="https://github.com/apache/arrow-datafusion/pull/1579"&gt;#1579&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Support &lt;code&gt;sort&lt;/code&gt; on unprojected columns &lt;a href="https://github.com/apache/arrow-datafusion/pull/1415"&gt;#1415&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Additional Integration Points&lt;/li&gt;
&lt;li&gt;A new public Expression simplification API &lt;a href="https://github.com/apache/arrow-datafusion/pull/1717"&gt;#1717&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib"&gt;DataFusion-Contrib&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;A new GitHub organization created as a home for both &lt;code&gt;DataFusion&lt;/code&gt; extensions and as a testing ground for new features.&lt;ul&gt;
&lt;li&gt;Extensions&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-python"&gt;DataFusion-Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-java"&gt;DataFusion-Java&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-hdfs-native"&gt;DataFusion-hdsfs-native&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-objectstore-s3"&gt;DataFusion-ObjectStore-s3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;New Features&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/datafusion-contrib/datafusion-streams"&gt;DataFusion-Streams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jorgecarleitao/arrow2"&gt;Arrow2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;An &lt;a href="https://github.com/apache/arrow-datafusion/tree/arrow2"&gt;Arrow2 Branch&lt;/a&gt; has been created.  There are ongoing discussions in &lt;a href="https://github.com/apache/arrow-datafusion/issues/1532"&gt;DataFusion&lt;/a&gt; and &lt;a href="https://github.com/apache/arrow-rs/issues/1176"&gt;arrow-rs&lt;/a&gt; about migrating &lt;code&gt;DataFusion&lt;/code&gt; to &lt;code&gt;Arrow2&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Documentation and Roadmap&lt;/h1&gt;
&lt;p&gt;We are working to consolidate the documentation into the &lt;a href="https://arrow.apache.org/datafusion"&gt;official site&lt;/a&gt;.  You can find more details there on topics such as the &lt;a href="https://arrow.apache.org/datafusion/user-guide/sql/index.html"&gt;SQL status&lt;/a&gt;  and a &lt;a href="https://arrow.apache.org/datafusion/user-guide/introduction.html#introduction"&gt;user guide&lt;/a&gt;. This is also an area we would love to get help from the broader community &lt;a href="https://github.com/apache/arrow-datafusion/issues/1821"&gt;#1821&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To provide transparency on DataFusion&amp;rsquo;s priorities to users and developers a three month roadmap will be published at the beginning of each quarter.  This can be found here &lt;a href="https://arrow.apache.org/datafusion/specification/roadmap.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Upcoming Attractions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Ballista is gaining momentum, and several groups are now evaluating and contributing to the project.&lt;/li&gt;
&lt;li&gt;Some of the proposed improvements&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/1701"&gt;Improvements Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/1675"&gt;Extensibility&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/1702"&gt;File system access&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion/issues/1704"&gt;Cluster state&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Continued improvements for working with limited resources and large datasets&lt;/li&gt;
&lt;li&gt;Memory limited joins&lt;a href="https://github.com/apache/arrow-datafusion/issues/1599"&gt;#1599&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sort-merge join&lt;a href="https://github.com/apache/arrow-datafusion/issues/141"&gt;#141&lt;/a&gt;&lt;a href="https://github.com/apache/arrow-datafusion/pull/1776"&gt;#1776&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Introduce row based bytes representation &lt;a href="https://github.com/apache/arrow-datafusion/pull/1708"&gt;#1708&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, and learning about state of
the art query processing, we would love to have you join us on the journey! You
can help by trying out DataFusion on some of your own data and projects and let us know how it goes or contribute a PR with documentation, tests or code. A list of open issues suitable for beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Check out our new &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 6.0.0 Release</title><link href="https://datafusion.apache.org/blog/2021/11/19/2021-11-8-datafusion-6.0.0.md" rel="alternate"></link><published>2021-11-19T00:00:00+00:00</published><updated>2021-11-19T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2021-11-19:/blog/2021/11/19/2021-11-8-datafusion-6.0.0.md</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an embedded
query engine which leverages the unique features of
&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache
Arrow&lt;/a&gt; to provide a system that is high
performance, easy to connect, easy to embed, and high quality.&lt;/p&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 6.0.0 release. This covers …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;&lt;a href="https://arrow.apache.org/datafusion/"&gt;DataFusion&lt;/a&gt; is an embedded
query engine which leverages the unique features of
&lt;a href="https://www.rust-lang.org/"&gt;Rust&lt;/a&gt; and &lt;a href="https://arrow.apache.org/"&gt;Apache
Arrow&lt;/a&gt; to provide a system that is high
performance, easy to connect, easy to embed, and high quality.&lt;/p&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 6.0.0 release. This covers 4 months of development work
and includes 134 commits from the following 28 distinct contributors.&lt;/p&gt;
&lt;!--
git log --pretty=oneline 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     134

git shortlog -sn 5.0.0..6.0.0 datafusion datafusion-cli datafusion-examples | wc -l
      29

      Carlos and xudong963 are same individual
--&gt;
&lt;pre&gt;&lt;code&gt;    28  Andrew Lamb
    26  Jiayu Liu
    13  xudong963
     9  rdettai
     9  QP Hou
     6  Matthew Turner
     5  Dani&amp;euml;l Heres
     4  Guillaume Balaine
     3  Francis Du
     3  Marco Neumann
     3  Jon Mease
     3  Nga Tran
     2  Yijie Shen
     2  Ruihang Xia
     2  Liang-Chi Hsieh
     2  baishen
     2  Andy Grove
     2  Jason Tianyi Wang
     1  Nan Zhu
     1  Antoine Wendlinger
     1  Kriszti&amp;aacute;n Sz&amp;udblac;cs
     1  Mike Seddon
     1  Conner Murphy
     1  Patrick More
     1  Taehoon Moon
     1  Tiphaine Ruy
     1  adsharma
     1  lichuan6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes
and improvements have been made: we refer you to the complete
&lt;a href="https://github.com/apache/arrow-datafusion/blob/6.0.0/datafusion/CHANGELOG.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;New Website&lt;/h1&gt;
&lt;p&gt;Befitting a growing project, DataFusion now has its
&lt;a href="https://arrow.apache.org/datafusion/"&gt;own website&lt;/a&gt; hosted as part of the
main &lt;a href="https://arrow.apache.org"&gt;Apache Arrow Website&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Roadmap&lt;/h1&gt;
&lt;p&gt;The community worked to gather their thoughts about where we are
taking DataFusion into a public
&lt;a href="https://arrow.apache.org/datafusion/specification/roadmap.html"&gt;Roadmap&lt;/a&gt;
for the first time&lt;/p&gt;
&lt;h1&gt;New Features&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Runtime operator metrics collection framework&lt;/li&gt;
&lt;li&gt;Object store abstraction for unified access to local or remote storage&lt;/li&gt;
&lt;li&gt;Hive style table partitioning support, for Parquet, CSV, Avro and Json files&lt;/li&gt;
&lt;li&gt;DataFrame API support for: &lt;code&gt;except&lt;/code&gt;, &lt;code&gt;intersect&lt;/code&gt;, &lt;code&gt;show&lt;/code&gt;, &lt;code&gt;limit&lt;/code&gt; and window functions&lt;/li&gt;
&lt;li&gt;SQL&lt;/li&gt;
&lt;li&gt;&lt;code&gt;EXPLAIN ANALYZE&lt;/code&gt; with runtime metrics&lt;/li&gt;
&lt;li&gt;&lt;code&gt;trim ( [ LEADING | TRAILING | BOTH ] [ FROM ] string text [, characters text ] )&lt;/code&gt; syntax&lt;/li&gt;
&lt;li&gt;Postgres style regular expression matching operators &lt;code&gt;~&lt;/code&gt;, &lt;code&gt;~*&lt;/code&gt;, &lt;code&gt;!~&lt;/code&gt;, and &lt;code&gt;!~*&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;SQL set operators &lt;code&gt;UNION&lt;/code&gt;, &lt;code&gt;INTERSECT&lt;/code&gt;, and &lt;code&gt;EXCEPT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cume_dist&lt;/code&gt;, &lt;code&gt;percent_rank&lt;/code&gt; window functions&lt;/li&gt;
&lt;li&gt;&lt;code&gt;digest&lt;/code&gt;, &lt;code&gt;blake2s&lt;/code&gt;, &lt;code&gt;blake2b&lt;/code&gt;, &lt;code&gt;blake3&lt;/code&gt; crypto functions&lt;/li&gt;
&lt;li&gt;HyperLogLog based &lt;code&gt;approx_distinct&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;is distinct from&lt;/code&gt; and &lt;code&gt;is not distinct from&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CREATE TABLE AS SELECT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Accessing elements of nested &lt;code&gt;Struct&lt;/code&gt; and &lt;code&gt;List&lt;/code&gt; columns (e.g. &lt;code&gt;SELECT struct_column['field_name'], array_column[0] FROM ...&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Boolean expressions in &lt;code&gt;CASE&lt;/code&gt; statement&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DROP TABLE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VALUES&lt;/code&gt; List&lt;/li&gt;
&lt;li&gt;Postgres regex match operators&lt;/li&gt;
&lt;li&gt;Support for Avro format&lt;/li&gt;
&lt;li&gt;Support for &lt;code&gt;ScalarValue::Struct&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Automatic schema inference for CSV files&lt;/li&gt;
&lt;li&gt;Better interactive editing support in &lt;code&gt;datafusion-cli&lt;/code&gt; as well as &lt;code&gt;psql&lt;/code&gt; style commands such as &lt;code&gt;\d&lt;/code&gt;, &lt;code&gt;\?&lt;/code&gt;, and &lt;code&gt;\q&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Generic constant evaluation and simplification framework&lt;/li&gt;
&lt;li&gt;Added common subexpression eliminate query plan optimization rule&lt;/li&gt;
&lt;li&gt;Python binding 0.4.0 with all Datafusion 6.0.0 features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With these new features, we are also now passing TPC-H queries 8, 13 and 21.&lt;/p&gt;
&lt;p&gt;For the full list of new features with their relevant PRs, see the
&lt;a href="https://github.com/apache/arrow-datafusion/blob/6.0.0/datafusion/CHANGELOG.md"&gt;enhancements section&lt;/a&gt;
in the changelog.&lt;/p&gt;
&lt;h1&gt;&lt;code&gt;async&lt;/code&gt; planning and decoupling file format from table layout&lt;/h1&gt;
&lt;p&gt;Driven by the need to support Hive style table partitioning, @rdettai
introduced the following design change to the Datafusion core.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The code for reading specific file formats (&lt;code&gt;Parquet&lt;/code&gt;, &lt;code&gt;Avro&lt;/code&gt;, &lt;code&gt;CSV&lt;/code&gt;, and
&lt;code&gt;JSON&lt;/code&gt;) was separated from the logic that handles grouping sets of
files into execution partitions.&lt;/li&gt;
&lt;li&gt;The query planning process was made &lt;code&gt;async&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a result, we are able to replace the old &lt;code&gt;Parquet&lt;/code&gt;, &lt;code&gt;CSV&lt;/code&gt; and &lt;code&gt;JSON&lt;/code&gt; table
providers with a single &lt;code&gt;ListingTable&lt;/code&gt; table provider.&lt;/p&gt;
&lt;p&gt;This also sets up DataFusion and its plug-in ecosystem to
supporting a wide range of catalogs and various object store implementations.
You can read more about this change in the
&lt;a href="https://docs.google.com/document/d/1Bd4-PLLH-pHj0BquMDsJ6cVr_awnxTuvwNJuWsTHxAQ"&gt;design document&lt;/a&gt;
and on the &lt;a href="https://github.com/apache/arrow-datafusion/pull/1010"&gt;arrow-datafusion#1010 PR&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to have you! You
can help by trying out DataFusion on some of your own data and projects and filing bug reports and helping to
improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for
beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;
and the full list is &lt;a href="https://github.com/apache/arrow-datafusion/issues"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Check out our new &lt;a href="https://arrow.apache.org/datafusion/community/communication.html"&gt;Communication Doc&lt;/a&gt; on more
ways to engage with the community.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow Ballista 0.5.0 Release</title><link href="https://datafusion.apache.org/blog/2021/08/18/ballista-0.5.0" rel="alternate"></link><published>2021-08-18T00:00:00+00:00</published><updated>2021-08-18T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2021-08-18:/blog/2021/08/18/ballista-0.5.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;Ballista extends DataFusion to provide support for distributed queries. This is the first release of Ballista since 
the project was &lt;a href="https://arrow.apache.org/blog/2021/04/12/ballista-donation/"&gt;donated&lt;/a&gt; to the Apache Arrow project 
and includes 80 commits from 11 contributors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git shortlog -sn 4.0.0..5.0.0 ballista/rust/client ballista/rust/core ballista/rust …&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;Ballista extends DataFusion to provide support for distributed queries. This is the first release of Ballista since 
the project was &lt;a href="https://arrow.apache.org/blog/2021/04/12/ballista-donation/"&gt;donated&lt;/a&gt; to the Apache Arrow project 
and includes 80 commits from 11 contributors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git shortlog -sn 4.0.0..5.0.0 ballista/rust/client ballista/rust/core ballista/rust/executor ballista/rust/scheduler
  27  Andy Grove
  15  Jiayu Liu
  12  Andrew Lamb
   8  Ximo Guanter
   6  Dani&amp;euml;l Heres
   5  QP Hou
   2  Jorge Leitao
   1  Javier Goday
   1  K.I. (Dennis) Jung
   1  Mike Seddon
   1  sathis
&lt;/code&gt;&lt;/pre&gt;
&lt;!--
$ git log --pretty=oneline 4.0.0..5.0.0 ballista/rust/client ballista/rust/core ballista/rust/executor ballista/rust/scheduler ballista-examples/ | wc -l
80
--&gt;
&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes 
and improvements have been made: we refer you to the &lt;a href="https://github.com/apache/arrow-datafusion/blob/5.0.0/ballista/CHANGELOG.md"&gt;complete changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Performance and Scalability&lt;/h1&gt;
&lt;p&gt;Ballista is now capable of running complex SQL queries at scale and supports scalable distributed joins. We have been 
benchmarking using individual queries from the TPC-H benchmark at scale factors up to 1000 (1 TB). When running against 
CSV files, performance is generally very close to DataFusion, and significantly faster in some cases due to the fact 
that the scheduler limits the number of concurrent tasks that run at any given time. Performance against large Parquet 
datasets is currently non ideal due to some issues (&lt;a href="https://github.com/apache/arrow-datafusion/issues/867"&gt;#867&lt;/a&gt;, 
&lt;a href="https://github.com/apache/arrow-datafusion/issues/868"&gt;#868&lt;/a&gt;) that we hope to resolve for the next release. &lt;/p&gt;
&lt;h1&gt;New Features&lt;/h1&gt;
&lt;p&gt;The main new features in this release are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ballista queries can now be executed by calling DataFrame.collect()&lt;/li&gt;
&lt;li&gt;The shuffle mechanism has been re-implemented&lt;/li&gt;
&lt;li&gt;Distributed hash-partitioned joins are now supported&lt;/li&gt;
&lt;li&gt;Keda autoscaling is supported&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To get started with Ballista, refer to the &lt;a href="https://docs.rs/ballista/0.5.0/ballista/"&gt;crate documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now that the basic functionality is in place, the focus for the next release will be to improve the performance and
scalability as well as improving the documentation.&lt;/p&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to Ballista, we would love to have you! You
can help by trying out Ballista on some of your own data and projects and filing bug reports and helping to
improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for
beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt;
and the full list is &lt;a href="https://github.com/apache/arrow-datafusion/issues"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Apache Arrow DataFusion 5.0.0 Release</title><link href="https://datafusion.apache.org/blog/2021/08/18/datafusion-5.0.0" rel="alternate"></link><published>2021-08-18T00:00:00+00:00</published><updated>2021-08-18T00:00:00+00:00</updated><author><name>pmc</name></author><id>tag:datafusion.apache.org,2021-08-18:/blog/2021/08/18/datafusion-5.0.0</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 5.0.0 release. This covers 4 months of development work 
and includes 211 commits from the following 31 distinct contributors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git shortlog -sn 4.0.0..5.0.0 datafusion datafusion-cli datafusion-examples
    61  Jiayu Liu
    47  Andrew Lamb
    27 …&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;The Apache Arrow team is pleased to announce the DataFusion 5.0.0 release. This covers 4 months of development work 
and includes 211 commits from the following 31 distinct contributors.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git shortlog -sn 4.0.0..5.0.0 datafusion datafusion-cli datafusion-examples
    61  Jiayu Liu
    47  Andrew Lamb
    27  Dani&amp;euml;l Heres
    13  QP Hou
    13  Andy Grove
     4  Javier Goday
     4  sathis
     3  Ruan Pearce-Authers
     3  Raphael Taylor-Davies
     3  Jorge Leitao
     3  Cui Wenzheng
     3  Mike Seddon
     3  Edd Robinson
     2  思维
     2  Liang-Chi Hsieh
     2  Michael Lu
     2  Parth Sarthy
     2  Patrick More
     2  Rich
     1  Charlie Evans
     1  Gang Liao
     1  Agata Naomichi
     1  Ritchie Vink
     1  Evan Chan
     1  Ruihang Xia
     1  Todd Treece
     1  Yichen Wang
     1  baishen
     1  Nga Tran
     1  rdettai
     1  Marco Neumann
&lt;/code&gt;&lt;/pre&gt;
&lt;!--
$ git log --pretty=oneline 4.0.0..5.0.0 datafusion datafusion-cli datafusion-examples | wc -l
     211
--&gt;
&lt;p&gt;The release notes below are not exhaustive and only expose selected highlights of the release. Many other bug fixes 
and improvements have been made: we refer you to the complete 
&lt;a href="https://github.com/apache/arrow-datafusion/blob/5.0.0/datafusion/CHANGELOG.md"&gt;changelog&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Performance&lt;/h1&gt;
&lt;p&gt;There have been numerous performance improvements in this release. The following chart shows the relative 
performance of individual TPC-H queries compared to the previous release.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;TPC-H @ scale factor 100, in parquet format. Concurrency 24.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/blog/images/2021-08-18-datafusion500perf.png"/&gt;&lt;/p&gt;
&lt;p&gt;We also extended support for more TPC-H queries: q7, q8, q9 and q13 are running successfully in DataFusion 5.0.&lt;/p&gt;
&lt;h1&gt;New Features&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Initial support for SQL-99 Analytics (WINDOW functions)&lt;/li&gt;
&lt;li&gt;Improved JOIN support: cross join, semi-join, anti join, and fixes to null handling&lt;/li&gt;
&lt;li&gt;Improved EXPLAIN support&lt;/li&gt;
&lt;li&gt;Initial implementation of metrics in the physical plan&lt;/li&gt;
&lt;li&gt;Support for SELECT DISTINCT&lt;/li&gt;
&lt;li&gt;Support for Json and NDJson formatted inputs&lt;/li&gt;
&lt;li&gt;Query column with relations&lt;/li&gt;
&lt;li&gt;Added more datetime related functions: &lt;code&gt;now&lt;/code&gt;, &lt;code&gt;date_trunc&lt;/code&gt;, &lt;code&gt;to_timestamp_millis&lt;/code&gt;, &lt;code&gt;to_timestamp_micros&lt;/code&gt;, &lt;code&gt;to_timestamp_seconds&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Streaming Dataframe.collect&lt;/li&gt;
&lt;li&gt;Support table column aliases&lt;/li&gt;
&lt;li&gt;Answer count(*), min() and max() queries using only statistics&lt;/li&gt;
&lt;li&gt;Non-equi-join filters in JOIN conditions&lt;/li&gt;
&lt;li&gt;Modulus operation&lt;/li&gt;
&lt;li&gt;Support group by column positions&lt;/li&gt;
&lt;li&gt;Added constant folding query optimizer&lt;/li&gt;
&lt;li&gt;Hash partitioned aggregation&lt;/li&gt;
&lt;li&gt;Added &lt;code&gt;random&lt;/code&gt; SQL function&lt;/li&gt;
&lt;li&gt;Implemented count distinct for floats and dictionary types&lt;/li&gt;
&lt;li&gt;Re-exported arrow and parquet crates in Datafusion&lt;/li&gt;
&lt;li&gt;General row group pruning logic that&amp;rsquo;s agnostic to storage format&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;How to Get Involved&lt;/h1&gt;
&lt;p&gt;If you are interested in contributing to DataFusion, we would love to have you! You 
can help by trying out DataFusion on some of your own data and projects and filing bug reports and helping to 
improve the documentation, or contribute to the documentation, tests or code. A list of open issues suitable for 
beginners is &lt;a href="https://github.com/apache/arrow-datafusion/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22"&gt;here&lt;/a&gt; 
and the full list is &lt;a href="https://github.com/apache/arrow-datafusion/issues"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>Ballista: A Distributed Scheduler for Apache Arrow</title><link href="https://datafusion.apache.org/blog/2021/04/12/ballista-donation" rel="alternate"></link><published>2021-04-12T00:00:00+00:00</published><updated>2021-04-12T00:00:00+00:00</updated><author><name>agrove</name></author><id>tag:datafusion.apache.org,2021-04-12:/blog/2021/04/12/ballista-donation</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are excited to announce that &lt;a href="https://github.com/apache/arrow-datafusion/tree/master/ballista"&gt;Ballista&lt;/a&gt; has been donated 
to the Apache Arrow project. &lt;/p&gt;
&lt;p&gt;Ballista is a distributed compute platform primarily implemented in Rust, and powered by Apache Arrow. It is built
on an architecture that allows other programming languages (such as Python, C++, and Java) to be supported …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are excited to announce that &lt;a href="https://github.com/apache/arrow-datafusion/tree/master/ballista"&gt;Ballista&lt;/a&gt; has been donated 
to the Apache Arrow project. &lt;/p&gt;
&lt;p&gt;Ballista is a distributed compute platform primarily implemented in Rust, and powered by Apache Arrow. It is built
on an architecture that allows other programming languages (such as Python, C++, and Java) to be supported as
first-class citizens without paying a penalty for serialization costs.&lt;/p&gt;
&lt;p&gt;The foundational technologies in Ballista are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://arrow.apache.org/"&gt;Apache Arrow&lt;/a&gt; memory model and compute kernels for efficient processing of data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/apache/arrow-datafusion"&gt;Apache Arrow DataFusion&lt;/a&gt; query planning and 
  execution framework, extended by Ballista to provide distributed planning and execution.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://arrow.apache.org/blog/2019/10/13/introducing-arrow-flight/"&gt;Apache Arrow Flight Protocol&lt;/a&gt; for efficient
  data transfer between processes.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developers.google.com/protocol-buffers"&gt;Google Protocol Buffers&lt;/a&gt; for serializing query plans.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; for packaging up executors along with user-defined code.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ballista can be deployed as a standalone cluster and also supports &lt;a href="https://kubernetes.io/"&gt;Kubernetes&lt;/a&gt;. In either
case, the scheduler can be configured to use &lt;a href="https://etcd.io/"&gt;etcd&lt;/a&gt; as a backing store to (eventually) provide
redundancy in the case of a scheduler failing.&lt;/p&gt;
&lt;h2&gt;Status&lt;/h2&gt;
&lt;p&gt;The Ballista project is at an early stage of development. However, it is capable of running complex analytics queries 
in a distributed cluster with reasonable performance (comparable to more established distributed query frameworks).&lt;/p&gt;
&lt;p&gt;One of the benefits of Ballista being part of the Arrow codebase is that there is now an opportunity to push parts of 
the scheduler down to DataFusion so that is possible to seamlessly scale across cores in DataFusion, and across nodes 
in Ballista, using the same unified query scheduler.&lt;/p&gt;
&lt;h2&gt;Contributors Welcome!&lt;/h2&gt;
&lt;p&gt;If you are excited about being able to use Rust for distributed compute and ETL and would like to contribute to this 
work then there are many ways to get involved. The simplest way to get started is to try out Ballista against your own 
datasets and file bug reports for any issues that you find. You could also check out the current 
&lt;a href="https://issues.apache.org/jira/issues/?jql=project%20%3D%20ARROW%20AND%20component%20%3D%20%22Rust%20-%20Ballista%22"&gt;list of issues&lt;/a&gt; and have a go at fixing one.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/apache/arrow/blob/master/rust/README.md#arrow-rust-community"&gt;Arrow Rust Community&lt;/a&gt;
section of the Rust README provides information on other ways to interact with the Ballista contributors and 
maintainers.&lt;/p&gt;</content><category term="blog"></category></entry><entry><title>DataFusion: A Rust-native Query Engine for Apache Arrow</title><link href="https://datafusion.apache.org/blog/2019/02/04/datafusion-donation" rel="alternate"></link><published>2019-02-04T00:00:00+00:00</published><updated>2019-02-04T00:00:00+00:00</updated><author><name>agrove</name></author><id>tag:datafusion.apache.org,2019-02-04:/blog/2019/02/04/datafusion-donation</id><summary type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are excited to announce that &lt;a href="https://github.com/apache/arrow-datafusion"&gt;DataFusion&lt;/a&gt; has been donated to the Apache Arrow project. DataFusion is an in-memory query engine for the Rust implementation of Apache Arrow.&lt;/p&gt;
&lt;p&gt;Although DataFusion was started two years ago, it was recently re-implemented to be Arrow-native and currently has limited capabilities but does support …&lt;/p&gt;</summary><content type="html">&lt;!--
{% comment %}
Licensed to the Apache Software Foundation (ASF) under one or more
contributor license agreements.  See the NOTICE file distributed with
this work for additional information regarding copyright ownership.
The ASF licenses this file to you under the Apache License, Version 2.0
(the "License"); you may not use this file except in compliance with
the License.  You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
{% endcomment %}
--&gt;
&lt;p&gt;We are excited to announce that &lt;a href="https://github.com/apache/arrow-datafusion"&gt;DataFusion&lt;/a&gt; has been donated to the Apache Arrow project. DataFusion is an in-memory query engine for the Rust implementation of Apache Arrow.&lt;/p&gt;
&lt;p&gt;Although DataFusion was started two years ago, it was recently re-implemented to be Arrow-native and currently has limited capabilities but does support SQL queries against iterators of RecordBatch and has support for CSV files. There are plans to &lt;a href="https://issues.apache.org/jira/browse/ARROW-4466"&gt;add support for Parquet files&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;SQL support is limited to projection (&lt;code&gt;SELECT&lt;/code&gt;), selection (&lt;code&gt;WHERE&lt;/code&gt;), and simple aggregates (&lt;code&gt;MIN&lt;/code&gt;, &lt;code&gt;MAX&lt;/code&gt;, &lt;code&gt;SUM&lt;/code&gt;) with an optional &lt;code&gt;GROUP BY&lt;/code&gt; clause.&lt;/p&gt;
&lt;p&gt;Supported expressions are identifiers, literals, simple math operations (&lt;code&gt;+&lt;/code&gt;, &lt;code&gt;-&lt;/code&gt;, &lt;code&gt;*&lt;/code&gt;, &lt;code&gt;/&lt;/code&gt;), binary expressions (&lt;code&gt;AND&lt;/code&gt;, &lt;code&gt;OR&lt;/code&gt;), equality and comparison operators (&lt;code&gt;=&lt;/code&gt;, &lt;code&gt;!=&lt;/code&gt;, &lt;code&gt;&amp;lt;&lt;/code&gt;, &lt;code&gt;&amp;lt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;=&lt;/code&gt;, &lt;code&gt;&amp;gt;&lt;/code&gt;), and &lt;code&gt;CAST(expr AS type)&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Example&lt;/h2&gt;
&lt;p&gt;The following example demonstrates running a simple aggregate SQL query against a CSV file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-rust"&gt;// create execution context
let mut ctx = ExecutionContext::new();

// define schema for data source (csv file)
let schema = Arc::new(Schema::new(vec![
    Field::new("c1", DataType::Utf8, false),
    Field::new("c2", DataType::UInt32, false),
    Field::new("c3", DataType::Int8, false),
    Field::new("c4", DataType::Int16, false),
    Field::new("c5", DataType::Int32, false),
    Field::new("c6", DataType::Int64, false),
    Field::new("c7", DataType::UInt8, false),
    Field::new("c8", DataType::UInt16, false),
    Field::new("c9", DataType::UInt32, false),
    Field::new("c10", DataType::UInt64, false),
    Field::new("c11", DataType::Float32, false),
    Field::new("c12", DataType::Float64, false),
    Field::new("c13", DataType::Utf8, false),
]));

// register csv file with the execution context
let csv_datasource =
    CsvDataSource::new("test/data/aggregate_test_100.csv", schema.clone(), 1024);
ctx.register_datasource("aggregate_test_100", Rc::new(RefCell::new(csv_datasource)));

let sql = "SELECT c1, MIN(c12), MAX(c12) FROM aggregate_test_100 WHERE c11 &amp;gt; 0.1 AND c11 &amp;lt; 0.9 GROUP BY c1";

// execute the query
let relation = ctx.sql(&amp;amp;sql).unwrap();
let mut results = relation.borrow_mut();

// iterate over the results
while let Some(batch) = results.next().unwrap() {
    println!(
        "RecordBatch has {} rows and {} columns",
        batch.num_rows(),
        batch.num_columns()
    );

    let c1 = batch
        .column(0)
        .as_any()
        .downcast_ref::&amp;lt;BinaryArray&amp;gt;()
        .unwrap();

    let min = batch
        .column(1)
        .as_any()
        .downcast_ref::&amp;lt;Float64Array&amp;gt;()
        .unwrap();

    let max = batch
        .column(2)
        .as_any()
        .downcast_ref::&amp;lt;Float64Array&amp;gt;()
        .unwrap();

    for i in 0..batch.num_rows() {
        let c1_value: String = String::from_utf8(c1.value(i).to_vec()).unwrap();
        println!("{}, Min: {}, Max: {}", c1_value, min.value(i), max.value(i),);
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Roadmap&lt;/h2&gt;
&lt;p&gt;The roadmap for DataFusion will depend on interest from the Rust community, but here are some of the short term items that are planned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extending test coverage of the existing functionality&lt;/li&gt;
&lt;li&gt;Adding support for Parquet data sources&lt;/li&gt;
&lt;li&gt;Implementing more SQL features such as &lt;code&gt;JOIN&lt;/code&gt;, &lt;code&gt;ORDER BY&lt;/code&gt; and &lt;code&gt;LIMIT&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Implement a DataFrame API as an alternative to SQL&lt;/li&gt;
&lt;li&gt;Adding support for partitioning and parallel query execution using Rust's async and await functionality&lt;/li&gt;
&lt;li&gt;Creating a Docker image to make it easy to use DataFusion as a standalone query tool for interactive and batch queries&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Contributors Welcome!&lt;/h2&gt;
&lt;p&gt;If you are excited about being able to use Rust for data science and would like to contribute to this work then there are many ways to get involved. The simplest way to get started is to try out DataFusion against your own data sources and file bug reports for any issues that you find. You could also check out the current &lt;a href="https://cwiki.apache.org/confluence/display/ARROW/Rust+JIRA+Dashboard"&gt;list of issues&lt;/a&gt; and have a go at fixing one. You can also join the &lt;a href="http://mail-archives.apache.org/mod_mbox/arrow-user/"&gt;user mailing list&lt;/a&gt; to ask questions.&lt;/p&gt;</content><category term="blog"></category></entry></feed>